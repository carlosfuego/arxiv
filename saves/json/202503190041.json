[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.13275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13275v1",
                "updated": "2025-03-17T15:27:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T15:27:02Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "title": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems"
                },
                "summary": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability."
                },
                "authors": [
                    {
                        "name": "Seyoung Song"
                    }
                ],
                "author_detail": {
                    "name": "Seyoung Song"
                },
                "author": "Seyoung Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7; I.2.11; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13064v1",
                "updated": "2025-03-17T11:10:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    11,
                    10,
                    49,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T11:10:49Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    11,
                    10,
                    49,
                    0,
                    76,
                    0
                ],
                "title": "HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads"
                },
                "summary": "The growth of machine learning (ML) workloads has underscored the importance\nof efficient memory hierarchies to address bandwidth, latency, and scalability\nchallenges. HERMES focuses on optimizing memory subsystems for RISC-V\narchitectures to meet the computational needs of ML models such as CNNs, RNNs,\nand Transformers. This project explores state-of-the-art techniques such as\nadvanced prefetching, tensor-aware caching, and hybrid memory models. The\ncornerstone of HERMES is the integration of shared L3 caches with fine-grained\ncoherence protocols and specialized pathways to deep learning accelerators like\nGemmini. Simulation tools like Gem5 and DRAMSim2 are used to evaluate baseline\nperformance and scalability under representative ML workloads. The findings of\nthis study highlight the design choices and anticipated challenges, paving the\nway for low-latency scalable memory operations for ML applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growth of machine learning (ML) workloads has underscored the importance\nof efficient memory hierarchies to address bandwidth, latency, and scalability\nchallenges. HERMES focuses on optimizing memory subsystems for RISC-V\narchitectures to meet the computational needs of ML models such as CNNs, RNNs,\nand Transformers. This project explores state-of-the-art techniques such as\nadvanced prefetching, tensor-aware caching, and hybrid memory models. The\ncornerstone of HERMES is the integration of shared L3 caches with fine-grained\ncoherence protocols and specialized pathways to deep learning accelerators like\nGemmini. Simulation tools like Gem5 and DRAMSim2 are used to evaluate baseline\nperformance and scalability under representative ML workloads. The findings of\nthis study highlight the design choices and anticipated challenges, paving the\nway for low-latency scalable memory operations for ML applications."
                },
                "authors": [
                    {
                        "name": "Pranav Suryadevara"
                    }
                ],
                "author_detail": {
                    "name": "Pranav Suryadevara"
                },
                "author": "Pranav Suryadevara",
                "arxiv_comment": "5 pages, 5 figures. Individual Project",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.3.2; C.1.3; C.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12991v1",
                "updated": "2025-03-17T09:46:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    46,
                    35,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T09:46:35Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    46,
                    35,
                    0,
                    76,
                    0
                ],
                "title": "Tuning the CMS Coffea-casa facility for 200 Gbps Challenge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tuning the CMS Coffea-casa facility for 200 Gbps Challenge"
                },
                "summary": "As a part of the IRIS-HEP \"Analysis Grand Challenge\" activities, the\nCoffea-casa AF team executed a \"200 Gbps Challenge\". One of the goals of this\nchallenge was to provide a setup for execution of a test notebook-style\nanalysis on the facility that could process a 200 TB CMS NanoAOD dataset in 20\nminutes.\n  We describe the solutions we deployed at the facility to execute the\nchallenge tasks. The facility was configured to provide 2000+ cores for quick\nturn-around, low-latency analysis. To reach the highest event processing rates\nwe tested different scaling backends, both scaling over HTCondor and Kubernetes\nresources and using Dask and Taskvine schedulers. This configuration also\nallowed us to compare two different services for managing Dask clusters, Dask\nlabextention, and Dask Gateway server, under extreme conditions.\n  A robust set of XCache servers with a redirector were deployed in Kubernetes\nto cache the dataset to minimize wide-area network traffic. The XCache servers\nwere backed with solid-state NVME drives deployed within the Kubernetes cluster\nnodes. All data access was authenticated using scitokens and was transparent to\nthe user. To ensure we could track and measure data throughput precisely, we\nused our existing Prometheus monitoring stack to monitor the XCache pod\nthroughput on the Kubernetes network layer. Using the rate query across all of\nthe 8 XCache pods we were able to view a stacked cumulative graph of the total\nthroughput for each XCache. This monitoring setup allowed us to ensure uniform\ndata rates across all nodes while verifying we had reached the 200 Gbps\nbenchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a part of the IRIS-HEP \"Analysis Grand Challenge\" activities, the\nCoffea-casa AF team executed a \"200 Gbps Challenge\". One of the goals of this\nchallenge was to provide a setup for execution of a test notebook-style\nanalysis on the facility that could process a 200 TB CMS NanoAOD dataset in 20\nminutes.\n  We describe the solutions we deployed at the facility to execute the\nchallenge tasks. The facility was configured to provide 2000+ cores for quick\nturn-around, low-latency analysis. To reach the highest event processing rates\nwe tested different scaling backends, both scaling over HTCondor and Kubernetes\nresources and using Dask and Taskvine schedulers. This configuration also\nallowed us to compare two different services for managing Dask clusters, Dask\nlabextention, and Dask Gateway server, under extreme conditions.\n  A robust set of XCache servers with a redirector were deployed in Kubernetes\nto cache the dataset to minimize wide-area network traffic. The XCache servers\nwere backed with solid-state NVME drives deployed within the Kubernetes cluster\nnodes. All data access was authenticated using scitokens and was transparent to\nthe user. To ensure we could track and measure data throughput precisely, we\nused our existing Prometheus monitoring stack to monitor the XCache pod\nthroughput on the Kubernetes network layer. Using the rate query across all of\nthe 8 XCache pods we were able to view a stacked cumulative graph of the total\nthroughput for each XCache. This monitoring setup allowed us to ensure uniform\ndata rates across all nodes while verifying we had reached the 200 Gbps\nbenchmark."
                },
                "authors": [
                    {
                        "name": "Sam Albin"
                    },
                    {
                        "name": "Garhan Attebury"
                    },
                    {
                        "name": "Kenneth Bloom"
                    },
                    {
                        "name": "Brian Paul Bockelman"
                    },
                    {
                        "name": "Benjamin Tovar Lopez"
                    },
                    {
                        "name": "Carl Lundstedt"
                    },
                    {
                        "name": "Oksana Shadura"
                    },
                    {
                        "name": "John Thiltges"
                    },
                    {
                        "name": "Derek Weitzel"
                    },
                    {
                        "name": "Andrew Wightman"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Wightman"
                },
                "arxiv_affiliation": "University of Nebraska-Lincoln",
                "author": "Andrew Wightman",
                "arxiv_comment": "Draft submitted to EPJ journal (CHEP 2024 conference proceedings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12988v1",
                "updated": "2025-03-17T09:44:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    44,
                    17,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T09:44:17Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    44,
                    17,
                    0,
                    76,
                    0
                ],
                "title": "ROMA: a Read-Only-Memory-based Accelerator for QLoRA-based On-Device LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ROMA: a Read-Only-Memory-based Accelerator for QLoRA-based On-Device LLM"
                },
                "summary": "As large language models (LLMs) demonstrate powerful capabilities, deploying\nthem on edge devices has become increasingly crucial, offering advantages in\nprivacy and real-time interaction. QLoRA has emerged as the standard approach\nfor on-device LLMs, leveraging quantized models to reduce memory and\ncomputational costs while utilizing LoRA for task-specific adaptability. In\nthis work, we propose ROMA, a QLoRA accelerator with a hybrid storage\narchitecture that uses ROM for quantized base models and SRAM for LoRA weights\nand KV cache. Our insight is that the quantized base model is stable and\nconverged, making it well-suited for ROM storage. Meanwhile, LoRA modules offer\nthe flexibility to adapt to new data without requiring updates to the base\nmodel. To further reduce the area cost of ROM, we introduce a novel B-ROM\ndesign and integrate it with the compute unit to form a fused cell for\nefficient use of chip resources. ROMA can effectively store both a 4-bit 3B and\na 2-bit 8B LLaMA model entirely on-chip, achieving a notable generation speed\nexceeding 20,000 tokens/s without requiring external memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) demonstrate powerful capabilities, deploying\nthem on edge devices has become increasingly crucial, offering advantages in\nprivacy and real-time interaction. QLoRA has emerged as the standard approach\nfor on-device LLMs, leveraging quantized models to reduce memory and\ncomputational costs while utilizing LoRA for task-specific adaptability. In\nthis work, we propose ROMA, a QLoRA accelerator with a hybrid storage\narchitecture that uses ROM for quantized base models and SRAM for LoRA weights\nand KV cache. Our insight is that the quantized base model is stable and\nconverged, making it well-suited for ROM storage. Meanwhile, LoRA modules offer\nthe flexibility to adapt to new data without requiring updates to the base\nmodel. To further reduce the area cost of ROM, we introduce a novel B-ROM\ndesign and integrate it with the compute unit to form a fused cell for\nefficient use of chip resources. ROMA can effectively store both a 4-bit 3B and\na 2-bit 8B LLaMA model entirely on-chip, achieving a notable generation speed\nexceeding 20,000 tokens/s without requiring external memory."
                },
                "authors": [
                    {
                        "name": "Wenqiang Wang"
                    },
                    {
                        "name": "Yijia Zhang"
                    },
                    {
                        "name": "Zikai Zhang"
                    },
                    {
                        "name": "Guanting Huo"
                    },
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Ningyi Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ningyi Xu"
                },
                "author": "Ningyi Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08407v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08407v2",
                "updated": "2025-03-17T03:30:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    3,
                    30,
                    29,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-11T13:10:41Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    10,
                    41,
                    1,
                    70,
                    0
                ],
                "title": "WildSeg3D: Segment Any 3D Objects in the Wild from 2D Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildSeg3D: Segment Any 3D Objects in the Wild from 2D Images"
                },
                "summary": "Recent advances in interactive 3D segmentation from 2D images have\ndemonstrated impressive performance. However, current models typically require\nextensive scene-specific training to accurately reconstruct and segment\nobjects, which limits their applicability in real-time scenarios. In this\npaper, we introduce WildSeg3D, an efficient approach that enables the\nsegmentation of arbitrary 3D objects across diverse environments using a\nfeed-forward mechanism. A key challenge of this feed-forward approach lies in\nthe accumulation of 3D alignment errors across multiple 2D views, which can\nlead to inaccurate 3D segmentation results. To address this issue, we propose\nDynamic Global Aligning (DGA), a technique that improves the accuracy of global\nmulti-view alignment by focusing on difficult-to-match 3D points across images,\nusing a dynamic adjustment function. Additionally, for real-time interactive\nsegmentation, we introduce Multi-view Group Mapping (MGM), a method that\nutilizes an object mask cache to integrate multi-view segmentations and respond\nrapidly to user prompts. WildSeg3D demonstrates robust generalization across\narbitrary scenes, thereby eliminating the need for scene-specific training.\nSpecifically, WildSeg3D not only attains the accuracy of state-of-the-art\n(SOTA) methods but also achieves a $40\\times$ speedup compared to existing SOTA\nmodels. Our code will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in interactive 3D segmentation from 2D images have\ndemonstrated impressive performance. However, current models typically require\nextensive scene-specific training to accurately reconstruct and segment\nobjects, which limits their applicability in real-time scenarios. In this\npaper, we introduce WildSeg3D, an efficient approach that enables the\nsegmentation of arbitrary 3D objects across diverse environments using a\nfeed-forward mechanism. A key challenge of this feed-forward approach lies in\nthe accumulation of 3D alignment errors across multiple 2D views, which can\nlead to inaccurate 3D segmentation results. To address this issue, we propose\nDynamic Global Aligning (DGA), a technique that improves the accuracy of global\nmulti-view alignment by focusing on difficult-to-match 3D points across images,\nusing a dynamic adjustment function. Additionally, for real-time interactive\nsegmentation, we introduce Multi-view Group Mapping (MGM), a method that\nutilizes an object mask cache to integrate multi-view segmentations and respond\nrapidly to user prompts. WildSeg3D demonstrates robust generalization across\narbitrary scenes, thereby eliminating the need for scene-specific training.\nSpecifically, WildSeg3D not only attains the accuracy of state-of-the-art\n(SOTA) methods but also achieves a $40\\times$ speedup compared to existing SOTA\nmodels. Our code will be publicly available."
                },
                "authors": [
                    {
                        "name": "Yansong Guo"
                    },
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Yansong Qu"
                    },
                    {
                        "name": "Liujuan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Liujuan Cao"
                },
                "author": "Liujuan Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08407v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08407v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v4",
                "updated": "2025-03-16T16:25:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    16,
                    16,
                    25,
                    31,
                    6,
                    75,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have made significant strides in\nvideo understanding but struggle with long videos due to the limitations of\ntheir backbone LLMs. Existing solutions rely on length extrapolation, which is\nmemory-constrained, or visual token compression, which primarily leverages\nlow-level temporal redundancy while overlooking the more effective high-level\nknowledge redundancy. To address this, we propose $\\textbf{ReTaKe}$, a\ntraining-free method with two novel modules DPSelect and PivotKV, to jointly\nreduce both temporal visual redundancy and knowledge redundancy for video\ncompression. To align with the way of human temporal perception, DPSelect\nidentifies keyframes based on inter-frame distance peaks. To leverage LLMs'\nlearned prior knowledge, PivotKV marks the keyframes as pivots and compress\nnon-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe\nenables VideoLLMs to process 8 times longer frames (up to 2048), outperforming\nsimilar-sized models by 3-5% and even rivaling much larger ones on VideoMME,\nMLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression\noperations with prefilling, ReTaKe introduces only ~10% prefilling latency\noverhead while reducing decoding latency by ~20%. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have made significant strides in\nvideo understanding but struggle with long videos due to the limitations of\ntheir backbone LLMs. Existing solutions rely on length extrapolation, which is\nmemory-constrained, or visual token compression, which primarily leverages\nlow-level temporal redundancy while overlooking the more effective high-level\nknowledge redundancy. To address this, we propose $\\textbf{ReTaKe}$, a\ntraining-free method with two novel modules DPSelect and PivotKV, to jointly\nreduce both temporal visual redundancy and knowledge redundancy for video\ncompression. To align with the way of human temporal perception, DPSelect\nidentifies keyframes based on inter-frame distance peaks. To leverage LLMs'\nlearned prior knowledge, PivotKV marks the keyframes as pivots and compress\nnon-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe\nenables VideoLLMs to process 8 times longer frames (up to 2048), outperforming\nsimilar-sized models by 3-5% and even rivaling much larger ones on VideoMME,\nMLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression\noperations with prefilling, ReTaKe introduces only ~10% prefilling latency\noverhead while reducing decoding latency by ~20%. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe."
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Rewrite the methods section. Add more ablation studies and results in\n  LongVideoBench. Update metadata",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12491v1",
                "updated": "2025-03-16T12:49:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    16,
                    12,
                    49,
                    44,
                    6,
                    75,
                    0
                ],
                "published": "2025-03-16T12:49:44Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    12,
                    49,
                    44,
                    6,
                    75,
                    0
                ],
                "title": "CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences"
                },
                "summary": "Large language models (LLMs) excel at processing long sequences, boosting\ndemand for key-value (KV) caching. While recent efforts to evict KV cache have\nalleviated the inference burden, they often fail to allocate resources\nrationally across layers with different attention patterns. In this paper, we\nintroduce Cascading and Adaptive KV cache Eviction (CAKE), a novel approach\nthat frames KV cache eviction as a \"cake-slicing problem.\" CAKE assesses\nlayer-specific preferences by considering attention dynamics in both spatial\nand temporal dimensions, allocates rational cache size for layers accordingly,\nand manages memory constraints in a cascading manner. This approach enables a\nglobal view of cache allocation, adaptively distributing resources across\ndiverse attention mechanisms while maintaining memory budgets. CAKE also\nemploys a new eviction indicator that considers the shifting importance of\ntokens over time, addressing limitations in existing methods that overlook\ntemporal dynamics. Comprehensive experiments on LongBench and NeedleBench show\nthat CAKE maintains model performance with only 3.2% of the KV cache and\nconsistently outperforms current baselines across various models and memory\nconstraints, particularly in low-memory settings. Additionally, CAKE achieves\nover 10x speedup in decoding latency compared to full cache when processing\ncontexts of 128K tokens with FlashAttention-2. Our code is available at\nhttps://github.com/antgroup/cakekv.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at processing long sequences, boosting\ndemand for key-value (KV) caching. While recent efforts to evict KV cache have\nalleviated the inference burden, they often fail to allocate resources\nrationally across layers with different attention patterns. In this paper, we\nintroduce Cascading and Adaptive KV cache Eviction (CAKE), a novel approach\nthat frames KV cache eviction as a \"cake-slicing problem.\" CAKE assesses\nlayer-specific preferences by considering attention dynamics in both spatial\nand temporal dimensions, allocates rational cache size for layers accordingly,\nand manages memory constraints in a cascading manner. This approach enables a\nglobal view of cache allocation, adaptively distributing resources across\ndiverse attention mechanisms while maintaining memory budgets. CAKE also\nemploys a new eviction indicator that considers the shifting importance of\ntokens over time, addressing limitations in existing methods that overlook\ntemporal dynamics. Comprehensive experiments on LongBench and NeedleBench show\nthat CAKE maintains model performance with only 3.2% of the KV cache and\nconsistently outperforms current baselines across various models and memory\nconstraints, particularly in low-memory settings. Additionally, CAKE achieves\nover 10x speedup in decoding latency compared to full cache when processing\ncontexts of 128K tokens with FlashAttention-2. Our code is available at\nhttps://github.com/antgroup/cakekv."
                },
                "authors": [
                    {
                        "name": "Ziran Qin"
                    },
                    {
                        "name": "Yuchen Cao"
                    },
                    {
                        "name": "Mingbao Lin"
                    },
                    {
                        "name": "Wen Hu"
                    },
                    {
                        "name": "Shixuan Fan"
                    },
                    {
                        "name": "Ke Cheng"
                    },
                    {
                        "name": "Weiyao Lin"
                    },
                    {
                        "name": "Jianguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianguo Li"
                },
                "author": "Jianguo Li",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12450v1",
                "updated": "2025-03-16T10:54:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    16,
                    10,
                    54,
                    59,
                    6,
                    75,
                    0
                ],
                "published": "2025-03-16T10:54:59Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    10,
                    54,
                    59,
                    6,
                    75,
                    0
                ],
                "title": "LazyMAR: Accelerating Masked Autoregressive Models via Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyMAR: Accelerating Masked Autoregressive Models via Feature Caching"
                },
                "summary": "Masked Autoregressive (MAR) models have emerged as a promising approach in\nimage generation, expected to surpass traditional autoregressive models in\ncomputational efficiency by leveraging the capability of parallel decoding.\nHowever, their dependence on bidirectional self-attention inherently conflicts\nwith conventional KV caching mechanisms, creating unexpected computational\nbottlenecks that undermine their expected efficiency. To address this problem,\nthis paper studies the caching mechanism for MAR by leveraging two types of\nredundancy: Token Redundancy indicates that a large portion of tokens have very\nsimilar representations in the adjacent decoding steps, which allows us to\nfirst cache them in previous steps and then reuse them in the later steps.\nCondition Redundancy indicates that the difference between conditional and\nunconditional output in classifier-free guidance exhibits very similar values\nin adjacent steps. Based on these two redundancies, we propose LazyMAR, which\nintroduces two caching mechanisms to handle them one by one. LazyMAR is\ntraining-free and plug-and-play for all MAR models. Experimental results\ndemonstrate that our method achieves 2.83 times acceleration with almost no\ndrop in generation quality. Our codes will be released in\nhttps://github.com/feihongyan1/LazyMAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Autoregressive (MAR) models have emerged as a promising approach in\nimage generation, expected to surpass traditional autoregressive models in\ncomputational efficiency by leveraging the capability of parallel decoding.\nHowever, their dependence on bidirectional self-attention inherently conflicts\nwith conventional KV caching mechanisms, creating unexpected computational\nbottlenecks that undermine their expected efficiency. To address this problem,\nthis paper studies the caching mechanism for MAR by leveraging two types of\nredundancy: Token Redundancy indicates that a large portion of tokens have very\nsimilar representations in the adjacent decoding steps, which allows us to\nfirst cache them in previous steps and then reuse them in the later steps.\nCondition Redundancy indicates that the difference between conditional and\nunconditional output in classifier-free guidance exhibits very similar values\nin adjacent steps. Based on these two redundancies, we propose LazyMAR, which\nintroduces two caching mechanisms to handle them one by one. LazyMAR is\ntraining-free and plug-and-play for all MAR models. Experimental results\ndemonstrate that our method achieves 2.83 times acceleration with almost no\ndrop in generation quality. Our codes will be released in\nhttps://github.com/feihongyan1/LazyMAR."
                },
                "authors": [
                    {
                        "name": "Feihong Yan"
                    },
                    {
                        "name": "Qingyan Wei"
                    },
                    {
                        "name": "Jiayi Tang"
                    },
                    {
                        "name": "Jiajun Li"
                    },
                    {
                        "name": "Yulin Wang"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Huiqi Li"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12150v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12150v1",
                "updated": "2025-03-15T14:13:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "published": "2025-03-15T14:13:23Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis"
                },
                "summary": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data-often inaccessible during online inference-and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\nPoint-Cache, a hierarchical cache model that captures essential clues of online\ntest samples, particularly focusing on the global structure of point clouds and\ntheir local-part details. Point-Cache, which serves as a rich 3D knowledge\nbase, is dynamically managed to prioritize the inclusion of high-quality\nsamples. Designed as a plug-and-play module, our method can be flexibly\nintegrated into large multimodal 3D models to support open-vocabulary point\ncloud recognition. Notably, our solution operates with efficiency comparable to\nzero-shot inference, as it is entirely training-free. Point-Cache demonstrates\nsubstantial gains across 8 challenging benchmarks and 4 representative large 3D\nmodels, highlighting its effectiveness. Code is available at\nhttps://github.com/auniquesun/Point-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data-often inaccessible during online inference-and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\nPoint-Cache, a hierarchical cache model that captures essential clues of online\ntest samples, particularly focusing on the global structure of point clouds and\ntheir local-part details. Point-Cache, which serves as a rich 3D knowledge\nbase, is dynamically managed to prioritize the inclusion of high-quality\nsamples. Designed as a plug-and-play module, our method can be flexibly\nintegrated into large multimodal 3D models to support open-vocabulary point\ncloud recognition. Notably, our solution operates with efficiency comparable to\nzero-shot inference, as it is entirely training-free. Point-Cache demonstrates\nsubstantial gains across 8 challenging benchmarks and 4 representative large 3D\nmodels, highlighting its effectiveness. Code is available at\nhttps://github.com/auniquesun/Point-Cache."
                },
                "authors": [
                    {
                        "name": "Hongyu Sun"
                    },
                    {
                        "name": "Qiuhong Ke"
                    },
                    {
                        "name": "Ming Cheng"
                    },
                    {
                        "name": "Yongcai Wang"
                    },
                    {
                        "name": "Deying Li"
                    },
                    {
                        "name": "Chenhui Gou"
                    },
                    {
                        "name": "Jianfei Cai"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Cai"
                },
                "author": "Jianfei Cai",
                "arxiv_comment": "Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12150v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12150v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11972v1",
                "updated": "2025-03-15T02:48:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    15,
                    2,
                    48,
                    27,
                    5,
                    74,
                    0
                ],
                "published": "2025-03-15T02:48:27Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    2,
                    48,
                    27,
                    5,
                    74,
                    0
                ],
                "title": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models"
                },
                "summary": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment."
                },
                "authors": [
                    {
                        "name": "Yuchen Xia"
                    },
                    {
                        "name": "Divyam Sharma"
                    },
                    {
                        "name": "Yichao Yuan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Nishil Talati"
                    }
                ],
                "author_detail": {
                    "name": "Nishil Talati"
                },
                "author": "Nishil Talati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11946v1",
                "updated": "2025-03-15T01:35:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    15,
                    1,
                    35,
                    53,
                    5,
                    74,
                    0
                ],
                "published": "2025-03-15T01:35:53Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    1,
                    35,
                    53,
                    5,
                    74,
                    0
                ],
                "title": "CCRSat: A Collaborative Computation Reuse Framework for Satellite Edge\n  Computing Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CCRSat: A Collaborative Computation Reuse Framework for Satellite Edge\n  Computing Networks"
                },
                "summary": "In satellite computing applications, such as remote sensing, tasks often\ninvolve similar or identical input data, leading to the same processing\nresults. Computation reuse is an emerging paradigm that leverages the execution\nresults of previous tasks to enhance the utilization of computational\nresources. While this paradigm has been extensively studied in terrestrial\nnetworks with abundant computing and caching resources, such as named data\nnetworking (NDN), it is essential to develop a framework appropriate for\nresource-constrained satellite networks, which are expected to have longer task\ncompletion times. In this paper, we propose CCRSat, a collaborative computation\nreuse framework for satellite edge computing networks. CCRSat initially\nimplements local computation reuse on an independent satellite, utilizing a\nsatellite reuse state (SRS) to assess the efficiency of computation reuse.\nAdditionally, an inter-satellite computation reuse algorithm is introduced,\nwhich utilizes the collaborative sharing of similarity in previously processed\ndata among multiple satellites. The evaluation results tested on real-world\ndatasets demonstrate that, compared to comparative scenarios, our proposed\nCCRSat can significantly reduce task completion time by up to 62.1% and\ncomputational resource consumption by up to 28.8%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In satellite computing applications, such as remote sensing, tasks often\ninvolve similar or identical input data, leading to the same processing\nresults. Computation reuse is an emerging paradigm that leverages the execution\nresults of previous tasks to enhance the utilization of computational\nresources. While this paradigm has been extensively studied in terrestrial\nnetworks with abundant computing and caching resources, such as named data\nnetworking (NDN), it is essential to develop a framework appropriate for\nresource-constrained satellite networks, which are expected to have longer task\ncompletion times. In this paper, we propose CCRSat, a collaborative computation\nreuse framework for satellite edge computing networks. CCRSat initially\nimplements local computation reuse on an independent satellite, utilizing a\nsatellite reuse state (SRS) to assess the efficiency of computation reuse.\nAdditionally, an inter-satellite computation reuse algorithm is introduced,\nwhich utilizes the collaborative sharing of similarity in previously processed\ndata among multiple satellites. The evaluation results tested on real-world\ndatasets demonstrate that, compared to comparative scenarios, our proposed\nCCRSat can significantly reduce task completion time by up to 62.1% and\ncomputational resource consumption by up to 28.8%."
                },
                "authors": [
                    {
                        "name": "Ye Zhang"
                    },
                    {
                        "name": "Zhishu Shen"
                    },
                    {
                        "name": "Dawen Jiang"
                    },
                    {
                        "name": "Xiangrui Liu"
                    },
                    {
                        "name": "Qiushi Zheng"
                    },
                    {
                        "name": "Jiong Jin"
                    }
                ],
                "author_detail": {
                    "name": "Jiong Jin"
                },
                "author": "Jiong Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06348v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06348v2",
                "updated": "2025-03-15T00:49:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    15,
                    0,
                    49,
                    55,
                    5,
                    74,
                    0
                ],
                "published": "2024-03-11T00:30:25Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    0,
                    30,
                    25,
                    0,
                    71,
                    0
                ],
                "title": "Accelerating Sparse Tensor Decomposition Using Adaptive Linearized\n  Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Sparse Tensor Decomposition Using Adaptive Linearized\n  Representation"
                },
                "summary": "High-dimensional sparse data emerge in many critical application domains such\nas healthcare and cybersecurity. To extract meaningful insights from massive\nvolumes of these multi-dimensional data, scientists employ unsupervised\nanalysis tools based on tensor decomposition (TD) methods. However, real-world\nsparse tensors exhibit highly irregular shapes and data distributions, which\npose significant challenges for making efficient use of modern parallel\nprocessors. This study breaks the prevailing assumption that compressing sparse\ntensors into coarse-grained structures or along a particular dimension/mode is\nmore efficient than keeping them in a fine-grained, mode-agnostic form. Our\nnovel sparse tensor representation, Adaptive Linearized Tensor Order (ALTO),\nencodes tensors in a compact format that can be easily streamed from memory and\nis amenable to both caching and parallel execution. In contrast to existing\ncompressed tensor formats, ALTO constructs one tensor copy that is agnostic to\nboth the mode orientation and the irregular distribution of nonzero elements.\nTo demonstrate the efficacy of ALTO, we propose a set of parallel TD algorithms\nthat exploit the inherent data reuse of tensor computations to substantially\nreduce synchronization overhead, decrease memory footprint, and improve\nparallel performance. Additionally, we characterize the major execution\nbottlenecks of TD methods on the latest Intel Xeon Scalable processors and\nintroduce dynamic adaptation heuristics to automatically select the best\nalgorithm based on the sparse tensor characteristics. Across a diverse set of\nreal-world data sets, ALTO outperforms the state-of-the-art approaches,\nachieving more than an order-of-magnitude speedup over the best mode-agnostic\nformats. Compared to the best mode-specific formats, ALTO achieves 5.1X\ngeometric mean speedup at a fraction (25%) of their storage costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-dimensional sparse data emerge in many critical application domains such\nas healthcare and cybersecurity. To extract meaningful insights from massive\nvolumes of these multi-dimensional data, scientists employ unsupervised\nanalysis tools based on tensor decomposition (TD) methods. However, real-world\nsparse tensors exhibit highly irregular shapes and data distributions, which\npose significant challenges for making efficient use of modern parallel\nprocessors. This study breaks the prevailing assumption that compressing sparse\ntensors into coarse-grained structures or along a particular dimension/mode is\nmore efficient than keeping them in a fine-grained, mode-agnostic form. Our\nnovel sparse tensor representation, Adaptive Linearized Tensor Order (ALTO),\nencodes tensors in a compact format that can be easily streamed from memory and\nis amenable to both caching and parallel execution. In contrast to existing\ncompressed tensor formats, ALTO constructs one tensor copy that is agnostic to\nboth the mode orientation and the irregular distribution of nonzero elements.\nTo demonstrate the efficacy of ALTO, we propose a set of parallel TD algorithms\nthat exploit the inherent data reuse of tensor computations to substantially\nreduce synchronization overhead, decrease memory footprint, and improve\nparallel performance. Additionally, we characterize the major execution\nbottlenecks of TD methods on the latest Intel Xeon Scalable processors and\nintroduce dynamic adaptation heuristics to automatically select the best\nalgorithm based on the sparse tensor characteristics. Across a diverse set of\nreal-world data sets, ALTO outperforms the state-of-the-art approaches,\nachieving more than an order-of-magnitude speedup over the best mode-agnostic\nformats. Compared to the best mode-specific formats, ALTO achieves 5.1X\ngeometric mean speedup at a fraction (25%) of their storage costs."
                },
                "authors": [
                    {
                        "name": "Jan Laukemann"
                    },
                    {
                        "name": "Ahmed E. Helal"
                    },
                    {
                        "name": "S. Isaac Geronimo Anderson"
                    },
                    {
                        "name": "Fabio Checconi"
                    },
                    {
                        "name": "Yongseok Soh"
                    },
                    {
                        "name": "Jesmin Jahan Tithi"
                    },
                    {
                        "name": "Teresa Ranadive"
                    },
                    {
                        "name": "Brian J Gravelle"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    },
                    {
                        "name": "Jee Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jee Choi"
                },
                "author": "Jee Choi",
                "arxiv_comment": "Accepted to TPDS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06348v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06348v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11816v1",
                "updated": "2025-03-14T19:02:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T19:02:16Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations."
                },
                "authors": [
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "arxiv_comment": "Invited paper to IEEE Custom Integrated Circuits Conference (CICC)\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11750v1",
                "updated": "2025-03-14T17:57:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    57,
                    42,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T17:57:42Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    57,
                    42,
                    4,
                    73,
                    0
                ],
                "title": "Making Every Step Effective: Jailbreaking Large Vision-Language Models\n  Through Hierarchical KV Equalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making Every Step Effective: Jailbreaking Large Vision-Language Models\n  Through Hierarchical KV Equalization"
                },
                "summary": "In the realm of large vision-language models (LVLMs), adversarial jailbreak\nattacks serve as a red-teaming approach to identify safety vulnerabilities of\nthese models and their associated defense mechanisms. However, we identify a\ncritical limitation: not every adversarial optimization step leads to a\npositive outcome, and indiscriminately accepting optimization results at each\nstep may reduce the overall attack success rate. To address this challenge, we\nintroduce HKVE (Hierarchical Key-Value Equalization), an innovative\njailbreaking framework that selectively accepts gradient optimization results\nbased on the distribution of attention scores across different layers, ensuring\nthat every optimization step positively contributes to the attack. Extensive\nexperiments demonstrate HKVE's significant effectiveness, achieving attack\nsuccess rates of 75.08% on MiniGPT4, 85.84% on LLaVA and 81.00% on Qwen-VL,\nsubstantially outperforming existing methods by margins of 20.43\\%, 21.01\\% and\n26.43\\% respectively. Furthermore, making every step effective not only leads\nto an increase in attack success rate but also allows for a reduction in the\nnumber of iterations, thereby lowering computational costs. Warning: This paper\ncontains potentially harmful example data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of large vision-language models (LVLMs), adversarial jailbreak\nattacks serve as a red-teaming approach to identify safety vulnerabilities of\nthese models and their associated defense mechanisms. However, we identify a\ncritical limitation: not every adversarial optimization step leads to a\npositive outcome, and indiscriminately accepting optimization results at each\nstep may reduce the overall attack success rate. To address this challenge, we\nintroduce HKVE (Hierarchical Key-Value Equalization), an innovative\njailbreaking framework that selectively accepts gradient optimization results\nbased on the distribution of attention scores across different layers, ensuring\nthat every optimization step positively contributes to the attack. Extensive\nexperiments demonstrate HKVE's significant effectiveness, achieving attack\nsuccess rates of 75.08% on MiniGPT4, 85.84% on LLaVA and 81.00% on Qwen-VL,\nsubstantially outperforming existing methods by margins of 20.43\\%, 21.01\\% and\n26.43\\% respectively. Furthermore, making every step effective not only leads\nto an increase in attack success rate but also allows for a reduction in the\nnumber of iterations, thereby lowering computational costs. Warning: This paper\ncontains potentially harmful example data."
                },
                "authors": [
                    {
                        "name": "Shuyang Hao"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Muhao Chen"
                    },
                    {
                        "name": "Zi Huang"
                    },
                    {
                        "name": "Yujun Cai"
                    }
                ],
                "author_detail": {
                    "name": "Yujun Cai"
                },
                "author": "Yujun Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01066v2",
                "updated": "2025-03-14T16:57:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    57,
                    12,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-03T00:14:34Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    0,
                    14,
                    34,
                    0,
                    62,
                    0
                ],
                "title": "Alchemist: Towards the Design of Efficient Online Continual Learning\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alchemist: Towards the Design of Efficient Online Continual Learning\n  System"
                },
                "summary": "Continual learning has become a promising solution to refine large language\nmodels incrementally by leveraging user feedback. In particular, online\ncontinual learning - iteratively training the model with small batches of user\nfeedback - has demonstrated notable performance improvements. However, the\nexisting practice of separating training and serving processes forces the\nonline trainer to recompute the intermediate results already done during\nserving. Such redundant computations can account for 30%-42% of total training\ntime.\n  In this paper, we propose Alchemist, to the best of our knowledge, the first\nonline continual learning system that efficiently reuses serving activations to\nincrease training throughput. Alchemist introduces two key techniques: (1)\nrecording and storing activations and KV cache only during the prefill phase to\nminimize latency and memory overhead; and (2) smart activation offloading and\nhedging. Evaluations with inputs of varied token length sampled from ShareGPT\ndataset show that compared with a separate training cluster, Alchemist\nsignificantly increases training throughput by up to 1.72x, reduces up to 47%\nmemory usage during training, and supports up to 2x more training tokens - all\nwhile maintaining negligible impact on serving latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning has become a promising solution to refine large language\nmodels incrementally by leveraging user feedback. In particular, online\ncontinual learning - iteratively training the model with small batches of user\nfeedback - has demonstrated notable performance improvements. However, the\nexisting practice of separating training and serving processes forces the\nonline trainer to recompute the intermediate results already done during\nserving. Such redundant computations can account for 30%-42% of total training\ntime.\n  In this paper, we propose Alchemist, to the best of our knowledge, the first\nonline continual learning system that efficiently reuses serving activations to\nincrease training throughput. Alchemist introduces two key techniques: (1)\nrecording and storing activations and KV cache only during the prefill phase to\nminimize latency and memory overhead; and (2) smart activation offloading and\nhedging. Evaluations with inputs of varied token length sampled from ShareGPT\ndataset show that compared with a separate training cluster, Alchemist\nsignificantly increases training throughput by up to 1.72x, reduces up to 47%\nmemory usage during training, and supports up to 2x more training tokens - all\nwhile maintaining negligible impact on serving latency."
                },
                "authors": [
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Haryadi S. Gunawi"
                    },
                    {
                        "name": "Beibin Li"
                    },
                    {
                        "name": "Changho Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Changho Hwang"
                },
                "author": "Changho Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11460v1",
                "updated": "2025-03-14T14:47:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    47,
                    55,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T14:47:55Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    47,
                    55,
                    4,
                    73,
                    0
                ],
                "title": "ARCAS: Adaptive Runtime System for Chiplet-Aware Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCAS: Adaptive Runtime System for Chiplet-Aware Scheduling"
                },
                "summary": "The growing disparity between CPU core counts and available memory bandwidth\nhas intensified memory contention in servers. This particularly affects highly\nparallelizable applications, which must achieve efficient cache utilization to\nmaintain performance as CPU core counts grow. Optimizing cache utilization,\nhowever, is complex for recent chiplet-based CPUs, whose partitioned L3 caches\nlead to varying latencies and bandwidths, even within a single NUMA domain.\nClassical NUMA optimizations and task scheduling approaches unfortunately fail\nto address the performance issues of chiplet-based CPUs.\n  We describe Adaptive Runtime system for Chiplet-Aware Scheduling (ARCAS), a\nnew runtime system designed for chiplet-based CPUs. ARCAS combines\nchiplet-aware task scheduling heuristics, hardware-aware memory allocation, and\nfine-grained performance monitoring to optimize workload execution. It\nimplements a lightweight concurrency model that combines user-level thread\nfeatures-such as individual stacks, per-task scheduling, and state\nmanagement-with coroutine-like behavior, allowing tasks to suspend and resume\nexecution at defined points while efficiently managing task migration across\nchiplets. Our evaluation across diverse scenarios shows ARCAS's effectiveness\nfor optimizing the performance of memory-intensive parallel applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing disparity between CPU core counts and available memory bandwidth\nhas intensified memory contention in servers. This particularly affects highly\nparallelizable applications, which must achieve efficient cache utilization to\nmaintain performance as CPU core counts grow. Optimizing cache utilization,\nhowever, is complex for recent chiplet-based CPUs, whose partitioned L3 caches\nlead to varying latencies and bandwidths, even within a single NUMA domain.\nClassical NUMA optimizations and task scheduling approaches unfortunately fail\nto address the performance issues of chiplet-based CPUs.\n  We describe Adaptive Runtime system for Chiplet-Aware Scheduling (ARCAS), a\nnew runtime system designed for chiplet-based CPUs. ARCAS combines\nchiplet-aware task scheduling heuristics, hardware-aware memory allocation, and\nfine-grained performance monitoring to optimize workload execution. It\nimplements a lightweight concurrency model that combines user-level thread\nfeatures-such as individual stacks, per-task scheduling, and state\nmanagement-with coroutine-like behavior, allowing tasks to suspend and resume\nexecution at defined points while efficiently managing task migration across\nchiplets. Our evaluation across diverse scenarios shows ARCAS's effectiveness\nfor optimizing the performance of memory-intensive parallel applications."
                },
                "authors": [
                    {
                        "name": "Alessandro Fogli"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Peter Pietzuch"
                    },
                    {
                        "name": "Jana Giceva"
                    }
                ],
                "author_detail": {
                    "name": "Jana Giceva"
                },
                "author": "Jana Giceva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11426v1",
                "updated": "2025-03-14T14:14:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    14,
                    5,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T14:14:05Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    14,
                    5,
                    4,
                    73,
                    0
                ],
                "title": "Text Compression for Efficient Language Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text Compression for Efficient Language Generation"
                },
                "summary": "We challenge the prevailing assumption that LLMs must rely fully on sub-word\ntokens for high-quality text generation. To this end, we propose the\n\"Generative Pretrained Thoughtformer\" (GPTHF), a hierarchical transformer\nlanguage model capable of text generation by compressing text into sentence\nembeddings and employing a sentence attention mechanism. GPTHF retains GPT's\narchitecture, modifying only token interactions via dynamic sparse attention\nmasks.\n  Our experiments show that GPTHF achieves an up to an order of magnitude\nimprovement in FLOPs efficiency and a threefold increase in runtime speed\ncompared to equally-sized GPT models in the low-size regime. This is achieved\nthrough a unique generation method that caches and reuses sentence embeddings,\nallowing significant portions of the input to bypass large parts of the\nnetwork.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We challenge the prevailing assumption that LLMs must rely fully on sub-word\ntokens for high-quality text generation. To this end, we propose the\n\"Generative Pretrained Thoughtformer\" (GPTHF), a hierarchical transformer\nlanguage model capable of text generation by compressing text into sentence\nembeddings and employing a sentence attention mechanism. GPTHF retains GPT's\narchitecture, modifying only token interactions via dynamic sparse attention\nmasks.\n  Our experiments show that GPTHF achieves an up to an order of magnitude\nimprovement in FLOPs efficiency and a threefold increase in runtime speed\ncompared to equally-sized GPT models in the low-size regime. This is achieved\nthrough a unique generation method that caches and reuses sentence embeddings,\nallowing significant portions of the input to bypass large parts of the\nnetwork."
                },
                "authors": [
                    {
                        "name": "David Gu"
                    },
                    {
                        "name": "Peter Belcak"
                    },
                    {
                        "name": "Roger Wattenhofer"
                    }
                ],
                "author_detail": {
                    "name": "Roger Wattenhofer"
                },
                "author": "Roger Wattenhofer",
                "arxiv_comment": "accepted to NAACL SRW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v1",
                "updated": "2025-03-14T06:49:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid (i.e., combination of regular attention and MLA\nlayers) or full MLA variant through lightweight post-training adaptation,\nbypassing the need for extensive pre-training. We demonstrate that leveraging\nthe dark knowledge of a well-trained model can enhance training accuracy and\nenable extreme KV cache compression in MLA without compromising model\nperformance. Our results show that using an 8B teacher model allows us to\ncompress the KV cache size of the Llama3.2-1B-Inst baseline by 6.4x while\npreserving 100% of its average score across multiple tasks on the LM Harness\nEvaluation benchmark. This is achieved with only 3.6B training tokens and about\n70 GPU hours on AMD MI300 GPUs, compared to the 370K GPU hours required for\npre-training the Llama3.2-1B model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid (i.e., combination of regular attention and MLA\nlayers) or full MLA variant through lightweight post-training adaptation,\nbypassing the need for extensive pre-training. We demonstrate that leveraging\nthe dark knowledge of a well-trained model can enhance training accuracy and\nenable extreme KV cache compression in MLA without compromising model\nperformance. Our results show that using an 8B teacher model allows us to\ncompress the KV cache size of the Llama3.2-1B-Inst baseline by 6.4x while\npreserving 100% of its average score across multiple tasks on the LM Harness\nEvaluation benchmark. This is achieved with only 3.6B training tokens and about\n70 GPU hours on AMD MI300 GPUs, compared to the 370K GPU hours required for\npre-training the Llama3.2-1B model."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11108v1",
                "updated": "2025-03-14T06:01:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    1,
                    42,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T06:01:42Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    1,
                    42,
                    4,
                    73,
                    0
                ],
                "title": "Limits of KV Cache Compression for Tensor Attention based Autoregressive\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limits of KV Cache Compression for Tensor Attention based Autoregressive\n  Transformers"
                },
                "summary": "The key-value (KV) cache in autoregressive transformers presents a\nsignificant bottleneck during inference, which restricts the context length\ncapabilities of large language models (LLMs). While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanism [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a novel\nreduction from communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. In the low\ndimensional regime where $d = o(\\log n)$, we analyze the theoretical bounds of\nthe space complexity as well. Overall, our work provides a theoretical\nfoundation for us to understand the compression-expressivity tradeoff in tensor\nattention mechanisms and offers more perspectives in developing more\nmemory-efficient transformer architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache in autoregressive transformers presents a\nsignificant bottleneck during inference, which restricts the context length\ncapabilities of large language models (LLMs). While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanism [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a novel\nreduction from communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. In the low\ndimensional regime where $d = o(\\log n)$, we analyze the theoretical bounds of\nthe space complexity as well. Overall, our work provides a theoretical\nfoundation for us to understand the compression-expressivity tradeoff in tensor\nattention mechanisms and offers more perspectives in developing more\nmemory-efficient transformer architectures."
                },
                "authors": [
                    {
                        "name": "Yifang Chen"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yu Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yu Tian"
                },
                "author": "Yu Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10589v1",
                "updated": "2025-03-13T17:40:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    40,
                    7,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:40:07Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    40,
                    7,
                    3,
                    72,
                    0
                ],
                "title": "Long Context Tuning for Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Context Tuning for Video Generation"
                },
                "summary": "Recent advances in video generation can produce realistic, minute-long\nsingle-shot videos with scalable diffusion transformers. However, real-world\nnarrative videos require multi-shot scenes with visual and dynamic consistency\nacross shots. In this work, we introduce Long Context Tuning (LCT), a training\nparadigm that expands the context window of pre-trained single-shot video\ndiffusion models to learn scene-level consistency directly from data. Our\nmethod expands full attention mechanisms from individual shots to encompass all\nshots within a scene, incorporating interleaved 3D position embedding and an\nasynchronous noise strategy, enabling both joint and auto-regressive shot\ngeneration without additional parameters. Models with bidirectional attention\nafter LCT can further be fine-tuned with context-causal attention, facilitating\nauto-regressive generation with efficient KV-cache. Experiments demonstrate\nsingle-shot models after LCT can produce coherent multi-shot scenes and exhibit\nemerging capabilities, including compositional generation and interactive shot\nextension, paving the way for more practical visual content creation. See\nhttps://guoyww.github.io/projects/long-context-video/ for more details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in video generation can produce realistic, minute-long\nsingle-shot videos with scalable diffusion transformers. However, real-world\nnarrative videos require multi-shot scenes with visual and dynamic consistency\nacross shots. In this work, we introduce Long Context Tuning (LCT), a training\nparadigm that expands the context window of pre-trained single-shot video\ndiffusion models to learn scene-level consistency directly from data. Our\nmethod expands full attention mechanisms from individual shots to encompass all\nshots within a scene, incorporating interleaved 3D position embedding and an\nasynchronous noise strategy, enabling both joint and auto-regressive shot\ngeneration without additional parameters. Models with bidirectional attention\nafter LCT can further be fine-tuned with context-causal attention, facilitating\nauto-regressive generation with efficient KV-cache. Experiments demonstrate\nsingle-shot models after LCT can produce coherent multi-shot scenes and exhibit\nemerging capabilities, including compositional generation and interactive shot\nextension, paving the way for more practical visual content creation. See\nhttps://guoyww.github.io/projects/long-context-video/ for more details."
                },
                "authors": [
                    {
                        "name": "Yuwei Guo"
                    },
                    {
                        "name": "Ceyuan Yang"
                    },
                    {
                        "name": "Ziyan Yang"
                    },
                    {
                        "name": "Zhibei Ma"
                    },
                    {
                        "name": "Zhijie Lin"
                    },
                    {
                        "name": "Zhenheng Yang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Lu Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Jiang"
                },
                "author": "Lu Jiang",
                "arxiv_comment": "Project Page: https://guoyww.github.io/projects/long-context-video/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10568v1",
                "updated": "2025-03-13T17:19:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    19,
                    51,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:19:51Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    19,
                    51,
                    3,
                    72,
                    0
                ],
                "title": "Autoregressive Image Generation with Randomized Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation with Randomized Parallel Decoding"
                },
                "summary": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel guided decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only\n64 sampling steps, achieving over a 20-fold increase in throughput while\nreducing memory consumption by over 75% compared to representative recent\nautoregressive models at a similar scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel guided decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only\n64 sampling steps, achieving over a 20-fold increase in throughput while\nreducing memory consumption by over 75% compared to representative recent\nautoregressive models at a similar scale."
                },
                "authors": [
                    {
                        "name": "Haopeng Li"
                    },
                    {
                        "name": "Jinyue Yang"
                    },
                    {
                        "name": "Guoqi Li"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07720v2",
                "updated": "2025-03-13T16:29:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    29,
                    17,
                    3,
                    72,
                    0
                ],
                "published": "2024-12-10T18:13:20Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "title": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer"
                },
                "summary": "We present ACDiT, a novel Autoregressive blockwise Conditional Diffusion\nTransformer, that innovatively combines autoregressive and diffusion paradigms\nfor modeling continuous visual information. By introducing a block-wise\nautoregressive unit, ACDiT offers a flexible interpolation between token-wise\nautoregression and full-sequence diffusion, bypassing the limitations of\ndiscrete tokenization. The generation of each block is formulated as a\nconditional diffusion process, conditioned on prior blocks. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) on\nstandard diffusion transformer during training. During inference, the process\niterates between diffusion denoising and autoregressive decoding that can make\nfull use of KV-Cache. We show that ACDiT performs best among all autoregressive\nbaselines under similar model scales on image and video generation tasks. We\nalso demonstrate that benefiting from autoregressive modeling, pretrained ACDiT\ncan be transferred in visual understanding tasks despite being trained with the\ndiffusion objective. The analysis of the trade-off between autoregressive\nmodeling and diffusion demonstrates the potential of ACDiT to be used in\nlong-horizon visual generation tasks. We hope that ACDiT offers a novel\nperspective on visual autoregressive generation and unlocks new avenues for\nunified models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ACDiT, a novel Autoregressive blockwise Conditional Diffusion\nTransformer, that innovatively combines autoregressive and diffusion paradigms\nfor modeling continuous visual information. By introducing a block-wise\nautoregressive unit, ACDiT offers a flexible interpolation between token-wise\nautoregression and full-sequence diffusion, bypassing the limitations of\ndiscrete tokenization. The generation of each block is formulated as a\nconditional diffusion process, conditioned on prior blocks. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) on\nstandard diffusion transformer during training. During inference, the process\niterates between diffusion denoising and autoregressive decoding that can make\nfull use of KV-Cache. We show that ACDiT performs best among all autoregressive\nbaselines under similar model scales on image and video generation tasks. We\nalso demonstrate that benefiting from autoregressive modeling, pretrained ACDiT\ncan be transferred in visual understanding tasks despite being trained with the\ndiffusion objective. The analysis of the trade-off between autoregressive\nmodeling and diffusion demonstrates the potential of ACDiT to be used in\nlong-horizon visual generation tasks. We hope that ACDiT offers a novel\nperspective on visual autoregressive generation and unlocks new avenues for\nunified models."
                },
                "authors": [
                    {
                        "name": "Jinyi Hu"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Yuxuan Song"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Mingxuan Wang"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Wei-Ying Ma"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10501v1",
                "updated": "2025-03-13T16:04:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    4,
                    31,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T16:04:31Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    4,
                    31,
                    3,
                    72,
                    0
                ],
                "title": "TokenCarve: Information-Preserving Visual Token Compression in\n  Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenCarve: Information-Preserving Visual Token Compression in\n  Multimodal Large Language Models"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are becoming increasingly popular,\nwhile the high computational cost associated with multimodal data input,\nparticularly from visual tokens, poses a significant challenge. Existing\ntraining-based token compression methods improve inference efficiency but\nrequire costly retraining, while training-free methods struggle to maintain\nperformance when aggressively reducing token counts. In this study, we reveal\nthat the performance degradation of MLLM closely correlates with the\naccelerated loss of information in the attention output matrix. This insight\nintroduces a novel information-preserving perspective, making it possible to\nmaintain performance even under extreme token compression. Based on this\nfinding, we propose TokenCarve, a training-free, plug-and-play, two-stage token\ncompression framework. The first stage employs an\nInformation-Preservation-Guided Selection (IPGS) strategy to prune\nlow-information tokens, while the second stage further leverages IPGS to guide\ntoken merging, minimizing information loss. Extensive experiments on 11\ndatasets and 2 model variants demonstrate the effectiveness of TokenCarve. It\ncan even reduce the number of visual tokens to 22.2% of the original count,\nachieving a 1.23x speedup in inference, a 64% reduction in KV cache storage,\nand only a 1.54% drop in accuracy. Our code is available at\nhttps://github.com/ShawnTan86/TokenCarve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are becoming increasingly popular,\nwhile the high computational cost associated with multimodal data input,\nparticularly from visual tokens, poses a significant challenge. Existing\ntraining-based token compression methods improve inference efficiency but\nrequire costly retraining, while training-free methods struggle to maintain\nperformance when aggressively reducing token counts. In this study, we reveal\nthat the performance degradation of MLLM closely correlates with the\naccelerated loss of information in the attention output matrix. This insight\nintroduces a novel information-preserving perspective, making it possible to\nmaintain performance even under extreme token compression. Based on this\nfinding, we propose TokenCarve, a training-free, plug-and-play, two-stage token\ncompression framework. The first stage employs an\nInformation-Preservation-Guided Selection (IPGS) strategy to prune\nlow-information tokens, while the second stage further leverages IPGS to guide\ntoken merging, minimizing information loss. Extensive experiments on 11\ndatasets and 2 model variants demonstrate the effectiveness of TokenCarve. It\ncan even reduce the number of visual tokens to 22.2% of the original count,\nachieving a 1.23x speedup in inference, a 64% reduction in KV cache storage,\nand only a 1.54% drop in accuracy. Our code is available at\nhttps://github.com/ShawnTan86/TokenCarve."
                },
                "authors": [
                    {
                        "name": "Xudong Tan"
                    },
                    {
                        "name": "Peng Ye"
                    },
                    {
                        "name": "Chongjun Tu"
                    },
                    {
                        "name": "Jianjian Cao"
                    },
                    {
                        "name": "Yaoxin Yang"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Tao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tao Chen"
                },
                "author": "Tao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10494v1",
                "updated": "2025-03-13T15:57:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    57,
                    50,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T15:57:50Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    57,
                    50,
                    3,
                    72,
                    0
                ],
                "title": "Source-primed Multi-turn Conversation Helps Large Language Models\n  Translate Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Source-primed Multi-turn Conversation Helps Large Language Models\n  Translate Documents"
                },
                "summary": "LLMs have paved the way for truly simple document-level machine translation,\nbut challenges such as omission errors remain. In this paper, we study a simple\nmethod for handling document-level machine translation, by leveraging previous\ncontexts in a multi-turn conversational manner. Specifically, by decomposing\ndocuments into segments and iteratively translating them while maintaining\nprevious turns, this method ensures coherent translations without additional\ntraining, and can fully re-use the KV cache of previous turns thus minimizing\ncomputational overhead. We further propose a `source-primed' method that first\nprovides the whole source document before multi-turn translation. We\nempirically show this multi-turn method outperforms both translating entire\ndocuments in a single turn and translating each segment independently according\nto multiple automatic metrics in representative LLMs, establishing a strong\nbaseline for document-level translation using LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have paved the way for truly simple document-level machine translation,\nbut challenges such as omission errors remain. In this paper, we study a simple\nmethod for handling document-level machine translation, by leveraging previous\ncontexts in a multi-turn conversational manner. Specifically, by decomposing\ndocuments into segments and iteratively translating them while maintaining\nprevious turns, this method ensures coherent translations without additional\ntraining, and can fully re-use the KV cache of previous turns thus minimizing\ncomputational overhead. We further propose a `source-primed' method that first\nprovides the whole source document before multi-turn translation. We\nempirically show this multi-turn method outperforms both translating entire\ndocuments in a single turn and translating each segment independently according\nto multiple automatic metrics in representative LLMs, establishing a strong\nbaseline for document-level translation using LLMs."
                },
                "authors": [
                    {
                        "name": "Hanxu Hu"
                    },
                    {
                        "name": "Jannis Vamvas"
                    },
                    {
                        "name": "Rico Sennrich"
                    }
                ],
                "author_detail": {
                    "name": "Rico Sennrich"
                },
                "author": "Rico Sennrich",
                "arxiv_comment": "9 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10337v1",
                "updated": "2025-03-13T13:15:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    15,
                    28,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T13:15:28Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    15,
                    28,
                    3,
                    72,
                    0
                ],
                "title": "KV-Distill: Nearly Lossless Learnable Context Compression for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Distill: Nearly Lossless Learnable Context Compression for LLMs"
                },
                "summary": "Sequence-to-sequence tasks often benefit from long contexts, but the\nquadratic complexity of self-attention in standard Transformers renders this\nnon-trivial. During generation, temporary representations -stored in the\nso-called KV cache-account for a large portion of GPU memory usage and scale\nlinearly with context length. We introduce KV-Distill, a Transformer\ncompression framework that distills long context KV caches into significantly\nshorter representations in a question-independent fashion. KV-Distill can be\ntrained as a parameter-efficient adaptor for pretrained models, and enables the\ncompression of arbitrary spans of a context while preserving pre-trained model\ncapabilities. We treat a compressed-uncompressed cache as a student-teacher\npairing and apply a KL-type divergence to match the generated outputs.\nKV-Distill outperforms other compression techniques in worst-case extractive\ntasks and approaches uncompressed performance in long context question\nanswering and summarization, and it can be fine-tuned on domain-specific\ncontexts to reduce lengths by up to 99% while preserving downstream\nperformance. We demonstrate the generalizability of KV-Distill across various\nmodel sizes and architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence-to-sequence tasks often benefit from long contexts, but the\nquadratic complexity of self-attention in standard Transformers renders this\nnon-trivial. During generation, temporary representations -stored in the\nso-called KV cache-account for a large portion of GPU memory usage and scale\nlinearly with context length. We introduce KV-Distill, a Transformer\ncompression framework that distills long context KV caches into significantly\nshorter representations in a question-independent fashion. KV-Distill can be\ntrained as a parameter-efficient adaptor for pretrained models, and enables the\ncompression of arbitrary spans of a context while preserving pre-trained model\ncapabilities. We treat a compressed-uncompressed cache as a student-teacher\npairing and apply a KL-type divergence to match the generated outputs.\nKV-Distill outperforms other compression techniques in worst-case extractive\ntasks and approaches uncompressed performance in long context question\nanswering and summarization, and it can be fine-tuned on domain-specific\ncontexts to reduce lengths by up to 99% while preserving downstream\nperformance. We demonstrate the generalizability of KV-Distill across various\nmodel sizes and architectures."
                },
                "authors": [
                    {
                        "name": "Vivek Chari"
                    },
                    {
                        "name": "Guanghui Qin"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10270v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10270v1",
                "updated": "2025-03-13T11:26:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T11:26:45Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "title": "EEdit : Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEdit : Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing"
                },
                "summary": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit"
                },
                "authors": [
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Wenteng Chen"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10270v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10270v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v3",
                "updated": "2025-03-13T11:14:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    14,
                    49,
                    3,
                    72,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: I/O-Aware Optimization of Traditional RNNs on modern hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: I/O-Aware Optimization of Traditional RNNs on modern hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: https://github.com/NX-AI/flashrnn",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: https://github.com/NX-AI/flashrnn"
                },
                "authors": [
                    {
                        "name": "Korbinian Pöppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10074v1",
                "updated": "2025-03-13T05:43:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    43,
                    14,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T05:43:14Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    43,
                    14,
                    3,
                    72,
                    0
                ],
                "title": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension"
                },
                "summary": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions."
                },
                "authors": [
                    {
                        "name": "Taehun Kim"
                    },
                    {
                        "name": "Hyerean Jang"
                    },
                    {
                        "name": "Youngjoo Shin"
                    }
                ],
                "author_detail": {
                    "name": "Youngjoo Shin"
                },
                "author": "Youngjoo Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17599v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17599v2",
                "updated": "2025-03-13T04:04:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    4,
                    4,
                    8,
                    3,
                    72,
                    0
                ],
                "published": "2025-02-24T19:34:52Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    34,
                    52,
                    0,
                    55,
                    0
                ],
                "title": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference"
                },
                "summary": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA."
                },
                "authors": [
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Hui Shen"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Che Liu"
                    },
                    {
                        "name": "Zheda Mai"
                    },
                    {
                        "name": "Mi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mi Zhang"
                },
                "author": "Mi Zhang",
                "arxiv_comment": "NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17599v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17599v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10714v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10714v1",
                "updated": "2025-03-13T03:36:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    36,
                    3,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T03:36:03Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    36,
                    3,
                    3,
                    72,
                    0
                ],
                "title": "ZeroMerge: Parameter-Free KV Cache Compression for Memory-Efficient\n  Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZeroMerge: Parameter-Free KV Cache Compression for Memory-Efficient\n  Long-Context LLMs"
                },
                "summary": "The linear growth of key-value (KV) cache memory and quadratic computational\ncomplexity pose significant bottlenecks for large language models (LLMs) in\nlong-context processing. While existing KV cache optimization methods address\nthese challenges through token pruning or feature merging, they often suffer\nfrom irreversible information loss or require costly parameter retraining. We\npropose ZeroMerge, a dynamic zero-shot compression framework that achieves\nefficient cache management through three key innovations: (1) Fine-grained\nmemory allocation guided by multi-dimensional token importance metrics at\nhead-level granularity, (2) A residual merging mechanism that preserves\ncritical context through compensated attention scoring, and (3) Parameter-free\nadaptation compatible with diverse LLM architectures without retraining.\nComprehensive evaluations across LLaMA-2 model demonstrate that ZeroMerge\nmaintains full-cache performance at 5\\% compression ratios while doubling\ninference throughput at 40K token lengths. The method effectively balances\nmemory efficiency, generation quality, and deployment flexibility, advancing\npractical long-context LLM applications. The code is available at\nhttps://github.com/SusCom-Lab/ZeroMerge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The linear growth of key-value (KV) cache memory and quadratic computational\ncomplexity pose significant bottlenecks for large language models (LLMs) in\nlong-context processing. While existing KV cache optimization methods address\nthese challenges through token pruning or feature merging, they often suffer\nfrom irreversible information loss or require costly parameter retraining. We\npropose ZeroMerge, a dynamic zero-shot compression framework that achieves\nefficient cache management through three key innovations: (1) Fine-grained\nmemory allocation guided by multi-dimensional token importance metrics at\nhead-level granularity, (2) A residual merging mechanism that preserves\ncritical context through compensated attention scoring, and (3) Parameter-free\nadaptation compatible with diverse LLM architectures without retraining.\nComprehensive evaluations across LLaMA-2 model demonstrate that ZeroMerge\nmaintains full-cache performance at 5\\% compression ratios while doubling\ninference throughput at 40K token lengths. The method effectively balances\nmemory efficiency, generation quality, and deployment flexibility, advancing\npractical long-context LLM applications. The code is available at\nhttps://github.com/SusCom-Lab/ZeroMerge."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Pei Liu"
                    },
                    {
                        "name": "Guoming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Guoming Tang"
                },
                "author": "Guoming Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10714v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10714v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13035v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13035v3",
                "updated": "2025-03-13T03:16:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    16,
                    43,
                    3,
                    72,
                    0
                ],
                "published": "2024-06-18T20:01:51Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    20,
                    1,
                    51,
                    1,
                    170,
                    0
                ],
                "title": "D2O: Dynamic Discriminative Operations for Efficient Long-Context\n  Inference of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D2O: Dynamic Discriminative Operations for Efficient Long-Context\n  Inference of Large Language Models"
                },
                "summary": "Generative inference in Large Language Models (LLMs) is impeded by the\ngrowing memory demands of Key-Value (KV) cache, especially for longer\nsequences. Traditional KV cache eviction strategies, which discard less\ncritical KV pairs based on attention scores, often degrade generation quality,\nleading to issues such as context loss or hallucinations. In this work, we\nintroduce Dynamic Discriminative Operations (D2O), a KV cache compression\nmethod that optimizes KV cache size dynamically and discriminatively at two\nlevels without fine-tuning, while preserving essential context. At layer level,\nD2O leverages the varying densities of attention weights between shallow and\ndeep layers to dynamically determine which layers should avoid excessive\neviction via a novel dynamic allocation strategy to minimize information loss.\nAt token level, D2O incorporates a compensation mechanism that maintains a\nsimilarity threshold to re-discriminate the importance of currently discarded\ntokens, determining whether they should be recalled and merged with similar\ntokens. We conduct experiments on various benchmarks and LLM architectures. Our\nresults show that D2O not only achieves significant memory savings and enhances\ninference throughput by more than 3$\\times$ but also maintains high-quality\nlong-text generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative inference in Large Language Models (LLMs) is impeded by the\ngrowing memory demands of Key-Value (KV) cache, especially for longer\nsequences. Traditional KV cache eviction strategies, which discard less\ncritical KV pairs based on attention scores, often degrade generation quality,\nleading to issues such as context loss or hallucinations. In this work, we\nintroduce Dynamic Discriminative Operations (D2O), a KV cache compression\nmethod that optimizes KV cache size dynamically and discriminatively at two\nlevels without fine-tuning, while preserving essential context. At layer level,\nD2O leverages the varying densities of attention weights between shallow and\ndeep layers to dynamically determine which layers should avoid excessive\neviction via a novel dynamic allocation strategy to minimize information loss.\nAt token level, D2O incorporates a compensation mechanism that maintains a\nsimilarity threshold to re-discriminate the importance of currently discarded\ntokens, determining whether they should be recalled and merged with similar\ntokens. We conduct experiments on various benchmarks and LLM architectures. Our\nresults show that D2O not only achieves significant memory savings and enhances\ninference throughput by more than 3$\\times$ but also maintains high-quality\nlong-text generation."
                },
                "authors": [
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Xinjian Wu"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Yi Xin"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Zhihong Zhu"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Siqi Luo"
                    },
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Mi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mi Zhang"
                },
                "author": "Mi Zhang",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13035v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13035v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14361v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14361v3",
                "updated": "2025-03-12T18:14:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    18,
                    14,
                    21,
                    2,
                    71,
                    0
                ],
                "published": "2024-01-25T18:07:50Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    18,
                    7,
                    50,
                    3,
                    25,
                    0
                ],
                "title": "MoE-Infinity: Efficient MoE Inference on Personal Machines with\n  Sparsity-Aware Expert Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Infinity: Efficient MoE Inference on Personal Machines with\n  Sparsity-Aware Expert Cache"
                },
                "summary": "This paper presents MoE-Infinity, an efficient MoE inference system designed\nfor personal machines with limited GPU memory capacity. The key idea for\nMoE-Infinity is that on personal machines, which are often single-user\nenvironments, MoE-based LLMs typically operate with a batch size of one. In\nthis setting, MoE models exhibit a high degree of activation sparsity, meaning\na small number of experts are frequently reused in generating tokens during the\ndecode phase. Leveraging this idea, we design a sparsity-aware expert cache,\nwhich can trace the sparse activation of experts during inference and carefully\nselect the trace that represents the sparsity pattern. By analyzing these\nselected traces, MoE-Infinity guides the replacement and prefetching of the\nexpert cache, providing 3.1-16.7x per-token latency improvements over numerous\nstate-of-the-art systems, including vLLM, Ollama, DeepSpeed and BrainStorm\nacross various MoE models (DeepSeek and Mixtral) when handling different LLM\ntasks. MoE-Infinity's source code is publicly available at\nhttps://github.com/EfficientMoE/MoE-Infinity",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents MoE-Infinity, an efficient MoE inference system designed\nfor personal machines with limited GPU memory capacity. The key idea for\nMoE-Infinity is that on personal machines, which are often single-user\nenvironments, MoE-based LLMs typically operate with a batch size of one. In\nthis setting, MoE models exhibit a high degree of activation sparsity, meaning\na small number of experts are frequently reused in generating tokens during the\ndecode phase. Leveraging this idea, we design a sparsity-aware expert cache,\nwhich can trace the sparse activation of experts during inference and carefully\nselect the trace that represents the sparsity pattern. By analyzing these\nselected traces, MoE-Infinity guides the replacement and prefetching of the\nexpert cache, providing 3.1-16.7x per-token latency improvements over numerous\nstate-of-the-art systems, including vLLM, Ollama, DeepSpeed and BrainStorm\nacross various MoE models (DeepSeek and Mixtral) when handling different LLM\ntasks. MoE-Infinity's source code is publicly available at\nhttps://github.com/EfficientMoE/MoE-Infinity"
                },
                "authors": [
                    {
                        "name": "Leyang Xue"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Zhan Lu"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Mahesh Marina"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Marina"
                },
                "author": "Mahesh Marina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14361v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14361v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v2",
                "updated": "2025-03-12T17:59:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    59,
                    18,
                    2,
                    71,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs"
                },
                "summary": "Long-range tasks demand reasoning over long inputs. Current solutions require\nlarge compute budgets, training data, model weight access, or complex\ntask-specific designs. We introduce PRISM, which processes information as a\nstream of chunks while maintaining a structured in-context memory specified\nwith a typed hierarchical schema. PRISM outperforms baselines on diverse tasks\nwhile using at least 4x shorter contexts than long-context models. This\napproach is token-efficient, producing concise outputs and efficiently\nleveraging key-value (KV) caches to reduce costs by up to 54% compared to\nalternative short-context methods. PRISM scales down to tiny chunks (<500\ntokens) without increasing encoding costs or sacrificing quality, and\ngeneralizes to new tasks with minimal effort by automatically generating\nschemas from task descriptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks demand reasoning over long inputs. Current solutions require\nlarge compute budgets, training data, model weight access, or complex\ntask-specific designs. We introduce PRISM, which processes information as a\nstream of chunks while maintaining a structured in-context memory specified\nwith a typed hierarchical schema. PRISM outperforms baselines on diverse tasks\nwhile using at least 4x shorter contexts than long-context models. This\napproach is token-efficient, producing concise outputs and efficiently\nleveraging key-value (KV) caches to reduce costs by up to 54% compared to\nalternative short-context methods. PRISM scales down to tiny chunks (<500\ntokens) without increasing encoding costs or sacrificing quality, and\ngeneralizes to new tasks with minimal effort by automatically generating\nschemas from task descriptions."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "28 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09573v1",
                "updated": "2025-03-12T17:43:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    43,
                    40,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T17:43:40Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    43,
                    40,
                    2,
                    71,
                    0
                ],
                "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models"
                },
                "summary": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/"
                },
                "authors": [
                    {
                        "name": "Marianne Arriola"
                    },
                    {
                        "name": "Aaron Gokaslan"
                    },
                    {
                        "name": "Justin T Chiu"
                    },
                    {
                        "name": "Zhihan Yang"
                    },
                    {
                        "name": "Zhixuan Qi"
                    },
                    {
                        "name": "Jiaqi Han"
                    },
                    {
                        "name": "Subham Sekhar Sahoo"
                    },
                    {
                        "name": "Volodymyr Kuleshov"
                    }
                ],
                "author_detail": {
                    "name": "Volodymyr Kuleshov"
                },
                "author": "Volodymyr Kuleshov",
                "arxiv_comment": "ICLR 2025 Oral. We provide the code at\n  https://github.com/kuleshov-group/bd3lms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09218v1",
                "updated": "2025-03-12T10:05:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    5,
                    5,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T10:05:05Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    5,
                    5,
                    2,
                    71,
                    0
                ],
                "title": "N2C2: Nearest Neighbor Enhanced Confidence Calibration for Cross-Lingual\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "N2C2: Nearest Neighbor Enhanced Confidence Calibration for Cross-Lingual\n  In-Context Learning"
                },
                "summary": "Recent advancements of in-context learning (ICL) show language models can\nsignificantly improve their performance when demonstrations are provided.\nHowever, little attention has been paid to model calibration and prediction\nconfidence of ICL in cross-lingual scenarios. To bridge this gap, we conduct a\nthorough analysis of ICL for cross-lingual sentiment classification. Our\nfindings suggest that ICL performs poorly in cross-lingual scenarios,\nexhibiting low accuracy and presenting high calibration errors. In response, we\npropose a novel approach, N2C2, which employs a -nearest neighbors augmented\nclassifier for prediction confidence calibration. N2C2 narrows the prediction\ngap by leveraging a datastore of cached few-shot instances. Specifically, N2C2\nintegrates the predictions from the datastore and incorporates confidence-aware\ndistribution, semantically consistent retrieval representation, and adaptive\nneighbor combination modules to effectively utilize the limited number of\nsupporting instances. Evaluation on two multilingual sentiment classification\ndatasets demonstrates that N2C2 outperforms traditional ICL. It surpasses fine\ntuning, prompt tuning and recent state-of-the-art methods in terms of accuracy\nand calibration errors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements of in-context learning (ICL) show language models can\nsignificantly improve their performance when demonstrations are provided.\nHowever, little attention has been paid to model calibration and prediction\nconfidence of ICL in cross-lingual scenarios. To bridge this gap, we conduct a\nthorough analysis of ICL for cross-lingual sentiment classification. Our\nfindings suggest that ICL performs poorly in cross-lingual scenarios,\nexhibiting low accuracy and presenting high calibration errors. In response, we\npropose a novel approach, N2C2, which employs a -nearest neighbors augmented\nclassifier for prediction confidence calibration. N2C2 narrows the prediction\ngap by leveraging a datastore of cached few-shot instances. Specifically, N2C2\nintegrates the predictions from the datastore and incorporates confidence-aware\ndistribution, semantically consistent retrieval representation, and adaptive\nneighbor combination modules to effectively utilize the limited number of\nsupporting instances. Evaluation on two multilingual sentiment classification\ndatasets demonstrates that N2C2 outperforms traditional ICL. It surpasses fine\ntuning, prompt tuning and recent state-of-the-art methods in terms of accuracy\nand calibration errors."
                },
                "authors": [
                    {
                        "name": "Jie He"
                    },
                    {
                        "name": "Simon Yu"
                    },
                    {
                        "name": "Deyi Xiong"
                    },
                    {
                        "name": "Víctor Gutiérrez-Basulto"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17363v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17363v3",
                "updated": "2025-03-12T07:23:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    7,
                    23,
                    32,
                    2,
                    71,
                    0
                ],
                "published": "2025-02-24T17:40:09Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    40,
                    9,
                    0,
                    55,
                    0
                ],
                "title": "KV-Edit: Training-Free Image Editing for Precise Background Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Edit: Training-Free Image Editing for Precise Background Preservation"
                },
                "summary": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit"
                },
                "authors": [
                    {
                        "name": "Tianrui Zhu"
                    },
                    {
                        "name": "Shiyi Zhang"
                    },
                    {
                        "name": "Jiawei Shao"
                    },
                    {
                        "name": "Yansong Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Tang"
                },
                "author": "Yansong Tang",
                "arxiv_comment": "Project webpage is available at\n  https://xilluill.github.io/projectpages/KV-Edit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17363v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17363v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19355v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19355v2",
                "updated": "2025-03-12T03:40:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    3,
                    40,
                    38,
                    2,
                    71,
                    0
                ],
                "published": "2024-10-25T07:24:38Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "title": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality"
                },
                "summary": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality."
                },
                "authors": [
                    {
                        "name": "Zhengyao Lv"
                    },
                    {
                        "name": "Chenyang Si"
                    },
                    {
                        "name": "Junhao Song"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Kwan-Yee K. Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Yee K. Wong"
                },
                "author": "Kwan-Yee K. Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19355v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19355v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08966v1",
                "updated": "2025-03-12T00:12:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    0,
                    12,
                    39,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T00:12:39Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    0,
                    12,
                    39,
                    2,
                    71,
                    0
                ],
                "title": "Performance Models for a Two-tiered Storage System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Models for a Two-tiered Storage System"
                },
                "summary": "This work describes the design, implementation and performance analysis of a\ndistributed two-tiered storage software. The first tier functions as a\ndistributed software cache implemented using solid-state devices~(NVMes) and\nthe second tier consists of multiple hard disks~(HDDs). We describe an online\nlearning algorithm that manages data movement between the tiers. The software\nis hybrid, i.e. both distributed and multi-threaded. The end-to-end performance\nmodel of the two-tier system was developed using queuing networks and\nbehavioral models of storage devices. We identified significant parameters that\naffect the performance of storage devices and created behavioral models for\neach device. The performance of the software was evaluated on a many-core\ncluster using non-trivial read/write workloads. The paper provides examples to\nillustrate the use of these models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work describes the design, implementation and performance analysis of a\ndistributed two-tiered storage software. The first tier functions as a\ndistributed software cache implemented using solid-state devices~(NVMes) and\nthe second tier consists of multiple hard disks~(HDDs). We describe an online\nlearning algorithm that manages data movement between the tiers. The software\nis hybrid, i.e. both distributed and multi-threaded. The end-to-end performance\nmodel of the two-tier system was developed using queuing networks and\nbehavioral models of storage devices. We identified significant parameters that\naffect the performance of storage devices and created behavioral models for\neach device. The performance of the software was evaluated on a many-core\ncluster using non-trivial read/write workloads. The paper provides examples to\nillustrate the use of these models."
                },
                "authors": [
                    {
                        "name": "Aparna Sasidharan"
                    },
                    {
                        "name": "Xian-He"
                    },
                    {
                        "name": "Jay Lofstead"
                    },
                    {
                        "name": "Scott Klasky"
                    }
                ],
                "author_detail": {
                    "name": "Scott Klasky"
                },
                "author": "Scott Klasky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08941v1",
                "updated": "2025-03-11T22:44:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    22,
                    44,
                    38,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T22:44:38Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    22,
                    44,
                    38,
                    1,
                    70,
                    0
                ],
                "title": "BCZT/LSMO/BCZT multilayer films for high temperature energy storage\n  capacitors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BCZT/LSMO/BCZT multilayer films for high temperature energy storage\n  capacitors"
                },
                "summary": "Ba0.85Ca0.15Zr0.1Ti0.9O3/La0.8Sr0.2MnO3/Ba0.85Ca0.15Zr0.1Ti0.9O3\n(BCZT/LSMO/BCZT) sandwich films were elaborated using the sol-gel spin coating\nprocess. The dielectric properties displayed excellent thermal stability with\nthe temperature coefficient of capacitance, TCC, remaining within 10% between\n-50 C and 300 C. The high energy storage density, Wrec, of 11.8 J/cm3 observed\nin this sandwich films, is nearly twice as high as that of the BCZT films, with\nan efficiency, n, of 77% under a weak electric field of 800 kV/cm. Furthermore,\nthe stability of Wrec and n was observed along the studied temperature interval\nmaking them promising candidates for high-temperature energy storage\ncapacitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ba0.85Ca0.15Zr0.1Ti0.9O3/La0.8Sr0.2MnO3/Ba0.85Ca0.15Zr0.1Ti0.9O3\n(BCZT/LSMO/BCZT) sandwich films were elaborated using the sol-gel spin coating\nprocess. The dielectric properties displayed excellent thermal stability with\nthe temperature coefficient of capacitance, TCC, remaining within 10% between\n-50 C and 300 C. The high energy storage density, Wrec, of 11.8 J/cm3 observed\nin this sandwich films, is nearly twice as high as that of the BCZT films, with\nan efficiency, n, of 77% under a weak electric field of 800 kV/cm. Furthermore,\nthe stability of Wrec and n was observed along the studied temperature interval\nmaking them promising candidates for high-temperature energy storage\ncapacitors."
                },
                "authors": [
                    {
                        "name": "Afaak Lakouader"
                    },
                    {
                        "name": "Abdelilah Lahmar"
                    },
                    {
                        "name": "Spela Kunej"
                    },
                    {
                        "name": "Daoud Mezzane"
                    },
                    {
                        "name": "Jamal Belhadi"
                    },
                    {
                        "name": "El Hassan Choukri"
                    },
                    {
                        "name": "Lahoucine Hajji"
                    },
                    {
                        "name": "Mbarek Amjoud"
                    },
                    {
                        "name": "Zdravko Kutnjak"
                    },
                    {
                        "name": "Igor A. Lukyanchuk"
                    },
                    {
                        "name": "Mimoun El Marssi"
                    }
                ],
                "author_detail": {
                    "name": "Mimoun El Marssi"
                },
                "author": "Mimoun El Marssi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08879v1",
                "updated": "2025-03-11T20:45:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    20,
                    45,
                    2,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T20:45:02Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    20,
                    45,
                    2,
                    1,
                    70,
                    0
                ],
                "title": "LLMs Know What to Drop: Self-Attention Guided KV Cache Eviction for\n  Efficient Long-Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Know What to Drop: Self-Attention Guided KV Cache Eviction for\n  Efficient Long-Context Inference"
                },
                "summary": "Efficient long-context inference is critical as large language models (LLMs)\nadopt context windows of ranging from 128K to 1M tokens. However, the growing\nkey-value (KV) cache and the high computational complexity of attention create\nsignificant bottlenecks in memory usage and latency. In this paper, we find\nthat attention in diverse long-context tasks exhibits sparsity, and LLMs\nimplicitly \"know\" which tokens can be dropped or evicted at the head level\nafter the pre-filling stage. Based on this insight, we propose Self-Attention\nGuided Eviction~(SAGE-KV), a simple and effective KV eviction cache method for\nlong-context inference. After prefilling, our method performs a one-time top-k\nselection at both the token and head levels to compress the KV cache, enabling\nefficient inference with the reduced cache. Evaluations on LongBench and three\nlong-context LLMs (Llama3.1-8B-Instruct-128k, Llama3-8B-Prolong-512k-Instruct,\nand Qwen2.5-7B-Instruct-128k) show that SAGE-KV maintains accuracy comparable\nto full attention while significantly improving efficiency. Specifically,\nSAGE-KV achieves 4x higher memory efficiency with improved accuracy over the\nstatic KV cache selection method StreamLLM, and 2x higher memory efficiency\nwith better accuracy than the dynamic KV cache selection method Quest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient long-context inference is critical as large language models (LLMs)\nadopt context windows of ranging from 128K to 1M tokens. However, the growing\nkey-value (KV) cache and the high computational complexity of attention create\nsignificant bottlenecks in memory usage and latency. In this paper, we find\nthat attention in diverse long-context tasks exhibits sparsity, and LLMs\nimplicitly \"know\" which tokens can be dropped or evicted at the head level\nafter the pre-filling stage. Based on this insight, we propose Self-Attention\nGuided Eviction~(SAGE-KV), a simple and effective KV eviction cache method for\nlong-context inference. After prefilling, our method performs a one-time top-k\nselection at both the token and head levels to compress the KV cache, enabling\nefficient inference with the reduced cache. Evaluations on LongBench and three\nlong-context LLMs (Llama3.1-8B-Instruct-128k, Llama3-8B-Prolong-512k-Instruct,\nand Qwen2.5-7B-Instruct-128k) show that SAGE-KV maintains accuracy comparable\nto full attention while significantly improving efficiency. Specifically,\nSAGE-KV achieves 4x higher memory efficiency with improved accuracy over the\nstatic KV cache selection method StreamLLM, and 2x higher memory efficiency\nwith better accuracy than the dynamic KV cache selection method Quest."
                },
                "authors": [
                    {
                        "name": "Guangtao Wang"
                    },
                    {
                        "name": "Shubhangi Upasani"
                    },
                    {
                        "name": "Chen Wu"
                    },
                    {
                        "name": "Darshan Gandhi"
                    },
                    {
                        "name": "Jonathan Li"
                    },
                    {
                        "name": "Changran Hu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Urmish Thakker"
                    }
                ],
                "author_detail": {
                    "name": "Urmish Thakker"
                },
                "author": "Urmish Thakker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08640v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08640v1",
                "updated": "2025-03-11T17:30:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    30,
                    58,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T17:30:58Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    30,
                    58,
                    1,
                    70,
                    0
                ],
                "title": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention"
                },
                "summary": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale."
                },
                "authors": [
                    {
                        "name": "Emily Xiao"
                    },
                    {
                        "name": "Chin-Jou Li"
                    },
                    {
                        "name": "Yilin Zhang"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Amanda Bertsch"
                    }
                ],
                "author_detail": {
                    "name": "Amanda Bertsch"
                },
                "author": "Amanda Bertsch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08640v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08461v1",
                "updated": "2025-03-11T14:10:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    10,
                    58,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T14:10:58Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    10,
                    58,
                    1,
                    70,
                    0
                ],
                "title": "FastCache: Optimizing Multimodal LLM Serving through Lightweight\n  KV-Cache Compression Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCache: Optimizing Multimodal LLM Serving through Lightweight\n  KV-Cache Compression Framework"
                },
                "summary": "Multi-modal Large Language Models (MLLMs) serving systems commonly employ\nKV-cache compression to reduce memory footprint. However, existing compression\nmethods introduce significant processing overhead and queuing delays,\nparticularly in concurrent serving scenarios. We present \\texttt{FastCache}, a\nnovel serving framework that effectively addresses these challenges through two\nkey innovations: (1) a dynamic batching strategy that optimizes request\nscheduling across prefill, compression, and decode stages, and (2) an efficient\nKV-cache memory pool mechanism that eliminates memory fragmentation while\nmaintaining high GPU utilization. Our comprehensive experiments on the GQA and\nMileBench datasets demonstrate that \\texttt{FastCache} achieves up to\n19.3$\\times$ reduction in Time-To-First-Token (TTFT) and 12.1$\\times$\nimprovement in throughput compared to state-of-the-art baselines. The system\nmaintains stable performance under high-concurrency scenarios (up to 40 req/s)\nwhile reducing average memory consumption by 20\\%. These results establish\n\\texttt{FastCache} as an efficient solution for real-world LLM serving systems\nwith KV-cache compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal Large Language Models (MLLMs) serving systems commonly employ\nKV-cache compression to reduce memory footprint. However, existing compression\nmethods introduce significant processing overhead and queuing delays,\nparticularly in concurrent serving scenarios. We present \\texttt{FastCache}, a\nnovel serving framework that effectively addresses these challenges through two\nkey innovations: (1) a dynamic batching strategy that optimizes request\nscheduling across prefill, compression, and decode stages, and (2) an efficient\nKV-cache memory pool mechanism that eliminates memory fragmentation while\nmaintaining high GPU utilization. Our comprehensive experiments on the GQA and\nMileBench datasets demonstrate that \\texttt{FastCache} achieves up to\n19.3$\\times$ reduction in Time-To-First-Token (TTFT) and 12.1$\\times$\nimprovement in throughput compared to state-of-the-art baselines. The system\nmaintains stable performance under high-concurrency scenarios (up to 40 req/s)\nwhile reducing average memory consumption by 20\\%. These results establish\n\\texttt{FastCache} as an efficient solution for real-world LLM serving systems\nwith KV-cache compression."
                },
                "authors": [
                    {
                        "name": "Jianian Zhu"
                    },
                    {
                        "name": "Hang Wu"
                    },
                    {
                        "name": "Haojie Wang"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Biao Hou"
                    },
                    {
                        "name": "Ruixuan Li"
                    },
                    {
                        "name": "Jidong Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Jidong Zhai"
                },
                "author": "Jidong Zhai",
                "arxiv_comment": "14 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10319v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10319v2",
                "updated": "2025-03-11T14:02:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    2,
                    4,
                    1,
                    70,
                    0
                ],
                "published": "2024-12-13T17:59:52Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods"
                },
                "summary": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench."
                },
                "authors": [
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Surin Ahn"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10319v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10319v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00857v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00857v3",
                "updated": "2025-03-11T13:13:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    13,
                    11,
                    1,
                    70,
                    0
                ],
                "published": "2024-12-01T15:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Coherent Video Inpainting Using Optical Flow-Guided Efficient Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherent Video Inpainting Using Optical Flow-Guided Efficient Diffusion"
                },
                "summary": "The text-guided video inpainting technique has significantly improved the\nperformance of content generation applications. A recent family for these\nimprovements uses diffusion models, which have become essential for achieving\nhigh-quality video inpainting results, yet they still face performance\nbottlenecks in temporal consistency and computational efficiency. This\nmotivates us to propose a new video inpainting framework using optical\nFlow-guided Efficient Diffusion (FloED) for higher video coherence.\nSpecifically, FloED employs a dual-branch architecture, where the time-agnostic\nflow branch restores corrupted flow first, and the multi-scale flow adapters\nprovide motion guidance to the main inpainting branch. Besides, a training-free\nlatent interpolation method is proposed to accelerate the multi-step denoising\nprocess using flow warping. With the flow attention cache mechanism, FLoED\nefficiently reduces the computational cost of incorporating optical flow.\nExtensive experiments on background restoration and object removal tasks show\nthat FloED outperforms state-of-the-art diffusion-based methods in both quality\nand efficiency. Our codes and models will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The text-guided video inpainting technique has significantly improved the\nperformance of content generation applications. A recent family for these\nimprovements uses diffusion models, which have become essential for achieving\nhigh-quality video inpainting results, yet they still face performance\nbottlenecks in temporal consistency and computational efficiency. This\nmotivates us to propose a new video inpainting framework using optical\nFlow-guided Efficient Diffusion (FloED) for higher video coherence.\nSpecifically, FloED employs a dual-branch architecture, where the time-agnostic\nflow branch restores corrupted flow first, and the multi-scale flow adapters\nprovide motion guidance to the main inpainting branch. Besides, a training-free\nlatent interpolation method is proposed to accelerate the multi-step denoising\nprocess using flow warping. With the flow attention cache mechanism, FLoED\nefficiently reduces the computational cost of incorporating optical flow.\nExtensive experiments on background restoration and object removal tasks show\nthat FloED outperforms state-of-the-art diffusion-based methods in both quality\nand efficiency. Our codes and models will be made publicly available."
                },
                "authors": [
                    {
                        "name": "Bohai Gu"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Peiran Dong"
                    },
                    {
                        "name": "Qihua Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Qihua Zhou"
                },
                "author": "Qihua Zhou",
                "arxiv_comment": "Project page: https://nevsnev.github.io/FloED/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00857v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00857v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07635v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07635v5",
                "updated": "2025-03-11T09:17:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    9,
                    17,
                    2,
                    1,
                    70,
                    0
                ],
                "published": "2024-11-12T08:30:59Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    30,
                    59,
                    1,
                    317,
                    0
                ],
                "title": "Breaking the Low-Rank Dilemma of Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Low-Rank Dilemma of Linear Attention"
                },
                "summary": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA."
                },
                "authors": [
                    {
                        "name": "Qihang Fan"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "arxiv_comment": "The paper is accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07635v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07635v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06304v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06304v2",
                "updated": "2025-03-11T03:26:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    3,
                    26,
                    20,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-08T18:42:34Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    18,
                    42,
                    34,
                    5,
                    67,
                    0
                ],
                "title": "Optimization and Benchmarking of Monolithically Stackable Gain Cell\n  Memory for Last-Level Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization and Benchmarking of Monolithically Stackable Gain Cell\n  Memory for Last-Level Cache"
                },
                "summary": "The Last Level Cache (LLC) is the processor's critical bridge between on-chip\nand off-chip memory levels - optimized for high density, high bandwidth, and\nlow operation energy. To date, high-density (HD) SRAM has been the conventional\ndevice of choice; however, with the slowing of transistor scaling, as reflected\nin the industry's almost identical HD SRAM cell size from 5 nm to 3 nm,\nalternative solutions such as 3D stacking with advanced packaging like hybrid\nbonding are pursued (as demonstrated in AMD's V-cache). Escalating data demands\nnecessitate ultra-large on-chip caches to decrease costly off-chip memory\nmovement, pushing the exploration of device technology toward monolithic 3D\n(M3D) integration where transistors can be stacked in the back-end-of-line\n(BEOL) at the interconnect level. M3D integration requires fabrication\ntechniques compatible with a low thermal budget (<400 degC). Among promising\nBEOL device candidates are amorphous oxide semiconductor (AOS) transistors,\nparticularly desirable for their ultra-low leakage (<fA/um), enabling\npersistent data retention (>seconds) when used in a gain-cell configuration.\nThis paper examines device, circuit, and system-level tradeoffs when optimizing\nBEOL-compatible AOS-based 2-transistor gain cell (2T-GC) for LLC. A cache\nearly-exploration tool, NS-Cache, is developed to model caches in advanced 7\nand 3 nm nodes and is integrated with the Gem5 simulator to systematically\nbenchmark the impact of the newfound density/performance when compared to\nHD-SRAM, MRAM, and 1T1C eDRAM alternatives for LLC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Last Level Cache (LLC) is the processor's critical bridge between on-chip\nand off-chip memory levels - optimized for high density, high bandwidth, and\nlow operation energy. To date, high-density (HD) SRAM has been the conventional\ndevice of choice; however, with the slowing of transistor scaling, as reflected\nin the industry's almost identical HD SRAM cell size from 5 nm to 3 nm,\nalternative solutions such as 3D stacking with advanced packaging like hybrid\nbonding are pursued (as demonstrated in AMD's V-cache). Escalating data demands\nnecessitate ultra-large on-chip caches to decrease costly off-chip memory\nmovement, pushing the exploration of device technology toward monolithic 3D\n(M3D) integration where transistors can be stacked in the back-end-of-line\n(BEOL) at the interconnect level. M3D integration requires fabrication\ntechniques compatible with a low thermal budget (<400 degC). Among promising\nBEOL device candidates are amorphous oxide semiconductor (AOS) transistors,\nparticularly desirable for their ultra-low leakage (<fA/um), enabling\npersistent data retention (>seconds) when used in a gain-cell configuration.\nThis paper examines device, circuit, and system-level tradeoffs when optimizing\nBEOL-compatible AOS-based 2-transistor gain cell (2T-GC) for LLC. A cache\nearly-exploration tool, NS-Cache, is developed to model caches in advanced 7\nand 3 nm nodes and is integrated with the Gem5 simulator to systematically\nbenchmark the impact of the newfound density/performance when compared to\nHD-SRAM, MRAM, and 1T1C eDRAM alternatives for LLC."
                },
                "authors": [
                    {
                        "name": "Faaiq Waqar"
                    },
                    {
                        "name": "Jungyoun Kwak"
                    },
                    {
                        "name": "Junmo Lee"
                    },
                    {
                        "name": "Minji Shon"
                    },
                    {
                        "name": "Mohammadhosein Gholamrezaei"
                    },
                    {
                        "name": "Kevin Skadron"
                    },
                    {
                        "name": "Shimeng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Shimeng Yu"
                },
                "author": "Shimeng Yu",
                "arxiv_comment": "14 pages, 15 Figures, 6 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06304v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06304v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2; B.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07545v1",
                "updated": "2025-03-10T17:12:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    17,
                    12,
                    47,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T17:12:47Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    17,
                    12,
                    47,
                    0,
                    69,
                    0
                ],
                "title": "Queueing, Predictions, and LLMs: Challenges and Open Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Queueing, Predictions, and LLMs: Challenges and Open Problems"
                },
                "summary": "Queueing systems present many opportunities for applying machine-learning\npredictions, such as estimated service times, to improve system performance.\nThis integration raises numerous open questions about how predictions can be\neffectively leveraged to improve scheduling decisions. Recent studies explore\nqueues with predicted service times, typically aiming to minimize job time in\nthe system. We review these works, highlight the effectiveness of predictions,\nand present open questions on queue performance. We then move to consider an\nimportant practical example of using predictions in scheduling, namely Large\nLanguage Model (LLM) systems, which presents novel scheduling challenges and\nhighlights the potential for predictions to improve performance. In particular,\nwe consider LLMs performing inference. Inference requests (jobs) in LLM systems\nare inherently complex; they have variable inference times, dynamic memory\nfootprints that are constrained by key-value (KV) store memory limitations, and\nmultiple possible preemption approaches that affect performance differently. We\nprovide background on the important aspects of scheduling in LLM systems, and\nintroduce new models and open problems that arise from them. We argue that\nthere are significant opportunities for applying insights and analysis from\nqueueing theory to scheduling in LLM systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Queueing systems present many opportunities for applying machine-learning\npredictions, such as estimated service times, to improve system performance.\nThis integration raises numerous open questions about how predictions can be\neffectively leveraged to improve scheduling decisions. Recent studies explore\nqueues with predicted service times, typically aiming to minimize job time in\nthe system. We review these works, highlight the effectiveness of predictions,\nand present open questions on queue performance. We then move to consider an\nimportant practical example of using predictions in scheduling, namely Large\nLanguage Model (LLM) systems, which presents novel scheduling challenges and\nhighlights the potential for predictions to improve performance. In particular,\nwe consider LLMs performing inference. Inference requests (jobs) in LLM systems\nare inherently complex; they have variable inference times, dynamic memory\nfootprints that are constrained by key-value (KV) store memory limitations, and\nmultiple possible preemption approaches that affect performance differently. We\nprovide background on the important aspects of scheduling in LLM systems, and\nintroduce new models and open problems that arise from them. We argue that\nthere are significant opportunities for applying insights and analysis from\nqueueing theory to scheduling in LLM systems."
                },
                "authors": [
                    {
                        "name": "Michael Mitzenmacher"
                    },
                    {
                        "name": "Rana Shahout"
                    }
                ],
                "author_detail": {
                    "name": "Rana Shahout"
                },
                "author": "Rana Shahout",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07518v1",
                "updated": "2025-03-10T16:41:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    16,
                    41,
                    14,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T16:41:14Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    16,
                    41,
                    14,
                    0,
                    69,
                    0
                ],
                "title": "TokenButler: Token Importance is Predictable",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenButler: Token Importance is Predictable"
                },
                "summary": "Large Language Models (LLMs) rely on the Key-Value (KV) Cache to store token\nhistory, enabling efficient decoding of tokens. As the KV-Cache grows, it\nbecomes a major memory and computation bottleneck, however, there is an\nopportunity to alleviate this bottleneck, especially because prior research has\nshown that only a small subset of tokens contribute meaningfully to each\ndecoding step. A key challenge in finding these critical tokens is that they\nare dynamic, and heavily input query-dependent. Existing methods either risk\nquality by evicting tokens permanently, or retain the full KV-Cache but rely on\nretrieving chunks (pages) of tokens at generation, failing at dense,\ncontext-rich tasks. Additionally, many existing KV-Cache sparsity methods rely\non inaccurate proxies for token importance. To address these limitations, we\nintroduce TokenButler, a high-granularity, query-aware predictor that learns to\nidentify these critical tokens. By training a light-weight predictor with less\nthan 1.2% parameter overhead, TokenButler prioritizes tokens based on their\ncontextual, predicted importance. This improves perplexity & downstream\naccuracy by over 8% relative to SoTA methods for estimating token importance.\nWe evaluate TokenButler on a novel synthetic small-context co-referential\nretrieval task, demonstrating near-oracle accuracy. Code, models and\nbenchmarks: https://github.com/abdelfattah-lab/TokenButler",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) rely on the Key-Value (KV) Cache to store token\nhistory, enabling efficient decoding of tokens. As the KV-Cache grows, it\nbecomes a major memory and computation bottleneck, however, there is an\nopportunity to alleviate this bottleneck, especially because prior research has\nshown that only a small subset of tokens contribute meaningfully to each\ndecoding step. A key challenge in finding these critical tokens is that they\nare dynamic, and heavily input query-dependent. Existing methods either risk\nquality by evicting tokens permanently, or retain the full KV-Cache but rely on\nretrieving chunks (pages) of tokens at generation, failing at dense,\ncontext-rich tasks. Additionally, many existing KV-Cache sparsity methods rely\non inaccurate proxies for token importance. To address these limitations, we\nintroduce TokenButler, a high-granularity, query-aware predictor that learns to\nidentify these critical tokens. By training a light-weight predictor with less\nthan 1.2% parameter overhead, TokenButler prioritizes tokens based on their\ncontextual, predicted importance. This improves perplexity & downstream\naccuracy by over 8% relative to SoTA methods for estimating token importance.\nWe evaluate TokenButler on a novel synthetic small-context co-referential\nretrieval task, demonstrating near-oracle accuracy. Code, models and\nbenchmarks: https://github.com/abdelfattah-lab/TokenButler"
                },
                "authors": [
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Ahmed F AbouElhamayed"
                    },
                    {
                        "name": "Yifei Gao"
                    },
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Nilesh Jain"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07474v1",
                "updated": "2025-03-10T15:49:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    49,
                    20,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T15:49:20Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    49,
                    20,
                    0,
                    69,
                    0
                ],
                "title": "Revealing Rotational Symmetry Breaking Charge-density Wave Order in\n  Kagome Superconductor (Rb, K)V$_3$Sb$_5$ by Ultrafast Pump-probe Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing Rotational Symmetry Breaking Charge-density Wave Order in\n  Kagome Superconductor (Rb, K)V$_3$Sb$_5$ by Ultrafast Pump-probe Experiments"
                },
                "summary": "The recently discovered Kagome superconductor AV$_3$Sb$_5$ (where A refers to\nK, Rb, Cs) has stimulated widespread research interest due to its interplay of\nnon-trivial topology and unconventional correlated physics including\ncharge-density waves (CDW) and superconductivity. The essential prerequisite to\nunderstanding the microscopic mechanisms of this complex electronic landscape\nis to unveil the configuration and symmetry of the charge-density wave order.\nAs to now, little consensus has been made on what symmetry is broken. Herein,\nwe clarify the microscopic structure and symmetry breaking of the CDW phase in\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$ by ultrafast time-resolved reflectivity. Our\napproach is based on extracting coherent phonon spectra induced by\nthree-dimensional CDW and comparing them to calculated phonon frequencies via\ndensity-functional theory. The combination of these experimental results and\ncalculations provides compelling evidence that the CDW structure of both\ncompounds prevailing up to T$_{\\text{CDW}}$ is the 2 $\\times$ 2 $\\times$ 2\nstaggered inverse Star-of-David pattern with interlayer $\\pi$ phase shift, in\nwhich the six-fold rotational symmetry is broken. These observations thus\ncorroborate six-fold rotational symmetry breaking throughout the CDW phase of\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recently discovered Kagome superconductor AV$_3$Sb$_5$ (where A refers to\nK, Rb, Cs) has stimulated widespread research interest due to its interplay of\nnon-trivial topology and unconventional correlated physics including\ncharge-density waves (CDW) and superconductivity. The essential prerequisite to\nunderstanding the microscopic mechanisms of this complex electronic landscape\nis to unveil the configuration and symmetry of the charge-density wave order.\nAs to now, little consensus has been made on what symmetry is broken. Herein,\nwe clarify the microscopic structure and symmetry breaking of the CDW phase in\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$ by ultrafast time-resolved reflectivity. Our\napproach is based on extracting coherent phonon spectra induced by\nthree-dimensional CDW and comparing them to calculated phonon frequencies via\ndensity-functional theory. The combination of these experimental results and\ncalculations provides compelling evidence that the CDW structure of both\ncompounds prevailing up to T$_{\\text{CDW}}$ is the 2 $\\times$ 2 $\\times$ 2\nstaggered inverse Star-of-David pattern with interlayer $\\pi$ phase shift, in\nwhich the six-fold rotational symmetry is broken. These observations thus\ncorroborate six-fold rotational symmetry breaking throughout the CDW phase of\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$."
                },
                "authors": [
                    {
                        "name": "Qinwen Deng"
                    },
                    {
                        "name": "Hengxin Tan"
                    },
                    {
                        "name": "Brenden R. Ortiz"
                    },
                    {
                        "name": "Stephen D. Wilson"
                    },
                    {
                        "name": "Binghai Yan"
                    },
                    {
                        "name": "Liang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wu"
                },
                "author": "Liang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10167v2",
                "updated": "2025-03-10T12:10:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    10,
                    30,
                    0,
                    69,
                    0
                ],
                "published": "2025-02-14T13:55:01Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    55,
                    1,
                    4,
                    45,
                    0
                ],
                "title": "Modeling and Simulating Emerging Memory Technologies: A Tutorial",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Simulating Emerging Memory Technologies: A Tutorial"
                },
                "summary": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Tristan Seidl"
                    },
                    {
                        "name": "Nils Hölscher"
                    },
                    {
                        "name": "Christian Hakert"
                    },
                    {
                        "name": "Minh Duy Truong"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "João Paulo C. de Lima"
                    },
                    {
                        "name": "Asif Ali Khan"
                    },
                    {
                        "name": "Jeronimo Castrillon"
                    },
                    {
                        "name": "Ali Nezhadi"
                    },
                    {
                        "name": "Lokesh Siddhu"
                    },
                    {
                        "name": "Hassan Nassar"
                    },
                    {
                        "name": "Mahta Mayahinia"
                    },
                    {
                        "name": "Mehdi Baradaran Tahoori"
                    },
                    {
                        "name": "Jörg Henkel"
                    },
                    {
                        "name": "Nils Wilbert"
                    },
                    {
                        "name": "Stefan Wildermann"
                    },
                    {
                        "name": "Jürgen Teich"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Teich"
                },
                "author": "Jürgen Teich",
                "arxiv_comment": "DFG Priority Program 2377 - Disruptive Memory Technologies",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07120v1",
                "updated": "2025-03-10T09:49:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    49,
                    18,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T09:49:18Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    49,
                    18,
                    0,
                    69,
                    0
                ],
                "title": "Exposure Bias Reduction for Enhancing Diffusion Transformer Feature\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposure Bias Reduction for Enhancing Diffusion Transformer Feature\n  Caching"
                },
                "summary": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis problem, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing the impact of caching on the generation of intermediate processes. So\nthe lack of exploration provides us with room for analysis and improvement. In\nthis paper, we analyze the impact of caching on the SNR of the diffusion\nprocess and discern that feature caching intensifies the denoising procedure,\nand we further identify this as a more severe exposure bias issue. Drawing on\nthis insight, we introduce EB-Cache, a joint cache strategy that aligns the\nNon-exposure bias (which gives us a higher performance ceiling) diffusion\nprocess. Our approach incorporates a comprehensive understanding of caching\nmechanisms and offers a novel perspective on leveraging caches to expedite\ndiffusion processes. Empirical results indicate that EB-Cache optimizes model\nperformance while concurrently facilitating acceleration. Specifically, in the\n50-step generation process, EB-Cache achieves 1.49$\\times$ acceleration with\n0.63 FID reduction from 3.69, surpassing prior acceleration methods. Code will\nbe available at\n\\href{https://github.com/aSleepyTree/EB-Cache}{https://github.com/aSleepyTree/EB-Cache}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis problem, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing the impact of caching on the generation of intermediate processes. So\nthe lack of exploration provides us with room for analysis and improvement. In\nthis paper, we analyze the impact of caching on the SNR of the diffusion\nprocess and discern that feature caching intensifies the denoising procedure,\nand we further identify this as a more severe exposure bias issue. Drawing on\nthis insight, we introduce EB-Cache, a joint cache strategy that aligns the\nNon-exposure bias (which gives us a higher performance ceiling) diffusion\nprocess. Our approach incorporates a comprehensive understanding of caching\nmechanisms and offers a novel perspective on leveraging caches to expedite\ndiffusion processes. Empirical results indicate that EB-Cache optimizes model\nperformance while concurrently facilitating acceleration. Specifically, in the\n50-step generation process, EB-Cache achieves 1.49$\\times$ acceleration with\n0.63 FID reduction from 3.69, surpassing prior acceleration methods. Code will\nbe available at\n\\href{https://github.com/aSleepyTree/EB-Cache}{https://github.com/aSleepyTree/EB-Cache}."
                },
                "authors": [
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Hu Yu"
                    },
                    {
                        "name": "Jie Xiao"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07027v1",
                "updated": "2025-03-10T08:07:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    7,
                    17,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T08:07:17Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    7,
                    17,
                    0,
                    69,
                    0
                ],
                "title": "EasyControl: Adding Efficient and Flexible Control for Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EasyControl: Adding Efficient and Flexible Control for Diffusion\n  Transformer"
                },
                "summary": "Recent advancements in Unet-based diffusion models, such as ControlNet and\nIP-Adapter, have introduced effective spatial and subject control mechanisms.\nHowever, the DiT (Diffusion Transformer) architecture still struggles with\nefficient and flexible control. To tackle this issue, we propose EasyControl, a\nnovel framework designed to unify condition-guided diffusion transformers with\nhigh efficiency and flexibility. Our framework is built on three key\ninnovations. First, we introduce a lightweight Condition Injection LoRA Module.\nThis module processes conditional signals in isolation, acting as a\nplug-and-play solution. It avoids modifying the base model weights, ensuring\ncompatibility with customized models and enabling the flexible injection of\ndiverse conditions. Notably, this module also supports harmonious and robust\nzero-shot multi-condition generalization, even when trained only on\nsingle-condition data. Second, we propose a Position-Aware Training Paradigm.\nThis approach standardizes input conditions to fixed resolutions, allowing the\ngeneration of images with arbitrary aspect ratios and flexible resolutions. At\nthe same time, it optimizes computational efficiency, making the framework more\npractical for real-world applications. Third, we develop a Causal Attention\nMechanism combined with the KV Cache technique, adapted for conditional\ngeneration tasks. This innovation significantly reduces the latency of image\nsynthesis, improving the overall efficiency of the framework. Through extensive\nexperiments, we demonstrate that EasyControl achieves exceptional performance\nacross various application scenarios. These innovations collectively make our\nframework highly efficient, flexible, and suitable for a wide range of tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Unet-based diffusion models, such as ControlNet and\nIP-Adapter, have introduced effective spatial and subject control mechanisms.\nHowever, the DiT (Diffusion Transformer) architecture still struggles with\nefficient and flexible control. To tackle this issue, we propose EasyControl, a\nnovel framework designed to unify condition-guided diffusion transformers with\nhigh efficiency and flexibility. Our framework is built on three key\ninnovations. First, we introduce a lightweight Condition Injection LoRA Module.\nThis module processes conditional signals in isolation, acting as a\nplug-and-play solution. It avoids modifying the base model weights, ensuring\ncompatibility with customized models and enabling the flexible injection of\ndiverse conditions. Notably, this module also supports harmonious and robust\nzero-shot multi-condition generalization, even when trained only on\nsingle-condition data. Second, we propose a Position-Aware Training Paradigm.\nThis approach standardizes input conditions to fixed resolutions, allowing the\ngeneration of images with arbitrary aspect ratios and flexible resolutions. At\nthe same time, it optimizes computational efficiency, making the framework more\npractical for real-world applications. Third, we develop a Causal Attention\nMechanism combined with the KV Cache technique, adapted for conditional\ngeneration tasks. This innovation significantly reduces the latency of image\nsynthesis, improving the overall efficiency of the framework. Through extensive\nexperiments, we demonstrate that EasyControl achieves exceptional performance\nacross various application scenarios. These innovations collectively make our\nframework highly efficient, flexible, and suitable for a wide range of tasks."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhang"
                    },
                    {
                        "name": "Yirui Yuan"
                    },
                    {
                        "name": "Yiren Song"
                    },
                    {
                        "name": "Haofan Wang"
                    },
                    {
                        "name": "Jiaming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiaming Liu"
                },
                "author": "Jiaming Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06923v1",
                "updated": "2025-03-10T05:09:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    5,
                    9,
                    42,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T05:09:42Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    5,
                    9,
                    42,
                    0,
                    69,
                    0
                ],
                "title": "From Reusing to Forecasting: Accelerating Diffusion Models with\n  TaylorSeers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Reusing to Forecasting: Accelerating Diffusion Models with\n  TaylorSeers"
                },
                "summary": "Diffusion Transformers (DiT) have revolutionized high-fidelity image and\nvideo synthesis, yet their computational demands remain prohibitive for\nreal-time applications. To solve this problem, feature caching has been\nproposed to accelerate diffusion models by caching the features in the previous\ntimesteps and then reusing them in the following timesteps. However, at\ntimesteps with significant intervals, the feature similarity in diffusion\nmodels decreases substantially, leading to a pronounced increase in errors\nintroduced by feature caching, significantly harming the generation quality. To\nsolve this problem, we propose TaylorSeer, which firstly shows that features of\ndiffusion models at future timesteps can be predicted based on their values at\nprevious timesteps. Based on the fact that features change slowly and\ncontinuously across timesteps, TaylorSeer employs a differential method to\napproximate the higher-order derivatives of features and predict features in\nfuture timesteps with Taylor series expansion. Extensive experiments\ndemonstrate its significant effectiveness in both image and video synthesis,\nespecially in high acceleration ratios. For instance, it achieves an almost\nlossless acceleration of 4.99$\\times$ on FLUX and 5.00$\\times$ on HunyuanVideo\nwithout additional training. On DiT, it achieves $3.41$ lower FID compared with\nprevious SOTA at $4.53$$\\times$ acceleration. %Our code is provided in the\nsupplementary materials and will be made publicly available on GitHub. Our\ncodes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have revolutionized high-fidelity image and\nvideo synthesis, yet their computational demands remain prohibitive for\nreal-time applications. To solve this problem, feature caching has been\nproposed to accelerate diffusion models by caching the features in the previous\ntimesteps and then reusing them in the following timesteps. However, at\ntimesteps with significant intervals, the feature similarity in diffusion\nmodels decreases substantially, leading to a pronounced increase in errors\nintroduced by feature caching, significantly harming the generation quality. To\nsolve this problem, we propose TaylorSeer, which firstly shows that features of\ndiffusion models at future timesteps can be predicted based on their values at\nprevious timesteps. Based on the fact that features change slowly and\ncontinuously across timesteps, TaylorSeer employs a differential method to\napproximate the higher-order derivatives of features and predict features in\nfuture timesteps with Taylor series expansion. Extensive experiments\ndemonstrate its significant effectiveness in both image and video synthesis,\nespecially in high acceleration ratios. For instance, it achieves an almost\nlossless acceleration of 4.99$\\times$ on FLUX and 5.00$\\times$ on HunyuanVideo\nwithout additional training. On DiT, it achieves $3.41$ lower FID compared with\nprevious SOTA at $4.53$$\\times$ acceleration. %Our code is provided in the\nsupplementary materials and will be made publicly available on GitHub. Our\ncodes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer"
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yuanhuiyi Lyu"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "13 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05116v2",
                "updated": "2025-03-10T02:41:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    2,
                    41,
                    21,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-07T03:27:33Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    3,
                    27,
                    33,
                    4,
                    66,
                    0
                ],
                "title": "Piccolo: Large-Scale Graph Processing with Fine-Grained In-Memory\n  Scatter-Gather",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Piccolo: Large-Scale Graph Processing with Fine-Grained In-Memory\n  Scatter-Gather"
                },
                "summary": "Graph processing requires irregular, fine-grained random access patterns\nincompatible with contemporary off-chip memory architecture, leading to\ninefficient data access. This inefficiency makes graph processing an extremely\nmemory-bound application. Because of this, existing graph processing\naccelerators typically employ a graph tiling-based or processing-in-memory\n(PIM) approach to relieve the memory bottleneck. In the tiling-based approach,\na graph is split into chunks that fit within the on-chip cache to maximize data\nreuse. In the PIM approach, arithmetic units are placed within memory to\nperform operations such as reduction or atomic addition. However, both\napproaches have several limitations, especially when implemented on current\nmemory standards (i.e., DDR). Because the access granularity provided by DDR is\nmuch larger than that of the graph vertex property data, much of the bandwidth\nand cache capacity are wasted. PIM is meant to alleviate such issues, but it is\ndifficult to use in conjunction with the tiling-based approach, resulting in a\nsignificant disadvantage. Furthermore, placing arithmetic units inside a memory\nchip is expensive, thereby supporting multiple types of operation is thought to\nbe impractical. To address the above limitations, we present Piccolo, an\nend-to-end efficient graph processing accelerator with fine-grained in-memory\nrandom scatter-gather. Instead of placing expensive arithmetic units in\noff-chip memory, Piccolo focuses on reducing the off-chip traffic with\nnon-arithmetic function-in-memory of random scatter-gather. To fully benefit\nfrom in-memory scatter-gather, Piccolo redesigns the cache and MHA of the\naccelerator such that it can enjoy both the advantage of tiling and in-memory\noperations. Piccolo achieves a maximum speedup of 3.28$\\times$ and a geometric\nmean speedup of 1.62$\\times$ across various and extensive benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph processing requires irregular, fine-grained random access patterns\nincompatible with contemporary off-chip memory architecture, leading to\ninefficient data access. This inefficiency makes graph processing an extremely\nmemory-bound application. Because of this, existing graph processing\naccelerators typically employ a graph tiling-based or processing-in-memory\n(PIM) approach to relieve the memory bottleneck. In the tiling-based approach,\na graph is split into chunks that fit within the on-chip cache to maximize data\nreuse. In the PIM approach, arithmetic units are placed within memory to\nperform operations such as reduction or atomic addition. However, both\napproaches have several limitations, especially when implemented on current\nmemory standards (i.e., DDR). Because the access granularity provided by DDR is\nmuch larger than that of the graph vertex property data, much of the bandwidth\nand cache capacity are wasted. PIM is meant to alleviate such issues, but it is\ndifficult to use in conjunction with the tiling-based approach, resulting in a\nsignificant disadvantage. Furthermore, placing arithmetic units inside a memory\nchip is expensive, thereby supporting multiple types of operation is thought to\nbe impractical. To address the above limitations, we present Piccolo, an\nend-to-end efficient graph processing accelerator with fine-grained in-memory\nrandom scatter-gather. Instead of placing expensive arithmetic units in\noff-chip memory, Piccolo focuses on reducing the off-chip traffic with\nnon-arithmetic function-in-memory of random scatter-gather. To fully benefit\nfrom in-memory scatter-gather, Piccolo redesigns the cache and MHA of the\naccelerator such that it can enjoy both the advantage of tiling and in-memory\noperations. Piccolo achieves a maximum speedup of 3.28$\\times$ and a geometric\nmean speedup of 1.62$\\times$ across various and extensive benchmarks."
                },
                "authors": [
                    {
                        "name": "Changmin Shin"
                    },
                    {
                        "name": "Jaeyong Song"
                    },
                    {
                        "name": "Hongsun Jang"
                    },
                    {
                        "name": "Dogeun Kim"
                    },
                    {
                        "name": "Jun Sung"
                    },
                    {
                        "name": "Taehee Kwon"
                    },
                    {
                        "name": "Jae Hyung Ju"
                    },
                    {
                        "name": "Frank Liu"
                    },
                    {
                        "name": "Yeonkyu Choi"
                    },
                    {
                        "name": "Jinho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jinho Lee"
                },
                "author": "Jinho Lee",
                "arxiv_comment": "HPCA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v3",
                "updated": "2025-03-09T17:43:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    17,
                    43,
                    28,
                    6,
                    68,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11706v2",
                "updated": "2025-03-09T16:14:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    16,
                    14,
                    51,
                    6,
                    68,
                    0
                ],
                "published": "2024-12-16T12:28:22Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "title": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration"
                },
                "summary": "Diffusion Transformers (DiTs) have proven effective in generating\nhigh-quality videos but are hindered by high computational costs. Existing\nvideo DiT sampling acceleration methods often rely on costly fine-tuning or\nexhibit limited generalization capabilities. We propose Asymmetric Reduction\nand Restoration (AsymRnR), a training-free and model-agnostic method to\naccelerate video DiTs. It builds on the observation that redundancies of\nfeature tokens in DiTs vary significantly across different model blocks,\ndenoising steps, and feature types. Our AsymRnR asymmetrically reduces\nredundant tokens in the attention operation, achieving acceleration with\nnegligible degradation in output quality and, in some cases, even improving it.\nWe also tailored a reduction schedule to distribute the reduction across\ncomponents adaptively. To further accelerate this process, we introduce a\nmatching cache for more efficient reduction. Backed by theoretical foundations\nand extensive experimental validation, AsymRnR integrates into state-of-the-art\nvideo DiTs and offers substantial speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have proven effective in generating\nhigh-quality videos but are hindered by high computational costs. Existing\nvideo DiT sampling acceleration methods often rely on costly fine-tuning or\nexhibit limited generalization capabilities. We propose Asymmetric Reduction\nand Restoration (AsymRnR), a training-free and model-agnostic method to\naccelerate video DiTs. It builds on the observation that redundancies of\nfeature tokens in DiTs vary significantly across different model blocks,\ndenoising steps, and feature types. Our AsymRnR asymmetrically reduces\nredundant tokens in the attention operation, achieving acceleration with\nnegligible degradation in output quality and, in some cases, even improving it.\nWe also tailored a reduction schedule to distribute the reduction across\ncomponents adaptively. To further accelerate this process, we introduce a\nmatching cache for more efficient reduction. Backed by theoretical foundations\nand extensive experimental validation, AsymRnR integrates into state-of-the-art\nvideo DiTs and offers substantial speedup."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "16 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06594v1",
                "updated": "2025-03-09T12:54:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    12,
                    54,
                    5,
                    6,
                    68,
                    0
                ],
                "published": "2025-03-09T12:54:05Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    12,
                    54,
                    5,
                    6,
                    68,
                    0
                ],
                "title": "Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation"
                },
                "summary": "The field of neural machine translation (NMT) has changed with the advent of\nlarge language models (LLMs). Much of the recent emphasis in natural language\nprocessing (NLP) has been on modeling machine translation and many other\nproblems using a single pre-trained Transformer decoder, while encoder-decoder\narchitectures, which were the standard in earlier NMT models, have received\nrelatively less attention. In this paper, we explore translation models that\nare universal, efficient, and easy to optimize, by marrying the world of LLMs\nwith the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder\nunchanged. We also develop methods for adapting LLMs to work better with the\nNMT decoder. Furthermore, we construct a new dataset involving multiple tasks\nto assess how well the machine translation system generalizes across various\ntasks. Evaluations on the WMT and our datasets show that results using our\nmethod match or surpass a range of baselines in terms of translation quality,\nbut achieve $2.4 \\sim 6.5 \\times$ inference speedups and a $75\\%$ reduction in\nthe memory footprint of the KV cache. It also demonstrates strong\ngeneralization across a variety of translation-related tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of neural machine translation (NMT) has changed with the advent of\nlarge language models (LLMs). Much of the recent emphasis in natural language\nprocessing (NLP) has been on modeling machine translation and many other\nproblems using a single pre-trained Transformer decoder, while encoder-decoder\narchitectures, which were the standard in earlier NMT models, have received\nrelatively less attention. In this paper, we explore translation models that\nare universal, efficient, and easy to optimize, by marrying the world of LLMs\nwith the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder\nunchanged. We also develop methods for adapting LLMs to work better with the\nNMT decoder. Furthermore, we construct a new dataset involving multiple tasks\nto assess how well the machine translation system generalizes across various\ntasks. Evaluations on the WMT and our datasets show that results using our\nmethod match or surpass a range of baselines in terms of translation quality,\nbut achieve $2.4 \\sim 6.5 \\times$ inference speedups and a $75\\%$ reduction in\nthe memory footprint of the KV cache. It also demonstrates strong\ngeneralization across a variety of translation-related tasks."
                },
                "authors": [
                    {
                        "name": "Yingfeng Luo"
                    },
                    {
                        "name": "Tong Zheng"
                    },
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Bei Li"
                    },
                    {
                        "name": "Qinghong Zhang"
                    },
                    {
                        "name": "Yongqi Gao"
                    },
                    {
                        "name": "Ziqiang Xu"
                    },
                    {
                        "name": "Peinan Feng"
                    },
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06545v1",
                "updated": "2025-03-09T10:31:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    10,
                    31,
                    51,
                    6,
                    68,
                    0
                ],
                "published": "2025-03-09T10:31:51Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    10,
                    31,
                    51,
                    6,
                    68,
                    0
                ],
                "title": "QuantCache: Adaptive Importance-Guided Quantization with Hierarchical\n  Latent and Layer Caching for Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuantCache: Adaptive Importance-Guided Quantization with Hierarchical\n  Latent and Layer Caching for Video Generation"
                },
                "summary": "Recently, Diffusion Transformers (DiTs) have emerged as a dominant\narchitecture in video generation, surpassing U-Net-based models in terms of\nperformance. However, the enhanced capabilities of DiTs come with significant\ndrawbacks, including increased computational and memory costs, which hinder\ntheir deployment on resource-constrained devices. Current acceleration\ntechniques, such as quantization and cache mechanism, offer limited speedup and\nare often applied in isolation, failing to fully address the complexities of\nDiT architectures. In this paper, we propose QuantCache, a novel training-free\ninference acceleration framework that jointly optimizes hierarchical latent\ncaching, adaptive importance-guided quantization, and structural\nredundancy-aware pruning. QuantCache achieves an end-to-end latency speedup of\n6.72$\\times$ on Open-Sora with minimal loss in generation quality. Extensive\nexperiments across multiple video generation benchmarks demonstrate the\neffectiveness of our method, setting a new standard for efficient DiT\ninference. The code and models will be available at\nhttps://github.com/JunyiWuCode/QuantCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Diffusion Transformers (DiTs) have emerged as a dominant\narchitecture in video generation, surpassing U-Net-based models in terms of\nperformance. However, the enhanced capabilities of DiTs come with significant\ndrawbacks, including increased computational and memory costs, which hinder\ntheir deployment on resource-constrained devices. Current acceleration\ntechniques, such as quantization and cache mechanism, offer limited speedup and\nare often applied in isolation, failing to fully address the complexities of\nDiT architectures. In this paper, we propose QuantCache, a novel training-free\ninference acceleration framework that jointly optimizes hierarchical latent\ncaching, adaptive importance-guided quantization, and structural\nredundancy-aware pruning. QuantCache achieves an end-to-end latency speedup of\n6.72$\\times$ on Open-Sora with minimal loss in generation quality. Extensive\nexperiments across multiple video generation benchmarks demonstrate the\neffectiveness of our method, setting a new standard for efficient DiT\ninference. The code and models will be available at\nhttps://github.com/JunyiWuCode/QuantCache."
                },
                "authors": [
                    {
                        "name": "Junyi Wu"
                    },
                    {
                        "name": "Zhiteng Li"
                    },
                    {
                        "name": "Zheng Hui"
                    },
                    {
                        "name": "Yulun Zhang"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Xiaokang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokang Yang"
                },
                "author": "Xiaokang Yang",
                "arxiv_comment": "The code and models will be available at\n  https://github.com/JunyiWuCode/QuantCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06433v1",
                "updated": "2025-03-09T04:14:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    4,
                    14,
                    6,
                    6,
                    68,
                    0
                ],
                "published": "2025-03-09T04:14:06Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    4,
                    14,
                    6,
                    6,
                    68,
                    0
                ],
                "title": "Seesaw: High-throughput LLM Inference via Model Re-sharding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seesaw: High-throughput LLM Inference via Model Re-sharding"
                },
                "summary": "To improve the efficiency of distributed large language model (LLM)\ninference, various parallelization strategies, such as tensor and pipeline\nparallelism, have been proposed. However, the distinct computational\ncharacteristics inherent in the two stages of LLM inference-prefilling and\ndecoding-render a single static parallelization strategy insufficient for the\neffective optimization of both stages. In this work, we present Seesaw, an LLM\ninference engine optimized for throughput-oriented tasks. The key idea behind\nSeesaw is dynamic model re-sharding, a technique that facilitates the dynamic\nreconfiguration of parallelization strategies across stages, thereby maximizing\nthroughput at both phases. To mitigate re-sharding overhead and optimize\ncomputational efficiency, we employ tiered KV cache buffering and\ntransition-minimizing scheduling. These approaches work synergistically to\nreduce the overhead caused by frequent stage transitions while ensuring maximum\nbatching efficiency. Our evaluation demonstrates that Seesaw achieves a\nthroughput increase of up to 1.78x (1.36x on average) compared to vLLM, the\nmost widely used state-of-the-art LLM inference engine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To improve the efficiency of distributed large language model (LLM)\ninference, various parallelization strategies, such as tensor and pipeline\nparallelism, have been proposed. However, the distinct computational\ncharacteristics inherent in the two stages of LLM inference-prefilling and\ndecoding-render a single static parallelization strategy insufficient for the\neffective optimization of both stages. In this work, we present Seesaw, an LLM\ninference engine optimized for throughput-oriented tasks. The key idea behind\nSeesaw is dynamic model re-sharding, a technique that facilitates the dynamic\nreconfiguration of parallelization strategies across stages, thereby maximizing\nthroughput at both phases. To mitigate re-sharding overhead and optimize\ncomputational efficiency, we employ tiered KV cache buffering and\ntransition-minimizing scheduling. These approaches work synergistically to\nreduce the overhead caused by frequent stage transitions while ensuring maximum\nbatching efficiency. Our evaluation demonstrates that Seesaw achieves a\nthroughput increase of up to 1.78x (1.36x on average) compared to vLLM, the\nmost widely used state-of-the-art LLM inference engine."
                },
                "authors": [
                    {
                        "name": "Qidong Su"
                    },
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Muralidhar Andoorveedu"
                    },
                    {
                        "name": "Chenhao Jiang"
                    },
                    {
                        "name": "Zhanda Zhu"
                    },
                    {
                        "name": "Kevin Song"
                    },
                    {
                        "name": "Christina Giannoula"
                    },
                    {
                        "name": "Gennady Pekhimenko"
                    }
                ],
                "author_detail": {
                    "name": "Gennady Pekhimenko"
                },
                "author": "Gennady Pekhimenko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00776v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00776v3",
                "updated": "2025-03-09T02:19:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    2,
                    19,
                    22,
                    6,
                    68,
                    0
                ],
                "published": "2024-12-01T11:43:46Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    11,
                    43,
                    46,
                    6,
                    336,
                    0
                ],
                "title": "Learning Mamba as a Continual Learner: Meta-learning Selective State\n  Space Models for Efficient Continual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Mamba as a Continual Learner: Meta-learning Selective State\n  Space Models for Efficient Continual Learning"
                },
                "summary": "Continual learning (CL) aims to efficiently learn from a non-stationary data\nstream, without storing or recomputing all seen samples. CL enables prediction\non new tasks by incorporating sequential training samples. Building on this\nconnection between CL and sequential modeling, meta-continual learning (MCL)\naims to meta-learn an efficient continual learner as a sequence prediction\nmodel, with advanced sequence models like Transformers being natural choices.\nHowever, despite decent performance, Transformers rely on a linearly growing\ncache to store all past representations, conflicting with CL's objective of not\nstoring all seen samples and limiting efficiency. In this paper, we focus on\nmeta-learning sequence-prediction-based continual learners without retaining\nall past representations. While attention-free models with fixed-size hidden\nstates (e.g., Linear Transformers) align with CL's essential goal and\nefficiency needs, they have shown limited effectiveness in MCL in previous\nliterature. Given Mamba's strong sequence modeling performance and\nattention-free nature, we explore a key question: Can attention-free models\nlike Mamba perform well on MCL? By formulating Mamba and the SSM for MCL tasks,\nwe propose MambaCL, a meta-learned continual learner. To enhance MambaCL's\ntraining, we introduce selectivity regularization, leveraging the connection\nbetween Mamba and Transformers to guide its behavior over sequences.\nFurthermore, we study how Mamba and other models perform across various MCL\nscenarios through extensive and well-designed experiments. Our results\nhighlight the promising performance and strong generalization of Mamba and\nattention-free models in MCL, demonstrating its potential for efficient\ncontinual learning and adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning (CL) aims to efficiently learn from a non-stationary data\nstream, without storing or recomputing all seen samples. CL enables prediction\non new tasks by incorporating sequential training samples. Building on this\nconnection between CL and sequential modeling, meta-continual learning (MCL)\naims to meta-learn an efficient continual learner as a sequence prediction\nmodel, with advanced sequence models like Transformers being natural choices.\nHowever, despite decent performance, Transformers rely on a linearly growing\ncache to store all past representations, conflicting with CL's objective of not\nstoring all seen samples and limiting efficiency. In this paper, we focus on\nmeta-learning sequence-prediction-based continual learners without retaining\nall past representations. While attention-free models with fixed-size hidden\nstates (e.g., Linear Transformers) align with CL's essential goal and\nefficiency needs, they have shown limited effectiveness in MCL in previous\nliterature. Given Mamba's strong sequence modeling performance and\nattention-free nature, we explore a key question: Can attention-free models\nlike Mamba perform well on MCL? By formulating Mamba and the SSM for MCL tasks,\nwe propose MambaCL, a meta-learned continual learner. To enhance MambaCL's\ntraining, we introduce selectivity regularization, leveraging the connection\nbetween Mamba and Transformers to guide its behavior over sequences.\nFurthermore, we study how Mamba and other models perform across various MCL\nscenarios through extensive and well-designed experiments. Our results\nhighlight the promising performance and strong generalization of Mamba and\nattention-free models in MCL, demonstrating its potential for efficient\ncontinual learning and adaptation."
                },
                "authors": [
                    {
                        "name": "Chongyang Zhao"
                    },
                    {
                        "name": "Dong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Dong Gong"
                },
                "author": "Dong Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00776v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00776v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03227v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03227v2",
                "updated": "2025-03-08T21:55:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    21,
                    55,
                    15,
                    5,
                    67,
                    0
                ],
                "published": "2024-04-04T06:24:11Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    6,
                    24,
                    11,
                    3,
                    95,
                    0
                ],
                "title": "Decentralized Learning Strategies for Estimation Error Minimization with\n  Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Learning Strategies for Estimation Error Minimization with\n  Graph Neural Networks"
                },
                "summary": "We address the challenge of sampling and remote estimation for autoregressive\nMarkovian processes in a multi-hop wireless network with\nstatistically-identical agents. Agents cache the most recent samples from\nothers and communicate over wireless collision channels governed by an\nunderlying graph topology. Our goal is to minimize time-average estimation\nerror and/or age of information with decentralized scalable sampling and\ntransmission policies, considering both oblivious (where decision-making is\nindependent of the physical processes) and non-oblivious policies (where\ndecision-making depends on physical processes). We prove that in oblivious\npolicies, minimizing estimation error is equivalent to minimizing the age of\ninformation. The complexity of the problem, especially the multi-dimensional\naction spaces and arbitrary network topologies, makes theoretical methods for\nfinding optimal transmission policies intractable. We optimize the policies\nusing a graphical multi-agent reinforcement learning framework, where each\nagent employs a permutation-equivariant graph neural network architecture.\nTheoretically, we prove that our proposed framework exhibits desirable\ntransferability properties, allowing transmission policies trained on small- or\nmoderate-size networks to be executed effectively on large-scale topologies.\nNumerical experiments demonstrate that (i) Our proposed framework outperforms\nstate-of-the-art baselines; (ii) The trained policies are transferable to\nlarger networks, and their performance gains increase with the number of\nagents; (iii) The training procedure withstands non-stationarity even if we\nutilize independent learning techniques; and, (iv) Recurrence is pivotal in\nboth independent learning and centralized training and decentralized execution,\nand improves the resilience to non-stationarity in independent learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of sampling and remote estimation for autoregressive\nMarkovian processes in a multi-hop wireless network with\nstatistically-identical agents. Agents cache the most recent samples from\nothers and communicate over wireless collision channels governed by an\nunderlying graph topology. Our goal is to minimize time-average estimation\nerror and/or age of information with decentralized scalable sampling and\ntransmission policies, considering both oblivious (where decision-making is\nindependent of the physical processes) and non-oblivious policies (where\ndecision-making depends on physical processes). We prove that in oblivious\npolicies, minimizing estimation error is equivalent to minimizing the age of\ninformation. The complexity of the problem, especially the multi-dimensional\naction spaces and arbitrary network topologies, makes theoretical methods for\nfinding optimal transmission policies intractable. We optimize the policies\nusing a graphical multi-agent reinforcement learning framework, where each\nagent employs a permutation-equivariant graph neural network architecture.\nTheoretically, we prove that our proposed framework exhibits desirable\ntransferability properties, allowing transmission policies trained on small- or\nmoderate-size networks to be executed effectively on large-scale topologies.\nNumerical experiments demonstrate that (i) Our proposed framework outperforms\nstate-of-the-art baselines; (ii) The trained policies are transferable to\nlarger networks, and their performance gains increase with the number of\nagents; (iii) The training procedure withstands non-stationarity even if we\nutilize independent learning techniques; and, (iv) Recurrence is pivotal in\nboth independent learning and centralized training and decentralized execution,\nand improves the resilience to non-stationarity in independent learning."
                },
                "authors": [
                    {
                        "name": "Xingran Chen"
                    },
                    {
                        "name": "Navid NaderiAlizadeh"
                    },
                    {
                        "name": "Alejandro Ribeiro"
                    },
                    {
                        "name": "Shirin Saeedi Bidokhti"
                    }
                ],
                "author_detail": {
                    "name": "Shirin Saeedi Bidokhti"
                },
                "author": "Shirin Saeedi Bidokhti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03227v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06302v1",
                "updated": "2025-03-08T18:30:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    18,
                    30,
                    54,
                    5,
                    67,
                    0
                ],
                "published": "2025-03-08T18:30:54Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    18,
                    30,
                    54,
                    5,
                    67,
                    0
                ],
                "title": "Synergizing AI and Digital Twins for Next-Generation Network\n  Optimization, Forecasting, and Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synergizing AI and Digital Twins for Next-Generation Network\n  Optimization, Forecasting, and Security"
                },
                "summary": "Digital network twins (DNTs) are virtual representations of physical\nnetworks, designed to enable real-time monitoring, simulation, and optimization\nof network performance. When integrated with machine learning (ML) techniques,\nparticularly federated learning (FL) and reinforcement learning (RL), DNTs\nemerge as powerful solutions for managing the complexities of network\noperations. This article presents a comprehensive analysis of the synergy of\nDNTs, FL, and RL techniques, showcasing their collective potential to address\ncritical challenges in 6G networks. We highlight key technical challenges that\nneed to be addressed, such as ensuring network reliability, achieving joint\ndata-scenario forecasting, and maintaining security in high-risk environments.\nAdditionally, we propose several pipelines that integrate DNT and ML within\ncoherent frameworks to enhance network optimization and security. Case studies\ndemonstrate the practical applications of our proposed pipelines in edge\ncaching and vehicular networks. In edge caching, the pipeline achieves over 80%\ncache hit rates while balancing base station loads. In autonomous vehicular\nsystem, it ensure a 100% no-collision rate, showcasing its reliability in\nsafety-critical scenarios. By exploring these synergies, we offer insights into\nthe future of intelligent and adaptive network systems that automate\ndecision-making and problem-solving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs) are virtual representations of physical\nnetworks, designed to enable real-time monitoring, simulation, and optimization\nof network performance. When integrated with machine learning (ML) techniques,\nparticularly federated learning (FL) and reinforcement learning (RL), DNTs\nemerge as powerful solutions for managing the complexities of network\noperations. This article presents a comprehensive analysis of the synergy of\nDNTs, FL, and RL techniques, showcasing their collective potential to address\ncritical challenges in 6G networks. We highlight key technical challenges that\nneed to be addressed, such as ensuring network reliability, achieving joint\ndata-scenario forecasting, and maintaining security in high-risk environments.\nAdditionally, we propose several pipelines that integrate DNT and ML within\ncoherent frameworks to enhance network optimization and security. Case studies\ndemonstrate the practical applications of our proposed pipelines in edge\ncaching and vehicular networks. In edge caching, the pipeline achieves over 80%\ncache hit rates while balancing base station loads. In autonomous vehicular\nsystem, it ensure a 100% no-collision rate, showcasing its reliability in\nsafety-critical scenarios. By exploring these synergies, we offer insights into\nthe future of intelligent and adaptive network systems that automate\ndecision-making and problem-solving."
                },
                "authors": [
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Minghong Fang"
                    },
                    {
                        "name": "Dianwei Chen"
                    },
                    {
                        "name": "Xianfeng Yang"
                    },
                    {
                        "name": "Yuchen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Liu"
                },
                "author": "Yuchen Liu",
                "arxiv_comment": "Accepted by IEEE Wireless Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03708v2",
                "updated": "2025-03-08T14:48:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    14,
                    48,
                    15,
                    5,
                    67,
                    0
                ],
                "published": "2025-03-05T17:59:19Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    59,
                    19,
                    2,
                    64,
                    0
                ],
                "title": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach"
                },
                "summary": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights will be released shortly, so please stay\ntuned for updates!",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights will be released shortly, so please stay\ntuned for updates!"
                },
                "authors": [
                    {
                        "name": "Nianzu Yang"
                    },
                    {
                        "name": "Pandeng Li"
                    },
                    {
                        "name": "Liming Zhao"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Chen-Wei Xie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Zhihang Liu"
                    },
                    {
                        "name": "Yun Zheng"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Junchi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Junchi Yan"
                },
                "author": "Junchi Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06015v1",
                "updated": "2025-03-08T02:35:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    2,
                    35,
                    16,
                    5,
                    67,
                    0
                ],
                "published": "2025-03-08T02:35:16Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    2,
                    35,
                    16,
                    5,
                    67,
                    0
                ],
                "title": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems"
                },
                "summary": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e. they do not adapt to changing cache access\npatterns. Newer developments such as High Luminosity - Large Hadron Collider\n(HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move toward\nstreaming readout based Data Acquisition systems (DAQs) will increase the data\nproduction exponentially and hence burden the storage, compute \\& network\ninfrastructures. Moreover, existing caching frameworks are optimized to reduce\nlatency, but not optimized for storage. This in combination with limited cache\ncapacities relative to total data makes it difficult to achieve data locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, first we present a Long Short-Term Memory-based (LSTM) hourly\ncache usage prediction. Second, we present an hourly file-level access\nprediction model based on CatboostRegressor. To date, most ML-based cache\nprediction strategies in HEP have focused on daily cache usage and limited\nworks tackled hourly cache usage and even less strategies addressed hourly\nfile-level access prediction. File-level access prediction allows for the\ndesign of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e. they do not adapt to changing cache access\npatterns. Newer developments such as High Luminosity - Large Hadron Collider\n(HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move toward\nstreaming readout based Data Acquisition systems (DAQs) will increase the data\nproduction exponentially and hence burden the storage, compute \\& network\ninfrastructures. Moreover, existing caching frameworks are optimized to reduce\nlatency, but not optimized for storage. This in combination with limited cache\ncapacities relative to total data makes it difficult to achieve data locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, first we present a Long Short-Term Memory-based (LSTM) hourly\ncache usage prediction. Second, we present an hourly file-level access\nprediction model based on CatboostRegressor. To date, most ML-based cache\nprediction strategies in HEP have focused on daily cache usage and limited\nworks tackled hourly cache usage and even less strategies addressed hourly\nfile-level access prediction. File-level access prediction allows for the\ndesign of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations."
                },
                "authors": [
                    {
                        "name": "Venkat Sai Suman Lamba Karanam"
                    },
                    {
                        "name": "Sarat Sasank Barla"
                    },
                    {
                        "name": "Byrav Ramamurthy"
                    },
                    {
                        "name": "Derek Weitzel"
                    }
                ],
                "author_detail": {
                    "name": "Derek Weitzel"
                },
                "author": "Derek Weitzel",
                "arxiv_comment": "Submitted as a contribution to the CHEP 2024 proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05941v1",
                "updated": "2025-03-07T21:16:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    21,
                    16,
                    41,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T21:16:41Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    21,
                    16,
                    41,
                    4,
                    66,
                    0
                ],
                "title": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions"
                },
                "summary": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example."
                },
                "authors": [
                    {
                        "name": "Avinash Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Avinash Kumar"
                },
                "author": "Avinash Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18668v2",
                "updated": "2025-03-07T18:57:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    57,
                    52,
                    4,
                    66,
                    0
                ],
                "published": "2024-02-28T19:28:27Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    19,
                    28,
                    27,
                    2,
                    59,
                    0
                ],
                "title": "Simple linear attention language models balance the recall-throughput\n  tradeoff",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple linear attention language models balance the recall-throughput\n  tradeoff"
                },
                "summary": "Recent work has shown that attention-based language models excel at recall,\nthe ability to ground generations in tokens previously seen in context.\nHowever, the efficiency of attention-based models is bottle-necked during\ninference by the KV-cache's aggressive memory consumption. In this work, we\nexplore whether we can improve language model efficiency (e.g. by reducing\nmemory consumption) without compromising on recall. By applying experiments and\ntheory to a broad set of architectures, we identify a key tradeoff between a\nmodel's state size and recall ability. We show that efficient alternatives to\nattention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but\nstruggle at recall. We propose BASED a simple architecture combining linear and\nsliding window attention. By varying BASED window size and linear attention\nfeature dimension, we can dial the state size and traverse the pareto frontier\nof the recall-memory tradeoff curve, recovering the full quality of attention\non one end and the small state size of attention-alternatives on the other. We\ntrain language models up to 1.3b parameters and show that BASED matches the\nstrongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them\non real-world recall-intensive tasks by 6.22 accuracy points. Implementations\nof linear attention are often less efficient than optimized standard attention\nimplementations. To make BASED competitive, we develop IO-aware algorithms that\nenable 24x higher throughput on language generation than FlashAttention-2, when\ngenerating 1024 tokens using 1.3b parameter models. Code for this work is\nprovided at: https://github.com/HazyResearch/based.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has shown that attention-based language models excel at recall,\nthe ability to ground generations in tokens previously seen in context.\nHowever, the efficiency of attention-based models is bottle-necked during\ninference by the KV-cache's aggressive memory consumption. In this work, we\nexplore whether we can improve language model efficiency (e.g. by reducing\nmemory consumption) without compromising on recall. By applying experiments and\ntheory to a broad set of architectures, we identify a key tradeoff between a\nmodel's state size and recall ability. We show that efficient alternatives to\nattention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but\nstruggle at recall. We propose BASED a simple architecture combining linear and\nsliding window attention. By varying BASED window size and linear attention\nfeature dimension, we can dial the state size and traverse the pareto frontier\nof the recall-memory tradeoff curve, recovering the full quality of attention\non one end and the small state size of attention-alternatives on the other. We\ntrain language models up to 1.3b parameters and show that BASED matches the\nstrongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them\non real-world recall-intensive tasks by 6.22 accuracy points. Implementations\nof linear attention are often less efficient than optimized standard attention\nimplementations. To make BASED competitive, we develop IO-aware algorithms that\nenable 24x higher throughput on language generation than FlashAttention-2, when\ngenerating 1024 tokens using 1.3b parameter models. Code for this work is\nprovided at: https://github.com/HazyResearch/based."
                },
                "authors": [
                    {
                        "name": "Simran Arora"
                    },
                    {
                        "name": "Sabri Eyuboglu"
                    },
                    {
                        "name": "Michael Zhang"
                    },
                    {
                        "name": "Aman Timalsina"
                    },
                    {
                        "name": "Silas Alberti"
                    },
                    {
                        "name": "Dylan Zinsley"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Atri Rudra"
                    },
                    {
                        "name": "Christopher Ré"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Ré"
                },
                "author": "Christopher Ré",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00242v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00242v4",
                "updated": "2025-03-07T17:47:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    47,
                    42,
                    4,
                    66,
                    0
                ],
                "published": "2024-03-30T04:34:54Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    4,
                    34,
                    54,
                    5,
                    90,
                    0
                ],
                "title": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference"
                },
                "summary": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99% KV cache IO and\nnearly 100% IO for partial results during attention calculation, DeFT achieves\nup to 2.23/3.59x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms. Our code is available at https://github.com/LINs-lab/DeFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99% KV cache IO and\nnearly 100% IO for partial results during attention calculation, DeFT achieves\nup to 2.23/3.59x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms. Our code is available at https://github.com/LINs-lab/DeFT."
                },
                "authors": [
                    {
                        "name": "Jinwei Yao"
                    },
                    {
                        "name": "Kaiqi Chen"
                    },
                    {
                        "name": "Kexun Zhang"
                    },
                    {
                        "name": "Jiaxuan You"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Zeke Wang"
                    },
                    {
                        "name": "Tao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Tao Lin"
                },
                "author": "Tao Lin",
                "arxiv_comment": "Update DeFT-v4, accepted by ICLR'25\n  (https://openreview.net/forum?id=2c7pfOqu9k). Our code is available at\n  https://github.com/LINs-lab/DeFT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00242v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00242v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05530v1",
                "updated": "2025-03-07T15:54:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T15:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "title": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, reducing reliance on expensive vector database lookups.\nWe evaluate Proximity on the MMLU and MedRAG benchmarks, demonstrating that it\nsignificantly improves retrieval efficiency while maintaining response\naccuracy. Proximity reduces retrieval latency by up to 59% while maintaining\naccuracy and lowers the computational burden on the vector database. We also\nexperiment with different similarity thresholds and quantify the trade-off\nbetween speed and recall. Our work shows that approximate caching is a viable\nand effective strategy for optimizing RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, reducing reliance on expensive vector database lookups.\nWe evaluate Proximity on the MMLU and MedRAG benchmarks, demonstrating that it\nsignificantly improves retrieval efficiency while maintaining response\naccuracy. Proximity reduces retrieval latency by up to 59% while maintaining\naccuracy and lowers the computational burden on the vector database. We also\nexperiment with different similarity thresholds and quantify the trade-off\nbetween speed and recall. Our work shows that approximate caching is a viable\nand effective strategy for optimizing RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Shai Bergman"
                    },
                    {
                        "name": "Zhang Ji"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Diana Petrescu"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Mathis Randl"
                    },
                    {
                        "name": "Martijn de Vos"
                    }
                ],
                "author_detail": {
                    "name": "Martijn de Vos"
                },
                "author": "Martijn de Vos",
                "arxiv_doi": "10.1145/3721146.3721941",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721146.3721941",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.05530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02694v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02694v4",
                "updated": "2025-03-07T14:49:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    49,
                    7,
                    4,
                    66,
                    0
                ],
                "published": "2024-03-05T06:23:50Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    6,
                    23,
                    50,
                    1,
                    65,
                    0
                ],
                "title": "MeanCache: User-Centric Semantic Caching for LLM Web Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeanCache: User-Centric Semantic Caching for LLM Web Services"
                },
                "summary": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%."
                },
                "authors": [
                    {
                        "name": "Waris Gill"
                    },
                    {
                        "name": "Mohamed Elidrisi"
                    },
                    {
                        "name": "Pallavi Kalapatapu"
                    },
                    {
                        "name": "Ammar Ahmed"
                    },
                    {
                        "name": "Ali Anwar"
                    },
                    {
                        "name": "Muhammad Ali Gulzar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Ali Gulzar"
                },
                "arxiv_affiliation": "Virginia Tech, USA",
                "author": "Muhammad Ali Gulzar",
                "arxiv_comment": "Accepted at 2025 IEEE 39th International Parallel and Distributed\n  Processing Symposium (IPDPS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02694v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02694v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05156v1",
                "updated": "2025-03-07T05:31:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    5,
                    31,
                    47,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T05:31:47Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    5,
                    31,
                    47,
                    4,
                    66,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Gradient-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Gradient-Optimized Cache"
                },
                "summary": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Kezhou Chen"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04982v1",
                "updated": "2025-03-06T21:21:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    21,
                    18,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T21:21:18Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    21,
                    18,
                    3,
                    65,
                    0
                ],
                "title": "LVLM-Compress-Bench: Benchmarking the Broader Impact of Large\n  Vision-Language Model Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LVLM-Compress-Bench: Benchmarking the Broader Impact of Large\n  Vision-Language Model Compression"
                },
                "summary": "Despite recent efforts in understanding the compression impact on large\nlanguage models (LLMs) in terms of their downstream task performance and\ntrustworthiness on relatively simpler uni-modal benchmarks (for example,\nquestion answering, common sense reasoning), their detailed study on\nmulti-modal Large Vision-Language Models (LVLMs) is yet to be unveiled. Towards\nmitigating this gap, we present LVLM-Compress-Bench, a framework to first\nthoroughly study the broad impact of compression on the generative performance\nof LVLMs with multi-modal input driven tasks. In specific, we consider two\nmajor classes of compression for autoregressive models, namely KV cache and\nweight compression, for the dynamically growing intermediate cache and static\nweights, respectively.\n  We use four LVLM variants of the popular LLaVA framework to present our\nanalysis via integrating various state-of-the-art KV and weight compression\nmethods including uniform, outlier-reduced, and group quantization for the KV\ncache and weights. With this framework we demonstrate on ten different\nmulti-modal datasets with different capabilities including recognition,\nknowledge, language generation, spatial awareness, visual reasoning,\nhallucination and visual illusion identification, toxicity, stereotypes and\nbias. In specific, our framework demonstrates the compression impact on both\ngeneral and ethically critical metrics leveraging a combination of real world\nand synthetic datasets to encompass diverse societal intersectional attributes.\nExtensive experimental evaluations yield diverse and intriguing observations on\nthe behavior of LVLMs at different quantization budget of KV and weights, in\nboth maintaining and losing performance as compared to the baseline model with\nFP16 data format.\n  Code will be open-sourced at\nhttps://github.com/opengear-project/LVLM-compress-bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent efforts in understanding the compression impact on large\nlanguage models (LLMs) in terms of their downstream task performance and\ntrustworthiness on relatively simpler uni-modal benchmarks (for example,\nquestion answering, common sense reasoning), their detailed study on\nmulti-modal Large Vision-Language Models (LVLMs) is yet to be unveiled. Towards\nmitigating this gap, we present LVLM-Compress-Bench, a framework to first\nthoroughly study the broad impact of compression on the generative performance\nof LVLMs with multi-modal input driven tasks. In specific, we consider two\nmajor classes of compression for autoregressive models, namely KV cache and\nweight compression, for the dynamically growing intermediate cache and static\nweights, respectively.\n  We use four LVLM variants of the popular LLaVA framework to present our\nanalysis via integrating various state-of-the-art KV and weight compression\nmethods including uniform, outlier-reduced, and group quantization for the KV\ncache and weights. With this framework we demonstrate on ten different\nmulti-modal datasets with different capabilities including recognition,\nknowledge, language generation, spatial awareness, visual reasoning,\nhallucination and visual illusion identification, toxicity, stereotypes and\nbias. In specific, our framework demonstrates the compression impact on both\ngeneral and ethically critical metrics leveraging a combination of real world\nand synthetic datasets to encompass diverse societal intersectional attributes.\nExtensive experimental evaluations yield diverse and intriguing observations on\nthe behavior of LVLMs at different quantization budget of KV and weights, in\nboth maintaining and losing performance as compared to the baseline model with\nFP16 data format.\n  Code will be open-sourced at\nhttps://github.com/opengear-project/LVLM-compress-bench."
                },
                "authors": [
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Anahita Bhiwandiwalla"
                    },
                    {
                        "name": "Sungduk Yu"
                    },
                    {
                        "name": "Phillip Howard"
                    },
                    {
                        "name": "Tiep Le"
                    },
                    {
                        "name": "Sharath Nittur Sridhar"
                    },
                    {
                        "name": "David Cobbley"
                    },
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Vasudev Lal"
                    }
                ],
                "author_detail": {
                    "name": "Vasudev Lal"
                },
                "author": "Vasudev Lal",
                "arxiv_comment": "This work has been accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04973v1",
                "updated": "2025-03-06T21:07:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    7,
                    41,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T21:07:41Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    7,
                    41,
                    3,
                    65,
                    0
                ],
                "title": "Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge\n  Reasoning"
                },
                "summary": "Incorporating external knowledge in large language models (LLMs) enhances\ntheir utility across diverse applications, but existing methods have\ntrade-offs. Retrieval-Augmented Generation (RAG) fetches evidence via\nsimilarity search, but key information may fall outside top ranked results.\nLong-context models can process multiple documents but are computationally\nexpensive and limited by context window size. Inspired by students condensing\nstudy material for open-book exams, we propose task-aware key-value (KV) cache\ncompression, which compresses external knowledge in a zero- or few-shot setup.\nThis enables LLMs to reason efficiently over a compacted representation of all\nrelevant information. Experiments show our approach outperforms both RAG and\ntask-agnostic compression methods. On LongBench v2, it improves accuracy by up\nto 7 absolute points over RAG with a 30x compression rate, while reducing\ninference latency from 0.43s to 0.16s. A synthetic dataset highlights that RAG\nperforms well when sparse evidence suffices, whereas task-aware compression is\nsuperior for broad knowledge tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incorporating external knowledge in large language models (LLMs) enhances\ntheir utility across diverse applications, but existing methods have\ntrade-offs. Retrieval-Augmented Generation (RAG) fetches evidence via\nsimilarity search, but key information may fall outside top ranked results.\nLong-context models can process multiple documents but are computationally\nexpensive and limited by context window size. Inspired by students condensing\nstudy material for open-book exams, we propose task-aware key-value (KV) cache\ncompression, which compresses external knowledge in a zero- or few-shot setup.\nThis enables LLMs to reason efficiently over a compacted representation of all\nrelevant information. Experiments show our approach outperforms both RAG and\ntask-agnostic compression methods. On LongBench v2, it improves accuracy by up\nto 7 absolute points over RAG with a 30x compression rate, while reducing\ninference latency from 0.43s to 0.16s. A synthetic dataset highlights that RAG\nperforms well when sparse evidence suffices, whereas task-aware compression is\nsuperior for broad knowledge tasks."
                },
                "authors": [
                    {
                        "name": "Giulio Corallo"
                    },
                    {
                        "name": "Orion Weller"
                    },
                    {
                        "name": "Fabio Petroni"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17635v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17635v2",
                "updated": "2025-03-06T06:39:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    6,
                    39,
                    56,
                    3,
                    65,
                    0
                ],
                "published": "2024-10-23T07:53:29Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "title": "Markov Chain of Thought for Efficient Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov Chain of Thought for Efficient Mathematical Reasoning"
                },
                "summary": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, \"derive, then reduce\", we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the $\\texttt{MCoTInstruct}$ dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs. The\ncode is available at https://github.com/james-yw/Markov-Chain-of-Thought",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, \"derive, then reduce\", we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the $\\texttt{MCoTInstruct}$ dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs. The\ncode is available at https://github.com/james-yw/Markov-Chain-of-Thought"
                },
                "authors": [
                    {
                        "name": "Wen Yang"
                    },
                    {
                        "name": "Minpeng Liao"
                    },
                    {
                        "name": "Kai Fan"
                    }
                ],
                "author_detail": {
                    "name": "Kai Fan"
                },
                "author": "Kai Fan",
                "arxiv_comment": "Camera ready version for NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17635v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17635v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01801v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01801v2",
                "updated": "2025-03-05T20:36:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    20,
                    36,
                    51,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-03T18:32:31Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    18,
                    32,
                    31,
                    0,
                    62,
                    0
                ],
                "title": "TUNA: Tuning Unstable and Noisy Cloud Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TUNA: Tuning Unstable and Noisy Cloud Applications"
                },
                "summary": "Autotuning plays a pivotal role in optimizing the performance of systems,\nparticularly in large-scale cloud deployments. One of the main challenges in\nperforming autotuning in the cloud arises from performance variability. We\nfirst investigate the extent to which noise slows autotuning and find that as\nlittle as $5\\%$ noise can lead to a $2.5$x slowdown in converging to the\nbest-performing configuration. We measure the magnitude of noise in cloud\ncomputing settings and find that while some components (CPU, disk) have almost\nno performance variability, there are still sources of significant variability\n(caches, memory). Furthermore, variability leads to autotuning finding unstable\nconfigurations. As many as $63.3\\%$ of the configurations selected as \"best\"\nduring tuning can have their performance degrade by $30\\%$ or more when\ndeployed. Using this as motivation, we propose a novel approach to improve the\nefficiency of autotuning systems by (a) detecting and removing outlier\nconfigurations and (b) using ML-based approaches to provide a more stable true\nsignal of de-noised experiment results to the optimizer. The resulting system,\nTUNA (Tuning Unstable and Noisy Cloud Applications) enables faster convergence\nand robust configurations. Tuning postgres running mssales, an enterprise\nproduction workload, we find that TUNA can lead to $1.88$x lower running time\non average with $2.58x$ lower standard deviation compared to traditional\nsampling methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autotuning plays a pivotal role in optimizing the performance of systems,\nparticularly in large-scale cloud deployments. One of the main challenges in\nperforming autotuning in the cloud arises from performance variability. We\nfirst investigate the extent to which noise slows autotuning and find that as\nlittle as $5\\%$ noise can lead to a $2.5$x slowdown in converging to the\nbest-performing configuration. We measure the magnitude of noise in cloud\ncomputing settings and find that while some components (CPU, disk) have almost\nno performance variability, there are still sources of significant variability\n(caches, memory). Furthermore, variability leads to autotuning finding unstable\nconfigurations. As many as $63.3\\%$ of the configurations selected as \"best\"\nduring tuning can have their performance degrade by $30\\%$ or more when\ndeployed. Using this as motivation, we propose a novel approach to improve the\nefficiency of autotuning systems by (a) detecting and removing outlier\nconfigurations and (b) using ML-based approaches to provide a more stable true\nsignal of de-noised experiment results to the optimizer. The resulting system,\nTUNA (Tuning Unstable and Noisy Cloud Applications) enables faster convergence\nand robust configurations. Tuning postgres running mssales, an enterprise\nproduction workload, we find that TUNA can lead to $1.88$x lower running time\non average with $2.58x$ lower standard deviation compared to traditional\nsampling methodologies."
                },
                "authors": [
                    {
                        "name": "Johannes Freischuetz"
                    },
                    {
                        "name": "Konstantinos Kanellis"
                    },
                    {
                        "name": "Brian Kroth"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "arxiv_doi": "10.1145/3689031.3717480",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689031.3717480",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.01801v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01801v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 20 figures, EuroSys'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03751v1",
                "updated": "2025-03-05T18:59:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    59,
                    50,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T18:59:50Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    59,
                    50,
                    2,
                    64,
                    0
                ],
                "title": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera\n  Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera\n  Control"
                },
                "summary": "We present GEN3C, a generative video model with precise Camera Control and\ntemporal 3D Consistency. Prior video models already generate realistic videos,\nbut they tend to leverage little 3D information, leading to inconsistencies,\nsuch as objects popping in and out of existence. Camera control, if implemented\nat all, is imprecise, because camera parameters are mere inputs to the neural\nnetwork which must then infer how the video depends on the camera. In contrast,\nGEN3C is guided by a 3D cache: point clouds obtained by predicting the\npixel-wise depth of seed images or previously generated frames. When generating\nthe next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with\nthe new camera trajectory provided by the user. Crucially, this means that\nGEN3C neither has to remember what it previously generated nor does it have to\ninfer the image structure from the camera pose. The model, instead, can focus\nall its generative power on previously unobserved regions, as well as advancing\nthe scene state to the next frame. Our results demonstrate more precise camera\ncontrol than prior work, as well as state-of-the-art results in sparse-view\nnovel view synthesis, even in challenging settings such as driving scenes and\nmonocular dynamic video. Results are best viewed in videos. Check out our\nwebpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present GEN3C, a generative video model with precise Camera Control and\ntemporal 3D Consistency. Prior video models already generate realistic videos,\nbut they tend to leverage little 3D information, leading to inconsistencies,\nsuch as objects popping in and out of existence. Camera control, if implemented\nat all, is imprecise, because camera parameters are mere inputs to the neural\nnetwork which must then infer how the video depends on the camera. In contrast,\nGEN3C is guided by a 3D cache: point clouds obtained by predicting the\npixel-wise depth of seed images or previously generated frames. When generating\nthe next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with\nthe new camera trajectory provided by the user. Crucially, this means that\nGEN3C neither has to remember what it previously generated nor does it have to\ninfer the image structure from the camera pose. The model, instead, can focus\nall its generative power on previously unobserved regions, as well as advancing\nthe scene state to the next frame. Our results demonstrate more precise camera\ncontrol than prior work, as well as state-of-the-art results in sparse-view\nnovel view synthesis, even in challenging settings such as driving scenes and\nmonocular dynamic video. Results are best viewed in videos. Check out our\nwebpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/"
                },
                "authors": [
                    {
                        "name": "Xuanchi Ren"
                    },
                    {
                        "name": "Tianchang Shen"
                    },
                    {
                        "name": "Jiahui Huang"
                    },
                    {
                        "name": "Huan Ling"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Merlin Nimier-David"
                    },
                    {
                        "name": "Thomas Müller"
                    },
                    {
                        "name": "Alexander Keller"
                    },
                    {
                        "name": "Sanja Fidler"
                    },
                    {
                        "name": "Jun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Gao"
                },
                "author": "Jun Gao",
                "arxiv_comment": "To appear in CVPR 2025. Website:\n  https://research.nvidia.com/labs/toronto-ai/GEN3C/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07115v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07115v3",
                "updated": "2025-03-05T14:43:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    43,
                    1,
                    2,
                    64,
                    0
                ],
                "published": "2025-02-10T23:11:44Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    11,
                    44,
                    0,
                    41,
                    0
                ],
                "title": "Online Scheduling for LLM Inference with KV Cache Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Scheduling for LLM Inference with KV Cache Constraints"
                },
                "summary": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment."
                },
                "authors": [
                    {
                        "name": "Patrick Jaillet"
                    },
                    {
                        "name": "Jiashuo Jiang"
                    },
                    {
                        "name": "Chara Podimata"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "arxiv_comment": "Will add a lemma in the proof of Theorem 5.3 to make the statement\n  and proof more rigorous",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07115v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07115v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.07714v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.07714v5",
                "updated": "2025-03-05T07:39:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    7,
                    39,
                    3,
                    2,
                    64,
                    0
                ],
                "published": "2024-03-12T14:57:40Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    14,
                    57,
                    40,
                    1,
                    72,
                    0
                ],
                "title": "StableToolBench: Towards Stable Large-Scale Benchmarking on Tool\n  Learning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StableToolBench: Towards Stable Large-Scale Benchmarking on Tool\n  Learning of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have witnessed remarkable advancements in recent\nyears, prompting the exploration of tool learning, which integrates LLMs with\nexternal tools to address diverse real-world challenges. Assessing the\ncapability of LLMs to utilise tools necessitates large-scale and stable\nbenchmarks. However, previous works relied on either hand-crafted online tools\nwith limited scale, or large-scale real online APIs suffering from instability\nof API status. To address this problem, we introduce StableToolBench, a\nbenchmark evolving from ToolBench, proposing a virtual API server and stable\nevaluation system. The virtual API server contains a caching system and API\nsimulators which are complementary to alleviate the change in API status.\nMeanwhile, the stable evaluation system designs solvable pass and win rates\nusing GPT-4 as the automatic evaluator to eliminate the randomness during\nevaluation. Experimental results demonstrate the stability of StableToolBench,\nand further discuss the effectiveness of API simulators, the caching system,\nand the evaluator system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have witnessed remarkable advancements in recent\nyears, prompting the exploration of tool learning, which integrates LLMs with\nexternal tools to address diverse real-world challenges. Assessing the\ncapability of LLMs to utilise tools necessitates large-scale and stable\nbenchmarks. However, previous works relied on either hand-crafted online tools\nwith limited scale, or large-scale real online APIs suffering from instability\nof API status. To address this problem, we introduce StableToolBench, a\nbenchmark evolving from ToolBench, proposing a virtual API server and stable\nevaluation system. The virtual API server contains a caching system and API\nsimulators which are complementary to alleviate the change in API status.\nMeanwhile, the stable evaluation system designs solvable pass and win rates\nusing GPT-4 as the automatic evaluator to eliminate the randomness during\nevaluation. Experimental results demonstrate the stability of StableToolBench,\nand further discuss the effectiveness of API simulators, the caching system,\nand the evaluator system."
                },
                "authors": [
                    {
                        "name": "Zhicheng Guo"
                    },
                    {
                        "name": "Sijie Cheng"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Shihao Liang"
                    },
                    {
                        "name": "Yujia Qin"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.07714v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.07714v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03182v1",
                "updated": "2025-03-05T04:54:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    4,
                    54,
                    50,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T04:54:50Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    4,
                    54,
                    50,
                    2,
                    64,
                    0
                ],
                "title": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism"
                },
                "summary": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation."
                },
                "authors": [
                    {
                        "name": "Xinyuan Lin"
                    },
                    {
                        "name": "Chenlu Li"
                    },
                    {
                        "name": "Zongle Huang"
                    },
                    {
                        "name": "Chunyu Wang"
                    },
                    {
                        "name": "Bo Xiao"
                    },
                    {
                        "name": "Huazhong Yang"
                    },
                    {
                        "name": "Shishi Duan"
                    },
                    {
                        "name": "Yongpan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yongpan Liu"
                },
                "author": "Yongpan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02969v1",
                "updated": "2025-03-04T19:51:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    19,
                    51,
                    29,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T19:51:29Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    19,
                    51,
                    29,
                    1,
                    63,
                    0
                ],
                "title": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model"
                },
                "summary": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code at\nhttps://github.com/LeiLiLab/InfiniSST",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code at\nhttps://github.com/LeiLiLab/InfiniSST"
                },
                "authors": [
                    {
                        "name": "Siqi Ouyang"
                    },
                    {
                        "name": "Xi Xu"
                    },
                    {
                        "name": "Lei Li"
                    }
                ],
                "author_detail": {
                    "name": "Lei Li"
                },
                "author": "Lei Li",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02812v1",
                "updated": "2025-03-04T17:37:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    37,
                    49,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T17:37:49Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    37,
                    49,
                    1,
                    63,
                    0
                ],
                "title": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression"
                },
                "summary": "Autoregressive language models rely on a Key-Value (KV) Cache, which avoids\nre-computing past hidden states during generation, making it faster. As model\nsizes and context lengths grow, the KV Cache becomes a significant memory\nbottleneck, which calls for compression methods that limit its size during\ngeneration. In this paper, we discover surprising properties of Query (Q) and\nKey (K) vectors that allow us to efficiently approximate attention scores\nwithout computing the attention maps. We propose Q-Filters, a training-free KV\nCache compression method that filters out less crucial Key-Value pairs based on\na single context-agnostic projection. Contrarily to many alternatives,\nQ-Filters is compatible with FlashAttention, as it does not require direct\naccess to attention weights. Experimental results in long-context settings\ndemonstrate that Q-Filters is competitive with attention-based compression\nmethods such as SnapKV in retrieval tasks while consistently outperforming\nefficient compression schemes such as Streaming-LLM in generation setups.\nNotably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task\nwith a x32 compression level while reducing the generation perplexity drop by\nup to 65% in text generation compared to Streaming-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive language models rely on a Key-Value (KV) Cache, which avoids\nre-computing past hidden states during generation, making it faster. As model\nsizes and context lengths grow, the KV Cache becomes a significant memory\nbottleneck, which calls for compression methods that limit its size during\ngeneration. In this paper, we discover surprising properties of Query (Q) and\nKey (K) vectors that allow us to efficiently approximate attention scores\nwithout computing the attention maps. We propose Q-Filters, a training-free KV\nCache compression method that filters out less crucial Key-Value pairs based on\na single context-agnostic projection. Contrarily to many alternatives,\nQ-Filters is compatible with FlashAttention, as it does not require direct\naccess to attention weights. Experimental results in long-context settings\ndemonstrate that Q-Filters is competitive with attention-based compression\nmethods such as SnapKV in retrieval tasks while consistently outperforming\nefficient compression schemes such as Streaming-LLM in generation setups.\nNotably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task\nwith a x32 compression level while reducing the generation perplexity drop by\nup to 65% in text generation compared to Streaming-LLM."
                },
                "authors": [
                    {
                        "name": "Nathan Godey"
                    },
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    },
                    {
                        "name": "Éric de la Clergerie"
                    },
                    {
                        "name": "Benoît Sagot"
                    }
                ],
                "author_detail": {
                    "name": "Benoît Sagot"
                },
                "author": "Benoît Sagot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02758v1",
                "updated": "2025-03-04T16:21:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    21,
                    33,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T16:21:33Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    21,
                    33,
                    1,
                    63,
                    0
                ],
                "title": "Efficient and Optimal No-Regret Caching under Partial Observation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Optimal No-Regret Caching under Partial Observation"
                },
                "summary": "Online learning algorithms have been successfully used to design caching\npolicies with sublinear regret in the total number of requests, with no\nstatistical assumption about the request sequence. Most existing algorithms\ninvolve computationally expensive operations and require knowledge of all past\nrequests. However, this may not be feasible in practical scenarios like caching\nat a cellular base station. Therefore, we study the caching problem in a more\nrestrictive setting where only a fraction of past requests are observed, and we\npropose a randomized caching policy with sublinear regret based on the classic\nonline learning algorithm Follow-the-Perturbed-Leader (FPL). Our caching policy\nis the first to attain the asymptotically optimal regret bound while ensuring\nasymptotically constant amortized time complexity in the partial observability\nsetting of requests. The experimental evaluation compares the proposed solution\nagainst classic caching policies and validates the proposed approach under\nsynthetic and real-world request traces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online learning algorithms have been successfully used to design caching\npolicies with sublinear regret in the total number of requests, with no\nstatistical assumption about the request sequence. Most existing algorithms\ninvolve computationally expensive operations and require knowledge of all past\nrequests. However, this may not be feasible in practical scenarios like caching\nat a cellular base station. Therefore, we study the caching problem in a more\nrestrictive setting where only a fraction of past requests are observed, and we\npropose a randomized caching policy with sublinear regret based on the classic\nonline learning algorithm Follow-the-Perturbed-Leader (FPL). Our caching policy\nis the first to attain the asymptotically optimal regret bound while ensuring\nasymptotically constant amortized time complexity in the partial observability\nsetting of requests. The experimental evaluation compares the proposed solution\nagainst classic caching policies and validates the proposed approach under\nsynthetic and real-world request traces."
                },
                "authors": [
                    {
                        "name": "Younes Ben Mazziane"
                    },
                    {
                        "name": "Francescomaria Faticanti"
                    },
                    {
                        "name": "Sara Alouf"
                    },
                    {
                        "name": "Giovanni Neglia"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Neglia"
                },
                "author": "Giovanni Neglia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03157v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03157v2",
                "updated": "2025-03-04T13:01:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    1,
                    7,
                    1,
                    63,
                    0
                ],
                "published": "2024-07-03T14:34:03Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    14,
                    34,
                    3,
                    2,
                    185,
                    0
                ],
                "title": "Let the Code LLM Edit Itself When You Edit the Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let the Code LLM Edit Itself When You Edit the Code"
                },
                "summary": "In this work, we investigate a typical scenario in code generation where a\ndeveloper edits existing code in real time and requests a code assistant, e.g.,\na large language model, to re-predict the next token or next line on the fly.\nNaively, the LLM needs to re-encode the entire KV cache to provide an accurate\nprediction. However, this process is computationally expensive, especially when\nthe sequence length is long. Simply encoding the edited subsequence and\nintegrating it to the original KV cache meets the temporal confusion problem,\nleading to significantly worse performance. We address this efficiency and\naccuracy trade-off by introducing \\underline{\\textbf{Positional\n\\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary\npositional encoding, PIE first removes the rotary matrices in the Key cache\nthat introduce temporal confusion and then reapplies the correct rotary\nmatrices. This process ensures that positional relationships between tokens are\ncorrect and requires only a single round of matrix multiplication. We validate\nthe effectiveness of PIE through extensive experiments on the RepoBench-C-8k\ndataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.\nOur evaluation includes three real-world coding tasks: code insertion, code\ndeletion, and multi-place code editing. Results demonstrate that PIE reduces\ncomputational overhead by over 85% compared to the standard full recomputation\napproach across all model sizes and tasks while well approximating the model\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we investigate a typical scenario in code generation where a\ndeveloper edits existing code in real time and requests a code assistant, e.g.,\na large language model, to re-predict the next token or next line on the fly.\nNaively, the LLM needs to re-encode the entire KV cache to provide an accurate\nprediction. However, this process is computationally expensive, especially when\nthe sequence length is long. Simply encoding the edited subsequence and\nintegrating it to the original KV cache meets the temporal confusion problem,\nleading to significantly worse performance. We address this efficiency and\naccuracy trade-off by introducing \\underline{\\textbf{Positional\n\\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary\npositional encoding, PIE first removes the rotary matrices in the Key cache\nthat introduce temporal confusion and then reapplies the correct rotary\nmatrices. This process ensures that positional relationships between tokens are\ncorrect and requires only a single round of matrix multiplication. We validate\nthe effectiveness of PIE through extensive experiments on the RepoBench-C-8k\ndataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.\nOur evaluation includes three real-world coding tasks: code insertion, code\ndeletion, and multi-place code editing. Results demonstrate that PIE reduces\ncomputational overhead by over 85% compared to the standard full recomputation\napproach across all model sizes and tasks while well approximating the model\nperformance."
                },
                "authors": [
                    {
                        "name": "Zhenyu He"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Shengjie Luo"
                    },
                    {
                        "name": "Jingjing Xu"
                    },
                    {
                        "name": "Zhi Zhang"
                    },
                    {
                        "name": "Di He"
                    }
                ],
                "author_detail": {
                    "name": "Di He"
                },
                "author": "Di He",
                "arxiv_comment": "ICLR 2025 Camera Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03157v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03157v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02508v1",
                "updated": "2025-03-04T11:19:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    19,
                    2,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T11:19:02Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    19,
                    2,
                    1,
                    63,
                    0
                ],
                "title": "Q&C: When Quantization Meets Cache in Efficient Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q&C: When Quantization Meets Cache in Efficient Image Generation"
                },
                "summary": "Quantization and cache mechanisms are typically applied individually for\nefficient Diffusion Transformers (DiTs), each demonstrating notable potential\nfor acceleration. However, the promoting effect of combining the two mechanisms\non efficient generation remains under-explored. Through empirical\ninvestigation, we find that the combination of quantization and cache\nmechanisms for DiT is not straightforward, and two key challenges lead to\nsevere catastrophic performance degradation: (i) the sample efficacy of\ncalibration datasets in post-training quantization (PTQ) is significantly\neliminated by cache operation; (ii) the combination of the above mechanisms\nintroduces more severe exposure bias within sampling distribution, resulting in\namplified error accumulation in the image generation process. In this work, we\ntake advantage of these two acceleration mechanisms and propose a hybrid\nacceleration method by tackling the above challenges, aiming to further improve\nthe efficiency of DiTs while maintaining excellent generation capability.\nConcretely, a temporal-aware parallel clustering (TAP) is designed to\ndynamically improve the sample selection efficacy for the calibration within\nPTQ for different diffusion steps. A variance compensation (VC) strategy is\nderived to correct the sampling distribution. It mitigates exposure bias\nthrough an adaptive correction factor generation. Extensive experiments have\nshown that our method has accelerated DiTs by 12.7x while preserving\ncompetitive generation capability. The code will be available at\nhttps://github.com/xinding-sys/Quant-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization and cache mechanisms are typically applied individually for\nefficient Diffusion Transformers (DiTs), each demonstrating notable potential\nfor acceleration. However, the promoting effect of combining the two mechanisms\non efficient generation remains under-explored. Through empirical\ninvestigation, we find that the combination of quantization and cache\nmechanisms for DiT is not straightforward, and two key challenges lead to\nsevere catastrophic performance degradation: (i) the sample efficacy of\ncalibration datasets in post-training quantization (PTQ) is significantly\neliminated by cache operation; (ii) the combination of the above mechanisms\nintroduces more severe exposure bias within sampling distribution, resulting in\namplified error accumulation in the image generation process. In this work, we\ntake advantage of these two acceleration mechanisms and propose a hybrid\nacceleration method by tackling the above challenges, aiming to further improve\nthe efficiency of DiTs while maintaining excellent generation capability.\nConcretely, a temporal-aware parallel clustering (TAP) is designed to\ndynamically improve the sample selection efficacy for the calibration within\nPTQ for different diffusion steps. A variance compensation (VC) strategy is\nderived to correct the sampling distribution. It mitigates exposure bias\nthrough an adaptive correction factor generation. Extensive experiments have\nshown that our method has accelerated DiTs by 12.7x while preserving\ncompetitive generation capability. The code will be available at\nhttps://github.com/xinding-sys/Quant-Cache."
                },
                "authors": [
                    {
                        "name": "Xin Ding"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Zhibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhibo Chen"
                },
                "author": "Zhibo Chen",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02504v1",
                "updated": "2025-03-04T11:15:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    15,
                    47,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T11:15:47Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    15,
                    47,
                    1,
                    63,
                    0
                ],
                "title": "Energy efficiency of cache eviction algorithms for Zipf distributed\n  objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy efficiency of cache eviction algorithms for Zipf distributed\n  objects"
                },
                "summary": "This paper presents a summary analysis of the Least Frequently Used (LFU) and\nPerfect Least Frequently Used (PLFU) cache eviction algorithms on real data,\ntransferred on Content Delivery Nettworks (CDNs), as well as on Zipf\ndistributed samples. In light of the growing emphasis on energy efficiency in\nCDNs in recent years due to rising energy costs, this paper considers and\ndiscusses the total CPU time required to run a cache algorithm. The total CPU\ntime represents a novel metric for evaluating cache performance, and it is\ncontrasted with the conventional Cache Hit Ratio (CHR) metric. Furthermore, a\nnew algorithm with an admission policy and the eviction strategy that of PLFU\nis presented. The results demonstrate that it is a simple and straightforward\nalgorithm to implement and offers high CHR and low CPU time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a summary analysis of the Least Frequently Used (LFU) and\nPerfect Least Frequently Used (PLFU) cache eviction algorithms on real data,\ntransferred on Content Delivery Nettworks (CDNs), as well as on Zipf\ndistributed samples. In light of the growing emphasis on energy efficiency in\nCDNs in recent years due to rising energy costs, this paper considers and\ndiscusses the total CPU time required to run a cache algorithm. The total CPU\ntime represents a novel metric for evaluating cache performance, and it is\ncontrasted with the conventional Cache Hit Ratio (CHR) metric. Furthermore, a\nnew algorithm with an admission policy and the eviction strategy that of PLFU\nis presented. The results demonstrate that it is a simple and straightforward\nalgorithm to implement and offers high CHR and low CPU time."
                },
                "authors": [
                    {
                        "name": "Emese Sziklay"
                    },
                    {
                        "name": "Tamás Jursonovics"
                    }
                ],
                "author_detail": {
                    "name": "Tamás Jursonovics"
                },
                "author": "Tamás Jursonovics",
                "arxiv_comment": "13 pages, 7 figures, ICRIC 2023, Volume 2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02398v1",
                "updated": "2025-03-04T08:41:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    41,
                    40,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T08:41:40Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    41,
                    40,
                    1,
                    63,
                    0
                ],
                "title": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence"
                },
                "summary": "Recommendation agents leverage large language models for user modeling LLM UM\nto construct textual personas guiding alignment with real users. However\nexisting LLM UM methods struggle with long user generated content UGC due to\ncontext limitations and performance degradation. To address this sampling\nstrategies prioritize relevance or recency are often applied yet they\ninevitably neglect the diverse user interests embedded within the discarded\nbehaviors resulting in incomplete modeling and degraded profiling quality.\nFurthermore relevance based sampling requires real time retrieval forcing the\nuser modeling process to operate online which introduces significant latency\noverhead. In this paper we propose PersonaX an agent agnostic LLM UM framework\nthat tackles these challenges through sub behavior sequence SBS selection and\noffline multi persona construction. PersonaX extracts compact SBS segments\noffline to capture diverse user interests generating fine grained textual\npersonas that are cached for efficient online retrieval. This approach ensures\nthat the user persona used for prompting remains highly relevant to the current\ncontext while eliminating the need for online user modeling. For SBS selection\nwe ensure both efficiency length less than five and high representational\nquality by balancing prototypicality and diversity within the sampled data.\nExtensive experiments validate the effectiveness and versatility of PersonaX in\nhigh quality user profiling. Utilizing only 30 to 50 percent of the behavioral\ndata with a sequence length of 480 integrating PersonaX with AgentCF yields an\nabsolute performance improvement of 3 to 11 percent while integration with\nAgent4Rec results in a gain of 10 to 50 percent. PersonaX as an agent agnostic\nframework sets a new benchmark for scalable user modeling paving the way for\nmore accurate and efficient LLM driven recommendation agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommendation agents leverage large language models for user modeling LLM UM\nto construct textual personas guiding alignment with real users. However\nexisting LLM UM methods struggle with long user generated content UGC due to\ncontext limitations and performance degradation. To address this sampling\nstrategies prioritize relevance or recency are often applied yet they\ninevitably neglect the diverse user interests embedded within the discarded\nbehaviors resulting in incomplete modeling and degraded profiling quality.\nFurthermore relevance based sampling requires real time retrieval forcing the\nuser modeling process to operate online which introduces significant latency\noverhead. In this paper we propose PersonaX an agent agnostic LLM UM framework\nthat tackles these challenges through sub behavior sequence SBS selection and\noffline multi persona construction. PersonaX extracts compact SBS segments\noffline to capture diverse user interests generating fine grained textual\npersonas that are cached for efficient online retrieval. This approach ensures\nthat the user persona used for prompting remains highly relevant to the current\ncontext while eliminating the need for online user modeling. For SBS selection\nwe ensure both efficiency length less than five and high representational\nquality by balancing prototypicality and diversity within the sampled data.\nExtensive experiments validate the effectiveness and versatility of PersonaX in\nhigh quality user profiling. Utilizing only 30 to 50 percent of the behavioral\ndata with a sequence length of 480 integrating PersonaX with AgentCF yields an\nabsolute performance improvement of 3 to 11 percent while integration with\nAgent4Rec results in a gain of 10 to 50 percent. PersonaX as an agent agnostic\nframework sets a new benchmark for scalable user modeling paving the way for\nmore accurate and efficient LLM driven recommendation agents."
                },
                "authors": [
                    {
                        "name": "Yunxiao Shi"
                    },
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Zeqi Zhang"
                    },
                    {
                        "name": "Xing Zi"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Min Xu"
                    }
                ],
                "author_detail": {
                    "name": "Min Xu"
                },
                "author": "Min Xu",
                "arxiv_comment": "draft paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02236v1",
                "updated": "2025-03-04T03:18:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    3,
                    18,
                    56,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T03:18:56Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    3,
                    18,
                    56,
                    1,
                    63,
                    0
                ],
                "title": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference"
                },
                "summary": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy."
                },
                "authors": [
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Xinhao Luo"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Wentao Ni"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Yuhao Zhu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Chen Jin"
                    }
                ],
                "author_detail": {
                    "name": "Chen Jin"
                },
                "author": "Chen Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05787v2",
                "updated": "2025-03-03T18:23:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    18,
                    23,
                    47,
                    0,
                    62,
                    0
                ],
                "published": "2024-11-08T18:57:07Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    57,
                    7,
                    4,
                    313,
                    0
                ],
                "title": "RefreshKV: Updating Small KV Cache During Long-form Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RefreshKV: Updating Small KV Cache During Long-form Generation"
                },
                "summary": "Generating long sequences of tokens given a long-context input is a very\ncompute-intensive inference scenario for large language models (LLMs). One\nprominent inference speed-up approach is to construct a smaller key-value (KV)\ncache, relieving LLMs from computing attention over a long sequence of tokens.\nWhile such methods work well to generate short sequences, their performance\ndegrades rapidly for long-form generation. Most KV compression happens once,\nprematurely removing tokens that can be useful later in the generation. We\npropose a new inference method, RefreshKV, that flexibly alternates between\nfull context attention and attention over a subset of input tokens during\ngeneration. After each full attention step, we update the smaller KV cache\nbased on the attention pattern over the entire input. Applying our method to\noff-the-shelf LLMs achieves comparable speedup to eviction-based methods while\nimproving performance for various long-form generation tasks. Lastly, we show\nthat continued pretraining with our inference setting brings further gains in\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating long sequences of tokens given a long-context input is a very\ncompute-intensive inference scenario for large language models (LLMs). One\nprominent inference speed-up approach is to construct a smaller key-value (KV)\ncache, relieving LLMs from computing attention over a long sequence of tokens.\nWhile such methods work well to generate short sequences, their performance\ndegrades rapidly for long-form generation. Most KV compression happens once,\nprematurely removing tokens that can be useful later in the generation. We\npropose a new inference method, RefreshKV, that flexibly alternates between\nfull context attention and attention over a subset of input tokens during\ngeneration. After each full attention step, we update the smaller KV cache\nbased on the attention pattern over the entire input. Applying our method to\noff-the-shelf LLMs achieves comparable speedup to eviction-based methods while\nimproving performance for various long-form generation tasks. Lastly, we show\nthat continued pretraining with our inference setting brings further gains in\nperformance."
                },
                "authors": [
                    {
                        "name": "Fangyuan Xu"
                    },
                    {
                        "name": "Tanya Goyal"
                    },
                    {
                        "name": "Eunsol Choi"
                    }
                ],
                "author_detail": {
                    "name": "Eunsol Choi"
                },
                "author": "Eunsol Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01586v1",
                "updated": "2025-03-03T14:26:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    14,
                    26,
                    51,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T14:26:51Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    14,
                    26,
                    51,
                    0,
                    62,
                    0
                ],
                "title": "EliteKV: Scalable KV Cache Compression via RoPE Frequency Selection and\n  Joint Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EliteKV: Scalable KV Cache Compression via RoPE Frequency Selection and\n  Joint Low-Rank Projection"
                },
                "summary": "Rotary Position Embedding (RoPE) enables each attention head to capture\nmulti-frequency information along the sequence dimension and is widely applied\nin foundation models. However, the nonlinearity introduced by RoPE complicates\noptimization of the key state in the Key-Value (KV) cache for RoPE-based\nattention. Existing KV cache compression methods typically store key state\nbefore rotation and apply the transformation during decoding, introducing\nadditional computational overhead. This paper introduces EliteKV, a flexible\nmodification framework for RoPE-based models supporting variable KV cache\ncompression ratios. EliteKV first identifies the intrinsic frequency preference\nof each head using RoPElite, selectively restoring linearity to certain\ndimensions of key within attention computation. Building on this, joint\nlow-rank compression of key and value enables partial cache sharing.\nExperimental results show that with minimal uptraining on only $0.6\\%$ of the\noriginal training data, RoPE-based models achieve a $75\\%$ reduction in KV\ncache size while preserving performance within a negligible margin.\nFurthermore, EliteKV consistently performs well across models of different\nscales within the same family.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotary Position Embedding (RoPE) enables each attention head to capture\nmulti-frequency information along the sequence dimension and is widely applied\nin foundation models. However, the nonlinearity introduced by RoPE complicates\noptimization of the key state in the Key-Value (KV) cache for RoPE-based\nattention. Existing KV cache compression methods typically store key state\nbefore rotation and apply the transformation during decoding, introducing\nadditional computational overhead. This paper introduces EliteKV, a flexible\nmodification framework for RoPE-based models supporting variable KV cache\ncompression ratios. EliteKV first identifies the intrinsic frequency preference\nof each head using RoPElite, selectively restoring linearity to certain\ndimensions of key within attention computation. Building on this, joint\nlow-rank compression of key and value enables partial cache sharing.\nExperimental results show that with minimal uptraining on only $0.6\\%$ of the\noriginal training data, RoPE-based models achieve a $75\\%$ reduction in KV\ncache size while preserving performance within a negligible margin.\nFurthermore, EliteKV consistently performs well across models of different\nscales within the same family."
                },
                "authors": [
                    {
                        "name": "Yuhao Zhou"
                    },
                    {
                        "name": "Sirui Song"
                    },
                    {
                        "name": "Boyang Liu"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Senjie Jin"
                    },
                    {
                        "name": "Xiaoran Fan"
                    },
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "13 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01483v1",
                "updated": "2025-03-03T12:43:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    12,
                    43,
                    6,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T12:43:06Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    12,
                    43,
                    6,
                    0,
                    62,
                    0
                ],
                "title": "KurTail : Kurtosis-based LLM Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KurTail : Kurtosis-based LLM Quantization"
                },
                "summary": "One of the challenges of quantizing a large language model (LLM) is the\npresence of outliers. Outliers often make uniform quantization schemes less\neffective, particularly in extreme cases such as 4-bit quantization. We\nintroduce KurTail, a new post-training quantization (PTQ) scheme that leverages\nKurtosis-based rotation to mitigate outliers in the activations of LLMs. Our\nmethod optimizes Kurtosis as a measure of tailedness. This approach enables the\nquantization of weights, activations, and the KV cache in 4 bits. We utilize\nlayer-wise optimization, ensuring memory efficiency. KurTail outperforms\nexisting quantization methods, offering a 13.3\\% boost in MMLU accuracy and a\n15.5\\% drop in Wiki perplexity compared to QuaRot. It also outperforms\nSpinQuant with a 2.6\\% MMLU gain and reduces perplexity by 2.9\\%, all while\nreducing the training cost. For comparison, learning the rotation using\nSpinQuant for Llama3-70B requires at least four NVIDIA H100 80GB GPUs, whereas\nour method requires only a single GPU, making it a more accessible solution for\nconsumer GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the challenges of quantizing a large language model (LLM) is the\npresence of outliers. Outliers often make uniform quantization schemes less\neffective, particularly in extreme cases such as 4-bit quantization. We\nintroduce KurTail, a new post-training quantization (PTQ) scheme that leverages\nKurtosis-based rotation to mitigate outliers in the activations of LLMs. Our\nmethod optimizes Kurtosis as a measure of tailedness. This approach enables the\nquantization of weights, activations, and the KV cache in 4 bits. We utilize\nlayer-wise optimization, ensuring memory efficiency. KurTail outperforms\nexisting quantization methods, offering a 13.3\\% boost in MMLU accuracy and a\n15.5\\% drop in Wiki perplexity compared to QuaRot. It also outperforms\nSpinQuant with a 2.6\\% MMLU gain and reduces perplexity by 2.9\\%, all while\nreducing the training cost. For comparison, learning the rotation using\nSpinQuant for Llama3-70B requires at least four NVIDIA H100 80GB GPUs, whereas\nour method requires only a single GPU, making it a more accessible solution for\nconsumer GPU."
                },
                "authors": [
                    {
                        "name": "Mohammad Sadegh Akhondzadeh"
                    },
                    {
                        "name": "Aleksandar Bojchevski"
                    },
                    {
                        "name": "Evangelos Eleftheriou"
                    },
                    {
                        "name": "Martino Dazzi"
                    }
                ],
                "author_detail": {
                    "name": "Martino Dazzi"
                },
                "author": "Martino Dazzi",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01348v1",
                "updated": "2025-03-03T09:38:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    38,
                    20,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T09:38:20Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    38,
                    20,
                    0,
                    62,
                    0
                ],
                "title": "Performance Optimization of 3D Stencil Computation on ARM Scalable\n  Vector Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Optimization of 3D Stencil Computation on ARM Scalable\n  Vector Extension"
                },
                "summary": "Stencil computation is essential in high-performance computing, especially\nfor large-scale tasks like liquid simulation and weather forecasting.\nOptimizing its performance can reduce both energy consumption and computation\ntime, which is critical in disaster prediction. This paper explores\noptimization techniques for 7-point 3D stencil computation on ARM's Scalable\nVector Extension (SVE), using the Roofline model and tools like Gem5 and cacti.\nWe evaluate software optimizations such as vectorization and tiling, as well as\nhardware adjustments in ARM SVE vector lengths and cache configurations. The\nstudy also examines performance, power consumption, and chip area trade-offs to\nidentify optimal configurations for ARM-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stencil computation is essential in high-performance computing, especially\nfor large-scale tasks like liquid simulation and weather forecasting.\nOptimizing its performance can reduce both energy consumption and computation\ntime, which is critical in disaster prediction. This paper explores\noptimization techniques for 7-point 3D stencil computation on ARM's Scalable\nVector Extension (SVE), using the Roofline model and tools like Gem5 and cacti.\nWe evaluate software optimizations such as vectorization and tiling, as well as\nhardware adjustments in ARM SVE vector lengths and cache configurations. The\nstudy also examines performance, power consumption, and chip area trade-offs to\nidentify optimal configurations for ARM-based systems."
                },
                "authors": [
                    {
                        "name": "Hongguang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hongguang Chen"
                },
                "author": "Hongguang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01330v1",
                "updated": "2025-03-03T09:12:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    12,
                    34,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T09:12:34Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    12,
                    34,
                    0,
                    62,
                    0
                ],
                "title": "WeightedKV: Attention Scores Weighted Key-Value Cache Merging for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WeightedKV: Attention Scores Weighted Key-Value Cache Merging for Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) use key-value (KV) cache to reduce redundant\ncomputation in autoregressive generation. However, the KV cache size increases\nlinearly during generation, leading to excessive memory usage, especially for\nlong texts. Most KV cache compression methods evict the unimportant KV pairs to\nmaintain a fixed cache size, which leads to the permanent loss of tokens during\ngeneration. However, singular value decomposition shows that \\textit{values} do\nnot exhibit a strong low-rank property as \\textit{keys} do, suggesting that\ninformation is distributed more evenly across \\textit{values}, in contrast to\nits more redundant distribution within \\textit{keys}. Therefore, methods that\nevict both \\textit{keys} and \\textit{values} risk losing crucial information\nand compromise context integrity, ultimately degrading the output quality. To\naddress this problem, we propose WeightedKV, a novel, training-free approach\nthat discards the \\textit{keys} of less important tokens, while merging their\n\\textit{values} into neighboring tokens via a convex combination weighted by\ntheir average attention scores. In this way, the retained \\textit{keys} serve\nas anchors that guide the generation process, while the merged \\textit{values}\nprovide a rich contextual backdrop. We assess our method on four widely used\nlanguage modeling datasets, demonstrating superior performance compared to all\nbaseline methods, particularly with a lower budget ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) use key-value (KV) cache to reduce redundant\ncomputation in autoregressive generation. However, the KV cache size increases\nlinearly during generation, leading to excessive memory usage, especially for\nlong texts. Most KV cache compression methods evict the unimportant KV pairs to\nmaintain a fixed cache size, which leads to the permanent loss of tokens during\ngeneration. However, singular value decomposition shows that \\textit{values} do\nnot exhibit a strong low-rank property as \\textit{keys} do, suggesting that\ninformation is distributed more evenly across \\textit{values}, in contrast to\nits more redundant distribution within \\textit{keys}. Therefore, methods that\nevict both \\textit{keys} and \\textit{values} risk losing crucial information\nand compromise context integrity, ultimately degrading the output quality. To\naddress this problem, we propose WeightedKV, a novel, training-free approach\nthat discards the \\textit{keys} of less important tokens, while merging their\n\\textit{values} into neighboring tokens via a convex combination weighted by\ntheir average attention scores. In this way, the retained \\textit{keys} serve\nas anchors that guide the generation process, while the merged \\textit{values}\nprovide a rich contextual backdrop. We assess our method on four widely used\nlanguage modeling datasets, demonstrating superior performance compared to all\nbaseline methods, particularly with a lower budget ratio."
                },
                "authors": [
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01323v1",
                "updated": "2025-03-03T09:04:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    4,
                    51,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T09:04:51Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    4,
                    51,
                    0,
                    62,
                    0
                ],
                "title": "CacheQuant: Comprehensively Accelerated Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheQuant: Comprehensively Accelerated Diffusion Models"
                },
                "summary": "Diffusion models have gradually gained prominence in the field of image\nsynthesis, showcasing remarkable generative capabilities. Nevertheless, the\nslow inference and complex networks, resulting from redundancy at both temporal\nand structural levels, hinder their low-latency applications in real-world\nscenarios. Current acceleration methods for diffusion models focus separately\non temporal and structural levels. However, independent optimization at each\nlevel to further push the acceleration limits results in significant\nperformance degradation. On the other hand, integrating optimizations at both\nlevels can compound the acceleration effects. Unfortunately, we find that the\noptimizations at these two levels are not entirely orthogonal. Performing\nseparate optimizations and then simply integrating them results in\nunsatisfactory performance. To tackle this issue, we propose CacheQuant, a\nnovel training-free paradigm that comprehensively accelerates diffusion models\nby jointly optimizing model caching and quantization techniques. Specifically,\nwe employ a dynamic programming approach to determine the optimal cache\nschedule, in which the properties of caching and quantization are carefully\nconsidered to minimize errors. Additionally, we propose decoupled error\ncorrection to further mitigate the coupled and accumulated errors step by step.\nExperimental results show that CacheQuant achieves a 5.18 speedup and 4\ncompression for Stable Diffusion on MS-COCO, with only a 0.02 loss in CLIP\nscore. Our code are open-sourced: https://github.com/BienLuky/CacheQuant .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have gradually gained prominence in the field of image\nsynthesis, showcasing remarkable generative capabilities. Nevertheless, the\nslow inference and complex networks, resulting from redundancy at both temporal\nand structural levels, hinder their low-latency applications in real-world\nscenarios. Current acceleration methods for diffusion models focus separately\non temporal and structural levels. However, independent optimization at each\nlevel to further push the acceleration limits results in significant\nperformance degradation. On the other hand, integrating optimizations at both\nlevels can compound the acceleration effects. Unfortunately, we find that the\noptimizations at these two levels are not entirely orthogonal. Performing\nseparate optimizations and then simply integrating them results in\nunsatisfactory performance. To tackle this issue, we propose CacheQuant, a\nnovel training-free paradigm that comprehensively accelerates diffusion models\nby jointly optimizing model caching and quantization techniques. Specifically,\nwe employ a dynamic programming approach to determine the optimal cache\nschedule, in which the properties of caching and quantization are carefully\nconsidered to minimize errors. Additionally, we propose decoupled error\ncorrection to further mitigate the coupled and accumulated errors step by step.\nExperimental results show that CacheQuant achieves a 5.18 speedup and 4\ncompression for Stable Diffusion on MS-COCO, with only a 0.02 loss in CLIP\nscore. Our code are open-sourced: https://github.com/BienLuky/CacheQuant ."
                },
                "authors": [
                    {
                        "name": "Xuewen Liu"
                    },
                    {
                        "name": "Zhikai Li"
                    },
                    {
                        "name": "Qingyi Gu"
                    }
                ],
                "author_detail": {
                    "name": "Qingyi Gu"
                },
                "author": "Qingyi Gu",
                "arxiv_comment": "CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01281v1",
                "updated": "2025-03-03T08:06:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    8,
                    6,
                    55,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T08:06:55Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    8,
                    6,
                    55,
                    0,
                    62,
                    0
                ],
                "title": "DCI: A Coordinated Allocation and Filling Workload-Aware Dual-Cache\n  Allocation GNN Inference Acceleration System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DCI: A Coordinated Allocation and Filling Workload-Aware Dual-Cache\n  Allocation GNN Inference Acceleration System"
                },
                "summary": "Graph Neural Networks (GNNs) are powerful tools for processing\ngraph-structured data, increasingly used for large-scale real-world graphs via\nsampling-based inference methods. However, inherent characteristics of neighbor\nsampling lead to redundant data loading during GNN inference, compounded by\ninefficient data transfers between host and GPU memory, resulting in slow\ninference and low resource utilization. Existing methods to accelerate GNN\ninference face several challenges: (1) low practical GPU memory utilization,\n(2) overlooking adjacency matrix locality, and (3) long preprocessing time. To\naddress these challenges, we introduce DCI, an efficient workload-aware\ndual-cache allocation system for GNN inference acceleration. DCI allocates\ncache capacities for both node features and adjacency matrices based on\nworkload patterns during the pre-sampling phase, leveraging a lightweight\ncache-filling algorithm to optimize data loading efficiency. Experimental\nresults demonstrate that DCI accelerates sampling and node feature loading,\nachieving end-to-end inference speedups of 1.18$\\times$ to 11.26$\\times$\ncompared to DGL, and 1.14$\\times$ to 13.68$\\times$ over RAIN, while reducing\npreprocessing time by 52.8\\% to 98.7\\%. Additionally, DCI outperforms\nstate-of-the-art single-cache inference systems by achieving speedup of\n1.08$\\times$ to 1.32$\\times$. We also compared DCI with DUCATI's dual-cache\npopulation strategy. Our lightweight population algorithm allows DCI to achieve\nnearly the same inference speed while keeping preprocessing time to less than\n20\\% of that required by DUCATI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) are powerful tools for processing\ngraph-structured data, increasingly used for large-scale real-world graphs via\nsampling-based inference methods. However, inherent characteristics of neighbor\nsampling lead to redundant data loading during GNN inference, compounded by\ninefficient data transfers between host and GPU memory, resulting in slow\ninference and low resource utilization. Existing methods to accelerate GNN\ninference face several challenges: (1) low practical GPU memory utilization,\n(2) overlooking adjacency matrix locality, and (3) long preprocessing time. To\naddress these challenges, we introduce DCI, an efficient workload-aware\ndual-cache allocation system for GNN inference acceleration. DCI allocates\ncache capacities for both node features and adjacency matrices based on\nworkload patterns during the pre-sampling phase, leveraging a lightweight\ncache-filling algorithm to optimize data loading efficiency. Experimental\nresults demonstrate that DCI accelerates sampling and node feature loading,\nachieving end-to-end inference speedups of 1.18$\\times$ to 11.26$\\times$\ncompared to DGL, and 1.14$\\times$ to 13.68$\\times$ over RAIN, while reducing\npreprocessing time by 52.8\\% to 98.7\\%. Additionally, DCI outperforms\nstate-of-the-art single-cache inference systems by achieving speedup of\n1.08$\\times$ to 1.32$\\times$. We also compared DCI with DUCATI's dual-cache\npopulation strategy. Our lightweight population algorithm allows DCI to achieve\nnearly the same inference speed while keeping preprocessing time to less than\n20\\% of that required by DUCATI."
                },
                "authors": [
                    {
                        "name": "Yi Luo"
                    },
                    {
                        "name": "Yaobin Wang"
                    },
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Yingchen Song"
                    },
                    {
                        "name": "Huan Wu"
                    },
                    {
                        "name": "Qingfeng Wang"
                    },
                    {
                        "name": "Jun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Huang"
                },
                "author": "Jun Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v2",
                "updated": "2025-03-03T05:49:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    5,
                    49,
                    41,
                    0,
                    62,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has driven growing\ndemand for processing extended context sequences in contemporary applications.\nHowever, this progress faces two major challenges: performance degradation due\nto sequence lengths out-of-distribution, and excessively long inference times\ncaused by the quadratic computational complexity of attention. These issues\nhinder the application of LLMs in long-context scenarios. In this paper, we\npropose Dynamic Token-Level KV Cache Selection (TokenSelect), a training-free\nmethod for efficient and accurate long-context inference. TokenSelect builds\nupon the observation of non-contiguous attention sparsity, using Query-Key dot\nproducts to measure per-head KV Cache criticality at token-level. By per-head\nsoft voting mechanism, TokenSelect selectively involves a few critical KV cache\ntokens in attention calculation without sacrificing accuracy. To further\naccelerate TokenSelect, we design the Selection Cache based on observations of\nconsecutive Query similarity and implemented efficient dot product kernel,\nsignificantly reducing the overhead. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has driven growing\ndemand for processing extended context sequences in contemporary applications.\nHowever, this progress faces two major challenges: performance degradation due\nto sequence lengths out-of-distribution, and excessively long inference times\ncaused by the quadratic computational complexity of attention. These issues\nhinder the application of LLMs in long-context scenarios. In this paper, we\npropose Dynamic Token-Level KV Cache Selection (TokenSelect), a training-free\nmethod for efficient and accurate long-context inference. TokenSelect builds\nupon the observation of non-contiguous attention sparsity, using Query-Key dot\nproducts to measure per-head KV Cache criticality at token-level. By per-head\nsoft voting mechanism, TokenSelect selectively involves a few critical KV cache\ntokens in attention calculation without sacrificing accuracy. To further\naccelerate TokenSelect, we design the Selection Cache based on observations of\nconsecutive Query similarity and implemented efficient dot product kernel,\nsignificantly reducing the overhead. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Tianfu Wang"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00979v1",
                "updated": "2025-03-02T18:12:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    18,
                    12,
                    50,
                    6,
                    61,
                    0
                ],
                "published": "2025-03-02T18:12:50Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    18,
                    12,
                    50,
                    6,
                    61,
                    0
                ],
                "title": "Dialogue Without Limits: Constant-Sized KV Caches for Extended Responses\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialogue Without Limits: Constant-Sized KV Caches for Extended Responses\n  in LLMs"
                },
                "summary": "Autoregressive Transformers rely on Key-Value (KV) caching to accelerate\ninference. However, the linear growth of the KV cache with context length leads\nto excessive memory consumption and bandwidth constraints. This bottleneck is\nparticularly problematic in real-time applications -- such as chatbots and\ninteractive assistants -- where low latency and high memory efficiency are\ncritical. Existing methods drop distant tokens or compress states in a lossy\nmanner, sacrificing accuracy by discarding vital context or introducing bias.\n  We propose MorphKV, an inference-time technique that maintains a\nconstant-sized KV cache while preserving accuracy. MorphKV balances long-range\ndependencies and local coherence during text generation. It eliminates\nearly-token bias while retaining high-fidelity context by adaptively ranking\ntokens through correlation-aware selection. Unlike heuristic retention or lossy\ncompression, MorphKV iteratively refines the KV cache via lightweight updates\nguided by attention patterns of recent tokens. This approach captures\ninter-token correlation with greater accuracy, crucial for tasks like content\ncreation and code generation. Our studies on long-response tasks show 52.9$\\%$\nmemory savings and 18.2$\\%$ higher accuracy on average compared to\nstate-of-the-art prior works, enabling efficient real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Transformers rely on Key-Value (KV) caching to accelerate\ninference. However, the linear growth of the KV cache with context length leads\nto excessive memory consumption and bandwidth constraints. This bottleneck is\nparticularly problematic in real-time applications -- such as chatbots and\ninteractive assistants -- where low latency and high memory efficiency are\ncritical. Existing methods drop distant tokens or compress states in a lossy\nmanner, sacrificing accuracy by discarding vital context or introducing bias.\n  We propose MorphKV, an inference-time technique that maintains a\nconstant-sized KV cache while preserving accuracy. MorphKV balances long-range\ndependencies and local coherence during text generation. It eliminates\nearly-token bias while retaining high-fidelity context by adaptively ranking\ntokens through correlation-aware selection. Unlike heuristic retention or lossy\ncompression, MorphKV iteratively refines the KV cache via lightweight updates\nguided by attention patterns of recent tokens. This approach captures\ninter-token correlation with greater accuracy, crucial for tasks like content\ncreation and code generation. Our studies on long-response tasks show 52.9$\\%$\nmemory savings and 18.2$\\%$ higher accuracy on average compared to\nstate-of-the-art prior works, enabling efficient real-world deployment."
                },
                "authors": [
                    {
                        "name": "Ravi Ghadia"
                    },
                    {
                        "name": "Avinash Kumar"
                    },
                    {
                        "name": "Gaurav Jain"
                    },
                    {
                        "name": "Prashant Nair"
                    },
                    {
                        "name": "Poulami Das"
                    }
                ],
                "author_detail": {
                    "name": "Poulami Das"
                },
                "author": "Poulami Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10781v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10781v2",
                "updated": "2025-03-02T14:37:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    14,
                    37,
                    53,
                    6,
                    61,
                    0
                ],
                "published": "2024-10-14T17:50:28Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    28,
                    0,
                    288,
                    0
                ],
                "title": "When Attention Sink Emerges in Language Models: An Empirical View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Attention Sink Emerges in Language Models: An Empirical View"
                },
                "summary": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink."
                },
                "authors": [
                    {
                        "name": "Xiangming Gu"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "arxiv_comment": "ICLR 2025 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10781v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10781v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00695v1",
                "updated": "2025-03-02T02:26:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    2,
                    26,
                    21,
                    6,
                    61,
                    0
                ],
                "published": "2025-03-02T02:26:21Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    2,
                    26,
                    21,
                    6,
                    61,
                    0
                ],
                "title": "MoSFormer: Augmenting Temporal Context with Memory of Surgery for\n  Surgical Phase Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoSFormer: Augmenting Temporal Context with Memory of Surgery for\n  Surgical Phase Recognition"
                },
                "summary": "Surgical phase recognition from video enables various downstream\napplications. Transformer-based sliding window approaches have set the\nstate-of-the-art by capturing rich spatial-temporal features. However, while\ntransformers can theoretically handle arbitrary-length sequences, in practice\nthey are limited by memory and compute constraints, resulting in fixed context\nwindows that struggle with maintaining temporal consistency across lengthy\nsurgical procedures. This often leads to fragmented predictions and limited\nprocedure-level understanding. To address these challenges, we propose Memory\nof Surgery (MoS), a framework that enriches temporal modeling by incorporating\nboth semantic interpretable long-term surgical history and short-term\nimpressions. MoSFormer, our enhanced transformer architecture, integrates MoS\nusing a carefully designed encoding and fusion mechanism. We further introduce\nstep filtering to refine history representation and develop a memory caching\npipeline to improve training and inference stability, mitigating shortcut\nlearning and overfitting. MoSFormer demonstrates state-of-the-art performance\non multiple benchmarks. On the Challenging BernBypass70 benchmark, it attains\n88.0 video-level accuracy and phase-level metrics of 70.7 precision, 68.7\nrecall, and 66.3 F1 score, outperforming its baseline with 2.1 video-level\naccuracy and phase-level metrics of 4.6 precision, 3.6 recall, and 3.8 F1\nscore. Further studies confirms the individual and combined benefits of\nlong-term and short-term memory components through ablation and counterfactual\ninference. Qualitative results shows improved temporal consistency. The\naugmented temporal context enables procedure-level understanding, paving the\nway for more comprehensive surgical video analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surgical phase recognition from video enables various downstream\napplications. Transformer-based sliding window approaches have set the\nstate-of-the-art by capturing rich spatial-temporal features. However, while\ntransformers can theoretically handle arbitrary-length sequences, in practice\nthey are limited by memory and compute constraints, resulting in fixed context\nwindows that struggle with maintaining temporal consistency across lengthy\nsurgical procedures. This often leads to fragmented predictions and limited\nprocedure-level understanding. To address these challenges, we propose Memory\nof Surgery (MoS), a framework that enriches temporal modeling by incorporating\nboth semantic interpretable long-term surgical history and short-term\nimpressions. MoSFormer, our enhanced transformer architecture, integrates MoS\nusing a carefully designed encoding and fusion mechanism. We further introduce\nstep filtering to refine history representation and develop a memory caching\npipeline to improve training and inference stability, mitigating shortcut\nlearning and overfitting. MoSFormer demonstrates state-of-the-art performance\non multiple benchmarks. On the Challenging BernBypass70 benchmark, it attains\n88.0 video-level accuracy and phase-level metrics of 70.7 precision, 68.7\nrecall, and 66.3 F1 score, outperforming its baseline with 2.1 video-level\naccuracy and phase-level metrics of 4.6 precision, 3.6 recall, and 3.8 F1\nscore. Further studies confirms the individual and combined benefits of\nlong-term and short-term memory components through ablation and counterfactual\ninference. Qualitative results shows improved temporal consistency. The\naugmented temporal context enables procedure-level understanding, paving the\nway for more comprehensive surgical video analysis."
                },
                "authors": [
                    {
                        "name": "Hao Ding"
                    },
                    {
                        "name": "Xu Lian"
                    },
                    {
                        "name": "Mathias Unberath"
                    }
                ],
                "author_detail": {
                    "name": "Mathias Unberath"
                },
                "author": "Mathias Unberath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07295v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07295v2",
                "updated": "2025-03-02T01:39:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    1,
                    39,
                    57,
                    6,
                    61,
                    0
                ],
                "published": "2024-10-09T16:21:38Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    21,
                    38,
                    2,
                    283,
                    0
                ],
                "title": "IterGen: Iterative Semantic-aware Structured LLM Generation with\n  Backtracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IterGen: Iterative Semantic-aware Structured LLM Generation with\n  Backtracking"
                },
                "summary": "Large Language Models (LLMs) are widely used for tasks such as natural\nlanguage and code generation, but their outputs often suffer from issues like\nhallucination, toxicity, and incorrect results. Current libraries for\nstructured LLM generation rely on left-to-right decoding without support for\nbacktracking, limiting the ability to correct or refine outputs mid-generation.\n  To address this, we introduce IterGen, a user-friendly library for iterative,\ngrammar-guided LLM generation that enables users to move both forward and\nbackward within the generated output based on grammar symbols. By leveraging a\nsymbol-to-position mapping and maintaining the key-value (KV) cache state,\nIterGen ensures efficient and structured generation while allowing for\ncorrections during the process. We demonstrate IterGen's effectiveness in two\nimportant applications: reducing privacy leakage in LLM outputs and improving\nthe accuracy of LLM-generated SQL and Vega-Lite queries.\n  Our code and additional resources are available at https://structuredllm.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used for tasks such as natural\nlanguage and code generation, but their outputs often suffer from issues like\nhallucination, toxicity, and incorrect results. Current libraries for\nstructured LLM generation rely on left-to-right decoding without support for\nbacktracking, limiting the ability to correct or refine outputs mid-generation.\n  To address this, we introduce IterGen, a user-friendly library for iterative,\ngrammar-guided LLM generation that enables users to move both forward and\nbackward within the generated output based on grammar symbols. By leveraging a\nsymbol-to-position mapping and maintaining the key-value (KV) cache state,\nIterGen ensures efficient and structured generation while allowing for\ncorrections during the process. We demonstrate IterGen's effectiveness in two\nimportant applications: reducing privacy leakage in LLM outputs and improving\nthe accuracy of LLM-generated SQL and Vega-Lite queries.\n  Our code and additional resources are available at https://structuredllm.com."
                },
                "authors": [
                    {
                        "name": "Shubham Ugare"
                    },
                    {
                        "name": "Rohan Gumaste"
                    },
                    {
                        "name": "Tarun Suresh"
                    },
                    {
                        "name": "Gagandeep Singh"
                    },
                    {
                        "name": "Sasa Misailovic"
                    }
                ],
                "author_detail": {
                    "name": "Sasa Misailovic"
                },
                "author": "Sasa Misailovic",
                "arxiv_comment": "Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07295v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07295v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00540v1",
                "updated": "2025-03-01T15:53:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    15,
                    53,
                    33,
                    5,
                    60,
                    0
                ],
                "published": "2025-03-01T15:53:33Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    15,
                    53,
                    33,
                    5,
                    60,
                    0
                ],
                "title": "Streaming Video Question-Answering with In-context Video KV-Cache\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming Video Question-Answering with In-context Video KV-Cache\n  Retrieval"
                },
                "summary": "We propose ReKV, a novel training-free approach that enables efficient\nstreaming video question-answering (StreamingVQA), by seamlessly integrating\nwith existing Video Large Language Models (Video-LLMs). Traditional VideoQA\nsystems struggle with long videos, as they must process entire videos before\nresponding to queries, and repeat this process for each new question. In\ncontrast, our approach analyzes long videos in a streaming manner, allowing for\nprompt responses as soon as user queries are received. Building on a common\nVideo-LLM, we first incorporate a sliding-window attention mechanism, ensuring\nthat input frames attend to a limited number of preceding frames, thereby\nreducing computational overhead. To prevent information loss, we store\nprocessed video key-value caches (KV-Caches) in RAM and disk, reloading them\ninto GPU memory as needed. Additionally, we introduce a retrieval method that\nleverages an external retriever or the parameters within Video-LLMs to retrieve\nonly query-relevant KV-Caches, ensuring both efficiency and accuracy in\nquestion answering. ReKV enables the separation of video encoding and\nquestion-answering across different processes and GPUs, significantly enhancing\nthe efficiency of StreamingVQA. Through comprehensive experimentation, we\nvalidate the efficacy and practicality of our approach, which significantly\nboosts efficiency and enhances applicability over existing VideoQA models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose ReKV, a novel training-free approach that enables efficient\nstreaming video question-answering (StreamingVQA), by seamlessly integrating\nwith existing Video Large Language Models (Video-LLMs). Traditional VideoQA\nsystems struggle with long videos, as they must process entire videos before\nresponding to queries, and repeat this process for each new question. In\ncontrast, our approach analyzes long videos in a streaming manner, allowing for\nprompt responses as soon as user queries are received. Building on a common\nVideo-LLM, we first incorporate a sliding-window attention mechanism, ensuring\nthat input frames attend to a limited number of preceding frames, thereby\nreducing computational overhead. To prevent information loss, we store\nprocessed video key-value caches (KV-Caches) in RAM and disk, reloading them\ninto GPU memory as needed. Additionally, we introduce a retrieval method that\nleverages an external retriever or the parameters within Video-LLMs to retrieve\nonly query-relevant KV-Caches, ensuring both efficiency and accuracy in\nquestion answering. ReKV enables the separation of video encoding and\nquestion-answering across different processes and GPUs, significantly enhancing\nthe efficiency of StreamingVQA. Through comprehensive experimentation, we\nvalidate the efficacy and practicality of our approach, which significantly\nboosts efficiency and enhances applicability over existing VideoQA models."
                },
                "authors": [
                    {
                        "name": "Shangzhe Di"
                    },
                    {
                        "name": "Zhelun Yu"
                    },
                    {
                        "name": "Guanghao Zhang"
                    },
                    {
                        "name": "Haoyuan Li"
                    },
                    {
                        "name": "Tao Zhong"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Bolin Li"
                    },
                    {
                        "name": "Wanggui He"
                    },
                    {
                        "name": "Fangxun Shu"
                    },
                    {
                        "name": "Hao Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Jiang"
                },
                "author": "Hao Jiang",
                "arxiv_comment": "Accepted to ICLR 2025. Code: https://github.com/Becomebright/ReKV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00392v1",
                "updated": "2025-03-01T07:56:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    7,
                    56,
                    42,
                    5,
                    60,
                    0
                ],
                "published": "2025-03-01T07:56:42Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    7,
                    56,
                    42,
                    5,
                    60,
                    0
                ],
                "title": "Progressive Sparse Attention: Algorithm and System Co-design for\n  Efficient Attention in LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progressive Sparse Attention: Algorithm and System Co-design for\n  Efficient Attention in LLM Serving"
                },
                "summary": "Processing long contexts has become a critical capability for modern large\nlanguage models (LLMs). However, serving long-context LLMs comes with\nsignificant inference costs due to the high memory overhead of the key-value\n(KV) cache. Existing work leverages dynamic sparse attention algorithms (DSAes)\nto mitigate the KV cache overhead, but these algorithms rely on top-$k$ KV\ncache selection, which results in a trade-off between accuracy and efficiency.\nA larger $k$ improves accuracy but decreases efficiency, while a smaller $k$\nboosts efficiency but compromises accuracy. To overcome this trade-off, this\npaper presents PSA, a $\\underline{P}$rogressive $\\underline{S}$parse\n$\\underline{A}$ttention mechanism that integrates algorithmic innovations with\nsystem co-design to achieve both high inference accuracy and improved\nefficiency in LLM serving. The PSA algorithm adaptively adjusts the KV cache\nbudget of different tokens and layers according to their real attention weight\ndistributions, rather than relying on a fixed budget $k$. This enables high\naccuracy while minimizing KV cache usage. To further enhance execution\nefficiency, we introduce a pipelined iteration scheme that reduces CPU-GPU\ninterleaving and synchronization overhead during PSA computation. Additionally,\nwe implement unified GPU memory management that optimizes PSA's memory\nutilization by accounting for uneven memory requirements across different model\nlayers. Extensive experimental results demonstrate that PSA reduces KV cache\nusage for attention computation by up to 2.4$\\times$ and 8.8$\\times$, and\nincreases end-to-end serving throughput by up to 1.4$\\times$ and 2.0$\\times$,\ncompared to state-of-the-art DSAes and systems without sparse attention,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts has become a critical capability for modern large\nlanguage models (LLMs). However, serving long-context LLMs comes with\nsignificant inference costs due to the high memory overhead of the key-value\n(KV) cache. Existing work leverages dynamic sparse attention algorithms (DSAes)\nto mitigate the KV cache overhead, but these algorithms rely on top-$k$ KV\ncache selection, which results in a trade-off between accuracy and efficiency.\nA larger $k$ improves accuracy but decreases efficiency, while a smaller $k$\nboosts efficiency but compromises accuracy. To overcome this trade-off, this\npaper presents PSA, a $\\underline{P}$rogressive $\\underline{S}$parse\n$\\underline{A}$ttention mechanism that integrates algorithmic innovations with\nsystem co-design to achieve both high inference accuracy and improved\nefficiency in LLM serving. The PSA algorithm adaptively adjusts the KV cache\nbudget of different tokens and layers according to their real attention weight\ndistributions, rather than relying on a fixed budget $k$. This enables high\naccuracy while minimizing KV cache usage. To further enhance execution\nefficiency, we introduce a pipelined iteration scheme that reduces CPU-GPU\ninterleaving and synchronization overhead during PSA computation. Additionally,\nwe implement unified GPU memory management that optimizes PSA's memory\nutilization by accounting for uneven memory requirements across different model\nlayers. Extensive experimental results demonstrate that PSA reduces KV cache\nusage for attention computation by up to 2.4$\\times$ and 8.8$\\times$, and\nincreases end-to-end serving throughput by up to 1.4$\\times$ and 2.0$\\times$,\ncompared to state-of-the-art DSAes and systems without sparse attention,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Qihui Zhou"
                    },
                    {
                        "name": "Peiqi Yin"
                    },
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "James Cheng"
                    }
                ],
                "author_detail": {
                    "name": "James Cheng"
                },
                "author": "James Cheng",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.13447v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13447v1",
                "updated": "2025-03-17T17:59:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    59,
                    54,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T17:59:54Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    59,
                    54,
                    0,
                    76,
                    0
                ],
                "title": "MetaScale: Test-Time Scaling with Evolving Meta-Thoughts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaScale: Test-Time Scaling with Evolving Meta-Thoughts"
                },
                "summary": "One critical challenge for large language models (LLMs) for making complex\nreasoning is their reliance on matching reasoning patterns from training data,\ninstead of proactively selecting the most appropriate cognitive strategy to\nsolve a given task. Existing approaches impose fixed cognitive structures that\nenhance performance in specific tasks but lack adaptability across diverse\nscenarios. To address this limitation, we introduce METASCALE, a test-time\nscaling framework based on meta-thoughts -- adaptive thinking strategies\ntailored to each task. METASCALE initializes a pool of candidate meta-thoughts,\nthen iteratively selects and evaluates them using a multi-armed bandit\nalgorithm with upper confidence bound selection, guided by a reward model. To\nfurther enhance adaptability, a genetic algorithm evolves high-reward\nmeta-thoughts, refining and extending the strategy pool over time. By\ndynamically proposing and optimizing meta-thoughts at inference time, METASCALE\nimproves both accuracy and generalization across a wide range of tasks.\nExperimental results demonstrate that MetaScale consistently outperforms\nstandard inference approaches, achieving an 11% performance gain in win rate on\nArena-Hard for GPT-4o, surpassing o1-mini by 0.9% under style control. Notably,\nMETASCALE scales more effectively with increasing sampling budgets and produces\nmore structured, expert-level responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One critical challenge for large language models (LLMs) for making complex\nreasoning is their reliance on matching reasoning patterns from training data,\ninstead of proactively selecting the most appropriate cognitive strategy to\nsolve a given task. Existing approaches impose fixed cognitive structures that\nenhance performance in specific tasks but lack adaptability across diverse\nscenarios. To address this limitation, we introduce METASCALE, a test-time\nscaling framework based on meta-thoughts -- adaptive thinking strategies\ntailored to each task. METASCALE initializes a pool of candidate meta-thoughts,\nthen iteratively selects and evaluates them using a multi-armed bandit\nalgorithm with upper confidence bound selection, guided by a reward model. To\nfurther enhance adaptability, a genetic algorithm evolves high-reward\nmeta-thoughts, refining and extending the strategy pool over time. By\ndynamically proposing and optimizing meta-thoughts at inference time, METASCALE\nimproves both accuracy and generalization across a wide range of tasks.\nExperimental results demonstrate that MetaScale consistently outperforms\nstandard inference approaches, achieving an 11% performance gain in win rate on\nArena-Hard for GPT-4o, surpassing o1-mini by 0.9% under style control. Notably,\nMETASCALE scales more effectively with increasing sampling budgets and produces\nmore structured, expert-level responses."
                },
                "authors": [
                    {
                        "name": "Qin Liu"
                    },
                    {
                        "name": "Wenxuan Zhou"
                    },
                    {
                        "name": "Nan Xu"
                    },
                    {
                        "name": "James Y. Huang"
                    },
                    {
                        "name": "Fei Wang"
                    },
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Hoifung Poon"
                    },
                    {
                        "name": "Muhao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Muhao Chen"
                },
                "author": "Muhao Chen",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13447v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13447v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13445v1",
                "updated": "2025-03-17T17:59:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    59,
                    39,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T17:59:39Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    59,
                    39,
                    0,
                    76,
                    0
                ],
                "title": "Faithfulness of LLM Self-Explanations for Commonsense Tasks: Larger Is\n  Better, and Instruction-Tuning Allows Trade-Offs but Not Pareto Dominance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faithfulness of LLM Self-Explanations for Commonsense Tasks: Larger Is\n  Better, and Instruction-Tuning Allows Trade-Offs but Not Pareto Dominance"
                },
                "summary": "As large language models (LLMs) become increasingly capable, ensuring that\ntheir self-generated explanations are faithful to their internal\ndecision-making process is critical for safety and oversight. In this work, we\nconduct a comprehensive counterfactual faithfulness analysis across 62 models\nfrom 8 families, encompassing both pretrained and instruction-tuned variants\nand significantly extending prior studies of counterfactual tests. We introduce\nphi-CCT, a simplified variant of the Correlational Counterfactual Test, which\navoids the need for token probabilities while explaining most of the variance\nof the original test. Our findings reveal clear scaling trends: larger models\nare consistently more faithful on our metrics. However, when comparing\ninstruction-tuned and human-imitated explanations, we find that observed\ndifferences in faithfulness can often be attributed to explanation verbosity,\nleading to shifts along the true-positive/false-positive Pareto frontier. While\ninstruction-tuning and prompting can influence this trade-off, we find limited\nevidence that they fundamentally expand the frontier of explanatory\nfaithfulness beyond what is achievable with pretrained models of comparable\nsize. Our analysis highlights the nuanced relationship between\ninstruction-tuning, verbosity, and the faithful representation of model\ndecision processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become increasingly capable, ensuring that\ntheir self-generated explanations are faithful to their internal\ndecision-making process is critical for safety and oversight. In this work, we\nconduct a comprehensive counterfactual faithfulness analysis across 62 models\nfrom 8 families, encompassing both pretrained and instruction-tuned variants\nand significantly extending prior studies of counterfactual tests. We introduce\nphi-CCT, a simplified variant of the Correlational Counterfactual Test, which\navoids the need for token probabilities while explaining most of the variance\nof the original test. Our findings reveal clear scaling trends: larger models\nare consistently more faithful on our metrics. However, when comparing\ninstruction-tuned and human-imitated explanations, we find that observed\ndifferences in faithfulness can often be attributed to explanation verbosity,\nleading to shifts along the true-positive/false-positive Pareto frontier. While\ninstruction-tuning and prompting can influence this trade-off, we find limited\nevidence that they fundamentally expand the frontier of explanatory\nfaithfulness beyond what is achievable with pretrained models of comparable\nsize. Our analysis highlights the nuanced relationship between\ninstruction-tuning, verbosity, and the faithful representation of model\ndecision processes."
                },
                "authors": [
                    {
                        "name": "Noah Y. Siegel"
                    },
                    {
                        "name": "Nicolas Heess"
                    },
                    {
                        "name": "Maria Perez-Ortiz"
                    },
                    {
                        "name": "Oana-Maria Camburu"
                    }
                ],
                "author_detail": {
                    "name": "Oana-Maria Camburu"
                },
                "author": "Oana-Maria Camburu",
                "arxiv_comment": "38 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13440v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13440v2",
                "updated": "2025-03-18T07:07:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    7,
                    7,
                    28,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-17T17:59:01Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    59,
                    1,
                    0,
                    76,
                    0
                ],
                "title": "MaTVLM: Hybrid Mamba-Transformer for Efficient Vision-Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MaTVLM: Hybrid Mamba-Transformer for Efficient Vision-Language Modeling"
                },
                "summary": "With the advancement of RNN models with linear complexity, the quadratic\ncomplexity challenge of transformers has the potential to be overcome. Notably,\nthe emerging Mamba-2 has demonstrated competitive performance, bridging the gap\nbetween RNN models and transformers. However, due to sequential processing and\nvanishing gradients, RNN models struggle to capture long-range dependencies,\nlimiting contextual understanding. This results in slow convergence, high\nresource demands, and poor performance on downstream understanding and complex\nreasoning tasks. In this work, we present a hybrid model MaTVLM by substituting\na portion of the transformer decoder layers in a pre-trained VLM with Mamba-2\nlayers. Leveraging the inherent relationship between attention and Mamba-2, we\ninitialize Mamba-2 with corresponding attention weights to accelerate\nconvergence. Subsequently, we employ a single-stage distillation process, using\nthe pre-trained VLM as the teacher model to transfer knowledge to the MaTVLM,\nfurther enhancing convergence speed and performance. Furthermore, we\ninvestigate the impact of differential distillation loss within our training\nframework. We evaluate the MaTVLM on multiple benchmarks, demonstrating\ncompetitive performance against the teacher model and existing VLMs while\nsurpassing both Mamba-based VLMs and models of comparable parameter scales.\nRemarkably, the MaTVLM achieves up to 3.6x faster inference than the teacher\nmodel while reducing GPU memory consumption by 27.5%, all without compromising\nperformance. Code and models are released at http://github.com/hustvl/MaTVLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancement of RNN models with linear complexity, the quadratic\ncomplexity challenge of transformers has the potential to be overcome. Notably,\nthe emerging Mamba-2 has demonstrated competitive performance, bridging the gap\nbetween RNN models and transformers. However, due to sequential processing and\nvanishing gradients, RNN models struggle to capture long-range dependencies,\nlimiting contextual understanding. This results in slow convergence, high\nresource demands, and poor performance on downstream understanding and complex\nreasoning tasks. In this work, we present a hybrid model MaTVLM by substituting\na portion of the transformer decoder layers in a pre-trained VLM with Mamba-2\nlayers. Leveraging the inherent relationship between attention and Mamba-2, we\ninitialize Mamba-2 with corresponding attention weights to accelerate\nconvergence. Subsequently, we employ a single-stage distillation process, using\nthe pre-trained VLM as the teacher model to transfer knowledge to the MaTVLM,\nfurther enhancing convergence speed and performance. Furthermore, we\ninvestigate the impact of differential distillation loss within our training\nframework. We evaluate the MaTVLM on multiple benchmarks, demonstrating\ncompetitive performance against the teacher model and existing VLMs while\nsurpassing both Mamba-based VLMs and models of comparable parameter scales.\nRemarkably, the MaTVLM achieves up to 3.6x faster inference than the teacher\nmodel while reducing GPU memory consumption by 27.5%, all without compromising\nperformance. Code and models are released at http://github.com/hustvl/MaTVLM."
                },
                "authors": [
                    {
                        "name": "Yingyue Li"
                    },
                    {
                        "name": "Bencheng Liao"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang",
                "arxiv_comment": "Code and model are available at http://github.com/hustvl/MaTVLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13440v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13438v1",
                "updated": "2025-03-17T17:58:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    58,
                    45,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T17:58:45Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    58,
                    45,
                    0,
                    76,
                    0
                ],
                "title": "Deep Belief Markov Models for POMDP Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Belief Markov Models for POMDP Inference"
                },
                "summary": "This work introduces a novel deep learning-based architecture, termed the\nDeep Belief Markov Model (DBMM), which provides efficient, model-formulation\nagnostic inference in Partially Observable Markov Decision Process (POMDP)\nproblems. The POMDP framework allows for modeling and solving sequential\ndecision-making problems under observation uncertainty. In complex,\nhigh-dimensional, partially observable environments, existing methods for\ninference based on exact computations (e.g., via Bayes' theorem) or sampling\nalgorithms do not scale well. Furthermore, ground truth states may not be\navailable for learning the exact transition dynamics. DBMMs extend deep Markov\nmodels into the partially observable decision-making framework and allow\nefficient belief inference entirely based on available observation data via\nvariational inference methods. By leveraging the potency of neural networks,\nDBMMs can infer and simulate non-linear relationships in the system dynamics\nand naturally scale to problems with high dimensionality and discrete or\ncontinuous variables. In addition, neural network parameters can be dynamically\nupdated efficiently based on data availability. DBMMs can thus be used to infer\na belief variable, thus enabling the derivation of POMDP solutions over the\nbelief space. We evaluate the efficacy of the proposed methodology by\nevaluating the capability of model-formulation agnostic inference of DBMMs in\nbenchmark problems that include discrete and continuous variables.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work introduces a novel deep learning-based architecture, termed the\nDeep Belief Markov Model (DBMM), which provides efficient, model-formulation\nagnostic inference in Partially Observable Markov Decision Process (POMDP)\nproblems. The POMDP framework allows for modeling and solving sequential\ndecision-making problems under observation uncertainty. In complex,\nhigh-dimensional, partially observable environments, existing methods for\ninference based on exact computations (e.g., via Bayes' theorem) or sampling\nalgorithms do not scale well. Furthermore, ground truth states may not be\navailable for learning the exact transition dynamics. DBMMs extend deep Markov\nmodels into the partially observable decision-making framework and allow\nefficient belief inference entirely based on available observation data via\nvariational inference methods. By leveraging the potency of neural networks,\nDBMMs can infer and simulate non-linear relationships in the system dynamics\nand naturally scale to problems with high dimensionality and discrete or\ncontinuous variables. In addition, neural network parameters can be dynamically\nupdated efficiently based on data availability. DBMMs can thus be used to infer\na belief variable, thus enabling the derivation of POMDP solutions over the\nbelief space. We evaluate the efficacy of the proposed methodology by\nevaluating the capability of model-formulation agnostic inference of DBMMs in\nbenchmark problems that include discrete and continuous variables."
                },
                "authors": [
                    {
                        "name": "Giacomo Arcieri"
                    },
                    {
                        "name": "Konstantinos G. Papakonstantinou"
                    },
                    {
                        "name": "Daniel Straub"
                    },
                    {
                        "name": "Eleni Chatzi"
                    }
                ],
                "author_detail": {
                    "name": "Eleni Chatzi"
                },
                "author": "Eleni Chatzi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13436v1",
                "updated": "2025-03-17T17:58:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    58,
                    30,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T17:58:30Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    58,
                    30,
                    0,
                    76,
                    0
                ],
                "title": "Unified Autoregressive Visual Generation and Understanding with\n  Continuous Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Autoregressive Visual Generation and Understanding with\n  Continuous Tokens"
                },
                "summary": "We present UniFluid, a unified autoregressive framework for joint visual\ngeneration and understanding leveraging continuous visual tokens. Our unified\nautoregressive architecture processes multimodal image and text inputs,\ngenerating discrete tokens for text and continuous tokens for image. We find\nthough there is an inherent trade-off between the image generation and\nunderstanding task, a carefully tuned training recipe enables them to improve\neach other. By selecting an appropriate loss balance weight, the unified model\nachieves results comparable to or exceeding those of single-task baselines on\nboth tasks. Furthermore, we demonstrate that employing stronger pre-trained\nLLMs and random-order generation during training is important to achieve\nhigh-fidelity image generation within this unified framework. Built upon the\nGemma model series, UniFluid exhibits competitive performance across both image\ngeneration and understanding, demonstrating strong transferability to various\ndownstream tasks, including image editing for generation, as well as visual\ncaptioning and question answering for understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present UniFluid, a unified autoregressive framework for joint visual\ngeneration and understanding leveraging continuous visual tokens. Our unified\nautoregressive architecture processes multimodal image and text inputs,\ngenerating discrete tokens for text and continuous tokens for image. We find\nthough there is an inherent trade-off between the image generation and\nunderstanding task, a carefully tuned training recipe enables them to improve\neach other. By selecting an appropriate loss balance weight, the unified model\nachieves results comparable to or exceeding those of single-task baselines on\nboth tasks. Furthermore, we demonstrate that employing stronger pre-trained\nLLMs and random-order generation during training is important to achieve\nhigh-fidelity image generation within this unified framework. Built upon the\nGemma model series, UniFluid exhibits competitive performance across both image\ngeneration and understanding, demonstrating strong transferability to various\ndownstream tasks, including image editing for generation, as well as visual\ncaptioning and question answering for understanding."
                },
                "authors": [
                    {
                        "name": "Lijie Fan"
                    },
                    {
                        "name": "Luming Tang"
                    },
                    {
                        "name": "Siyang Qin"
                    },
                    {
                        "name": "Tianhong Li"
                    },
                    {
                        "name": "Xuan Yang"
                    },
                    {
                        "name": "Siyuan Qiao"
                    },
                    {
                        "name": "Andreas Steiner"
                    },
                    {
                        "name": "Chen Sun"
                    },
                    {
                        "name": "Yuanzhen Li"
                    },
                    {
                        "name": "Tao Zhu"
                    },
                    {
                        "name": "Michael Rubinstein"
                    },
                    {
                        "name": "Michalis Raptis"
                    },
                    {
                        "name": "Deqing Sun"
                    },
                    {
                        "name": "Radu Soricut"
                    }
                ],
                "author_detail": {
                    "name": "Radu Soricut"
                },
                "author": "Radu Soricut",
                "arxiv_comment": "Tech report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18573v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18573v2",
                "updated": "2025-03-17T17:58:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    58,
                    13,
                    0,
                    76,
                    0
                ],
                "published": "2024-12-24T17:56:08Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    17,
                    56,
                    8,
                    1,
                    359,
                    0
                ],
                "title": "Top General Performance = Top Domain Performance? DomainCodeBench: A\n  Multi-domain Code Generation Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Top General Performance = Top Domain Performance? DomainCodeBench: A\n  Multi-domain Code Generation Benchmark"
                },
                "summary": "With the rapid advancement of large language models (LLMs), extensive\nresearch has been conducted to investigate the code generation capabilities of\nLLMs. However, existing efforts primarily focus on general-domain tasks,\nleaving LLMs' code generation performance in real-world application domains\nunderexplored. This raises a critical question: can a model's general-domain\ncoding ability reliably represent its ability in specialized domains? In this\npaper, we introduce DomainCodeBench, a multi-domain code generation benchmark\ndesigned to systematically evaluate LLMs across 12 software application domains\nand 15 programming languages. DomainCodeBench contains 2,400 manually verified\ntasks with ground truth, human-annotated docstrings, and fine-grained\ndependency information to ensure more coverage of domain-specific challenges.\nSpecifically, we first identify the most popular application domains by topic\nmining. Then, we curate coding tasks based on commonly used frameworks and\nplatforms in each domain. We obtain several findings through extensive\nexperiments on DomainCodeBench with ten mainstream LLMs. (1) Performance\ndecoupling: experiments reveal that top general-domain models do not\nconsistently excel in specific application domains; (2) Domain-specific\nweaknesses: LLMs often fail due to domain knowledge gaps and third-party\nlibrary misusage; (3) Contextual enhancement: we show that augmenting prompts\nwith domain-specific knowledge improves performance by around 38.17%, providing\nactionable insights for performance optimization. Our replication package,\nincluding the benchmark, source code, and experimental results, is available at\nhttps://github.com/DeepSoftwareAnalytics/DomainCodeBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of large language models (LLMs), extensive\nresearch has been conducted to investigate the code generation capabilities of\nLLMs. However, existing efforts primarily focus on general-domain tasks,\nleaving LLMs' code generation performance in real-world application domains\nunderexplored. This raises a critical question: can a model's general-domain\ncoding ability reliably represent its ability in specialized domains? In this\npaper, we introduce DomainCodeBench, a multi-domain code generation benchmark\ndesigned to systematically evaluate LLMs across 12 software application domains\nand 15 programming languages. DomainCodeBench contains 2,400 manually verified\ntasks with ground truth, human-annotated docstrings, and fine-grained\ndependency information to ensure more coverage of domain-specific challenges.\nSpecifically, we first identify the most popular application domains by topic\nmining. Then, we curate coding tasks based on commonly used frameworks and\nplatforms in each domain. We obtain several findings through extensive\nexperiments on DomainCodeBench with ten mainstream LLMs. (1) Performance\ndecoupling: experiments reveal that top general-domain models do not\nconsistently excel in specific application domains; (2) Domain-specific\nweaknesses: LLMs often fail due to domain knowledge gaps and third-party\nlibrary misusage; (3) Contextual enhancement: we show that augmenting prompts\nwith domain-specific knowledge improves performance by around 38.17%, providing\nactionable insights for performance optimization. Our replication package,\nincluding the benchmark, source code, and experimental results, is available at\nhttps://github.com/DeepSoftwareAnalytics/DomainCodeBench."
                },
                "authors": [
                    {
                        "name": "Dewu Zheng"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Ensheng Shi"
                    },
                    {
                        "name": "Xilin Liu"
                    },
                    {
                        "name": "Yuchi Ma"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18573v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13427v1",
                "updated": "2025-03-17T17:54:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    54,
                    55,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T17:54:55Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    54,
                    55,
                    0,
                    76,
                    0
                ],
                "title": "xLSTM 7B: A Recurrent LLM for Fast and Efficient Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xLSTM 7B: A Recurrent LLM for Fast and Efficient Inference"
                },
                "summary": "Recent breakthroughs in solving reasoning, math and coding problems with\nLarge Language Models (LLMs) have been enabled by investing substantial\ncomputation budgets at inference time. Therefore, inference speed is one of the\nmost critical properties of LLM architectures, and there is a growing need for\nLLMs that are efficient and fast at inference. Recently, LLMs built on the\nxLSTM architecture have emerged as a powerful alternative to Transformers,\noffering linear compute scaling with sequence length and constant memory usage,\nboth highly desirable properties for efficient inference. However, such\nxLSTM-based LLMs have yet to be scaled to larger models and assessed and\ncompared with respect to inference speed and efficiency. In this work, we\nintroduce xLSTM 7B, a 7-billion-parameter LLM that combines xLSTM's\narchitectural benefits with targeted optimizations for fast and efficient\ninference. Our experiments demonstrate that xLSTM 7B achieves performance on\ndownstream tasks comparable to other similar-sized LLMs, while providing\nsignificantly faster inference speeds and greater efficiency compared to Llama-\nand Mamba-based LLMs. These results establish xLSTM 7B as the fastest and most\nefficient 7B LLM, offering a solution for tasks that require large amounts of\ntest-time computation. Our work highlights xLSTM's potential as a foundational\narchitecture for methods building on heavy use of LLM inference. Our model\nweights, model code and training code are open-source.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent breakthroughs in solving reasoning, math and coding problems with\nLarge Language Models (LLMs) have been enabled by investing substantial\ncomputation budgets at inference time. Therefore, inference speed is one of the\nmost critical properties of LLM architectures, and there is a growing need for\nLLMs that are efficient and fast at inference. Recently, LLMs built on the\nxLSTM architecture have emerged as a powerful alternative to Transformers,\noffering linear compute scaling with sequence length and constant memory usage,\nboth highly desirable properties for efficient inference. However, such\nxLSTM-based LLMs have yet to be scaled to larger models and assessed and\ncompared with respect to inference speed and efficiency. In this work, we\nintroduce xLSTM 7B, a 7-billion-parameter LLM that combines xLSTM's\narchitectural benefits with targeted optimizations for fast and efficient\ninference. Our experiments demonstrate that xLSTM 7B achieves performance on\ndownstream tasks comparable to other similar-sized LLMs, while providing\nsignificantly faster inference speeds and greater efficiency compared to Llama-\nand Mamba-based LLMs. These results establish xLSTM 7B as the fastest and most\nefficient 7B LLM, offering a solution for tasks that require large amounts of\ntest-time computation. Our work highlights xLSTM's potential as a foundational\narchitecture for methods building on heavy use of LLM inference. Our model\nweights, model code and training code are open-source."
                },
                "authors": [
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Korbinian Pöppel"
                    },
                    {
                        "name": "Phillip Lippe"
                    },
                    {
                        "name": "Richard Kurle"
                    },
                    {
                        "name": "Patrick M. Blies"
                    },
                    {
                        "name": "Günter Klambauer"
                    },
                    {
                        "name": "Sebastian Böck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "arxiv_comment": "Code available at: https://github.com/NX-AI/xlstm and\n  https://github.com/NX-AI/xlstm-jax",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13423v1",
                "updated": "2025-03-17T17:53:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    53,
                    23,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T17:53:23Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    53,
                    23,
                    0,
                    76,
                    0
                ],
                "title": "SuperBPE: Space Travel for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperBPE: Space Travel for Language Models"
                },
                "summary": "The assumption across nearly all language model (LM) tokenization schemes is\nthat tokens should be subwords, i.e., contained within word boundaries. While\nproviding a seemingly reasonable inductive bias, is this common practice\nlimiting the potential of modern LMs? Whitespace is not a reliable delimiter of\nmeaning, as evidenced by multi-word expressions (e.g., \"by the way\"),\ncrosslingual variation in the number of words needed to express a concept\n(e.g., \"spacesuit helmet\" in German is \"raumanzughelm\"), and languages that do\nnot use whitespace at all (e.g., Chinese). To explore the potential of\ntokenization beyond subwords, we introduce a \"superword\" tokenizer, SuperBPE,\nwhich incorporates a simple pretokenization curriculum into the byte-pair\nencoding (BPE) algorithm to first learn subwords, then superwords that bridge\nwhitespace. This brings dramatic improvements in encoding efficiency: when\nfixing the vocabulary size to 200k, SuperBPE encodes a fixed piece of text with\nup to 33% fewer tokens than BPE on average. In experiments, we pretrain 8B\ntransformer LMs from scratch while fixing the model size, vocabulary size, and\ntrain compute, varying *only* the algorithm for learning the vocabulary. Our\nmodel trained with SuperBPE achieves an average +4.0% absolute improvement over\nthe BPE baseline across 30 downstream tasks (including +8.2% on MMLU), while\nsimultaneously requiring 27% less compute at inference time. In analysis, we\nfind that SuperBPE results in segmentations of text that are more uniform in\nper-token difficulty. Qualitatively, this may be because SuperBPE tokens often\ncapture common multi-word expressions that function semantically as a single\nunit. SuperBPE is a straightforward, local modification to tokenization that\nimproves both encoding efficiency and downstream performance, yielding better\nlanguage models overall.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The assumption across nearly all language model (LM) tokenization schemes is\nthat tokens should be subwords, i.e., contained within word boundaries. While\nproviding a seemingly reasonable inductive bias, is this common practice\nlimiting the potential of modern LMs? Whitespace is not a reliable delimiter of\nmeaning, as evidenced by multi-word expressions (e.g., \"by the way\"),\ncrosslingual variation in the number of words needed to express a concept\n(e.g., \"spacesuit helmet\" in German is \"raumanzughelm\"), and languages that do\nnot use whitespace at all (e.g., Chinese). To explore the potential of\ntokenization beyond subwords, we introduce a \"superword\" tokenizer, SuperBPE,\nwhich incorporates a simple pretokenization curriculum into the byte-pair\nencoding (BPE) algorithm to first learn subwords, then superwords that bridge\nwhitespace. This brings dramatic improvements in encoding efficiency: when\nfixing the vocabulary size to 200k, SuperBPE encodes a fixed piece of text with\nup to 33% fewer tokens than BPE on average. In experiments, we pretrain 8B\ntransformer LMs from scratch while fixing the model size, vocabulary size, and\ntrain compute, varying *only* the algorithm for learning the vocabulary. Our\nmodel trained with SuperBPE achieves an average +4.0% absolute improvement over\nthe BPE baseline across 30 downstream tasks (including +8.2% on MMLU), while\nsimultaneously requiring 27% less compute at inference time. In analysis, we\nfind that SuperBPE results in segmentations of text that are more uniform in\nper-token difficulty. Qualitatively, this may be because SuperBPE tokens often\ncapture common multi-word expressions that function semantically as a single\nunit. SuperBPE is a straightforward, local modification to tokenization that\nimproves both encoding efficiency and downstream performance, yielding better\nlanguage models overall."
                },
                "authors": [
                    {
                        "name": "Alisa Liu"
                    },
                    {
                        "name": "Jonathan Hayase"
                    },
                    {
                        "name": "Valentin Hofmann"
                    },
                    {
                        "name": "Sewoong Oh"
                    },
                    {
                        "name": "Noah A. Smith"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "arxiv_comment": "preprint, code and artifacts will become available at\n  https://superbpe.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13421v1",
                "updated": "2025-03-17T17:51:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    51,
                    49,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T17:51:49Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    51,
                    49,
                    0,
                    76,
                    0
                ],
                "title": "Optimal Expert Selection for Distributed Mixture-of-Experts at the\n  Wireless Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Expert Selection for Distributed Mixture-of-Experts at the\n  Wireless Edge"
                },
                "summary": "The emergence of distributed Mixture-of-Experts (DMoE) systems, which deploy\nexpert models at edge nodes, offers a pathway to achieving connected\nintelligence in sixth-generation (6G) mobile networks and edge artificial\nintelligence (AI). However, current DMoE systems lack an effective expert\nselection algorithm to address the simultaneous task-expert relevance and\nchannel diversity inherent in these systems. Traditional AI or communication\nsystems focus on either performance or channel conditions, and direct\napplication of these methods leads to high communication overhead or low\nperformance. To address this, we propose the DMoE protocol to schedule the\nexpert inference and inter-expert transmission. This protocol identifies expert\nselection and subcarrier allocation as key optimization problems. We formulate\nan expert selection problem by incorporating both AI performance and channel\nconditions, and further extend it to a Joint Expert and Subcarrier Allocation\n(JESA) problem for comprehensive AI and channel management within the DMoE\nframework. For the NP-hard expert selection problem, we introduce the Dynamic\nExpert Selection (DES) algorithm, which leverages a linear relaxation as a\nbounding criterion to significantly reduce search complexity. For the JESA\nproblem, we discover a unique structural property that ensures asymptotic\noptimality in most scenarios. We propose an iterative algorithm that addresses\nsubcarrier allocation as a subproblem and integrates it with the DES algorithm.\nThe proposed framework effectively manages the tradeoff between task relevance\nand channel conditions through a tunable importance factor, enabling flexible\nadaptation to diverse scenarios. Numerical experiments validate the dual\nbenefits of the proposed expert selection algorithm: high performance and\nsignificantly reduced cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of distributed Mixture-of-Experts (DMoE) systems, which deploy\nexpert models at edge nodes, offers a pathway to achieving connected\nintelligence in sixth-generation (6G) mobile networks and edge artificial\nintelligence (AI). However, current DMoE systems lack an effective expert\nselection algorithm to address the simultaneous task-expert relevance and\nchannel diversity inherent in these systems. Traditional AI or communication\nsystems focus on either performance or channel conditions, and direct\napplication of these methods leads to high communication overhead or low\nperformance. To address this, we propose the DMoE protocol to schedule the\nexpert inference and inter-expert transmission. This protocol identifies expert\nselection and subcarrier allocation as key optimization problems. We formulate\nan expert selection problem by incorporating both AI performance and channel\nconditions, and further extend it to a Joint Expert and Subcarrier Allocation\n(JESA) problem for comprehensive AI and channel management within the DMoE\nframework. For the NP-hard expert selection problem, we introduce the Dynamic\nExpert Selection (DES) algorithm, which leverages a linear relaxation as a\nbounding criterion to significantly reduce search complexity. For the JESA\nproblem, we discover a unique structural property that ensures asymptotic\noptimality in most scenarios. We propose an iterative algorithm that addresses\nsubcarrier allocation as a subproblem and integrates it with the DES algorithm.\nThe proposed framework effectively manages the tradeoff between task relevance\nand channel conditions through a tunable importance factor, enabling flexible\nadaptation to diverse scenarios. Numerical experiments validate the dual\nbenefits of the proposed expert selection algorithm: high performance and\nsignificantly reduced cost."
                },
                "authors": [
                    {
                        "name": "Shengling Qin"
                    },
                    {
                        "name": "Hai Wu"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13415v1",
                "updated": "2025-03-17T17:45:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    45,
                    46,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T17:45:46Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    45,
                    46,
                    0,
                    76,
                    0
                ],
                "title": "A Comprehensive Survey on Multi-Agent Cooperative Decision-Making:\n  Scenarios, Approaches, Challenges and Perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey on Multi-Agent Cooperative Decision-Making:\n  Scenarios, Approaches, Challenges and Perspectives"
                },
                "summary": "With the rapid development of artificial intelligence, intelligent\ndecision-making techniques have gradually surpassed human levels in various\nhuman-machine competitions, especially in complex multi-agent cooperative task\nscenarios. Multi-agent cooperative decision-making involves multiple agents\nworking together to complete established tasks and achieve specific objectives.\nThese techniques are widely applicable in real-world scenarios such as\nautonomous driving, drone navigation, disaster rescue, and simulated military\nconfrontations. This paper begins with a comprehensive survey of the leading\nsimulation environments and platforms used for multi-agent cooperative\ndecision-making. Specifically, we provide an in-depth analysis for these\nsimulation environments from various perspectives, including task formats,\nreward allocation, and the underlying technologies employed. Subsequently, we\nprovide a comprehensive overview of the mainstream intelligent decision-making\napproaches, algorithms and models for multi-agent systems (MAS).\nTheseapproaches can be broadly categorized into five types: rule-based\n(primarily fuzzy logic), game theory-based, evolutionary algorithms-based, deep\nmulti-agent reinforcement learning (MARL)-based, and large language\nmodels(LLMs)reasoning-based. Given the significant advantages of MARL\nandLLMs-baseddecision-making methods over the traditional rule, game theory,\nand evolutionary algorithms, this paper focuses on these multi-agent methods\nutilizing MARL and LLMs-based techniques. We provide an in-depth discussion of\nthese approaches, highlighting their methodology taxonomies, advantages, and\ndrawbacks. Further, several prominent research directions in the future and\npotential challenges of multi-agent cooperative decision-making are also\ndetailed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of artificial intelligence, intelligent\ndecision-making techniques have gradually surpassed human levels in various\nhuman-machine competitions, especially in complex multi-agent cooperative task\nscenarios. Multi-agent cooperative decision-making involves multiple agents\nworking together to complete established tasks and achieve specific objectives.\nThese techniques are widely applicable in real-world scenarios such as\nautonomous driving, drone navigation, disaster rescue, and simulated military\nconfrontations. This paper begins with a comprehensive survey of the leading\nsimulation environments and platforms used for multi-agent cooperative\ndecision-making. Specifically, we provide an in-depth analysis for these\nsimulation environments from various perspectives, including task formats,\nreward allocation, and the underlying technologies employed. Subsequently, we\nprovide a comprehensive overview of the mainstream intelligent decision-making\napproaches, algorithms and models for multi-agent systems (MAS).\nTheseapproaches can be broadly categorized into five types: rule-based\n(primarily fuzzy logic), game theory-based, evolutionary algorithms-based, deep\nmulti-agent reinforcement learning (MARL)-based, and large language\nmodels(LLMs)reasoning-based. Given the significant advantages of MARL\nandLLMs-baseddecision-making methods over the traditional rule, game theory,\nand evolutionary algorithms, this paper focuses on these multi-agent methods\nutilizing MARL and LLMs-based techniques. We provide an in-depth discussion of\nthese approaches, highlighting their methodology taxonomies, advantages, and\ndrawbacks. Further, several prominent research directions in the future and\npotential challenges of multi-agent cooperative decision-making are also\ndetailed."
                },
                "authors": [
                    {
                        "name": "Weiqiang Jin"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Biao Zhao"
                    },
                    {
                        "name": "Xingwu Tian"
                    },
                    {
                        "name": "Bohang Shi"
                    },
                    {
                        "name": "Guang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guang Yang"
                },
                "author": "Guang Yang",
                "arxiv_comment": "54 pages, 24 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13413v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13413v2",
                "updated": "2025-03-18T04:41:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    4,
                    41,
                    37,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-17T17:42:51Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    42,
                    51,
                    0,
                    76,
                    0
                ],
                "title": "DLPO: Towards a Robust, Efficient, and Generalizable Prompt Optimization\n  Framework from a Deep-Learning Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DLPO: Towards a Robust, Efficient, and Generalizable Prompt Optimization\n  Framework from a Deep-Learning Perspective"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across diverse\ntasks, largely driven by well-designed prompts. However, crafting and selecting\nsuch prompts often requires considerable human effort, significantly limiting\nits scalability. To mitigate this, recent studies have explored automated\nprompt optimization as a promising solution. Despite these efforts, existing\nmethods still face critical challenges in robustness, efficiency, and\ngeneralization. To systematically address these challenges, we first conduct an\nempirical analysis to identify the limitations of current reflection-based\nprompt optimization paradigm. Building on these insights, we propose 7\ninnovative approaches inspired by traditional deep learning paradigms for\nprompt optimization (DLPO), seamlessly integrating these concepts into\ntext-based gradient optimization. Through these advancements, we progressively\ntackle the aforementioned challenges and validate our methods through extensive\nexperimentation. We hope our study not only provides valuable guidance for\nfuture research but also offers a comprehensive understanding of the challenges\nand potential solutions in prompt optimization. Our code is available at\nhttps://github.com/sfasfaffa/DLPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across diverse\ntasks, largely driven by well-designed prompts. However, crafting and selecting\nsuch prompts often requires considerable human effort, significantly limiting\nits scalability. To mitigate this, recent studies have explored automated\nprompt optimization as a promising solution. Despite these efforts, existing\nmethods still face critical challenges in robustness, efficiency, and\ngeneralization. To systematically address these challenges, we first conduct an\nempirical analysis to identify the limitations of current reflection-based\nprompt optimization paradigm. Building on these insights, we propose 7\ninnovative approaches inspired by traditional deep learning paradigms for\nprompt optimization (DLPO), seamlessly integrating these concepts into\ntext-based gradient optimization. Through these advancements, we progressively\ntackle the aforementioned challenges and validate our methods through extensive\nexperimentation. We hope our study not only provides valuable guidance for\nfuture research but also offers a comprehensive understanding of the challenges\nand potential solutions in prompt optimization. Our code is available at\nhttps://github.com/sfasfaffa/DLPO."
                },
                "authors": [
                    {
                        "name": "Dengyun Peng"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Jinhao Liu"
                    },
                    {
                        "name": "Jingjing Chen"
                    },
                    {
                        "name": "Libo Qin"
                    }
                ],
                "author_detail": {
                    "name": "Libo Qin"
                },
                "author": "Libo Qin",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13413v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13413v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13402v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13402v1",
                "updated": "2025-03-17T17:34:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    34,
                    4,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T17:34:04Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    34,
                    4,
                    0,
                    76,
                    0
                ],
                "title": "Toward Generative 6G Simulation: An Experimental Multi-Agent LLM and\n  ns-3 Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Generative 6G Simulation: An Experimental Multi-Agent LLM and\n  ns-3 Integration"
                },
                "summary": "The move toward open Sixth-Generation (6G) networks necessitates a novel\napproach to full-stack simulation environments for evaluating complex\ntechnology developments before prototyping and real-world implementation. This\npaper introduces an innovative approach\\footnote{A lightweight, mock version of\nthe code is available on GitHub at that combines a multi-agent framework with\nthe Network Simulator 3 (ns-3) to automate and optimize the generation,\ndebugging, execution, and analysis of complex 5G network scenarios. Our\nframework orchestrates a suite of specialized agents -- namely, the Simulation\nGeneration Agent, Test Designer Agent, Test Executor Agent, and Result\nInterpretation Agent -- using advanced LangChain coordination. The Simulation\nGeneration Agent employs a structured chain-of-thought (CoT) reasoning process,\nleveraging LLMs and retrieval-augmented generation (RAG) to translate natural\nlanguage simulation specifications into precise ns-3 scripts. Concurrently, the\nTest Designer Agent generates comprehensive automated test suites by\nintegrating knowledge retrieval techniques with dynamic test case synthesis.\nThe Test Executor Agent dynamically deploys and runs simulations, managing\ndependencies and parsing detailed performance metrics. At the same time, the\nResult Interpretation Agent utilizes LLM-driven analysis to extract actionable\ninsights from the simulation outputs. By integrating external resources such as\nlibrary documentation and ns-3 testing frameworks, our experimental approach\ncan enhance simulation accuracy and adaptability, reducing reliance on\nextensive programming expertise. A detailed case study using the ns-3 5G-LENA\nmodule validates the effectiveness of the proposed approach. The code\ngeneration process converges in an average of 1.8 iterations, has a syntax\nerror rate of 17.0%, a mean response time of 7.3 seconds, and receives a human\nevaluation score of 7.5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The move toward open Sixth-Generation (6G) networks necessitates a novel\napproach to full-stack simulation environments for evaluating complex\ntechnology developments before prototyping and real-world implementation. This\npaper introduces an innovative approach\\footnote{A lightweight, mock version of\nthe code is available on GitHub at that combines a multi-agent framework with\nthe Network Simulator 3 (ns-3) to automate and optimize the generation,\ndebugging, execution, and analysis of complex 5G network scenarios. Our\nframework orchestrates a suite of specialized agents -- namely, the Simulation\nGeneration Agent, Test Designer Agent, Test Executor Agent, and Result\nInterpretation Agent -- using advanced LangChain coordination. The Simulation\nGeneration Agent employs a structured chain-of-thought (CoT) reasoning process,\nleveraging LLMs and retrieval-augmented generation (RAG) to translate natural\nlanguage simulation specifications into precise ns-3 scripts. Concurrently, the\nTest Designer Agent generates comprehensive automated test suites by\nintegrating knowledge retrieval techniques with dynamic test case synthesis.\nThe Test Executor Agent dynamically deploys and runs simulations, managing\ndependencies and parsing detailed performance metrics. At the same time, the\nResult Interpretation Agent utilizes LLM-driven analysis to extract actionable\ninsights from the simulation outputs. By integrating external resources such as\nlibrary documentation and ns-3 testing frameworks, our experimental approach\ncan enhance simulation accuracy and adaptability, reducing reliance on\nextensive programming expertise. A detailed case study using the ns-3 5G-LENA\nmodule validates the effectiveness of the proposed approach. The code\ngeneration process converges in an average of 1.8 iterations, has a syntax\nerror rate of 17.0%, a mean response time of 7.3 seconds, and receives a human\nevaluation score of 7.5."
                },
                "authors": [
                    {
                        "name": "Farhad Rezazadeh"
                    },
                    {
                        "name": "Amir Ashtari Gargari"
                    },
                    {
                        "name": "Sandra Lagen"
                    },
                    {
                        "name": "Houbing Song"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Lingjia Liu"
                    }
                ],
                "author_detail": {
                    "name": "Lingjia Liu"
                },
                "author": "Lingjia Liu",
                "arxiv_comment": "6 pages, 4 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13402v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13402v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13399v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13399v1",
                "updated": "2025-03-17T17:33:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    33,
                    10,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T17:33:10Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    33,
                    10,
                    0,
                    76,
                    0
                ],
                "title": "MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based\n  Scientific Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based\n  Scientific Research"
                },
                "summary": "Scientific research demands sophisticated reasoning over multimodal data, a\nchallenge especially prevalent in biology. Despite recent advances in\nmultimodal large language models (MLLMs) for AI-assisted research, existing\nmultimodal reasoning benchmarks only target up to college-level difficulty,\nwhile research-level benchmarks emphasize lower-level perception, falling short\nof the complex multimodal reasoning needed for scientific discovery. To bridge\nthis gap, we introduce MicroVQA, a visual-question answering (VQA) benchmark\ndesigned to assess three reasoning capabilities vital in research workflows:\nexpert image understanding, hypothesis generation, and experiment proposal.\nMicroVQA consists of 1,042 multiple-choice questions (MCQs) curated by biology\nexperts across diverse microscopy modalities, ensuring VQA samples represent\nreal scientific practice. In constructing the benchmark, we find that standard\nMCQ generation methods induce language shortcuts, motivating a new two-stage\npipeline: an optimized LLM prompt structures question-answer pairs into MCQs;\nthen, an agent-based `RefineBot' updates them to remove shortcuts. Benchmarking\non state-of-the-art MLLMs reveal a peak performance of 53\\%; models with\nsmaller LLMs only slightly underperform top models, suggesting that\nlanguage-based reasoning is less challenging than multimodal reasoning; and\ntuning with scientific articles enhances performance. Expert analysis of\nchain-of-thought responses shows that perception errors are the most frequent,\nfollowed by knowledge errors and then overgeneralization errors. These insights\nhighlight the challenges in multimodal scientific reasoning, showing MicroVQA\nis a valuable resource advancing AI-driven biomedical research. MicroVQA is\navailable at https://huggingface.co/datasets/jmhb/microvqa, and project page at\nhttps://jmhb0.github.io/microvqa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific research demands sophisticated reasoning over multimodal data, a\nchallenge especially prevalent in biology. Despite recent advances in\nmultimodal large language models (MLLMs) for AI-assisted research, existing\nmultimodal reasoning benchmarks only target up to college-level difficulty,\nwhile research-level benchmarks emphasize lower-level perception, falling short\nof the complex multimodal reasoning needed for scientific discovery. To bridge\nthis gap, we introduce MicroVQA, a visual-question answering (VQA) benchmark\ndesigned to assess three reasoning capabilities vital in research workflows:\nexpert image understanding, hypothesis generation, and experiment proposal.\nMicroVQA consists of 1,042 multiple-choice questions (MCQs) curated by biology\nexperts across diverse microscopy modalities, ensuring VQA samples represent\nreal scientific practice. In constructing the benchmark, we find that standard\nMCQ generation methods induce language shortcuts, motivating a new two-stage\npipeline: an optimized LLM prompt structures question-answer pairs into MCQs;\nthen, an agent-based `RefineBot' updates them to remove shortcuts. Benchmarking\non state-of-the-art MLLMs reveal a peak performance of 53\\%; models with\nsmaller LLMs only slightly underperform top models, suggesting that\nlanguage-based reasoning is less challenging than multimodal reasoning; and\ntuning with scientific articles enhances performance. Expert analysis of\nchain-of-thought responses shows that perception errors are the most frequent,\nfollowed by knowledge errors and then overgeneralization errors. These insights\nhighlight the challenges in multimodal scientific reasoning, showing MicroVQA\nis a valuable resource advancing AI-driven biomedical research. MicroVQA is\navailable at https://huggingface.co/datasets/jmhb/microvqa, and project page at\nhttps://jmhb0.github.io/microvqa."
                },
                "authors": [
                    {
                        "name": "James Burgess"
                    },
                    {
                        "name": "Jeffrey J Nirschl"
                    },
                    {
                        "name": "Laura Bravo-Sánchez"
                    },
                    {
                        "name": "Alejandro Lozano"
                    },
                    {
                        "name": "Sanket Rajan Gupte"
                    },
                    {
                        "name": "Jesus G. Galaz-Montoya"
                    },
                    {
                        "name": "Yuhui Zhang"
                    },
                    {
                        "name": "Yuchang Su"
                    },
                    {
                        "name": "Disha Bhowmik"
                    },
                    {
                        "name": "Zachary Coman"
                    },
                    {
                        "name": "Sarina M. Hasan"
                    },
                    {
                        "name": "Alexandra Johannesson"
                    },
                    {
                        "name": "William D. Leineweber"
                    },
                    {
                        "name": "Malvika G Nair"
                    },
                    {
                        "name": "Ridhi Yarlagadda"
                    },
                    {
                        "name": "Connor Zuraski"
                    },
                    {
                        "name": "Wah Chiu"
                    },
                    {
                        "name": "Sarah Cohen"
                    },
                    {
                        "name": "Jan N. Hansen"
                    },
                    {
                        "name": "Manuel D Leonetti"
                    },
                    {
                        "name": "Chad Liu"
                    },
                    {
                        "name": "Emma Lundberg"
                    },
                    {
                        "name": "Serena Yeung-Levy"
                    }
                ],
                "author_detail": {
                    "name": "Serena Yeung-Levy"
                },
                "author": "Serena Yeung-Levy",
                "arxiv_comment": "CVPR 2025 (Conference on Computer Vision and Pattern Recognition)\n  Project page at https://jmhb0.github.io/microvqa Benchmark at\n  https://huggingface.co/datasets/jmhb/microvqa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13399v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13399v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.CB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13383v1",
                "updated": "2025-03-17T17:11:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    11,
                    22,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T17:11:22Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    11,
                    22,
                    0,
                    76,
                    0
                ],
                "title": "Cream of the Crop: Harvesting Rich, Scalable and Transferable\n  Multi-Modal Data for Instruction Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cream of the Crop: Harvesting Rich, Scalable and Transferable\n  Multi-Modal Data for Instruction Fine-Tuning"
                },
                "summary": "The hypothesis that pretrained large language models (LLMs) necessitate only\nminimal supervision during the fine-tuning (SFT) stage (Zhou et al., 2024) has\nbeen substantiated by recent advancements in data curation and selection\nresearch. However, their stability and generalizability are compromised due to\nthe vulnerability to experimental setups and validation protocols, falling\nshort of surpassing random sampling (Diddee & Ippolito, 2024; Xia et al.,\n2024b). Built upon LLMs, multi-modal LLMs (MLLMs), combined with the sheer\ntoken volume and heightened heterogeneity of data sources, amplify both the\nsignificance and complexity of data selection.\n  To harvest multi-modal instructional data in a robust and efficient manner,\nwe re-define the granularity of the quality metric by decomposing it into 14\nvision-language-related capabilities, and introduce multi-modal rich scorers to\nevaluate the capabilities of each data candidate. To promote diversity, in\nlight of the inherent objective of the alignment stage, we take interaction\nstyle as diversity indicator and use a multi-modal rich styler to identify data\ninstruction patterns. In doing so, our multi-modal rich scorers and styler\n(mmSSR) guarantee that high-scoring information is conveyed to users in\ndiversified forms. Free from embedding-based clustering or greedy sampling,\nmmSSR efficiently scales to millions of data with varying budget constraints,\nsupports customization for general or specific capability acquisition, and\nfacilitates training-free generalization to new domains for curation. Across\n10+ experimental settings, validated by 14 multi-modal benchmarks, we\ndemonstrate consistent improvements over random sampling, baseline strategies\nand state-of-the-art selection methods, achieving 99.1% of full performance\nwith only 30% of the 2.6M data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The hypothesis that pretrained large language models (LLMs) necessitate only\nminimal supervision during the fine-tuning (SFT) stage (Zhou et al., 2024) has\nbeen substantiated by recent advancements in data curation and selection\nresearch. However, their stability and generalizability are compromised due to\nthe vulnerability to experimental setups and validation protocols, falling\nshort of surpassing random sampling (Diddee & Ippolito, 2024; Xia et al.,\n2024b). Built upon LLMs, multi-modal LLMs (MLLMs), combined with the sheer\ntoken volume and heightened heterogeneity of data sources, amplify both the\nsignificance and complexity of data selection.\n  To harvest multi-modal instructional data in a robust and efficient manner,\nwe re-define the granularity of the quality metric by decomposing it into 14\nvision-language-related capabilities, and introduce multi-modal rich scorers to\nevaluate the capabilities of each data candidate. To promote diversity, in\nlight of the inherent objective of the alignment stage, we take interaction\nstyle as diversity indicator and use a multi-modal rich styler to identify data\ninstruction patterns. In doing so, our multi-modal rich scorers and styler\n(mmSSR) guarantee that high-scoring information is conveyed to users in\ndiversified forms. Free from embedding-based clustering or greedy sampling,\nmmSSR efficiently scales to millions of data with varying budget constraints,\nsupports customization for general or specific capability acquisition, and\nfacilitates training-free generalization to new domains for curation. Across\n10+ experimental settings, validated by 14 multi-modal benchmarks, we\ndemonstrate consistent improvements over random sampling, baseline strategies\nand state-of-the-art selection methods, achieving 99.1% of full performance\nwith only 30% of the 2.6M data."
                },
                "authors": [
                    {
                        "name": "Mengyao Lyu"
                    },
                    {
                        "name": "Yan Li"
                    },
                    {
                        "name": "Huasong Zhong"
                    },
                    {
                        "name": "Wenhao Yang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    },
                    {
                        "name": "Zhenheng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zhenheng Yang"
                },
                "author": "Zhenheng Yang",
                "arxiv_comment": "update comparison with sota and analysis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13377v1",
                "updated": "2025-03-17T17:04:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    4,
                    20,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T17:04:20Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    4,
                    20,
                    0,
                    76,
                    0
                ],
                "title": "TimeZero: Temporal Video Grounding with Reasoning-Guided LVLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TimeZero: Temporal Video Grounding with Reasoning-Guided LVLM"
                },
                "summary": "We introduce TimeZero, a reasoning-guided LVLM designed for the temporal\nvideo grounding (TVG) task. This task requires precisely localizing relevant\nvideo segments within long videos based on a given language query. TimeZero\ntackles this challenge by extending the inference process, enabling the model\nto reason about video-language relationships solely through reinforcement\nlearning. To evaluate the effectiveness of TimeZero, we conduct experiments on\ntwo benchmarks, where TimeZero achieves state-of-the-art performance on\nCharades-STA. Code is available at https://github.com/www-Ye/TimeZero.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce TimeZero, a reasoning-guided LVLM designed for the temporal\nvideo grounding (TVG) task. This task requires precisely localizing relevant\nvideo segments within long videos based on a given language query. TimeZero\ntackles this challenge by extending the inference process, enabling the model\nto reason about video-language relationships solely through reinforcement\nlearning. To evaluate the effectiveness of TimeZero, we conduct experiments on\ntwo benchmarks, where TimeZero achieves state-of-the-art performance on\nCharades-STA. Code is available at https://github.com/www-Ye/TimeZero."
                },
                "authors": [
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Boshen Xu"
                    },
                    {
                        "name": "Zihao Yue"
                    },
                    {
                        "name": "Zihan Xiao"
                    },
                    {
                        "name": "Ziheng Wang"
                    },
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Dingyi Yang"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Qin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Qin Jin"
                },
                "author": "Qin Jin",
                "arxiv_comment": "Code: https://github.com/www-Ye/TimeZero",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13369v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13369v1",
                "updated": "2025-03-17T16:52:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    52,
                    46,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T16:52:46Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    52,
                    46,
                    0,
                    76,
                    0
                ],
                "title": "Sightation Counts: Leveraging Sighted User Feedback in Building a\n  BLV-aligned Dataset of Diagram Descriptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sightation Counts: Leveraging Sighted User Feedback in Building a\n  BLV-aligned Dataset of Diagram Descriptions"
                },
                "summary": "Often, the needs and visual abilities differ between the annotator group and\nthe end user group. Generating detailed diagram descriptions for blind and\nlow-vision (BLV) users is one such challenging domain. Sighted annotators could\ndescribe visuals with ease, but existing studies have shown that direct\ngenerations by them are costly, bias-prone, and somewhat lacking by BLV\nstandards. In this study, we ask sighted individuals to assess -- rather than\nproduce -- diagram descriptions generated by vision-language models (VLM) that\nhave been guided with latent supervision via a multi-pass inference. The\nsighted assessments prove effective and useful to professional educators who\nare themselves BLV and teach visually impaired learners. We release Sightation,\na collection of diagram description datasets spanning 5k diagrams and 137k\nsamples for completion, preference, retrieval, question answering, and\nreasoning training purposes and demonstrate their fine-tuning potential in\nvarious downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Often, the needs and visual abilities differ between the annotator group and\nthe end user group. Generating detailed diagram descriptions for blind and\nlow-vision (BLV) users is one such challenging domain. Sighted annotators could\ndescribe visuals with ease, but existing studies have shown that direct\ngenerations by them are costly, bias-prone, and somewhat lacking by BLV\nstandards. In this study, we ask sighted individuals to assess -- rather than\nproduce -- diagram descriptions generated by vision-language models (VLM) that\nhave been guided with latent supervision via a multi-pass inference. The\nsighted assessments prove effective and useful to professional educators who\nare themselves BLV and teach visually impaired learners. We release Sightation,\na collection of diagram description datasets spanning 5k diagrams and 137k\nsamples for completion, preference, retrieval, question answering, and\nreasoning training purposes and demonstrate their fine-tuning potential in\nvarious downstream tasks."
                },
                "authors": [
                    {
                        "name": "Wan Ju Kang"
                    },
                    {
                        "name": "Eunki Kim"
                    },
                    {
                        "name": "Na Min An"
                    },
                    {
                        "name": "Sangryul Kim"
                    },
                    {
                        "name": "Haemin Choi"
                    },
                    {
                        "name": "Ki Hoon Kwak"
                    },
                    {
                        "name": "James Thorne"
                    }
                ],
                "author_detail": {
                    "name": "James Thorne"
                },
                "author": "James Thorne",
                "arxiv_comment": "37 pages, 10 figures, 21 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13369v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13369v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12757v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12757v3",
                "updated": "2025-03-18T06:49:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    6,
                    49,
                    14,
                    1,
                    77,
                    0
                ],
                "published": "2024-06-18T16:24:48Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    16,
                    24,
                    48,
                    1,
                    170,
                    0
                ],
                "title": "MAC: A Benchmark for Multiple Attributes Compositional Zero-Shot\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAC: A Benchmark for Multiple Attributes Compositional Zero-Shot\n  Learning"
                },
                "summary": "Compositional Zero-Shot Learning (CZSL) aims to learn semantic primitives\n(attributes and objects) from seen compositions and recognize unseen\nattribute-object compositions. Existing CZSL datasets focus on single\nattributes, neglecting the fact that objects naturally exhibit multiple\ninterrelated attributes. Their narrow attribute scope and single attribute\nlabeling introduce annotation biases, misleading the learning of attributes and\ncausing inaccurate evaluation. To address these issues, we introduce the\nMulti-Attribute Composition (MAC) dataset, encompassing 22,838 images and\n17,627 compositions with comprehensive and representative attribute\nannotations. MAC shows complex relationship between attributes and objects,\nwith each attribute type linked to an average of 82.2 object types, and each\nobject type associated with 31.4 attribute types. Based on MAC, we propose\nmulti-attribute compositional zero-shot learning that requires deeper semantic\nunderstanding and advanced attribute associations, establishing a more\nrealistic and challenging benchmark for CZSL. We also propose Multi-attribute\nVisual-Primitive Integrator (MVP-Integrator), a robust baseline for\nmulti-attribute CZSL, which disentangles semantic primitives and performs\neffective visual-primitive association. Experimental results demonstrate that\nMVP-Integrator significantly outperforms existing CZSL methods on MAC with\nimproved inference efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Zero-Shot Learning (CZSL) aims to learn semantic primitives\n(attributes and objects) from seen compositions and recognize unseen\nattribute-object compositions. Existing CZSL datasets focus on single\nattributes, neglecting the fact that objects naturally exhibit multiple\ninterrelated attributes. Their narrow attribute scope and single attribute\nlabeling introduce annotation biases, misleading the learning of attributes and\ncausing inaccurate evaluation. To address these issues, we introduce the\nMulti-Attribute Composition (MAC) dataset, encompassing 22,838 images and\n17,627 compositions with comprehensive and representative attribute\nannotations. MAC shows complex relationship between attributes and objects,\nwith each attribute type linked to an average of 82.2 object types, and each\nobject type associated with 31.4 attribute types. Based on MAC, we propose\nmulti-attribute compositional zero-shot learning that requires deeper semantic\nunderstanding and advanced attribute associations, establishing a more\nrealistic and challenging benchmark for CZSL. We also propose Multi-attribute\nVisual-Primitive Integrator (MVP-Integrator), a robust baseline for\nmulti-attribute CZSL, which disentangles semantic primitives and performs\neffective visual-primitive association. Experimental results demonstrate that\nMVP-Integrator significantly outperforms existing CZSL methods on MAC with\nimproved inference efficiency."
                },
                "authors": [
                    {
                        "name": "Shuo Xu"
                    },
                    {
                        "name": "Sai Wang"
                    },
                    {
                        "name": "Xinyue Hu"
                    },
                    {
                        "name": "Yutian Lin"
                    },
                    {
                        "name": "Sibei Yang"
                    },
                    {
                        "name": "Yu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wu"
                },
                "author": "Yu Wu",
                "arxiv_comment": "13pages,5figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12757v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12757v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11621v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11621v2",
                "updated": "2025-03-17T16:47:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    47,
                    7,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-14T17:41:08Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    41,
                    8,
                    4,
                    73,
                    0
                ],
                "title": "Euclid preparation. BAO analysis of photometric galaxy clustering in\n  configuration space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Euclid preparation. BAO analysis of photometric galaxy clustering in\n  configuration space"
                },
                "summary": "With about 1.5 billion galaxies expected to be observed, the very large\nnumber of objects in the Euclid photometric survey will allow for precise\nstudies of galaxy clustering from a single survey, over a large range of\nredshifts $0.2 < z < 2.5$. In this work, we use photometric redshifts to\nextract the baryon acoustic oscillation signal (BAO) from the Flagship galaxy\nmock catalogue with a tomographic approach to constrain the evolution of the\nUniverse and infer its cosmological parameters. We measure the two-point\nangular correlation function in 13 redshift bins. A template-fitting approach\nis applied to the measurement to extract the shift of the BAO peak through the\ntransverse Alcock--Paczynski parameter $\\alpha$. A joint analysis of all\nredshift bins is performed to constrain $\\alpha$ at the effective redshift\n$z_\\mathrm{eff}=0.77$ with MCMC and profile likelihood techniques. We also\nextract one $\\alpha_i$ parameter per redshift bin to quantify its evolution as\na function of time. From these 13 $\\alpha_i$, which are directly proportional\nto the ratio $D_\\mathrm{A}/\\,r_\\mathrm{s,\\,drag}$, we constrain $h$,\n$\\Omega_\\mathrm{b}$, and $\\Omega_\\mathrm{cdm}$. From the joint analysis, we\nconstrain $\\alpha(z_\\mathrm{eff}=0.77)=1.0011^{+0.0078}_{-0.0079}$, which\nrepresents a three-fold improvement over current constraints from the Dark\nEnergy Survey. As expected, the constraining power in the analysis of each\nredshift bin is lower, with an uncertainty ranging from $\\pm\\,0.13$ to\n$\\pm\\,0.024$. From these results, we constrain $h$ at 0.45 %,\n$\\Omega_\\mathrm{b}$ at 0.91 %, and $\\Omega_\\mathrm{cdm}$ at 7.7 %. We quantify\nthe influence of analysis choices like the template, scale cuts, redshift bins,\nand systematic effects like redshift-space distortions over our constraints\nboth at the level of the extracted $\\alpha_i$ parameters and at the level of\ncosmological inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With about 1.5 billion galaxies expected to be observed, the very large\nnumber of objects in the Euclid photometric survey will allow for precise\nstudies of galaxy clustering from a single survey, over a large range of\nredshifts $0.2 < z < 2.5$. In this work, we use photometric redshifts to\nextract the baryon acoustic oscillation signal (BAO) from the Flagship galaxy\nmock catalogue with a tomographic approach to constrain the evolution of the\nUniverse and infer its cosmological parameters. We measure the two-point\nangular correlation function in 13 redshift bins. A template-fitting approach\nis applied to the measurement to extract the shift of the BAO peak through the\ntransverse Alcock--Paczynski parameter $\\alpha$. A joint analysis of all\nredshift bins is performed to constrain $\\alpha$ at the effective redshift\n$z_\\mathrm{eff}=0.77$ with MCMC and profile likelihood techniques. We also\nextract one $\\alpha_i$ parameter per redshift bin to quantify its evolution as\na function of time. From these 13 $\\alpha_i$, which are directly proportional\nto the ratio $D_\\mathrm{A}/\\,r_\\mathrm{s,\\,drag}$, we constrain $h$,\n$\\Omega_\\mathrm{b}$, and $\\Omega_\\mathrm{cdm}$. From the joint analysis, we\nconstrain $\\alpha(z_\\mathrm{eff}=0.77)=1.0011^{+0.0078}_{-0.0079}$, which\nrepresents a three-fold improvement over current constraints from the Dark\nEnergy Survey. As expected, the constraining power in the analysis of each\nredshift bin is lower, with an uncertainty ranging from $\\pm\\,0.13$ to\n$\\pm\\,0.024$. From these results, we constrain $h$ at 0.45 %,\n$\\Omega_\\mathrm{b}$ at 0.91 %, and $\\Omega_\\mathrm{cdm}$ at 7.7 %. We quantify\nthe influence of analysis choices like the template, scale cuts, redshift bins,\nand systematic effects like redshift-space distortions over our constraints\nboth at the level of the extracted $\\alpha_i$ parameters and at the level of\ncosmological inference."
                },
                "authors": [
                    {
                        "name": "Euclid Collaboration"
                    },
                    {
                        "name": "V. Duret"
                    },
                    {
                        "name": "S. Escoffier"
                    },
                    {
                        "name": "W. Gillard"
                    },
                    {
                        "name": "I. Tutusaus"
                    },
                    {
                        "name": "S. Camera"
                    },
                    {
                        "name": "N. Tessore"
                    },
                    {
                        "name": "F. J. Castander"
                    },
                    {
                        "name": "N. Aghanim"
                    },
                    {
                        "name": "A. Amara"
                    },
                    {
                        "name": "L. Amendola"
                    },
                    {
                        "name": "S. Andreon"
                    },
                    {
                        "name": "N. Auricchio"
                    },
                    {
                        "name": "C. Baccigalupi"
                    },
                    {
                        "name": "M. Baldi"
                    },
                    {
                        "name": "S. Bardelli"
                    },
                    {
                        "name": "P. Battaglia"
                    },
                    {
                        "name": "A. Biviano"
                    },
                    {
                        "name": "D. Bonino"
                    },
                    {
                        "name": "E. Branchini"
                    },
                    {
                        "name": "M. Brescia"
                    },
                    {
                        "name": "J. Brinchmann"
                    },
                    {
                        "name": "A. Caillat"
                    },
                    {
                        "name": "G. Cañas-Herrera"
                    },
                    {
                        "name": "V. Capobianco"
                    },
                    {
                        "name": "C. Carbone"
                    },
                    {
                        "name": "V. F. Cardone"
                    },
                    {
                        "name": "J. Carretero"
                    },
                    {
                        "name": "S. Casas"
                    },
                    {
                        "name": "M. Castellano"
                    },
                    {
                        "name": "G. Castignani"
                    },
                    {
                        "name": "S. Cavuoti"
                    },
                    {
                        "name": "K. C. Chambers"
                    },
                    {
                        "name": "A. Cimatti"
                    },
                    {
                        "name": "C. Colodro-Conde"
                    },
                    {
                        "name": "G. Congedo"
                    },
                    {
                        "name": "C. J. Conselice"
                    },
                    {
                        "name": "L. Conversi"
                    },
                    {
                        "name": "Y. Copin"
                    },
                    {
                        "name": "F. Courbin"
                    },
                    {
                        "name": "H. M. Courtois"
                    },
                    {
                        "name": "M. Cropper"
                    },
                    {
                        "name": "A. Da Silva"
                    },
                    {
                        "name": "H. Degaudenzi"
                    },
                    {
                        "name": "S. de la Torre"
                    },
                    {
                        "name": "G. De Lucia"
                    },
                    {
                        "name": "A. M. Di Giorgio"
                    },
                    {
                        "name": "H. Dole"
                    },
                    {
                        "name": "F. Dubath"
                    },
                    {
                        "name": "X. Dupac"
                    },
                    {
                        "name": "S. Dusini"
                    },
                    {
                        "name": "A. Ealet"
                    },
                    {
                        "name": "M. Farina"
                    },
                    {
                        "name": "R. Farinelli"
                    },
                    {
                        "name": "S. Farrens"
                    },
                    {
                        "name": "F. Faustini"
                    },
                    {
                        "name": "S. Ferriol"
                    },
                    {
                        "name": "F. Finelli"
                    },
                    {
                        "name": "S. Fotopoulou"
                    },
                    {
                        "name": "N. Fourmanoit"
                    },
                    {
                        "name": "M. Frailis"
                    },
                    {
                        "name": "E. Franceschi"
                    },
                    {
                        "name": "M. Fumana"
                    },
                    {
                        "name": "S. Galeotta"
                    },
                    {
                        "name": "B. Gillis"
                    },
                    {
                        "name": "C. Giocoli"
                    },
                    {
                        "name": "J. Gracia-Carpio"
                    },
                    {
                        "name": "A. Grazian"
                    },
                    {
                        "name": "F. Grupp"
                    },
                    {
                        "name": "S. V. H. Haugan"
                    },
                    {
                        "name": "W. Holmes"
                    },
                    {
                        "name": "F. Hormuth"
                    },
                    {
                        "name": "A. Hornstrup"
                    },
                    {
                        "name": "P. Hudelot"
                    },
                    {
                        "name": "K. Jahnke"
                    },
                    {
                        "name": "M. Jhabvala"
                    },
                    {
                        "name": "B. Joachimi"
                    },
                    {
                        "name": "E. Keihänen"
                    },
                    {
                        "name": "S. Kermiche"
                    },
                    {
                        "name": "A. Kiessling"
                    },
                    {
                        "name": "M. Kilbinger"
                    },
                    {
                        "name": "B. Kubik"
                    },
                    {
                        "name": "M. Kunz"
                    },
                    {
                        "name": "H. Kurki-Suonio"
                    },
                    {
                        "name": "O. Lahav"
                    },
                    {
                        "name": "A. M. C. Le Brun"
                    },
                    {
                        "name": "S. Ligori"
                    },
                    {
                        "name": "P. B. Lilje"
                    },
                    {
                        "name": "V. Lindholm"
                    },
                    {
                        "name": "I. Lloro"
                    },
                    {
                        "name": "G. Mainetti"
                    },
                    {
                        "name": "D. Maino"
                    },
                    {
                        "name": "E. Maiorano"
                    },
                    {
                        "name": "O. Mansutti"
                    },
                    {
                        "name": "S. Marcin"
                    },
                    {
                        "name": "O. Marggraf"
                    },
                    {
                        "name": "K. Markovic"
                    },
                    {
                        "name": "M. Martinelli"
                    },
                    {
                        "name": "N. Martinet"
                    },
                    {
                        "name": "F. Marulli"
                    },
                    {
                        "name": "R. Massey"
                    },
                    {
                        "name": "S. Maurogordato"
                    },
                    {
                        "name": "E. Medinaceli"
                    },
                    {
                        "name": "S. Mei"
                    },
                    {
                        "name": "M. Melchior"
                    },
                    {
                        "name": "Y. Mellier"
                    },
                    {
                        "name": "M. Meneghetti"
                    },
                    {
                        "name": "E. Merlin"
                    },
                    {
                        "name": "G. Meylan"
                    },
                    {
                        "name": "A. Mora"
                    },
                    {
                        "name": "M. Moresco"
                    },
                    {
                        "name": "B. Morin"
                    },
                    {
                        "name": "L. Moscardini"
                    },
                    {
                        "name": "E. Munari"
                    },
                    {
                        "name": "R. Nakajima"
                    },
                    {
                        "name": "C. Neissner"
                    },
                    {
                        "name": "R. C. Nichol"
                    },
                    {
                        "name": "S. -M. Niemi"
                    },
                    {
                        "name": "C. Padilla"
                    },
                    {
                        "name": "S. Paltani"
                    },
                    {
                        "name": "F. Pasian"
                    },
                    {
                        "name": "K. Pedersen"
                    },
                    {
                        "name": "W. J. Percival"
                    },
                    {
                        "name": "V. Pettorino"
                    },
                    {
                        "name": "S. Pires"
                    },
                    {
                        "name": "G. Polenta"
                    },
                    {
                        "name": "M. Poncet"
                    },
                    {
                        "name": "L. A. Popa"
                    },
                    {
                        "name": "L. Pozzetti"
                    },
                    {
                        "name": "F. Raison"
                    },
                    {
                        "name": "R. Rebolo"
                    },
                    {
                        "name": "J. Rhodes"
                    },
                    {
                        "name": "G. Riccio"
                    },
                    {
                        "name": "E. Romelli"
                    },
                    {
                        "name": "M. Roncarelli"
                    },
                    {
                        "name": "R. Saglia"
                    },
                    {
                        "name": "Z. Sakr"
                    },
                    {
                        "name": "D. Sapone"
                    },
                    {
                        "name": "B. Sartoris"
                    },
                    {
                        "name": "J. A. Schewtschenko"
                    },
                    {
                        "name": "P. Schneider"
                    },
                    {
                        "name": "T. Schrabback"
                    },
                    {
                        "name": "A. Secroun"
                    },
                    {
                        "name": "E. Sefusatti"
                    },
                    {
                        "name": "G. Seidel"
                    },
                    {
                        "name": "S. Serrano"
                    },
                    {
                        "name": "P. Simon"
                    },
                    {
                        "name": "C. Sirignano"
                    },
                    {
                        "name": "G. Sirri"
                    },
                    {
                        "name": "A. Spurio Mancini"
                    },
                    {
                        "name": "L. Stanco"
                    },
                    {
                        "name": "J. -L. Starck"
                    },
                    {
                        "name": "J. Steinwagner"
                    },
                    {
                        "name": "P. Tallada-Crespí"
                    },
                    {
                        "name": "D. Tavagnacco"
                    },
                    {
                        "name": "A. N. Taylor"
                    },
                    {
                        "name": "I. Tereno"
                    },
                    {
                        "name": "S. Toft"
                    },
                    {
                        "name": "R. Toledo-Moreo"
                    },
                    {
                        "name": "F. Torradeflot"
                    },
                    {
                        "name": "J. Valiviita"
                    },
                    {
                        "name": "T. Vassallo"
                    },
                    {
                        "name": "G. Verdoes Kleijn"
                    },
                    {
                        "name": "A. Veropalumbo"
                    },
                    {
                        "name": "Y. Wang"
                    },
                    {
                        "name": "J. Weller"
                    },
                    {
                        "name": "G. Zamorani"
                    },
                    {
                        "name": "F. M. Zerbi"
                    },
                    {
                        "name": "E. Zucca"
                    },
                    {
                        "name": "M. Bolzonella"
                    },
                    {
                        "name": "C. Burigana"
                    },
                    {
                        "name": "M. Calabrese"
                    },
                    {
                        "name": "D. Di Ferdinando"
                    },
                    {
                        "name": "J. A. Escartin Vigo"
                    },
                    {
                        "name": "L. Gabarra"
                    },
                    {
                        "name": "S. Matthew"
                    },
                    {
                        "name": "N. Mauri"
                    },
                    {
                        "name": "A. Pezzotta"
                    },
                    {
                        "name": "M. Pöntinen"
                    },
                    {
                        "name": "C. Porciani"
                    },
                    {
                        "name": "V. Scottez"
                    },
                    {
                        "name": "M. Tenti"
                    },
                    {
                        "name": "M. Viel"
                    },
                    {
                        "name": "M. Wiesmann"
                    },
                    {
                        "name": "Y. Akrami"
                    },
                    {
                        "name": "V. Allevato"
                    },
                    {
                        "name": "I. T. Andika"
                    },
                    {
                        "name": "M. Archidiacono"
                    },
                    {
                        "name": "F. Atrio-Barandela"
                    },
                    {
                        "name": "A. Balaguera-Antolinez"
                    },
                    {
                        "name": "M. Ballardini"
                    },
                    {
                        "name": "D. Bertacca"
                    },
                    {
                        "name": "M. Bethermin"
                    },
                    {
                        "name": "A. Blanchard"
                    },
                    {
                        "name": "L. Blot"
                    },
                    {
                        "name": "H. Böhringer"
                    },
                    {
                        "name": "S. Borgani"
                    },
                    {
                        "name": "M. L. Brown"
                    },
                    {
                        "name": "S. Bruton"
                    },
                    {
                        "name": "R. Cabanac"
                    },
                    {
                        "name": "A. Calabro"
                    },
                    {
                        "name": "B. Camacho Quevedo"
                    },
                    {
                        "name": "A. Cappi"
                    },
                    {
                        "name": "F. Caro"
                    },
                    {
                        "name": "C. S. Carvalho"
                    },
                    {
                        "name": "T. Castro"
                    },
                    {
                        "name": "F. Cogato"
                    },
                    {
                        "name": "S. Contarini"
                    },
                    {
                        "name": "T. Contini"
                    },
                    {
                        "name": "A. R. Cooray"
                    },
                    {
                        "name": "S. Davini"
                    },
                    {
                        "name": "F. De Paolis"
                    },
                    {
                        "name": "G. Desprez"
                    },
                    {
                        "name": "A. Díaz-Sánchez"
                    },
                    {
                        "name": "S. Di Domizio"
                    },
                    {
                        "name": "J. M. Diego"
                    },
                    {
                        "name": "A. G. Ferrari"
                    },
                    {
                        "name": "P. G. Ferreira"
                    },
                    {
                        "name": "A. Finoguenov"
                    },
                    {
                        "name": "K. Ganga"
                    },
                    {
                        "name": "J. García-Bellido"
                    },
                    {
                        "name": "T. Gasparetto"
                    },
                    {
                        "name": "E. Gaztanaga"
                    },
                    {
                        "name": "F. Giacomini"
                    },
                    {
                        "name": "F. Gianotti"
                    },
                    {
                        "name": "G. Gozaliasl"
                    },
                    {
                        "name": "A. Gregorio"
                    },
                    {
                        "name": "M. Guidi"
                    },
                    {
                        "name": "C. M. Gutierrez"
                    },
                    {
                        "name": "A. Hall"
                    },
                    {
                        "name": "S. Hemmati"
                    },
                    {
                        "name": "H. Hildebrandt"
                    },
                    {
                        "name": "J. Hjorth"
                    },
                    {
                        "name": "J. J. E. Kajava"
                    },
                    {
                        "name": "Y. Kang"
                    },
                    {
                        "name": "V. Kansal"
                    },
                    {
                        "name": "D. Karagiannis"
                    },
                    {
                        "name": "C. C. Kirkpatrick"
                    },
                    {
                        "name": "S. Kruk"
                    },
                    {
                        "name": "M. Lattanzi"
                    },
                    {
                        "name": "M. Lembo"
                    },
                    {
                        "name": "G. Leroy"
                    },
                    {
                        "name": "J. Lesgourgues"
                    },
                    {
                        "name": "T. I. Liaudat"
                    },
                    {
                        "name": "S. J. Liu"
                    },
                    {
                        "name": "A. Loureiro"
                    },
                    {
                        "name": "G. Maggio"
                    },
                    {
                        "name": "M. Magliocchetti"
                    },
                    {
                        "name": "F. Mannucci"
                    },
                    {
                        "name": "R. Maoli"
                    },
                    {
                        "name": "J. Martín-Fleitas"
                    },
                    {
                        "name": "C. J. A. P. Martins"
                    },
                    {
                        "name": "L. Maurin"
                    },
                    {
                        "name": "R. B. Metcalf"
                    },
                    {
                        "name": "M. Miluzio"
                    },
                    {
                        "name": "P. Monaco"
                    },
                    {
                        "name": "C. Moretti"
                    },
                    {
                        "name": "C. Murray"
                    },
                    {
                        "name": "S. Nadathur"
                    },
                    {
                        "name": "K. Naidoo"
                    },
                    {
                        "name": "A. Navarro-Alsina"
                    },
                    {
                        "name": "S. Nesseris"
                    },
                    {
                        "name": "K. Paterson"
                    },
                    {
                        "name": "A. Pisani"
                    },
                    {
                        "name": "D. Potter"
                    },
                    {
                        "name": "I. Risso"
                    },
                    {
                        "name": "P. -F. Rocci"
                    },
                    {
                        "name": "M. Sahlén"
                    },
                    {
                        "name": "E. Sarpa"
                    },
                    {
                        "name": "A. Schneider"
                    },
                    {
                        "name": "D. Sciotti"
                    },
                    {
                        "name": "E. Sellentin"
                    },
                    {
                        "name": "M. Sereno"
                    },
                    {
                        "name": "A. Silvestri"
                    },
                    {
                        "name": "L. C. Smith"
                    },
                    {
                        "name": "K. Tanidis"
                    },
                    {
                        "name": "C. Tao"
                    },
                    {
                        "name": "G. Testera"
                    },
                    {
                        "name": "R. Teyssier"
                    },
                    {
                        "name": "S. Tosi"
                    },
                    {
                        "name": "A. Troja"
                    },
                    {
                        "name": "M. Tucci"
                    },
                    {
                        "name": "C. Valieri"
                    },
                    {
                        "name": "A. Venhola"
                    },
                    {
                        "name": "D. Vergani"
                    },
                    {
                        "name": "F. Vernizzi"
                    },
                    {
                        "name": "G. Verza"
                    },
                    {
                        "name": "P. Vielzeuf"
                    },
                    {
                        "name": "N. A. Walton"
                    }
                ],
                "author_detail": {
                    "name": "N. A. Walton"
                },
                "author": "N. A. Walton",
                "arxiv_affiliation": "Institute of Astronomy, University of Cambridge, Madingley Road, Cambridge CB3 0HA, UK",
                "arxiv_comment": "18 pages, 12 figures, submitted to A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11621v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11621v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13360v1",
                "updated": "2025-03-17T16:45:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    45,
                    12,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T16:45:12Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    45,
                    12,
                    0,
                    76,
                    0
                ],
                "title": "Mitigating Visual Forgetting via Take-along Visual Conditioning for\n  Multi-modal Long CoT Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Visual Forgetting via Take-along Visual Conditioning for\n  Multi-modal Long CoT Reasoning"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nenhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting\nto advanced, product-oriented solutions like OpenAI o1. During our\nre-implementation of this model, we noticed that in multimodal tasks requiring\nvisual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to\nmaintain focus on the visual information, in other words, MLLMs suffer from a\ngradual decline in attention to visual information as reasoning progresses,\ncausing text-over-relied outputs. To investigate this, we ablate image inputs\nduring long-chain reasoning. Concretely, we truncate the reasoning process\nmidway, then re-complete the reasoning process with the input image removed. We\nobserve only a ~2% accuracy drop on MathVista's test-hard subset, revealing the\nmodel's textual outputs dominate the following reasoning process. Motivated by\nthis, we propose Take-along Visual Conditioning (TVC), a strategy that shifts\nimage input to critical reasoning stages and compresses redundant visual tokens\nvia dynamic pruning. This methodology helps the model retain attention to the\nvisual components throughout the reasoning. Our approach achieves\nstate-of-the-art performance on average across five mathematical reasoning\nbenchmarks (+3.4% vs previous sota), demonstrating the effectiveness of TVC in\nenhancing multimodal reasoning systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have demonstrated\nenhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting\nto advanced, product-oriented solutions like OpenAI o1. During our\nre-implementation of this model, we noticed that in multimodal tasks requiring\nvisual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to\nmaintain focus on the visual information, in other words, MLLMs suffer from a\ngradual decline in attention to visual information as reasoning progresses,\ncausing text-over-relied outputs. To investigate this, we ablate image inputs\nduring long-chain reasoning. Concretely, we truncate the reasoning process\nmidway, then re-complete the reasoning process with the input image removed. We\nobserve only a ~2% accuracy drop on MathVista's test-hard subset, revealing the\nmodel's textual outputs dominate the following reasoning process. Motivated by\nthis, we propose Take-along Visual Conditioning (TVC), a strategy that shifts\nimage input to critical reasoning stages and compresses redundant visual tokens\nvia dynamic pruning. This methodology helps the model retain attention to the\nvisual components throughout the reasoning. Our approach achieves\nstate-of-the-art performance on average across five mathematical reasoning\nbenchmarks (+3.4% vs previous sota), demonstrating the effectiveness of TVC in\nenhancing multimodal reasoning systems."
                },
                "authors": [
                    {
                        "name": "Hai-Long Sun"
                    },
                    {
                        "name": "Zhun Sun"
                    },
                    {
                        "name": "Houwen Peng"
                    },
                    {
                        "name": "Han-Jia Ye"
                    }
                ],
                "author_detail": {
                    "name": "Han-Jia Ye"
                },
                "author": "Han-Jia Ye",
                "arxiv_comment": "The project page is available at\n  https://sun-hailong.github.io/projects/TVC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13356v1",
                "updated": "2025-03-17T16:42:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    42,
                    34,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T16:42:34Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    42,
                    34,
                    0,
                    76,
                    0
                ],
                "title": "Agents Play Thousands of 3D Video Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents Play Thousands of 3D Video Games"
                },
                "summary": "We present PORTAL, a novel framework for developing artificial intelligence\nagents capable of playing thousands of 3D video games through language-guided\npolicy generation. By transforming decision-making problems into language\nmodeling tasks, our approach leverages large language models (LLMs) to generate\nbehavior trees represented in domain-specific language (DSL). This method\neliminates the computational burden associated with traditional reinforcement\nlearning approaches while preserving strategic depth and rapid adaptability.\nOur framework introduces a hybrid policy structure that combines rule-based\nnodes with neural network components, enabling both high-level strategic\nreasoning and precise low-level control. A dual-feedback mechanism\nincorporating quantitative game metrics and vision-language model analysis\nfacilitates iterative policy improvement at both tactical and strategic levels.\nThe resulting policies are instantaneously deployable, human-interpretable, and\ncapable of generalizing across diverse gaming environments. Experimental\nresults demonstrate PORTAL's effectiveness across thousands of first-person\nshooter (FPS) games, showcasing significant improvements in development\nefficiency, policy generalization, and behavior diversity compared to\ntraditional approaches. PORTAL represents a significant advancement in game AI\ndevelopment, offering a practical solution for creating sophisticated agents\nthat can operate across thousands of commercial video games with minimal\ndevelopment overhead. Experiment results on the 3D video games are best viewed\non https://zhongwen.one/projects/portal .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present PORTAL, a novel framework for developing artificial intelligence\nagents capable of playing thousands of 3D video games through language-guided\npolicy generation. By transforming decision-making problems into language\nmodeling tasks, our approach leverages large language models (LLMs) to generate\nbehavior trees represented in domain-specific language (DSL). This method\neliminates the computational burden associated with traditional reinforcement\nlearning approaches while preserving strategic depth and rapid adaptability.\nOur framework introduces a hybrid policy structure that combines rule-based\nnodes with neural network components, enabling both high-level strategic\nreasoning and precise low-level control. A dual-feedback mechanism\nincorporating quantitative game metrics and vision-language model analysis\nfacilitates iterative policy improvement at both tactical and strategic levels.\nThe resulting policies are instantaneously deployable, human-interpretable, and\ncapable of generalizing across diverse gaming environments. Experimental\nresults demonstrate PORTAL's effectiveness across thousands of first-person\nshooter (FPS) games, showcasing significant improvements in development\nefficiency, policy generalization, and behavior diversity compared to\ntraditional approaches. PORTAL represents a significant advancement in game AI\ndevelopment, offering a practical solution for creating sophisticated agents\nthat can operate across thousands of commercial video games with minimal\ndevelopment overhead. Experiment results on the 3D video games are best viewed\non https://zhongwen.one/projects/portal ."
                },
                "authors": [
                    {
                        "name": "Zhongwen Xu"
                    },
                    {
                        "name": "Xianliang Wang"
                    },
                    {
                        "name": "Siyi Li"
                    },
                    {
                        "name": "Tao Yu"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Qiang Fu"
                    },
                    {
                        "name": "Wei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Yang"
                },
                "author": "Wei Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11506v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11506v2",
                "updated": "2025-03-17T16:22:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    22,
                    15,
                    0,
                    76,
                    0
                ],
                "published": "2024-10-15T11:17:19Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    11,
                    17,
                    19,
                    1,
                    289,
                    0
                ],
                "title": "Spatio-Temporal Distortion Aware Omnidirectional Video Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatio-Temporal Distortion Aware Omnidirectional Video Super-Resolution"
                },
                "summary": "Omnidirectional video (ODV) provides an immersive visual experience and is\nwidely utilized in virtual reality and augmented reality. However, restricted\ncapturing devices and transmission bandwidth lead to low-resolution ODVs. Video\nsuper-resolution (SR) is proposed to enhance resolution, but practical ODV\nspatial projection distortions and temporal flickering are not well addressed\ndirectly applying existing methods. To achieve better ODV-SR reconstruction, we\npropose a Spatio-Temporal Distortion Aware Network (STDAN) oriented to ODV\ncharacteristics. Specifically, a spatially continuous distortion modulation\nmodule is introduced to improve discrete projection distortions. Next, we\ndesign an interlaced multi-frame reconstruction mechanism to refine temporal\nconsistency across frames. Furthermore, we incorporate latitude-saliency\nadaptive weights during training to concentrate on regions with higher texture\ncomplexity and human-watching interest. In general, we explore inference-free\nand real-world viewing matched strategies to provide an application-friendly\nmethod on a novel ODV-SR dataset with practical scenarios. Extensive\nexperimental results demonstrate the superior performance of the proposed STDAN\nover state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Omnidirectional video (ODV) provides an immersive visual experience and is\nwidely utilized in virtual reality and augmented reality. However, restricted\ncapturing devices and transmission bandwidth lead to low-resolution ODVs. Video\nsuper-resolution (SR) is proposed to enhance resolution, but practical ODV\nspatial projection distortions and temporal flickering are not well addressed\ndirectly applying existing methods. To achieve better ODV-SR reconstruction, we\npropose a Spatio-Temporal Distortion Aware Network (STDAN) oriented to ODV\ncharacteristics. Specifically, a spatially continuous distortion modulation\nmodule is introduced to improve discrete projection distortions. Next, we\ndesign an interlaced multi-frame reconstruction mechanism to refine temporal\nconsistency across frames. Furthermore, we incorporate latitude-saliency\nadaptive weights during training to concentrate on regions with higher texture\ncomplexity and human-watching interest. In general, we explore inference-free\nand real-world viewing matched strategies to provide an application-friendly\nmethod on a novel ODV-SR dataset with practical scenarios. Extensive\nexperimental results demonstrate the superior performance of the proposed STDAN\nover state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Hongyu An"
                    },
                    {
                        "name": "Xinfeng Zhang"
                    },
                    {
                        "name": "Shijie Zhao"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Ruiqin Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Ruiqin Xiong"
                },
                "author": "Ruiqin Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11506v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11506v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13344v1",
                "updated": "2025-03-17T16:22:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    22,
                    0,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T16:22:00Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    22,
                    0,
                    0,
                    76,
                    0
                ],
                "title": "STEP: Simultaneous Tracking and Estimation of Pose for Animals and\n  Humans",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STEP: Simultaneous Tracking and Estimation of Pose for Animals and\n  Humans"
                },
                "summary": "We introduce STEP, a novel framework utilizing Transformer-based\ndiscriminative model prediction for simultaneous tracking and estimation of\npose across diverse animal species and humans. We are inspired by the fact that\nthe human brain exploits spatiotemporal continuity and performs concurrent\nlocalization and pose estimation despite the specialization of brain areas for\nform and motion processing. Traditional discriminative models typically require\npredefined target states for determining model weights, a challenge we address\nthrough Gaussian Map Soft Prediction (GMSP) and Offset Map Regression Adapter\n(OMRA) Modules. These modules remove the necessity of keypoint target states as\ninput, streamlining the process. Our method starts with a known target state\ninitialized through a pre-trained detector or manual initialization in the\ninitial frame of a given video sequence. It then seamlessly tracks the target\nand estimates keypoints of anatomical importance as output for subsequent\nframes. Unlike prevalent top-down pose estimation methods, our approach doesn't\nrely on per-frame target detections due to its tracking capability. This\nfacilitates a significant advancement in inference efficiency and potential\napplications. We train and validate our approach on datasets encompassing\ndiverse species. Our experiments demonstrate superior results compared to\nexisting methods, opening doors to various applications, including but not\nlimited to action recognition and behavioral analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce STEP, a novel framework utilizing Transformer-based\ndiscriminative model prediction for simultaneous tracking and estimation of\npose across diverse animal species and humans. We are inspired by the fact that\nthe human brain exploits spatiotemporal continuity and performs concurrent\nlocalization and pose estimation despite the specialization of brain areas for\nform and motion processing. Traditional discriminative models typically require\npredefined target states for determining model weights, a challenge we address\nthrough Gaussian Map Soft Prediction (GMSP) and Offset Map Regression Adapter\n(OMRA) Modules. These modules remove the necessity of keypoint target states as\ninput, streamlining the process. Our method starts with a known target state\ninitialized through a pre-trained detector or manual initialization in the\ninitial frame of a given video sequence. It then seamlessly tracks the target\nand estimates keypoints of anatomical importance as output for subsequent\nframes. Unlike prevalent top-down pose estimation methods, our approach doesn't\nrely on per-frame target detections due to its tracking capability. This\nfacilitates a significant advancement in inference efficiency and potential\napplications. We train and validate our approach on datasets encompassing\ndiverse species. Our experiments demonstrate superior results compared to\nexisting methods, opening doors to various applications, including but not\nlimited to action recognition and behavioral analysis."
                },
                "authors": [
                    {
                        "name": "Shashikant Verma"
                    },
                    {
                        "name": "Harish Katti"
                    },
                    {
                        "name": "Soumyaratna Debnath"
                    },
                    {
                        "name": "Yamuna Swamy"
                    },
                    {
                        "name": "Shanmuganathan Raman"
                    }
                ],
                "author_detail": {
                    "name": "Shanmuganathan Raman"
                },
                "author": "Shanmuganathan Raman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13340v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13340v1",
                "updated": "2025-03-17T16:18:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    18,
                    23,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T16:18:23Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    18,
                    23,
                    0,
                    76,
                    0
                ],
                "title": "LearnMate: Enhancing Online Education with LLM-Powered Personalized\n  Learning Plans and Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LearnMate: Enhancing Online Education with LLM-Powered Personalized\n  Learning Plans and Support"
                },
                "summary": "With the increasing prevalence of online learning, adapting education to\ndiverse learner needs remains a persistent challenge. Recent advancements in\nartificial intelligence (AI), particularly large language models (LLMs),\npromise powerful tools and capabilities to enhance personalized learning in\nonline educational environments. In this work, we explore how LLMs can improve\npersonalized learning experiences by catering to individual user needs toward\nenhancing the overall quality of online education. We designed personalization\nguidelines based on the growing literature on personalized learning to ground\nLLMs in generating tailored learning plans. To operationalize these guidelines,\nwe implemented LearnMate, an LLM-based system that generates personalized\nlearning plans and provides users with real-time learning support. We discuss\nthe implications and future directions of this work, aiming to move beyond the\ntraditional one-size-fits-all approach by integrating LLM-based personalized\nsupport into online learning environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing prevalence of online learning, adapting education to\ndiverse learner needs remains a persistent challenge. Recent advancements in\nartificial intelligence (AI), particularly large language models (LLMs),\npromise powerful tools and capabilities to enhance personalized learning in\nonline educational environments. In this work, we explore how LLMs can improve\npersonalized learning experiences by catering to individual user needs toward\nenhancing the overall quality of online education. We designed personalization\nguidelines based on the growing literature on personalized learning to ground\nLLMs in generating tailored learning plans. To operationalize these guidelines,\nwe implemented LearnMate, an LLM-based system that generates personalized\nlearning plans and provides users with real-time learning support. We discuss\nthe implications and future directions of this work, aiming to move beyond the\ntraditional one-size-fits-all approach by integrating LLM-based personalized\nsupport into online learning environments."
                },
                "authors": [
                    {
                        "name": "Xinyu Jessica Wang"
                    },
                    {
                        "name": "Christine Lee"
                    },
                    {
                        "name": "Bilge Mutlu"
                    }
                ],
                "author_detail": {
                    "name": "Bilge Mutlu"
                },
                "author": "Bilge Mutlu",
                "arxiv_doi": "10.1145/3706599.3719857",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706599.3719857",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.13340v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13340v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Extended Abstracts of the CHI Conference on Human Factors in\n  Computing Systems (CHI EA '25), April 26-May 1, 2025, Yokohama, Japan",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13335v1",
                "updated": "2025-03-17T16:15:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    15,
                    2,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T16:15:02Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    15,
                    2,
                    0,
                    76,
                    0
                ],
                "title": "Reliable and Efficient Amortized Model-based Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable and Efficient Amortized Model-based Evaluation"
                },
                "summary": "Comprehensive evaluations of language models (LM) during both development and\ndeployment phases are necessary because these models possess numerous\ncapabilities (e.g., mathematical reasoning, legal support, or medical\ndiagnostic) as well as safety risks (e.g., racial bias, toxicity, or\nmisinformation). The average score across a wide range of benchmarks provides a\nsignal that helps guide the use of these LMs in practice. Currently, holistic\nevaluations are costly due to the large volume of benchmark questions, making\nfrequent evaluations impractical. A popular attempt to lower the cost is to\ncompute the average score on a subset of the benchmark. This approach,\nunfortunately, often renders an unreliable measure of LM performance because\nthe average score is often confounded with the difficulty of the questions in\nthe benchmark subset. Item response theory (IRT) was designed to address this\nchallenge, providing a reliable measurement by careful controlling for question\ndifficulty. Unfortunately, question difficulty is expensive to estimate. Facing\nthis challenge, we train a model that predicts question difficulty from its\ncontent, enabling a reliable measurement at a fraction of the cost. In\naddition, we leverage this difficulty predictor to further improve the\nevaluation efficiency through training a question generator given a difficulty\nlevel. This question generator is essential in adaptive testing, where, instead\nof using a random subset of the benchmark questions, informative questions are\nadaptively chosen based on the current estimation of LLM performance.\nExperiments on 22 common natural language benchmarks and 172 LMs show that this\napproach is more reliable and efficient compared to current common practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehensive evaluations of language models (LM) during both development and\ndeployment phases are necessary because these models possess numerous\ncapabilities (e.g., mathematical reasoning, legal support, or medical\ndiagnostic) as well as safety risks (e.g., racial bias, toxicity, or\nmisinformation). The average score across a wide range of benchmarks provides a\nsignal that helps guide the use of these LMs in practice. Currently, holistic\nevaluations are costly due to the large volume of benchmark questions, making\nfrequent evaluations impractical. A popular attempt to lower the cost is to\ncompute the average score on a subset of the benchmark. This approach,\nunfortunately, often renders an unreliable measure of LM performance because\nthe average score is often confounded with the difficulty of the questions in\nthe benchmark subset. Item response theory (IRT) was designed to address this\nchallenge, providing a reliable measurement by careful controlling for question\ndifficulty. Unfortunately, question difficulty is expensive to estimate. Facing\nthis challenge, we train a model that predicts question difficulty from its\ncontent, enabling a reliable measurement at a fraction of the cost. In\naddition, we leverage this difficulty predictor to further improve the\nevaluation efficiency through training a question generator given a difficulty\nlevel. This question generator is essential in adaptive testing, where, instead\nof using a random subset of the benchmark questions, informative questions are\nadaptively chosen based on the current estimation of LLM performance.\nExperiments on 22 common natural language benchmarks and 172 LMs show that this\napproach is more reliable and efficient compared to current common practice."
                },
                "authors": [
                    {
                        "name": "Sang Truong"
                    },
                    {
                        "name": "Yuheng Tu"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Sanmi Koyejo"
                    }
                ],
                "author_detail": {
                    "name": "Sanmi Koyejo"
                },
                "author": "Sanmi Koyejo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13330v1",
                "updated": "2025-03-17T16:09:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    9,
                    22,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T16:09:22Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    9,
                    22,
                    0,
                    76,
                    0
                ],
                "title": "LEAVS: An LLM-based Labeler for Abdominal CT Supervision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEAVS: An LLM-based Labeler for Abdominal CT Supervision"
                },
                "summary": "Extracting structured labels from radiology reports has been employed to\ncreate vision models to simultaneously detect several types of abnormalities.\nHowever, existing works focus mainly on the chest region. Few works have been\ninvestigated on abdominal radiology reports due to more complex anatomy and a\nwider range of pathologies in the abdomen. We propose LEAVS (Large language\nmodel Extractor for Abdominal Vision Supervision). This labeler can annotate\nthe certainty of presence and the urgency of seven types of abnormalities for\nnine abdominal organs on CT radiology reports. To ensure broad coverage, we\nchose abnormalities that encompass most of the finding types from CT reports.\nOur approach employs a specialized chain-of-thought prompting strategy for a\nlocally-run LLM using sentence extraction and multiple-choice questions in a\ntree-based decision system. We demonstrate that the LLM can extract several\nabnormality types across abdominal organs with an average F1 score of 0.89,\nsignificantly outperforming competing labelers and humans. Additionally, we\nshow that extraction of urgency labels achieved performance comparable to human\nannotations. Finally, we demonstrate that the abnormality labels contain\nvaluable information for training a single vision model that classifies several\norgans as normal or abnormal. We release our code and structured annotations\nfor a public CT dataset containing over 1,000 CT volumes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting structured labels from radiology reports has been employed to\ncreate vision models to simultaneously detect several types of abnormalities.\nHowever, existing works focus mainly on the chest region. Few works have been\ninvestigated on abdominal radiology reports due to more complex anatomy and a\nwider range of pathologies in the abdomen. We propose LEAVS (Large language\nmodel Extractor for Abdominal Vision Supervision). This labeler can annotate\nthe certainty of presence and the urgency of seven types of abnormalities for\nnine abdominal organs on CT radiology reports. To ensure broad coverage, we\nchose abnormalities that encompass most of the finding types from CT reports.\nOur approach employs a specialized chain-of-thought prompting strategy for a\nlocally-run LLM using sentence extraction and multiple-choice questions in a\ntree-based decision system. We demonstrate that the LLM can extract several\nabnormality types across abdominal organs with an average F1 score of 0.89,\nsignificantly outperforming competing labelers and humans. Additionally, we\nshow that extraction of urgency labels achieved performance comparable to human\nannotations. Finally, we demonstrate that the abnormality labels contain\nvaluable information for training a single vision model that classifies several\norgans as normal or abnormal. We release our code and structured annotations\nfor a public CT dataset containing over 1,000 CT volumes."
                },
                "authors": [
                    {
                        "name": "Ricardo Bigolin Lanfredi"
                    },
                    {
                        "name": "Yan Zhuang"
                    },
                    {
                        "name": "Mark Finkelstein"
                    },
                    {
                        "name": "Praveen Thoppey Srinivasan Balamuralikrishna"
                    },
                    {
                        "name": "Luke Krembs"
                    },
                    {
                        "name": "Brandon Khoury"
                    },
                    {
                        "name": "Arthi Reddy"
                    },
                    {
                        "name": "Pritam Mukherjee"
                    },
                    {
                        "name": "Neil M. Rofsky"
                    },
                    {
                        "name": "Ronald M. Summers"
                    }
                ],
                "author_detail": {
                    "name": "Ronald M. Summers"
                },
                "author": "Ronald M. Summers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13817v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13817v3",
                "updated": "2025-03-17T16:05:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    5,
                    34,
                    0,
                    76,
                    0
                ],
                "published": "2024-12-18T13:04:30Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    4,
                    30,
                    2,
                    353,
                    0
                ],
                "title": "Nullu: Mitigating Object Hallucinations in Large Vision-Language Models\n  via HalluSpace Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nullu: Mitigating Object Hallucinations in Large Vision-Language Models\n  via HalluSpace Projection"
                },
                "summary": "Recent studies have shown that large vision-language models (LVLMs) often\nsuffer from the issue of object hallucinations (OH). To mitigate this issue, we\nintroduce an efficient method that edits the model weights based on an unsafe\nsubspace, which we call HalluSpace in this paper. With truthful and\nhallucinated text prompts accompanying the visual content as inputs, the\nHalluSpace can be identified by extracting the hallucinated embedding features\nand removing the truthful representations in LVLMs. By orthogonalizing the\nmodel weights, input features will be projected into the Null space of the\nHalluSpace to reduce OH, based on which we name our method Nullu. We reveal\nthat HalluSpaces generally contain prior information in the large language\nmodels (LLMs) applied to build LVLMs, which have been shown as essential causes\nof OH in previous studies. Therefore, null space projection suppresses the\nLLMs' priors to filter out the hallucinated features, resulting in contextually\naccurate outputs. Experiments show that our method can effectively mitigate OH\nacross different LVLM families without extra inference costs and also show\nstrong performance in general LVLM benchmarks. Code is released at\nhttps://github.com/Ziwei-Zheng/Nullu.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have shown that large vision-language models (LVLMs) often\nsuffer from the issue of object hallucinations (OH). To mitigate this issue, we\nintroduce an efficient method that edits the model weights based on an unsafe\nsubspace, which we call HalluSpace in this paper. With truthful and\nhallucinated text prompts accompanying the visual content as inputs, the\nHalluSpace can be identified by extracting the hallucinated embedding features\nand removing the truthful representations in LVLMs. By orthogonalizing the\nmodel weights, input features will be projected into the Null space of the\nHalluSpace to reduce OH, based on which we name our method Nullu. We reveal\nthat HalluSpaces generally contain prior information in the large language\nmodels (LLMs) applied to build LVLMs, which have been shown as essential causes\nof OH in previous studies. Therefore, null space projection suppresses the\nLLMs' priors to filter out the hallucinated features, resulting in contextually\naccurate outputs. Experiments show that our method can effectively mitigate OH\nacross different LVLM families without extra inference costs and also show\nstrong performance in general LVLM benchmarks. Code is released at\nhttps://github.com/Ziwei-Zheng/Nullu."
                },
                "authors": [
                    {
                        "name": "Le Yang"
                    },
                    {
                        "name": "Ziwei Zheng"
                    },
                    {
                        "name": "Boxu Chen"
                    },
                    {
                        "name": "Zhengyu Zhao"
                    },
                    {
                        "name": "Chenhao Lin"
                    },
                    {
                        "name": "Chao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chao Shen"
                },
                "author": "Chao Shen",
                "arxiv_comment": "CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13817v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13817v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13313v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13313v1",
                "updated": "2025-03-17T15:52:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    52,
                    53,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T15:52:53Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    52,
                    53,
                    0,
                    76,
                    0
                ],
                "title": "Detection of millimeter-wave coronal emission in a quasar at\n  cosmological distance using microlensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detection of millimeter-wave coronal emission in a quasar at\n  cosmological distance using microlensing"
                },
                "summary": "Determining the nature of emission processes at the heart of quasars is\ncritical for understanding environments of supermassive black holes. One of the\nkey open questions is the origin of long-wavelength emission from radio-quiet\nquasars. The proposed mechanisms span a broad range, from central star\nformation to dusty torus, low-power jets, or coronal emission from the\ninnermost accretion disk. Distinguishing between these scenarios requires\nprobing spatial scales $\\leq$0.01 pc, beyond the reach of any current\nmillimetre-wave telescope. Fortunately, in gravitationally lensed quasars,\ncompact mm-wave emission might be microlensed by stars in the foreground\ngalaxy, providing strong constraints on the source size. We report a striking\nchange in rest-frame 1.3-mm flux-ratios in RXJ1131-1231, a quadruply-lensed\nquasar at z = 0.658, observed by the Atacama Large Millimeter/submillimeter\nArray (ALMA) in 2015 and 2020. The observed flux-ratio variability is\nconsistent with microlensing of a very compact source with a half-light radius\n$\\leq$50 astronomical units. The compactness of the source leaves coronal\nemission as the most likely scenario. Furthermore, the inferred mm-wave and\nX-ray luminosities follow the characteristic G\\\"udel-Benz relationship for\ncoronal emission. These observations represent the first unambiguous evidence\nfor coronae as the dominant mechanism for long-wavelength emission in\nradio-quiet quasars.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Determining the nature of emission processes at the heart of quasars is\ncritical for understanding environments of supermassive black holes. One of the\nkey open questions is the origin of long-wavelength emission from radio-quiet\nquasars. The proposed mechanisms span a broad range, from central star\nformation to dusty torus, low-power jets, or coronal emission from the\ninnermost accretion disk. Distinguishing between these scenarios requires\nprobing spatial scales $\\leq$0.01 pc, beyond the reach of any current\nmillimetre-wave telescope. Fortunately, in gravitationally lensed quasars,\ncompact mm-wave emission might be microlensed by stars in the foreground\ngalaxy, providing strong constraints on the source size. We report a striking\nchange in rest-frame 1.3-mm flux-ratios in RXJ1131-1231, a quadruply-lensed\nquasar at z = 0.658, observed by the Atacama Large Millimeter/submillimeter\nArray (ALMA) in 2015 and 2020. The observed flux-ratio variability is\nconsistent with microlensing of a very compact source with a half-light radius\n$\\leq$50 astronomical units. The compactness of the source leaves coronal\nemission as the most likely scenario. Furthermore, the inferred mm-wave and\nX-ray luminosities follow the characteristic G\\\"udel-Benz relationship for\ncoronal emission. These observations represent the first unambiguous evidence\nfor coronae as the dominant mechanism for long-wavelength emission in\nradio-quiet quasars."
                },
                "authors": [
                    {
                        "name": "M. Rybak"
                    },
                    {
                        "name": "D. Sluse"
                    },
                    {
                        "name": "K. Gupta"
                    },
                    {
                        "name": "M. Millon"
                    },
                    {
                        "name": "E. Behar"
                    },
                    {
                        "name": "F. Courbin"
                    },
                    {
                        "name": "J. P. McKean"
                    },
                    {
                        "name": "H. R. Stacey"
                    }
                ],
                "author_detail": {
                    "name": "H. R. Stacey"
                },
                "author": "H. R. Stacey",
                "arxiv_comment": "Submitted to A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13313v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13313v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.04928v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.04928v3",
                "updated": "2025-03-17T15:50:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    50,
                    13,
                    0,
                    76,
                    0
                ],
                "published": "2023-11-03T18:27:21Z",
                "published_parsed": [
                    2023,
                    11,
                    3,
                    18,
                    27,
                    21,
                    4,
                    307,
                    0
                ],
                "title": "Leveraging Large Language Models for Collective Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Collective Decision-Making"
                },
                "summary": "In various work contexts, such as meeting scheduling, collaborating, and\nproject planning, collective decision-making is essential but often challenging\ndue to diverse individual preferences, varying work focuses, and power dynamics\namong members. To address this, we propose a system leveraging Large Language\nModels (LLMs) to facilitate group decision-making by managing conversations and\nbalancing preferences among individuals. Our system aims to extract individual\npreferences from each member's conversation with the system and suggest options\nthat satisfy the preferences of the members. We specifically apply this system\nto corporate meeting scheduling. We create synthetic employee profiles and\nsimulate conversations at scale, leveraging LLMs to evaluate the system\nperformance as a novel approach to conducting a user study. Our results\nindicate efficient coordination with reduced interactions between the members\nand the LLM-based system. The system refines and improves its proposed options\nover time, ensuring that many of the members' individual preferences are\nsatisfied in an equitable way. Finally, we conduct a survey study involving\nhuman participants to assess our system's ability to aggregate preferences and\nreasoning about them. Our findings show that the system exhibits strong\nperformance in both dimensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In various work contexts, such as meeting scheduling, collaborating, and\nproject planning, collective decision-making is essential but often challenging\ndue to diverse individual preferences, varying work focuses, and power dynamics\namong members. To address this, we propose a system leveraging Large Language\nModels (LLMs) to facilitate group decision-making by managing conversations and\nbalancing preferences among individuals. Our system aims to extract individual\npreferences from each member's conversation with the system and suggest options\nthat satisfy the preferences of the members. We specifically apply this system\nto corporate meeting scheduling. We create synthetic employee profiles and\nsimulate conversations at scale, leveraging LLMs to evaluate the system\nperformance as a novel approach to conducting a user study. Our results\nindicate efficient coordination with reduced interactions between the members\nand the LLM-based system. The system refines and improves its proposed options\nover time, ensuring that many of the members' individual preferences are\nsatisfied in an equitable way. Finally, we conduct a survey study involving\nhuman participants to assess our system's ability to aggregate preferences and\nreasoning about them. Our findings show that the system exhibits strong\nperformance in both dimensions."
                },
                "authors": [
                    {
                        "name": "Marios Papachristou"
                    },
                    {
                        "name": "Longqi Yang"
                    },
                    {
                        "name": "Chin-Chia Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Chin-Chia Hsu"
                },
                "author": "Chin-Chia Hsu",
                "arxiv_comment": "To appear at ACM CSCW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.04928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.04928v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13305v1",
                "updated": "2025-03-17T15:47:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    47,
                    37,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T15:47:37Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    47,
                    37,
                    0,
                    76,
                    0
                ],
                "title": "Computation Mechanism Behind LLM Position Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computation Mechanism Behind LLM Position Generalization"
                },
                "summary": "Most written natural languages are composed of sequences of words and\nsentences. Similar to humans, large language models (LLMs) exhibit flexibility\nin handling textual positions - a phenomenon we term position generalization.\nThey can understand texts with position perturbations and generalize to longer\ntexts than those encountered during training with the latest techniques. These\nphenomena suggest that LLMs handle positions tolerantly, but how LLMs\ncomputationally process positional relevance remains largely unexplored. This\nwork connects the linguistic phenomenon with LLMs' computational mechanisms. We\nshow how LLMs enforce certain computational mechanisms for the aforementioned\ntolerance in position perturbations. Despite the complex design of the\nself-attention mechanism, this work reveals that LLMs learn a counterintuitive\ndisentanglement of attention logits. Their values show a 0.959 linear\ncorrelation with an approximation of the arithmetic sum of positional relevance\nand semantic importance. Furthermore, we identify a prevalent pattern in\nintermediate features, which we prove theoretically enables this effect. The\npattern, which is different from how randomly initialized parameters would\nbehave, suggests that it is a learned behavior rather than a natural result of\nthe model architecture. Based on these findings, we provide computational\nexplanations and criteria for LLMs' position flexibilities. This work takes a\npioneering step in linking position generalization with modern LLMs' internal\nmechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most written natural languages are composed of sequences of words and\nsentences. Similar to humans, large language models (LLMs) exhibit flexibility\nin handling textual positions - a phenomenon we term position generalization.\nThey can understand texts with position perturbations and generalize to longer\ntexts than those encountered during training with the latest techniques. These\nphenomena suggest that LLMs handle positions tolerantly, but how LLMs\ncomputationally process positional relevance remains largely unexplored. This\nwork connects the linguistic phenomenon with LLMs' computational mechanisms. We\nshow how LLMs enforce certain computational mechanisms for the aforementioned\ntolerance in position perturbations. Despite the complex design of the\nself-attention mechanism, this work reveals that LLMs learn a counterintuitive\ndisentanglement of attention logits. Their values show a 0.959 linear\ncorrelation with an approximation of the arithmetic sum of positional relevance\nand semantic importance. Furthermore, we identify a prevalent pattern in\nintermediate features, which we prove theoretically enables this effect. The\npattern, which is different from how randomly initialized parameters would\nbehave, suggests that it is a learned behavior rather than a natural result of\nthe model architecture. Based on these findings, we provide computational\nexplanations and criteria for LLMs' position flexibilities. This work takes a\npioneering step in linking position generalization with modern LLMs' internal\nmechanisms."
                },
                "authors": [
                    {
                        "name": "Chi Han"
                    },
                    {
                        "name": "Heng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Heng Ji"
                },
                "author": "Heng Ji",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13301v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13301v1",
                "updated": "2025-03-17T15:45:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    45,
                    17,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T15:45:17Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    45,
                    17,
                    0,
                    76,
                    0
                ],
                "title": "LIMCA: LLM for Automating Analog In-Memory Computing Architecture Design\n  Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LIMCA: LLM for Automating Analog In-Memory Computing Architecture Design\n  Exploration"
                },
                "summary": "Resistive crossbars enabling analog In-Memory Computing (IMC) have emerged as\na promising architecture for Deep Neural Network (DNN) acceleration, offering\nhigh memory bandwidth and in-situ computation. However, the manual,\nknowledge-intensive design process and the lack of high-quality circuit\nnetlists have significantly constrained design space exploration and\noptimization to behavioral system-level tools. In this work, we introduce\nLIMCA, a novel fine-tune-free Large Language Model (LLM)-driven framework for\nautomating the design and evaluation of IMC crossbar architectures. Unlike\ntraditional approaches, LIMCA employs a No-Human-In-Loop (NHIL) automated\npipeline to generate and validate circuit netlists for SPICE simulations,\neliminating manual intervention. LIMCA systematically explores the IMC design\nspace by leveraging a structured dataset and LLM-based performance evaluation.\nOur experimental results on MNIST classification demonstrate that LIMCA\nsuccessfully generates crossbar designs achieving $\\geq$96% accuracy while\nmaintaining a power consumption $\\leq$3W, making this the first work in\nLLM-assisted IMC design space exploration. Compared to existing frameworks,\nLIMCA provides an automated, scalable, and hardware-aware solution, reducing\ndesign exploration time while ensuring user-constrained performance trade-offs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resistive crossbars enabling analog In-Memory Computing (IMC) have emerged as\na promising architecture for Deep Neural Network (DNN) acceleration, offering\nhigh memory bandwidth and in-situ computation. However, the manual,\nknowledge-intensive design process and the lack of high-quality circuit\nnetlists have significantly constrained design space exploration and\noptimization to behavioral system-level tools. In this work, we introduce\nLIMCA, a novel fine-tune-free Large Language Model (LLM)-driven framework for\nautomating the design and evaluation of IMC crossbar architectures. Unlike\ntraditional approaches, LIMCA employs a No-Human-In-Loop (NHIL) automated\npipeline to generate and validate circuit netlists for SPICE simulations,\neliminating manual intervention. LIMCA systematically explores the IMC design\nspace by leveraging a structured dataset and LLM-based performance evaluation.\nOur experimental results on MNIST classification demonstrate that LIMCA\nsuccessfully generates crossbar designs achieving $\\geq$96% accuracy while\nmaintaining a power consumption $\\leq$3W, making this the first work in\nLLM-assisted IMC design space exploration. Compared to existing frameworks,\nLIMCA provides an automated, scalable, and hardware-aware solution, reducing\ndesign exploration time while ensuring user-constrained performance trade-offs."
                },
                "authors": [
                    {
                        "name": "Deepak Vungarala"
                    },
                    {
                        "name": "Md Hasibul Amin"
                    },
                    {
                        "name": "Pietro Mercati"
                    },
                    {
                        "name": "Arnob Ghosh"
                    },
                    {
                        "name": "Arman Roohi"
                    },
                    {
                        "name": "Ramtin Zand"
                    },
                    {
                        "name": "Shaahin Angizi"
                    }
                ],
                "author_detail": {
                    "name": "Shaahin Angizi"
                },
                "author": "Shaahin Angizi",
                "arxiv_comment": "4 Figures, 5 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13301v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13299v1",
                "updated": "2025-03-17T15:44:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    44,
                    9,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T15:44:09Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    44,
                    9,
                    0,
                    76,
                    0
                ],
                "title": "A Survey on Transformer Context Extension: Approaches and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Transformer Context Extension: Approaches and Evaluation"
                },
                "summary": "Large language models (LLMs) based on Transformer have been widely applied in\nthe filed of natural language processing (NLP), demonstrating strong\nperformance, particularly in handling short text tasks. However, when it comes\nto long context scenarios, the performance of LLMs degrades due to some\nchallenges. To alleviate this phenomenon, there is a number of work proposed\nrecently. In this survey, we first list the challenges of applying pre-trained\nLLMs to process long contexts. Then systematically review the approaches\nrelated to long context and propose our taxonomy categorizing them into four\nmain types: positional encoding, context compression, retrieval augmented, and\nattention pattern. In addition to the approaches, we focus on the evaluation of\nlong context, organizing relevant data, tasks, and metrics based on existing\nlong context benchmarks. Finally, we summarize unresolved issues in the long\ncontext domain and put forward our views on future developments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) based on Transformer have been widely applied in\nthe filed of natural language processing (NLP), demonstrating strong\nperformance, particularly in handling short text tasks. However, when it comes\nto long context scenarios, the performance of LLMs degrades due to some\nchallenges. To alleviate this phenomenon, there is a number of work proposed\nrecently. In this survey, we first list the challenges of applying pre-trained\nLLMs to process long contexts. Then systematically review the approaches\nrelated to long context and propose our taxonomy categorizing them into four\nmain types: positional encoding, context compression, retrieval augmented, and\nattention pattern. In addition to the approaches, we focus on the evaluation of\nlong context, organizing relevant data, tasks, and metrics based on existing\nlong context benchmarks. Finally, we summarize unresolved issues in the long\ncontext domain and put forward our views on future developments."
                },
                "authors": [
                    {
                        "name": "Yijun Liu"
                    },
                    {
                        "name": "Jinzheng Yu"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Zhongyang Li"
                    },
                    {
                        "name": "Qingfu Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Qingfu Zhu"
                },
                "author": "Qingfu Zhu",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13292v1",
                "updated": "2025-03-17T15:39:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    39,
                    43,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T15:39:43Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    39,
                    43,
                    0,
                    76,
                    0
                ],
                "title": "Quantitative Image-Based Validation Framework for Assessing Global\n  Coronal Magnetic Field Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantitative Image-Based Validation Framework for Assessing Global\n  Coronal Magnetic Field Models"
                },
                "summary": "Coronagraph observations provide key information about the orientation of the\nSun's magnetic field. Previous studies used quasi-radial features detected in\ncoronagraph images to improve coronal magnetic field models by comparing the\norientation of the features to the projected orientation of the model field.\nVarious algorithms segment these coronal features to approximate the local\nplane-of-sky geometry of the coronal magnetic field, and their orientation can\nbe used as input for optimizing and constraining coronal magnetic field models.\nWe present a new framework that allows for further quantitative evaluations of\nimage-based coronal segmentation methods against magnetic field models, and\nvice-versa. We compare quasi-radial features identified from QRaFT, a global\ncoronal feature tracing algorithm, in white-light coronagraph images to outputs\nof MAS, an advanced magnetohydrodynamic model. We use the FORWARD toolset to\nproduce synthetic polarized brightness images co-aligned to real coronagraph\nobservations, segment features in these images, and quantify the difference\nbetween the inferred and model magnetic field. This approach allows us to\ngeometrically compare features segmented in artificial images to those\nsegmented in white-light coronagraph observations against the plane-of-sky\nprojected MAS coronal magnetic field. We quantify QRaFT's performance in the\nartificial images and observational data, and perform statistical analyses that\nmeasure the similarity of these results to compute the accuracy and uncertainty\nof the model output to the observational data. The results demonstrate through\na quantitative evaluation that a coronal segmentation method identifies the\nglobal large-scale orientation of the coronal magnetic field within\n$\\sim\\pm10^\\circ$ of the plane-of-sky projected MAS magnetic field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coronagraph observations provide key information about the orientation of the\nSun's magnetic field. Previous studies used quasi-radial features detected in\ncoronagraph images to improve coronal magnetic field models by comparing the\norientation of the features to the projected orientation of the model field.\nVarious algorithms segment these coronal features to approximate the local\nplane-of-sky geometry of the coronal magnetic field, and their orientation can\nbe used as input for optimizing and constraining coronal magnetic field models.\nWe present a new framework that allows for further quantitative evaluations of\nimage-based coronal segmentation methods against magnetic field models, and\nvice-versa. We compare quasi-radial features identified from QRaFT, a global\ncoronal feature tracing algorithm, in white-light coronagraph images to outputs\nof MAS, an advanced magnetohydrodynamic model. We use the FORWARD toolset to\nproduce synthetic polarized brightness images co-aligned to real coronagraph\nobservations, segment features in these images, and quantify the difference\nbetween the inferred and model magnetic field. This approach allows us to\ngeometrically compare features segmented in artificial images to those\nsegmented in white-light coronagraph observations against the plane-of-sky\nprojected MAS coronal magnetic field. We quantify QRaFT's performance in the\nartificial images and observational data, and perform statistical analyses that\nmeasure the similarity of these results to compute the accuracy and uncertainty\nof the model output to the observational data. The results demonstrate through\na quantitative evaluation that a coronal segmentation method identifies the\nglobal large-scale orientation of the coronal magnetic field within\n$\\sim\\pm10^\\circ$ of the plane-of-sky projected MAS magnetic field."
                },
                "authors": [
                    {
                        "name": "Christopher E. Rura"
                    },
                    {
                        "name": "Vadim M. Uritsky"
                    },
                    {
                        "name": "Shaela I. Jones"
                    },
                    {
                        "name": "Cooper Downs"
                    },
                    {
                        "name": "Nathalia Alzate"
                    },
                    {
                        "name": "Charles N. Arge"
                    }
                ],
                "author_detail": {
                    "name": "Charles N. Arge"
                },
                "author": "Charles N. Arge",
                "arxiv_comment": "30 pages, 11 figures, 4 tables. Submitted to ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13288v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13288v1",
                "updated": "2025-03-17T15:38:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    38,
                    33,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T15:38:33Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    38,
                    33,
                    0,
                    76,
                    0
                ],
                "title": "$φ$-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time\n  Exploration and Exploitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$φ$-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time\n  Exploration and Exploitation"
                },
                "summary": "Inference-time optimization scales computation to derive deliberate reasoning\nsteps for effective performance. While previous search-based strategies address\nthe short-sightedness of auto-regressive generation, the vast search space\nleads to excessive exploration and insufficient exploitation. To strike an\nefficient balance to derive the optimal step, we frame the decoding strategy as\nforesight sampling, leveraging simulated future steps to obtain globally\noptimal step estimation. Built on it, we propose a novel decoding strategy,\nnamed $\\phi$-Decoding. To provide a precise and expressive estimation of step\nvalue, $\\phi$-Decoding approximates two distributions via foresight and\nclustering. Sampling from the joint distribution, the optimal steps can be\nselected for exploitation. To support adaptive computation allocation, we\npropose in-width and in-depth pruning strategies, featuring a light-weight\nsolution to achieve inference efficiency. Extensive experiments across seven\nbenchmarks show $\\phi$-Decoding outperforms strong baselines in both\nperformance and efficiency. Additional analysis demonstrates its generalization\nacross various LLMs and scalability across a wide range of computing budgets.\nThe code will be released at https://github.com/xufangzhi/phi-Decoding, and the\nopen-source PyPI package is coming soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-time optimization scales computation to derive deliberate reasoning\nsteps for effective performance. While previous search-based strategies address\nthe short-sightedness of auto-regressive generation, the vast search space\nleads to excessive exploration and insufficient exploitation. To strike an\nefficient balance to derive the optimal step, we frame the decoding strategy as\nforesight sampling, leveraging simulated future steps to obtain globally\noptimal step estimation. Built on it, we propose a novel decoding strategy,\nnamed $\\phi$-Decoding. To provide a precise and expressive estimation of step\nvalue, $\\phi$-Decoding approximates two distributions via foresight and\nclustering. Sampling from the joint distribution, the optimal steps can be\nselected for exploitation. To support adaptive computation allocation, we\npropose in-width and in-depth pruning strategies, featuring a light-weight\nsolution to achieve inference efficiency. Extensive experiments across seven\nbenchmarks show $\\phi$-Decoding outperforms strong baselines in both\nperformance and efficiency. Additional analysis demonstrates its generalization\nacross various LLMs and scalability across a wide range of computing budgets.\nThe code will be released at https://github.com/xufangzhi/phi-Decoding, and the\nopen-source PyPI package is coming soon."
                },
                "authors": [
                    {
                        "name": "Fangzhi Xu"
                    },
                    {
                        "name": "Hang Yan"
                    },
                    {
                        "name": "Chang Ma"
                    },
                    {
                        "name": "Haiteng Zhao"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Qika Lin"
                    },
                    {
                        "name": "Zhiyong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Wu"
                },
                "author": "Zhiyong Wu",
                "arxiv_comment": "13 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13288v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13288v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13281v1",
                "updated": "2025-03-17T15:31:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    31,
                    55,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T15:31:55Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    31,
                    55,
                    0,
                    76,
                    0
                ],
                "title": "LLM-Match: An Open-Sourced Patient Matching Model Based on Large\n  Language Models and Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Match: An Open-Sourced Patient Matching Model Based on Large\n  Language Models and Retrieval-Augmented Generation"
                },
                "summary": "Patient matching is the process of linking patients to appropriate clinical\ntrials by accurately identifying and matching their medical records with trial\neligibility criteria. We propose LLM-Match, a novel framework for patient\nmatching leveraging fine-tuned open-source large language models. Our approach\nconsists of four key components. First, a retrieval-augmented generation (RAG)\nmodule extracts relevant patient context from a vast pool of electronic health\nrecords (EHRs). Second, a prompt generation module constructs input prompts by\nintegrating trial eligibility criteria (both inclusion and exclusion criteria),\npatient context, and system instructions. Third, a fine-tuning module with a\nclassification head optimizes the model parameters using structured prompts and\nground-truth labels. Fourth, an evaluation module assesses the fine-tuned\nmodel's performance on the testing datasets. We evaluated LLM-Match on four\nopen datasets, n2c2, SIGIR, TREC 2021, and TREC 2022, using open-source models,\ncomparing it against TrialGPT, Zero-Shot, and GPT-4-based closed models.\nLLM-Match outperformed all baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patient matching is the process of linking patients to appropriate clinical\ntrials by accurately identifying and matching their medical records with trial\neligibility criteria. We propose LLM-Match, a novel framework for patient\nmatching leveraging fine-tuned open-source large language models. Our approach\nconsists of four key components. First, a retrieval-augmented generation (RAG)\nmodule extracts relevant patient context from a vast pool of electronic health\nrecords (EHRs). Second, a prompt generation module constructs input prompts by\nintegrating trial eligibility criteria (both inclusion and exclusion criteria),\npatient context, and system instructions. Third, a fine-tuning module with a\nclassification head optimizes the model parameters using structured prompts and\nground-truth labels. Fourth, an evaluation module assesses the fine-tuned\nmodel's performance on the testing datasets. We evaluated LLM-Match on four\nopen datasets, n2c2, SIGIR, TREC 2021, and TREC 2022, using open-source models,\ncomparing it against TrialGPT, Zero-Shot, and GPT-4-based closed models.\nLLM-Match outperformed all baselines."
                },
                "authors": [
                    {
                        "name": "Xiaodi Li"
                    },
                    {
                        "name": "Shaika Chowdhury"
                    },
                    {
                        "name": "Chung Il Wi"
                    },
                    {
                        "name": "Maria Vassilaki"
                    },
                    {
                        "name": "Ken Liu"
                    },
                    {
                        "name": "Terence T Sio"
                    },
                    {
                        "name": "Owen Garrick"
                    },
                    {
                        "name": "Young J Juhn"
                    },
                    {
                        "name": "James R Cerhan"
                    },
                    {
                        "name": "Cui Tao"
                    },
                    {
                        "name": "Nansu Zong"
                    }
                ],
                "author_detail": {
                    "name": "Nansu Zong"
                },
                "author": "Nansu Zong",
                "arxiv_comment": "10 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13279v1",
                "updated": "2025-03-17T15:31:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    31,
                    20,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T15:31:20Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    31,
                    20,
                    0,
                    76,
                    0
                ],
                "title": "Goal2Story: A Multi-Agent Fleet based on Privately Enabled sLLMs for\n  Impacting Mapping on Requirements Elicitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Goal2Story: A Multi-Agent Fleet based on Privately Enabled sLLMs for\n  Impacting Mapping on Requirements Elicitation"
                },
                "summary": "As requirements drift with rapid iterations, agile development becomes the\ndominant paradigm. Goal-driven Requirements Elicitation (RE) is a pivotal yet\nchallenging task in agile project development due to its heavy tangling with\nadaptive planning and efficient collaboration. Recently, AI agents have shown\npromising ability in supporting requirements analysis by saving significant\ntime and effort for stakeholders. However, current research mainly focuses on\nfunctional RE, and research works have not been reported bridging the long\njourney from goal to user stories. Moreover, considering the cost of LLM\nfacilities and the need for data and idea protection, privately hosted\nsmall-sized LLM should be further utilized in RE. To address these challenges,\nwe propose Goal2Story, a multi-agent fleet that adopts the Impact Mapping (IM)\nframework while merely using cost-effective sLLMs for goal-driven RE. Moreover,\nwe introduce a StorySeek dataset that contains over 1,000 user stories (USs)\nwith corresponding goals and project context information, as well as the\nsemi-automatic dataset construction method. For evaluation, we proposed two\nmetrics: Factuality Hit Rate (FHR) to measure consistency between the generated\nUSs with the dataset and Quality And Consistency Evaluation (QuACE) to evaluate\nthe quality of the generated USs. Experimental results demonstrate that\nGoal2Story outperforms the baseline performance of the Super-Agent adopting\npowerful LLMs, while also showcasing the performance improvements in key\nmetrics brought by CoT and Agent Profile to Goal2Story, as well as its\nexploration in identifying latent needs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As requirements drift with rapid iterations, agile development becomes the\ndominant paradigm. Goal-driven Requirements Elicitation (RE) is a pivotal yet\nchallenging task in agile project development due to its heavy tangling with\nadaptive planning and efficient collaboration. Recently, AI agents have shown\npromising ability in supporting requirements analysis by saving significant\ntime and effort for stakeholders. However, current research mainly focuses on\nfunctional RE, and research works have not been reported bridging the long\njourney from goal to user stories. Moreover, considering the cost of LLM\nfacilities and the need for data and idea protection, privately hosted\nsmall-sized LLM should be further utilized in RE. To address these challenges,\nwe propose Goal2Story, a multi-agent fleet that adopts the Impact Mapping (IM)\nframework while merely using cost-effective sLLMs for goal-driven RE. Moreover,\nwe introduce a StorySeek dataset that contains over 1,000 user stories (USs)\nwith corresponding goals and project context information, as well as the\nsemi-automatic dataset construction method. For evaluation, we proposed two\nmetrics: Factuality Hit Rate (FHR) to measure consistency between the generated\nUSs with the dataset and Quality And Consistency Evaluation (QuACE) to evaluate\nthe quality of the generated USs. Experimental results demonstrate that\nGoal2Story outperforms the baseline performance of the Super-Agent adopting\npowerful LLMs, while also showcasing the performance improvements in key\nmetrics brought by CoT and Agent Profile to Goal2Story, as well as its\nexploration in identifying latent needs."
                },
                "authors": [
                    {
                        "name": "Xinkai Zou"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Xiongbo Shi"
                    },
                    {
                        "name": "Chen Yang"
                    }
                ],
                "author_detail": {
                    "name": "Chen Yang"
                },
                "author": "Chen Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13275v1",
                "updated": "2025-03-17T15:27:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T15:27:02Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "title": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems"
                },
                "summary": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability."
                },
                "authors": [
                    {
                        "name": "Seyoung Song"
                    }
                ],
                "author_detail": {
                    "name": "Seyoung Song"
                },
                "author": "Seyoung Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7; I.2.11; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14299v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14299v4",
                "updated": "2025-03-17T15:22:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    22,
                    28,
                    0,
                    76,
                    0
                ],
                "published": "2024-11-21T16:50:11Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    50,
                    11,
                    3,
                    326,
                    0
                ],
                "title": "Masala-CHAI: A Large-Scale SPICE Netlist Dataset for Analog Circuits by\n  Harnessing AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masala-CHAI: A Large-Scale SPICE Netlist Dataset for Analog Circuits by\n  Harnessing AI"
                },
                "summary": "Masala-CHAI is the first fully automated framework leveraging large language\nmodels (LLMs) to generate Simulation Programs with Integrated Circuit Emphasis\n(SPICE) netlists. It addresses a long-standing challenge in automating netlist\ngeneration for analog circuits within circuit design automation. Automating\nthis workflow could accelerate the creation of finetuned LLMs for analog\ncircuit design and verification. We identify key challenges in this automation\nand evaluate the multi-modal capabilities of state-of-the-art LLMs,\nparticularly GPT-4, to address these issues. We propose a three-step workflow\nto overcome current limitations: labeling analog circuits, prompt tuning, and\nnetlist verification. This approach aims to create an end-to-end SPICE netlist\ngenerator from circuit schematic images, tackling the long-standing hurdle of\naccurate netlist generation. Our framework demonstrates significant performance\nimprovements, tested on approximately 2,100 schematics of varying complexity.\nWe open-source this solution for community-driven development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masala-CHAI is the first fully automated framework leveraging large language\nmodels (LLMs) to generate Simulation Programs with Integrated Circuit Emphasis\n(SPICE) netlists. It addresses a long-standing challenge in automating netlist\ngeneration for analog circuits within circuit design automation. Automating\nthis workflow could accelerate the creation of finetuned LLMs for analog\ncircuit design and verification. We identify key challenges in this automation\nand evaluate the multi-modal capabilities of state-of-the-art LLMs,\nparticularly GPT-4, to address these issues. We propose a three-step workflow\nto overcome current limitations: labeling analog circuits, prompt tuning, and\nnetlist verification. This approach aims to create an end-to-end SPICE netlist\ngenerator from circuit schematic images, tackling the long-standing hurdle of\naccurate netlist generation. Our framework demonstrates significant performance\nimprovements, tested on approximately 2,100 schematics of varying complexity.\nWe open-source this solution for community-driven development."
                },
                "authors": [
                    {
                        "name": "Jitendra Bhandari"
                    },
                    {
                        "name": "Vineet Bhat"
                    },
                    {
                        "name": "Yuheng He"
                    },
                    {
                        "name": "Hamed Rahmani"
                    },
                    {
                        "name": "Siddharth Garg"
                    },
                    {
                        "name": "Ramesh Karri"
                    }
                ],
                "author_detail": {
                    "name": "Ramesh Karri"
                },
                "author": "Ramesh Karri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14299v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14299v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13269v1",
                "updated": "2025-03-17T15:22:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    22,
                    19,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T15:22:19Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    22,
                    19,
                    0,
                    76,
                    0
                ],
                "title": "DAgent: A Relational Database-Driven Data Analysis Report Generation\n  Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAgent: A Relational Database-Driven Data Analysis Report Generation\n  Agent"
                },
                "summary": "Relational database-driven data analysis (RDB-DA) report generation, which\naims to generate data analysis reports after querying relational databases, has\nbeen widely applied in fields such as finance and healthcare. Typically, these\ntasks are manually completed by data scientists, making the process very\nlabor-intensive and showing a clear need for automation. Although existing\nmethods (e.g., Table QA or Text-to-SQL) have been proposed to reduce human\ndependency, they cannot handle complex analytical tasks that require multi-step\nreasoning, cross-table associations, and synthesizing insights into reports.\nMoreover, there is no dataset available for developing automatic RDB-DA report\ngeneration. To fill this gap, this paper proposes an LLM agent system for\nRDB-DA report generation tasks, dubbed DAgent; moreover, we construct a\nbenchmark for automatic data analysis report generation, which includes a new\ndataset DA-Dataset and evaluation metrics. DAgent integrates planning, tools,\nand memory modules to decompose natural language questions into logically\nindependent sub-queries, accurately retrieve key information from relational\ndatabases, and generate analytical reports that meet the requirements of\ncompleteness, correctness, and conciseness through multi-step reasoning and\neffective data integration. Experimental analysis on the DA-Dataset\ndemonstrates that DAgent's superiority in retrieval performance and analysis\nreport generation quality, showcasing its strong potential for tackling complex\ndatabase analysis report generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relational database-driven data analysis (RDB-DA) report generation, which\naims to generate data analysis reports after querying relational databases, has\nbeen widely applied in fields such as finance and healthcare. Typically, these\ntasks are manually completed by data scientists, making the process very\nlabor-intensive and showing a clear need for automation. Although existing\nmethods (e.g., Table QA or Text-to-SQL) have been proposed to reduce human\ndependency, they cannot handle complex analytical tasks that require multi-step\nreasoning, cross-table associations, and synthesizing insights into reports.\nMoreover, there is no dataset available for developing automatic RDB-DA report\ngeneration. To fill this gap, this paper proposes an LLM agent system for\nRDB-DA report generation tasks, dubbed DAgent; moreover, we construct a\nbenchmark for automatic data analysis report generation, which includes a new\ndataset DA-Dataset and evaluation metrics. DAgent integrates planning, tools,\nand memory modules to decompose natural language questions into logically\nindependent sub-queries, accurately retrieve key information from relational\ndatabases, and generate analytical reports that meet the requirements of\ncompleteness, correctness, and conciseness through multi-step reasoning and\neffective data integration. Experimental analysis on the DA-Dataset\ndemonstrates that DAgent's superiority in retrieval performance and analysis\nreport generation quality, showcasing its strong potential for tackling complex\ndatabase analysis report generation tasks."
                },
                "authors": [
                    {
                        "name": "Wenyi Xu"
                    },
                    {
                        "name": "Yuren Mao"
                    },
                    {
                        "name": "Xiaolu Zhang"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Xuemei Dong"
                    },
                    {
                        "name": "Mengfei Zhang"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Yunjun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yunjun Gao"
                },
                "author": "Yunjun Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13263v1",
                "updated": "2025-03-17T15:17:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    17,
                    15,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T15:17:15Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    17,
                    15,
                    0,
                    76,
                    0
                ],
                "title": "On the accuracy of posterior recovery with neural network emulators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the accuracy of posterior recovery with neural network emulators"
                },
                "summary": "Neural network emulators are widely used in astrophysics and cosmology to\napproximate complex simulations inside Bayesian inference loops. Ad hoc rules\nof thumb are often used to justify the emulator accuracy required for reliable\nposterior recovery. We provide a theoretically motivated limit on the maximum\namount of incorrect information inferred by using an emulator with a given\naccuracy. Under assumptions of linearity in the model, uncorrelated noise in\nthe data and a Gaussian likelihood function, we demonstrate that the difference\nbetween the true underlying posterior and the recovered posterior can be\nquantified via a Kullback-Leibler divergence. We demonstrate how this limit can\nbe used in the field of 21-cm cosmology by comparing the posteriors recovered\nwhen fitting mock data sets generated with the 1D radiative transfer code ARES\ndirectly with the simulation code and separately with an emulator. This paper\nis partly in response to and builds upon recent discussions in the literature\nwhich call into question the use of emulators in Bayesian inference pipelines.\nUpon repeating some aspects of these analyses, we find these concerns\nquantitatively unjustified, with accurate posterior recovery possible even when\nthe mean RMSE error for the emulator is approximately 20% of the magnitude of\nthe noise in the data. For the purposes of community reproducibility, we make\nour analysis code public at this link\nhttps://github.com/htjb/validating_posteriors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural network emulators are widely used in astrophysics and cosmology to\napproximate complex simulations inside Bayesian inference loops. Ad hoc rules\nof thumb are often used to justify the emulator accuracy required for reliable\nposterior recovery. We provide a theoretically motivated limit on the maximum\namount of incorrect information inferred by using an emulator with a given\naccuracy. Under assumptions of linearity in the model, uncorrelated noise in\nthe data and a Gaussian likelihood function, we demonstrate that the difference\nbetween the true underlying posterior and the recovered posterior can be\nquantified via a Kullback-Leibler divergence. We demonstrate how this limit can\nbe used in the field of 21-cm cosmology by comparing the posteriors recovered\nwhen fitting mock data sets generated with the 1D radiative transfer code ARES\ndirectly with the simulation code and separately with an emulator. This paper\nis partly in response to and builds upon recent discussions in the literature\nwhich call into question the use of emulators in Bayesian inference pipelines.\nUpon repeating some aspects of these analyses, we find these concerns\nquantitatively unjustified, with accurate posterior recovery possible even when\nthe mean RMSE error for the emulator is approximately 20% of the magnitude of\nthe noise in the data. For the purposes of community reproducibility, we make\nour analysis code public at this link\nhttps://github.com/htjb/validating_posteriors."
                },
                "authors": [
                    {
                        "name": "H. T. J. Bevins"
                    },
                    {
                        "name": "T. Gessey-Jones"
                    },
                    {
                        "name": "W. J. Handley"
                    }
                ],
                "author_detail": {
                    "name": "W. J. Handley"
                },
                "author": "W. J. Handley",
                "arxiv_comment": "Comments welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03834v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03834v2",
                "updated": "2025-03-17T15:08:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    8,
                    47,
                    0,
                    76,
                    0
                ],
                "published": "2024-10-04T18:02:48Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    18,
                    2,
                    48,
                    4,
                    278,
                    0
                ],
                "title": "GraphRouter: A Graph-based Router for LLM Selections",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphRouter: A Graph-based Router for LLM Selections"
                },
                "summary": "The rapidly growing number and variety of Large Language Models (LLMs)\npresent significant challenges in efficiently selecting the appropriate LLM for\na given query, especially considering the trade-offs between performance and\ncomputational cost. Current LLM selection methods often struggle to generalize\nacross new LLMs and different tasks because of their limited ability to\nleverage contextual interactions among tasks, queries, and LLMs, as well as\ntheir dependence on a transductive learning framework. To address these\nshortcomings, we introduce a novel inductive graph framework, named as\nGraphRouter, which fully utilizes the contextual information among tasks,\nqueries, and LLMs to enhance the LLM selection process. GraphRouter constructs\na heterogeneous graph comprising task, query, and LLM nodes, with interactions\nrepresented as edges, which efficiently captures the contextual information\nbetween the query's requirements and the LLM's capabilities. Through an\ninnovative edge prediction mechanism, GraphRouter is able to predict attributes\n(the effect and cost of LLM response) of potential edges, allowing for\noptimized recommendations that adapt to both existing and newly introduced LLMs\nwithout requiring retraining. Comprehensive experiments across three distinct\neffect-cost weight scenarios have shown that GraphRouter substantially\nsurpasses existing routers, delivering a minimum performance improvement of\n12.3%. In addition, it achieves enhanced generalization across new LLMs\nsettings and supports diverse tasks with at least a 9.5% boost in effect and a\nsignificant reduction in computational demands. This work endeavors to apply a\ngraph-based approach for the contextual and adaptive selection of LLMs,\noffering insights for real-world applications. Our codes for GraphRouter is\nreleased at https://github.com/ulab-uiuc/GraphRouter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapidly growing number and variety of Large Language Models (LLMs)\npresent significant challenges in efficiently selecting the appropriate LLM for\na given query, especially considering the trade-offs between performance and\ncomputational cost. Current LLM selection methods often struggle to generalize\nacross new LLMs and different tasks because of their limited ability to\nleverage contextual interactions among tasks, queries, and LLMs, as well as\ntheir dependence on a transductive learning framework. To address these\nshortcomings, we introduce a novel inductive graph framework, named as\nGraphRouter, which fully utilizes the contextual information among tasks,\nqueries, and LLMs to enhance the LLM selection process. GraphRouter constructs\na heterogeneous graph comprising task, query, and LLM nodes, with interactions\nrepresented as edges, which efficiently captures the contextual information\nbetween the query's requirements and the LLM's capabilities. Through an\ninnovative edge prediction mechanism, GraphRouter is able to predict attributes\n(the effect and cost of LLM response) of potential edges, allowing for\noptimized recommendations that adapt to both existing and newly introduced LLMs\nwithout requiring retraining. Comprehensive experiments across three distinct\neffect-cost weight scenarios have shown that GraphRouter substantially\nsurpasses existing routers, delivering a minimum performance improvement of\n12.3%. In addition, it achieves enhanced generalization across new LLMs\nsettings and supports diverse tasks with at least a 9.5% boost in effect and a\nsignificant reduction in computational demands. This work endeavors to apply a\ngraph-based approach for the contextual and adaptive selection of LLMs,\noffering insights for real-world applications. Our codes for GraphRouter is\nreleased at https://github.com/ulab-uiuc/GraphRouter."
                },
                "authors": [
                    {
                        "name": "Tao Feng"
                    },
                    {
                        "name": "Yanzhen Shen"
                    },
                    {
                        "name": "Jiaxuan You"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxuan You"
                },
                "author": "Jiaxuan You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03834v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03834v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13250v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13250v1",
                "updated": "2025-03-17T15:06:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    6,
                    14,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T15:06:14Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    6,
                    14,
                    0,
                    76,
                    0
                ],
                "title": "MindEye-OmniAssist: A Gaze-Driven LLM-Enhanced Assistive Robot System\n  for Implicit Intention Recognition and Task Execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MindEye-OmniAssist: A Gaze-Driven LLM-Enhanced Assistive Robot System\n  for Implicit Intention Recognition and Task Execution"
                },
                "summary": "A promising effective human-robot interaction in assistive robotic systems is\ngaze-based control. However, current gaze-based assistive systems mainly help\nusers with basic grasping actions, offering limited support. Moreover, the\nrestricted intent recognition capability constrains the assistive system's\nability to provide diverse assistance functions. In this paper, we propose an\nopen implicit intention recognition framework powered by Large Language Model\n(LLM) and Vision Foundation Model (VFM), which can process gaze input and\nrecognize user intents that are not confined to predefined or specific\nscenarios. Furthermore, we implement a gaze-driven LLM-enhanced assistive robot\nsystem (MindEye-OmniAssist) that recognizes user's intentions through gaze and\nassists in completing task. To achieve this, the system utilizes open\nvocabulary object detector, intention recognition network and LLM to infer\ntheir full intentions. By integrating eye movement feedback and LLM, it\ngenerates action sequences to assist the user in completing tasks. Real-world\nexperiments have been conducted for assistive tasks, and the system achieved an\noverall success rate of 41/55 across various undefined tasks. Preliminary\nresults show that the proposed method holds the potential to provide a more\nuser-friendly human-computer interaction interface and significantly enhance\nthe versatility and effectiveness of assistive systems by supporting more\ncomplex and diverse task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A promising effective human-robot interaction in assistive robotic systems is\ngaze-based control. However, current gaze-based assistive systems mainly help\nusers with basic grasping actions, offering limited support. Moreover, the\nrestricted intent recognition capability constrains the assistive system's\nability to provide diverse assistance functions. In this paper, we propose an\nopen implicit intention recognition framework powered by Large Language Model\n(LLM) and Vision Foundation Model (VFM), which can process gaze input and\nrecognize user intents that are not confined to predefined or specific\nscenarios. Furthermore, we implement a gaze-driven LLM-enhanced assistive robot\nsystem (MindEye-OmniAssist) that recognizes user's intentions through gaze and\nassists in completing task. To achieve this, the system utilizes open\nvocabulary object detector, intention recognition network and LLM to infer\ntheir full intentions. By integrating eye movement feedback and LLM, it\ngenerates action sequences to assist the user in completing tasks. Real-world\nexperiments have been conducted for assistive tasks, and the system achieved an\noverall success rate of 41/55 across various undefined tasks. Preliminary\nresults show that the proposed method holds the potential to provide a more\nuser-friendly human-computer interaction interface and significantly enhance\nthe versatility and effectiveness of assistive systems by supporting more\ncomplex and diverse task."
                },
                "authors": [
                    {
                        "name": "Zejia Zhang"
                    },
                    {
                        "name": "Bo Yang"
                    },
                    {
                        "name": "Xinxing Chen"
                    },
                    {
                        "name": "Weizhuang Shi"
                    },
                    {
                        "name": "Haoyuan Wang"
                    },
                    {
                        "name": "Wei Luo"
                    },
                    {
                        "name": "Jian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Huang"
                },
                "author": "Jian Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13250v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13250v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08597v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08597v2",
                "updated": "2025-03-17T15:04:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    4,
                    6,
                    0,
                    76,
                    0
                ],
                "published": "2024-07-11T15:25:02Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    15,
                    25,
                    2,
                    3,
                    193,
                    0
                ],
                "title": "Learning Program Behavioral Models from Synthesized Input-Output Pairs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Program Behavioral Models from Synthesized Input-Output Pairs"
                },
                "summary": "We introduce Modelizer - a novel framework that, given a black-box program,\nlearns a model from its input/output behavior using neural machine translation\nalgorithms. The resulting model mocks the original program: Given an input, the\nmodel predicts the output that would have been produced by the program.\nHowever, the model is also reversible - that is, the model can predict the\ninput that would have produced a given output. Finally, the model is\ndifferentiable and can be efficiently restricted to predict only a certain\naspect of the program behavior. Modelizer uses grammars to synthesize and\ninputs and unsupervised tokenizers to decompose the resulting outputs, allowing\nit to learn sequence-to-sequence associations between token streams. Other than\ninput grammars, Modelizer only requires the ability to execute the program. The\nresulting models are small, requiring fewer than 6.3 million parameters for\nlanguages such as Markdown or HTML; and they are accurate, achieving up to\n95.4% accuracy and a BLEU score of 0.98 with standard error 0.04 in mocking\nreal-world applications. As it learns from and predicts executions rather than\ncode, Modelizer departs from the LLM-centric research trend, opening new\nopportunities for program-specific models that are fully tuned towards\nindividual programs. Indeed, we foresee several applications of these models,\nespecially as the output of the program can be any aspect of program behavior.\nBeyond mocking and predicting program behavior, the models can also synthesize\ninputs that are likely to produce a particular behavior, such as failures or\ncoverage, thus assisting in program understanding and maintenance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Modelizer - a novel framework that, given a black-box program,\nlearns a model from its input/output behavior using neural machine translation\nalgorithms. The resulting model mocks the original program: Given an input, the\nmodel predicts the output that would have been produced by the program.\nHowever, the model is also reversible - that is, the model can predict the\ninput that would have produced a given output. Finally, the model is\ndifferentiable and can be efficiently restricted to predict only a certain\naspect of the program behavior. Modelizer uses grammars to synthesize and\ninputs and unsupervised tokenizers to decompose the resulting outputs, allowing\nit to learn sequence-to-sequence associations between token streams. Other than\ninput grammars, Modelizer only requires the ability to execute the program. The\nresulting models are small, requiring fewer than 6.3 million parameters for\nlanguages such as Markdown or HTML; and they are accurate, achieving up to\n95.4% accuracy and a BLEU score of 0.98 with standard error 0.04 in mocking\nreal-world applications. As it learns from and predicts executions rather than\ncode, Modelizer departs from the LLM-centric research trend, opening new\nopportunities for program-specific models that are fully tuned towards\nindividual programs. Indeed, we foresee several applications of these models,\nespecially as the output of the program can be any aspect of program behavior.\nBeyond mocking and predicting program behavior, the models can also synthesize\ninputs that are likely to produce a particular behavior, such as failures or\ncoverage, thus assisting in program understanding and maintenance."
                },
                "authors": [
                    {
                        "name": "Tural Mammadov"
                    },
                    {
                        "name": "Dietrich Klakow"
                    },
                    {
                        "name": "Alexander Koller"
                    },
                    {
                        "name": "Andreas Zeller"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Zeller"
                },
                "author": "Andreas Zeller",
                "arxiv_comment": "42 pages, 9 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08597v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08597v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07 (Primary), 68N30 (Secondary), 68Q42",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.5; D.2.7; I.2.6; F.1.1; F.4.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13244v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13244v1",
                "updated": "2025-03-17T14:57:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    57,
                    59,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T14:57:59Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    57,
                    59,
                    0,
                    76,
                    0
                ],
                "title": "Pairwise vs Higher-order interactions: Can we identify the interaction\n  type in coupled oscillators from time series?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pairwise vs Higher-order interactions: Can we identify the interaction\n  type in coupled oscillators from time series?"
                },
                "summary": "Rhythmic phenomena, which are ubiquitous in biological systems, are typically\nmodelled as systems of coupled limit cycle oscillators. Recently, there has\nbeen an increased interest in understanding the impact of higher-order\ninteractions on the population dynamics of coupled oscillators. Meanwhile,\nestimating a mathematical model from experimental data is a vital step in\nunderstanding the dynamics of real-world complex systems. In coupled oscillator\nsystems, identifying the type of interaction (e.g. pairwise or three-body) of a\nnetwork is challenging, because different interactions can induce similar\ndynamical states and bifurcations. In this study, we have developed a method\nbased on the adaptive LASSO (Least Absolute Shrinkage and Selection Operator)\nto infer the interactions between the oscillators from time series data. The\nproposed method can successfully classify the type of interaction and infer the\nprobabilities of the existence of pairwise and three-body couplings. Through\nsystematic analysis of synthetic datasets, we have demonstrated that our method\noutperforms two baseline methods, LASSO and OLS (Ordinary Least Squares), in\naccurately inferring the topology and strength of couplings between\noscillators. Finally, we demonstrate the effectiveness of the proposed method\nby applying it to the synthetic data of 100 oscillators. These results imply\nthat the proposed method is promising for identifying interactions from\nrhythmic activities in real-world systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rhythmic phenomena, which are ubiquitous in biological systems, are typically\nmodelled as systems of coupled limit cycle oscillators. Recently, there has\nbeen an increased interest in understanding the impact of higher-order\ninteractions on the population dynamics of coupled oscillators. Meanwhile,\nestimating a mathematical model from experimental data is a vital step in\nunderstanding the dynamics of real-world complex systems. In coupled oscillator\nsystems, identifying the type of interaction (e.g. pairwise or three-body) of a\nnetwork is challenging, because different interactions can induce similar\ndynamical states and bifurcations. In this study, we have developed a method\nbased on the adaptive LASSO (Least Absolute Shrinkage and Selection Operator)\nto infer the interactions between the oscillators from time series data. The\nproposed method can successfully classify the type of interaction and infer the\nprobabilities of the existence of pairwise and three-body couplings. Through\nsystematic analysis of synthetic datasets, we have demonstrated that our method\noutperforms two baseline methods, LASSO and OLS (Ordinary Least Squares), in\naccurately inferring the topology and strength of couplings between\noscillators. Finally, we demonstrate the effectiveness of the proposed method\nby applying it to the synthetic data of 100 oscillators. These results imply\nthat the proposed method is promising for identifying interactions from\nrhythmic activities in real-world systems."
                },
                "authors": [
                    {
                        "name": "Weiwei Su"
                    },
                    {
                        "name": "Shigefumi Hata"
                    },
                    {
                        "name": "Hiroshi Kori"
                    },
                    {
                        "name": "Hiroya Nakao"
                    },
                    {
                        "name": "Ryota Kobayashi"
                    }
                ],
                "author_detail": {
                    "name": "Ryota Kobayashi"
                },
                "author": "Ryota Kobayashi",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13244v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13244v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nlin.CD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nlin.CD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13224v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13224v1",
                "updated": "2025-03-17T14:37:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    37,
                    42,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T14:37:42Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    37,
                    42,
                    0,
                    76,
                    0
                ],
                "title": "ProDiF: Protecting Domain-Invariant Features to Secure Pre-Trained\n  Models Against Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProDiF: Protecting Domain-Invariant Features to Secure Pre-Trained\n  Models Against Extraction"
                },
                "summary": "Pre-trained models are valuable intellectual property, capturing both\ndomain-specific and domain-invariant features within their weight spaces.\nHowever, model extraction attacks threaten these assets by enabling\nunauthorized source-domain inference and facilitating cross-domain transfer via\nthe exploitation of domain-invariant features. In this work, we introduce\n**ProDiF**, a novel framework that leverages targeted weight space manipulation\nto secure pre-trained models against extraction attacks. **ProDiF** quantifies\nthe transferability of filters and perturbs the weights of critical filters in\nunsecured memory, while preserving actual critical weights in a Trusted\nExecution Environment (TEE) for authorized users. A bi-level optimization\nfurther ensures resilience against adaptive fine-tuning attacks. Experimental\nresults show that **ProDiF** reduces source-domain accuracy to near-random\nlevels and decreases cross-domain transferability by 74.65\\%, providing robust\nprotection for pre-trained models. This work offers comprehensive protection\nfor pre-trained DNN models and highlights the potential of weight space\nmanipulation as a novel approach to model security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained models are valuable intellectual property, capturing both\ndomain-specific and domain-invariant features within their weight spaces.\nHowever, model extraction attacks threaten these assets by enabling\nunauthorized source-domain inference and facilitating cross-domain transfer via\nthe exploitation of domain-invariant features. In this work, we introduce\n**ProDiF**, a novel framework that leverages targeted weight space manipulation\nto secure pre-trained models against extraction attacks. **ProDiF** quantifies\nthe transferability of filters and perturbs the weights of critical filters in\nunsecured memory, while preserving actual critical weights in a Trusted\nExecution Environment (TEE) for authorized users. A bi-level optimization\nfurther ensures resilience against adaptive fine-tuning attacks. Experimental\nresults show that **ProDiF** reduces source-domain accuracy to near-random\nlevels and decreases cross-domain transferability by 74.65\\%, providing robust\nprotection for pre-trained models. This work offers comprehensive protection\nfor pre-trained DNN models and highlights the potential of weight space\nmanipulation as a novel approach to model security."
                },
                "authors": [
                    {
                        "name": "Tong Zhou"
                    },
                    {
                        "name": "Shijin Duan"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Charles Fleming"
                    },
                    {
                        "name": "Ramana Rao Kompella"
                    },
                    {
                        "name": "Shaolei Ren"
                    },
                    {
                        "name": "Xiaolin Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaolin Xu"
                },
                "author": "Xiaolin Xu",
                "arxiv_comment": "Accepted at the ICLR Workshop on Neural Network Weights as a New Data\n  Modality 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13224v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13224v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04194v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04194v4",
                "updated": "2025-03-17T14:33:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    33,
                    10,
                    0,
                    76,
                    0
                ],
                "published": "2024-07-05T00:40:03Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    0,
                    40,
                    3,
                    4,
                    187,
                    0
                ],
                "title": "Regularization Using Synthetic Data in High-Dimensional Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regularization Using Synthetic Data in High-Dimensional Models"
                },
                "summary": "To address the challenges of reliable statistical inference in\nhigh-dimensional models, we introduce the Synthetic-data Regularized Estimator\n(SRE). Unlike traditional regularization methods, the SRE regularizes the\ncomplex target model via a weighted likelihood based on synthetic data\ngenerated from a simpler, more stable model. This method provides a\ntheoretically sound and practically effective alternative to parameter\npenalization. We establish key theoretical properties of the SRE in generalized\nlinear models, including existence, stability, consistency, and minimax rate\noptimality. Applying the Convex Gaussian Min-Max Theorem, we derive a precise\nasymptotic characterization in the high-dimensional linear regime. To deal with\nthe non-separable regularization, we introduce a novel decomposition in our\nanalysis. Building upon these results, we develop practical methodologies for\ntuning parameter selection, confidence interval construction, and calibrated\nvariable selection in high-dimensional inference. The effectiveness of the SRE\nis demonstrated through simulation studies and real-data applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To address the challenges of reliable statistical inference in\nhigh-dimensional models, we introduce the Synthetic-data Regularized Estimator\n(SRE). Unlike traditional regularization methods, the SRE regularizes the\ncomplex target model via a weighted likelihood based on synthetic data\ngenerated from a simpler, more stable model. This method provides a\ntheoretically sound and practically effective alternative to parameter\npenalization. We establish key theoretical properties of the SRE in generalized\nlinear models, including existence, stability, consistency, and minimax rate\noptimality. Applying the Convex Gaussian Min-Max Theorem, we derive a precise\nasymptotic characterization in the high-dimensional linear regime. To deal with\nthe non-separable regularization, we introduce a novel decomposition in our\nanalysis. Building upon these results, we develop practical methodologies for\ntuning parameter selection, confidence interval construction, and calibrated\nvariable selection in high-dimensional inference. The effectiveness of the SRE\nis demonstrated through simulation studies and real-data applications."
                },
                "authors": [
                    {
                        "name": "Weihao Li"
                    },
                    {
                        "name": "Dongming Huang"
                    }
                ],
                "author_detail": {
                    "name": "Dongming Huang"
                },
                "author": "Dongming Huang",
                "arxiv_comment": "98 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04194v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04194v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14892v2",
                "updated": "2025-03-17T14:32:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    32,
                    8,
                    0,
                    76,
                    0
                ],
                "published": "2025-01-24T19:31:06Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    19,
                    31,
                    6,
                    4,
                    24,
                    0
                ],
                "title": "Causal Graphs Meet Thoughts: Enhancing Complex Reasoning in\n  Graph-Augmented LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Graphs Meet Thoughts: Enhancing Complex Reasoning in\n  Graph-Augmented LLMs"
                },
                "summary": "In knowledge-intensive tasks, especially in high-stakes domains like medicine\nand law, it is critical not only to retrieve relevant information but also to\nprovide causal reasoning and explainability. Large language models (LLMs) have\nachieved remarkable performance in natural language understanding and\ngeneration tasks. However, they often suffer from limitations such as\ndifficulty in incorporating new knowledge, generating hallucinations, and\nexplaining their reasoning process. To address these challenges, integrating\nknowledge graphs with Graph Retrieval-Augmented Generation (Graph RAG) has\nemerged as an effective solution. Traditional Graph RAG methods often rely on\nsimple graph traversal or semantic similarity, which do not capture causal\nrelationships or align well with the model's internal reasoning steps. This\npaper proposes a novel pipeline that filters large knowledge graphs to\nemphasize cause-effect edges, aligns the retrieval process with the model's\nchain-of-thought (CoT), and enhances reasoning through multi-stage path\nimprovements. Experiments on medical question-answering tasks show consistent\ngains, with up to a 10\\% absolute improvement across multiple large language\nmodels (LLMs). This approach demonstrates the value of combining causal\nreasoning with stepwise retrieval, leading to more interpretable and logically\ngrounded solutions for complex queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In knowledge-intensive tasks, especially in high-stakes domains like medicine\nand law, it is critical not only to retrieve relevant information but also to\nprovide causal reasoning and explainability. Large language models (LLMs) have\nachieved remarkable performance in natural language understanding and\ngeneration tasks. However, they often suffer from limitations such as\ndifficulty in incorporating new knowledge, generating hallucinations, and\nexplaining their reasoning process. To address these challenges, integrating\nknowledge graphs with Graph Retrieval-Augmented Generation (Graph RAG) has\nemerged as an effective solution. Traditional Graph RAG methods often rely on\nsimple graph traversal or semantic similarity, which do not capture causal\nrelationships or align well with the model's internal reasoning steps. This\npaper proposes a novel pipeline that filters large knowledge graphs to\nemphasize cause-effect edges, aligns the retrieval process with the model's\nchain-of-thought (CoT), and enhances reasoning through multi-stage path\nimprovements. Experiments on medical question-answering tasks show consistent\ngains, with up to a 10\\% absolute improvement across multiple large language\nmodels (LLMs). This approach demonstrates the value of combining causal\nreasoning with stepwise retrieval, leading to more interpretable and logically\ngrounded solutions for complex queries."
                },
                "authors": [
                    {
                        "name": "Hang Luo"
                    },
                    {
                        "name": "Jian Zhang"
                    },
                    {
                        "name": "Chujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Chujun Li"
                },
                "author": "Chujun Li",
                "arxiv_comment": "18 pages, 3 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13222v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13222v1",
                "updated": "2025-03-17T14:31:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    31,
                    37,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T14:31:37Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    31,
                    37,
                    0,
                    76,
                    0
                ],
                "title": "Can Language Models Follow Multiple Turns of Entangled Instructions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Language Models Follow Multiple Turns of Entangled Instructions?"
                },
                "summary": "Despite significant achievements in improving the instruction-following\ncapabilities of large language models (LLMs), the ability to process multiple\npotentially entangled or conflicting instructions remains a considerable\nchallenge. Real-world scenarios often require consistency across multiple\ninstructions over time, such as secret privacy, personal preferences, and\nprioritization, which demand sophisticated abilities to integrate multiple\nturns and carefully balance competing objectives when instructions intersect or\nconflict. This work presents a systematic investigation of LLMs' capabilities\nin handling multiple turns of instructions, covering three levels of\ndifficulty: (1) retrieving information from instructions, (2) tracking and\nreasoning across turns, and (3) resolving conflicts among instructions. We\nconstruct MultiTurnInstruct with around 1.1K high-quality multi-turn\nconversations through the human-in-the-loop approach and result in nine\ncapability categories, including statics and dynamics, reasoning, and\nmultitasking. Our finding reveals an intriguing trade-off between different\ncapabilities. While GPT models demonstrate superior memorization, they show\nreduced effectiveness in privacy-protection tasks requiring selective\ninformation withholding. Larger models exhibit stronger reasoning capabilities\nbut still struggle with resolving conflicting instructions. Importantly, these\nperformance gaps cannot be attributed solely to information loss, as models\ndemonstrate strong BLEU scores on memorization tasks but their attention\nmechanisms fail to integrate multiple related instructions effectively. These\nfindings highlight critical areas for improvement in complex real-world tasks\ninvolving multi-turn instructions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant achievements in improving the instruction-following\ncapabilities of large language models (LLMs), the ability to process multiple\npotentially entangled or conflicting instructions remains a considerable\nchallenge. Real-world scenarios often require consistency across multiple\ninstructions over time, such as secret privacy, personal preferences, and\nprioritization, which demand sophisticated abilities to integrate multiple\nturns and carefully balance competing objectives when instructions intersect or\nconflict. This work presents a systematic investigation of LLMs' capabilities\nin handling multiple turns of instructions, covering three levels of\ndifficulty: (1) retrieving information from instructions, (2) tracking and\nreasoning across turns, and (3) resolving conflicts among instructions. We\nconstruct MultiTurnInstruct with around 1.1K high-quality multi-turn\nconversations through the human-in-the-loop approach and result in nine\ncapability categories, including statics and dynamics, reasoning, and\nmultitasking. Our finding reveals an intriguing trade-off between different\ncapabilities. While GPT models demonstrate superior memorization, they show\nreduced effectiveness in privacy-protection tasks requiring selective\ninformation withholding. Larger models exhibit stronger reasoning capabilities\nbut still struggle with resolving conflicting instructions. Importantly, these\nperformance gaps cannot be attributed solely to information loss, as models\ndemonstrate strong BLEU scores on memorization tasks but their attention\nmechanisms fail to integrate multiple related instructions effectively. These\nfindings highlight critical areas for improvement in complex real-world tasks\ninvolving multi-turn instructions."
                },
                "authors": [
                    {
                        "name": "Chi Han"
                    }
                ],
                "author_detail": {
                    "name": "Chi Han"
                },
                "author": "Chi Han",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13222v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13222v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13221v1",
                "updated": "2025-03-17T14:31:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    31,
                    29,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T14:31:29Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    31,
                    29,
                    0,
                    76,
                    0
                ],
                "title": "Measurement of the inhomogeneity of the KATRIN tritium source electric\n  potential by high-resolution spectroscopy of conversion electrons from\n  $^{83m}$Kr",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurement of the inhomogeneity of the KATRIN tritium source electric\n  potential by high-resolution spectroscopy of conversion electrons from\n  $^{83m}$Kr"
                },
                "summary": "Precision spectroscopy of the electron spectrum of the tritium $\\beta$-decay\nnear the kinematic endpoint is a direct method to determine the effective\nelectron antineutrino mass. The KArlsruhe TRItium Neutrino (KATRIN) experiment\naims to determine this quantity with a sensitivity of better than 0.3$\\,$eV\n(90$\\,$% C.L.). An inhomogeneous electric potential in the tritium source of\nKATRIN can lead to distortions of the $\\beta$-spectrum, which directly impact\nthe neutrino-mass observable. This effect can be quantified through precision\nspectroscopy of the conversion-electrons of co-circulated metastable\n$^{83m}$Kr. Therefore, dedicated, several-weeks long measurement campaigns have\nbeen performed within the KATRIN data taking schedule. In this work, we infer\nthe tritium source potential observables from these measurements, and present\ntheir implications for the neutrino-mass determination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precision spectroscopy of the electron spectrum of the tritium $\\beta$-decay\nnear the kinematic endpoint is a direct method to determine the effective\nelectron antineutrino mass. The KArlsruhe TRItium Neutrino (KATRIN) experiment\naims to determine this quantity with a sensitivity of better than 0.3$\\,$eV\n(90$\\,$% C.L.). An inhomogeneous electric potential in the tritium source of\nKATRIN can lead to distortions of the $\\beta$-spectrum, which directly impact\nthe neutrino-mass observable. This effect can be quantified through precision\nspectroscopy of the conversion-electrons of co-circulated metastable\n$^{83m}$Kr. Therefore, dedicated, several-weeks long measurement campaigns have\nbeen performed within the KATRIN data taking schedule. In this work, we infer\nthe tritium source potential observables from these measurements, and present\ntheir implications for the neutrino-mass determination."
                },
                "authors": [
                    {
                        "name": "H. Acharya"
                    },
                    {
                        "name": "M. Aker"
                    },
                    {
                        "name": "D. Batzler"
                    },
                    {
                        "name": "A. Beglarian"
                    },
                    {
                        "name": "J. Beisenkötter"
                    },
                    {
                        "name": "M. Biassoni"
                    },
                    {
                        "name": "B. Bieringer"
                    },
                    {
                        "name": "Y. Biondi"
                    },
                    {
                        "name": "F. Block"
                    },
                    {
                        "name": "B. Bornschein"
                    },
                    {
                        "name": "L. Bornschein"
                    },
                    {
                        "name": "M. Böttcher"
                    },
                    {
                        "name": "M. Carminati"
                    },
                    {
                        "name": "A. Chatrabhuti"
                    },
                    {
                        "name": "S. Chilingaryan"
                    },
                    {
                        "name": "B. A. Daniel"
                    },
                    {
                        "name": "M. Descher"
                    },
                    {
                        "name": "D. Díaz Barrero"
                    },
                    {
                        "name": "O. Dragoun"
                    },
                    {
                        "name": "G. Drexlin"
                    },
                    {
                        "name": "F. Edzards"
                    },
                    {
                        "name": "K. Eitel"
                    },
                    {
                        "name": "E. Ellinger"
                    },
                    {
                        "name": "R. Engel"
                    },
                    {
                        "name": "S. Enomoto"
                    },
                    {
                        "name": "A. Felden"
                    },
                    {
                        "name": "C. Fengler"
                    },
                    {
                        "name": "C. Fiorini"
                    },
                    {
                        "name": "J. A. Formaggio"
                    },
                    {
                        "name": "C. Forstner"
                    },
                    {
                        "name": "F. M. Fränkle"
                    },
                    {
                        "name": "G. Gagliardi"
                    },
                    {
                        "name": "K. Gauda"
                    },
                    {
                        "name": "A. S. Gavin"
                    },
                    {
                        "name": "W. Gil"
                    },
                    {
                        "name": "F. Glück"
                    },
                    {
                        "name": "R. Größle"
                    },
                    {
                        "name": "V. Gupta"
                    },
                    {
                        "name": "K. Habib"
                    },
                    {
                        "name": "V. Hannen"
                    },
                    {
                        "name": "L. Hasselmann"
                    },
                    {
                        "name": "K. Helbing"
                    },
                    {
                        "name": "S. Heyns"
                    },
                    {
                        "name": "R. Hiller"
                    },
                    {
                        "name": "D. Hillesheimer"
                    },
                    {
                        "name": "D. Hinz"
                    },
                    {
                        "name": "T. Höhn"
                    },
                    {
                        "name": "A. Huber"
                    },
                    {
                        "name": "A. Jansen"
                    },
                    {
                        "name": "K. Khosonthongkee"
                    },
                    {
                        "name": "M. Klein"
                    },
                    {
                        "name": "J. Kohpeiß"
                    },
                    {
                        "name": "C. Köhler"
                    },
                    {
                        "name": "A. Kopmann"
                    },
                    {
                        "name": "N. Kovac"
                    },
                    {
                        "name": "L. La Cascio"
                    },
                    {
                        "name": "L. Laschinger"
                    },
                    {
                        "name": "T. Lasserre"
                    },
                    {
                        "name": "J. Lauer"
                    },
                    {
                        "name": "T. L. Le"
                    },
                    {
                        "name": "O. Lebeda"
                    },
                    {
                        "name": "B. Lehnert"
                    },
                    {
                        "name": "A. Lokhov"
                    },
                    {
                        "name": "M. Machatschek"
                    },
                    {
                        "name": "M. Mark"
                    },
                    {
                        "name": "A. Marsteller"
                    },
                    {
                        "name": "E. L. Martin"
                    },
                    {
                        "name": "K. McMichael"
                    },
                    {
                        "name": "C. Melzer"
                    },
                    {
                        "name": "S. Mertens"
                    },
                    {
                        "name": "S. Mohanty"
                    },
                    {
                        "name": "J. Mostafa"
                    },
                    {
                        "name": "A. Nava"
                    },
                    {
                        "name": "H. Neumann"
                    },
                    {
                        "name": "S. Niemes"
                    },
                    {
                        "name": "I. Nutini"
                    },
                    {
                        "name": "A. Onillon"
                    },
                    {
                        "name": "R. Ostertag"
                    },
                    {
                        "name": "D. S. Parno"
                    },
                    {
                        "name": "U. Pinsook"
                    },
                    {
                        "name": "J. Plößner"
                    },
                    {
                        "name": "A. W. P. Poon"
                    },
                    {
                        "name": "J. M. L. Poyato"
                    },
                    {
                        "name": "F. Priester"
                    },
                    {
                        "name": "J. Ráliš"
                    },
                    {
                        "name": "S. Ramachandran"
                    },
                    {
                        "name": "R. G. H. Robertson"
                    },
                    {
                        "name": "C. Rodenbeck"
                    },
                    {
                        "name": "M. Röllig"
                    },
                    {
                        "name": "R. Sack"
                    },
                    {
                        "name": "A. Saenz"
                    },
                    {
                        "name": "R. Salomon"
                    },
                    {
                        "name": "P. Schäfer"
                    },
                    {
                        "name": "M. Slezák"
                    },
                    {
                        "name": "K. Schlösser"
                    },
                    {
                        "name": "M. Schlösser"
                    },
                    {
                        "name": "L. Schlüter"
                    },
                    {
                        "name": "S. Schneidewind"
                    },
                    {
                        "name": "U. Schnurr"
                    },
                    {
                        "name": "J. Schürmann"
                    },
                    {
                        "name": "A. K. Schütz"
                    },
                    {
                        "name": "A. Schwemmer"
                    },
                    {
                        "name": "A. Schwenck"
                    },
                    {
                        "name": "J. Seeyangnok"
                    },
                    {
                        "name": "M. Šefčík"
                    },
                    {
                        "name": "D. Siegmann"
                    },
                    {
                        "name": "F. Simon"
                    },
                    {
                        "name": "J. Songwadhana"
                    },
                    {
                        "name": "F. Spanier"
                    },
                    {
                        "name": "D. Spreng"
                    },
                    {
                        "name": "W. Sreethawong"
                    },
                    {
                        "name": "M. Steidl"
                    },
                    {
                        "name": "J. Štorek"
                    },
                    {
                        "name": "X. Stribl"
                    },
                    {
                        "name": "M. Sturm"
                    },
                    {
                        "name": "N. Suwonjandee"
                    },
                    {
                        "name": "N. Tan Jerome"
                    },
                    {
                        "name": "H. H. Telle"
                    },
                    {
                        "name": "L. A. Thorne"
                    },
                    {
                        "name": "T. Thümmler"
                    },
                    {
                        "name": "K. Trost"
                    },
                    {
                        "name": "K. Valerius"
                    },
                    {
                        "name": "D. Vénos"
                    },
                    {
                        "name": "C. Weinheimer"
                    },
                    {
                        "name": "S. Welte"
                    },
                    {
                        "name": "J. Wendel"
                    },
                    {
                        "name": "C. Wiesinger"
                    },
                    {
                        "name": "J. F. Wilkerson"
                    },
                    {
                        "name": "J. Wolf"
                    },
                    {
                        "name": "S. Wüstling"
                    },
                    {
                        "name": "J. Wydra"
                    },
                    {
                        "name": "W. Xu"
                    },
                    {
                        "name": "G. Zeller"
                    }
                ],
                "author_detail": {
                    "name": "G. Zeller"
                },
                "author": "G. Zeller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nucl-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13217v1",
                "updated": "2025-03-17T14:28:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    28,
                    8,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T14:28:08Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    28,
                    8,
                    0,
                    76,
                    0
                ],
                "title": "Dense Policy: Bidirectional Autoregressive Learning of Actions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dense Policy: Bidirectional Autoregressive Learning of Actions"
                },
                "summary": "Mainstream visuomotor policies predominantly rely on generative models for\nholistic action prediction, while current autoregressive policies, predicting\nthe next token or chunk, have shown suboptimal results. This motivates a search\nfor more effective learning methods to unleash the potential of autoregressive\npolicies for robotic manipulation. This paper introduces a bidirectionally\nexpanded learning approach, termed Dense Policy, to establish a new paradigm\nfor autoregressive policies in action prediction. It employs a lightweight\nencoder-only architecture to iteratively unfold the action sequence from an\ninitial single frame into the target sequence in a coarse-to-fine manner with\nlogarithmic-time inference. Extensive experiments validate that our dense\npolicy has superior autoregressive learning capabilities and can surpass\nexisting holistic generative policies. Our policy, example data, and training\ncode will be publicly available upon publication. Project page: https:\n//selen-suyue.github.io/DspNet/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mainstream visuomotor policies predominantly rely on generative models for\nholistic action prediction, while current autoregressive policies, predicting\nthe next token or chunk, have shown suboptimal results. This motivates a search\nfor more effective learning methods to unleash the potential of autoregressive\npolicies for robotic manipulation. This paper introduces a bidirectionally\nexpanded learning approach, termed Dense Policy, to establish a new paradigm\nfor autoregressive policies in action prediction. It employs a lightweight\nencoder-only architecture to iteratively unfold the action sequence from an\ninitial single frame into the target sequence in a coarse-to-fine manner with\nlogarithmic-time inference. Extensive experiments validate that our dense\npolicy has superior autoregressive learning capabilities and can surpass\nexisting holistic generative policies. Our policy, example data, and training\ncode will be publicly available upon publication. Project page: https:\n//selen-suyue.github.io/DspNet/."
                },
                "authors": [
                    {
                        "name": "Yue Su"
                    },
                    {
                        "name": "Xinyu Zhan"
                    },
                    {
                        "name": "Hongjie Fang"
                    },
                    {
                        "name": "Han Xue"
                    },
                    {
                        "name": "Hao-Shu Fang"
                    },
                    {
                        "name": "Yong-Lu Li"
                    },
                    {
                        "name": "Cewu Lu"
                    },
                    {
                        "name": "Lixin Yang"
                    }
                ],
                "author_detail": {
                    "name": "Lixin Yang"
                },
                "author": "Lixin Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11601v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11601v3",
                "updated": "2025-03-17T14:26:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    26,
                    33,
                    0,
                    76,
                    0
                ],
                "published": "2024-06-17T14:52:21Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    14,
                    52,
                    21,
                    0,
                    169,
                    0
                ],
                "title": "Standardizing Structural Causal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standardizing Structural Causal Models"
                },
                "summary": "Synthetic datasets generated by structural causal models (SCMs) are commonly\nused for benchmarking causal structure learning algorithms. However, the\nvariances and pairwise correlations in SCM data tend to increase along the\ncausal ordering. Several popular algorithms exploit these artifacts, possibly\nleading to conclusions that do not generalize to real-world settings. Existing\nmetrics like $\\operatorname{Var}$-sortability and\n$\\operatorname{R^2}$-sortability quantify these patterns, but they do not\nprovide tools to remedy them. To address this, we propose\ninternally-standardized structural causal models (iSCMs), a modification of\nSCMs that introduces a standardization operation at each variable during the\ngenerative process. By construction, iSCMs are not\n$\\operatorname{Var}$-sortable. We also find empirical evidence that they are\nmostly not $\\operatorname{R^2}$-sortable for commonly-used graph families.\nMoreover, contrary to the post-hoc standardization of data generated by\nstandard SCMs, we prove that linear iSCMs are less identifiable from prior\nknowledge on the weights and do not collapse to deterministic relationships in\nlarge systems, which may make iSCMs a useful model in causal inference beyond\nthe benchmarking problem studied here. Our code is publicly available at:\nhttps://github.com/werkaaa/iscm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic datasets generated by structural causal models (SCMs) are commonly\nused for benchmarking causal structure learning algorithms. However, the\nvariances and pairwise correlations in SCM data tend to increase along the\ncausal ordering. Several popular algorithms exploit these artifacts, possibly\nleading to conclusions that do not generalize to real-world settings. Existing\nmetrics like $\\operatorname{Var}$-sortability and\n$\\operatorname{R^2}$-sortability quantify these patterns, but they do not\nprovide tools to remedy them. To address this, we propose\ninternally-standardized structural causal models (iSCMs), a modification of\nSCMs that introduces a standardization operation at each variable during the\ngenerative process. By construction, iSCMs are not\n$\\operatorname{Var}$-sortable. We also find empirical evidence that they are\nmostly not $\\operatorname{R^2}$-sortable for commonly-used graph families.\nMoreover, contrary to the post-hoc standardization of data generated by\nstandard SCMs, we prove that linear iSCMs are less identifiable from prior\nknowledge on the weights and do not collapse to deterministic relationships in\nlarge systems, which may make iSCMs a useful model in causal inference beyond\nthe benchmarking problem studied here. Our code is publicly available at:\nhttps://github.com/werkaaa/iscm."
                },
                "authors": [
                    {
                        "name": "Weronika Ormaniec"
                    },
                    {
                        "name": "Scott Sussex"
                    },
                    {
                        "name": "Lars Lorch"
                    },
                    {
                        "name": "Bernhard Schölkopf"
                    },
                    {
                        "name": "Andreas Krause"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Krause"
                },
                "author": "Andreas Krause",
                "arxiv_comment": "Added additional benchmarks, including PC algorithm, GES, GOLEM.\n  Evaluated Var-sortability and R2-sortability of the heuristics for mitigating\n  variance accumulation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11601v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11601v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13212v1",
                "updated": "2025-03-17T14:23:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    23,
                    4,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T14:23:04Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    23,
                    4,
                    0,
                    76,
                    0
                ],
                "title": "MAME: Multidimensional Adaptive Metamer Exploration with Human\n  Perceptual Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAME: Multidimensional Adaptive Metamer Exploration with Human\n  Perceptual Feedback"
                },
                "summary": "Alignment between human brain networks and artificial models is actively\nstudied in machine learning and neuroscience. A widely adopted approach to\nexplore their functional alignment is to identify metamers for both humans and\nmodels. Metamers refer to input stimuli that are physically different but\nequivalent within a given system. If a model's metameric space completely\nmatched the human metameric space, the model would achieve functional alignment\nwith humans. However, conventional methods lack direct ways to search for human\nmetamers. Instead, researchers first develop biologically inspired models and\nthen infer about human metamers indirectly by testing whether model metamers\nalso appear as metamers to humans. Here, we propose the Multidimensional\nAdaptive Metamer Exploration (MAME) framework, enabling direct high-dimensional\nexploration of human metameric space. MAME leverages online image generation\nguided by human perceptual feedback. Specifically, it modulates reference\nimages across multiple dimensions by leveraging hierarchical responses from\nconvolutional neural networks (CNNs). Generated images are presented to\nparticipants whose perceptual discriminability is assessed in a behavioral\ntask. Based on participants' responses, subsequent image generation parameters\nare adaptively updated online. Using our MAME framework, we successfully\nmeasured a human metameric space of over fifty dimensions within a single\nexperiment. Experimental results showed that human discrimination sensitivity\nwas lower for metameric images based on low-level features compared to\nhigh-level features, which image contrast metrics could not explain. The\nfinding suggests that the model computes low-level information not essential\nfor human perception. Our framework has the potential to contribute to\ndeveloping interpretable AI and understanding of brain function in\nneuroscience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment between human brain networks and artificial models is actively\nstudied in machine learning and neuroscience. A widely adopted approach to\nexplore their functional alignment is to identify metamers for both humans and\nmodels. Metamers refer to input stimuli that are physically different but\nequivalent within a given system. If a model's metameric space completely\nmatched the human metameric space, the model would achieve functional alignment\nwith humans. However, conventional methods lack direct ways to search for human\nmetamers. Instead, researchers first develop biologically inspired models and\nthen infer about human metamers indirectly by testing whether model metamers\nalso appear as metamers to humans. Here, we propose the Multidimensional\nAdaptive Metamer Exploration (MAME) framework, enabling direct high-dimensional\nexploration of human metameric space. MAME leverages online image generation\nguided by human perceptual feedback. Specifically, it modulates reference\nimages across multiple dimensions by leveraging hierarchical responses from\nconvolutional neural networks (CNNs). Generated images are presented to\nparticipants whose perceptual discriminability is assessed in a behavioral\ntask. Based on participants' responses, subsequent image generation parameters\nare adaptively updated online. Using our MAME framework, we successfully\nmeasured a human metameric space of over fifty dimensions within a single\nexperiment. Experimental results showed that human discrimination sensitivity\nwas lower for metameric images based on low-level features compared to\nhigh-level features, which image contrast metrics could not explain. The\nfinding suggests that the model computes low-level information not essential\nfor human perception. Our framework has the potential to contribute to\ndeveloping interpretable AI and understanding of brain function in\nneuroscience."
                },
                "authors": [
                    {
                        "name": "Mina Kamao"
                    },
                    {
                        "name": "Hayato Ono"
                    },
                    {
                        "name": "Ayumu Yamashita"
                    },
                    {
                        "name": "Kaoru Amano"
                    },
                    {
                        "name": "Masataka Sawayama"
                    }
                ],
                "author_detail": {
                    "name": "Masataka Sawayama"
                },
                "author": "Masataka Sawayama",
                "arxiv_comment": "14 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13208v1",
                "updated": "2025-03-17T14:20:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    20,
                    48,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T14:20:48Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    20,
                    48,
                    0,
                    76,
                    0
                ],
                "title": "Improving Complex Reasoning with Dynamic Prompt Corruption: A soft\n  prompt Optimization Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Complex Reasoning with Dynamic Prompt Corruption: A soft\n  prompt Optimization Approach"
                },
                "summary": "Prompt-tuning (PT) for large language models (LLMs) can facilitate the\nperformance on various conventional NLP tasks with significantly fewer\ntrainable parameters. However, our investigation reveals that PT provides\nlimited improvement and may even degrade the primitive performance of LLMs on\ncomplex reasoning tasks. Such a phenomenon suggests that soft prompts can\npositively impact certain instances while negatively affecting others,\nparticularly during the later phases of reasoning. To address these challenges,\nWe first identify an information accumulation within the soft prompts. Through\ndetailed analysis, we demonstrate that this phenomenon is often accompanied by\nerroneous information flow patterns in the deeper layers of the model, which\nultimately lead to incorrect reasoning outcomes. we propose a novel method\ncalled \\textbf{D}ynamic \\textbf{P}rompt \\textbf{C}orruption (DPC) to take\nbetter advantage of soft prompts in complex reasoning tasks, which dynamically\nadjusts the influence of soft prompts based on their impact on the reasoning\nprocess. Specifically, DPC consists of two stages: Dynamic Trigger and Dynamic\nCorruption. First, Dynamic Trigger measures the impact of soft prompts,\nidentifying whether beneficial or detrimental. Then, Dynamic Corruption\nmitigates the negative effects of soft prompts by selectively masking key\ntokens that interfere with the reasoning process. We validate the proposed\napproach through extensive experiments on various LLMs and reasoning tasks,\nincluding GSM8K, MATH, and AQuA. Experimental results demonstrate that DPC can\nconsistently enhance the performance of PT, achieving 4\\%-8\\% accuracy gains\ncompared to vanilla prompt tuning, highlighting the effectiveness of our\napproach and its potential to enhance complex reasoning in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-tuning (PT) for large language models (LLMs) can facilitate the\nperformance on various conventional NLP tasks with significantly fewer\ntrainable parameters. However, our investigation reveals that PT provides\nlimited improvement and may even degrade the primitive performance of LLMs on\ncomplex reasoning tasks. Such a phenomenon suggests that soft prompts can\npositively impact certain instances while negatively affecting others,\nparticularly during the later phases of reasoning. To address these challenges,\nWe first identify an information accumulation within the soft prompts. Through\ndetailed analysis, we demonstrate that this phenomenon is often accompanied by\nerroneous information flow patterns in the deeper layers of the model, which\nultimately lead to incorrect reasoning outcomes. we propose a novel method\ncalled \\textbf{D}ynamic \\textbf{P}rompt \\textbf{C}orruption (DPC) to take\nbetter advantage of soft prompts in complex reasoning tasks, which dynamically\nadjusts the influence of soft prompts based on their impact on the reasoning\nprocess. Specifically, DPC consists of two stages: Dynamic Trigger and Dynamic\nCorruption. First, Dynamic Trigger measures the impact of soft prompts,\nidentifying whether beneficial or detrimental. Then, Dynamic Corruption\nmitigates the negative effects of soft prompts by selectively masking key\ntokens that interfere with the reasoning process. We validate the proposed\napproach through extensive experiments on various LLMs and reasoning tasks,\nincluding GSM8K, MATH, and AQuA. Experimental results demonstrate that DPC can\nconsistently enhance the performance of PT, achieving 4\\%-8\\% accuracy gains\ncompared to vanilla prompt tuning, highlighting the effectiveness of our\napproach and its potential to enhance complex reasoning in LLMs."
                },
                "authors": [
                    {
                        "name": "Sinan Fan"
                    },
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Chen Shen"
                    },
                    {
                        "name": "Ge Teng"
                    },
                    {
                        "name": "Xiaosong Yuan"
                    },
                    {
                        "name": "Xiaofeng Zhang"
                    },
                    {
                        "name": "Chenxi Huang"
                    },
                    {
                        "name": "Wenxiao Wang"
                    },
                    {
                        "name": "Xiaofei He"
                    },
                    {
                        "name": "Jieping Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jieping Ye"
                },
                "author": "Jieping Ye",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02500v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02500v3",
                "updated": "2025-03-17T14:18:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    18,
                    42,
                    0,
                    76,
                    0
                ],
                "published": "2024-06-04T17:18:40Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    18,
                    40,
                    1,
                    156,
                    0
                ],
                "title": "Towards Efficient Mixture of Experts: A Holistic Study of Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Efficient Mixture of Experts: A Holistic Study of Compression\n  Techniques"
                },
                "summary": "Scaling large language models has driven remarkable advancements across\nvarious domains, yet the continual increase in model size presents significant\nchallenges for real-world deployment. The Mixture of Experts (MoE) architecture\noffers a promising solution by dynamically selecting and activating only a\nsubset of experts during inference, thus substantially reducing computational\ncosts while preserving high performance. Despite these benefits, MoE introduces\nnew inefficiencies, such as excessive parameters and communication overhead. In\nthis work, we present a holistic study of compression techniques for Mixture of\nExperts to enhance both efficiency and scalability. While recent efforts have\nfocused on Expert Trimming, which reduces the number of experts, these\napproaches still suffer from considerable communication and computational\ncosts. To address this, we propose more aggressive strategies, such as Layer\nDrop, which removes entire MoE layers, and Block Drop, which eliminates\ntransformer blocks. Surprisingly, these aggressive pruning techniques not only\npreserve model performance but also substantially improve computation and\nmemory efficiency. Furthermore, beyond Expert Trimming, we also introduce\nExpert Slimming, which compresses individual experts to further boost\nperformance and can be seamlessly integrated with Expert Trimming. Extensive\nexperimental results demonstrate the effectiveness of our proposed\nmethods-Layer Drop and Block Drop-along with the comprehensive recipe that\nintegrates Expert Slimming and Expert Trimming, achieving a 6.05x speedup with\n77.1% reduced memory usage while maintaining over 92% of performance on\nMixtral-8x7B. Our code is released at\nhttps://github.com/CASE-Lab-UMD/Unified-MoE-Compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling large language models has driven remarkable advancements across\nvarious domains, yet the continual increase in model size presents significant\nchallenges for real-world deployment. The Mixture of Experts (MoE) architecture\noffers a promising solution by dynamically selecting and activating only a\nsubset of experts during inference, thus substantially reducing computational\ncosts while preserving high performance. Despite these benefits, MoE introduces\nnew inefficiencies, such as excessive parameters and communication overhead. In\nthis work, we present a holistic study of compression techniques for Mixture of\nExperts to enhance both efficiency and scalability. While recent efforts have\nfocused on Expert Trimming, which reduces the number of experts, these\napproaches still suffer from considerable communication and computational\ncosts. To address this, we propose more aggressive strategies, such as Layer\nDrop, which removes entire MoE layers, and Block Drop, which eliminates\ntransformer blocks. Surprisingly, these aggressive pruning techniques not only\npreserve model performance but also substantially improve computation and\nmemory efficiency. Furthermore, beyond Expert Trimming, we also introduce\nExpert Slimming, which compresses individual experts to further boost\nperformance and can be seamlessly integrated with Expert Trimming. Extensive\nexperimental results demonstrate the effectiveness of our proposed\nmethods-Layer Drop and Block Drop-along with the comprehensive recipe that\nintegrates Expert Slimming and Expert Trimming, achieving a 6.05x speedup with\n77.1% reduced memory usage while maintaining over 92% of performance on\nMixtral-8x7B. Our code is released at\nhttps://github.com/CASE-Lab-UMD/Unified-MoE-Compression."
                },
                "authors": [
                    {
                        "name": "Shwai He"
                    },
                    {
                        "name": "Daize Dong"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Ang Li"
                    }
                ],
                "author_detail": {
                    "name": "Ang Li"
                },
                "author": "Ang Li",
                "arxiv_comment": "Transactions on Machine Learning Research (TMLR)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02500v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02500v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13205v1",
                "updated": "2025-03-17T14:14:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    14,
                    28,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T14:14:28Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    14,
                    28,
                    0,
                    76,
                    0
                ],
                "title": "MAP: Evaluation and Multi-Agent Enhancement of Large Language Models for\n  Inpatient Pathways",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAP: Evaluation and Multi-Agent Enhancement of Large Language Models for\n  Inpatient Pathways"
                },
                "summary": "Inpatient pathways demand complex clinical decision-making based on\ncomprehensive patient information, posing critical challenges for clinicians.\nDespite advancements in large language models (LLMs) in medical applications,\nlimited research focused on artificial intelligence (AI) inpatient pathways\nsystems, due to the lack of large-scale inpatient datasets. Moreover, existing\nmedical benchmarks typically concentrated on medical question-answering and\nexaminations, ignoring the multifaceted nature of clinical decision-making in\ninpatient settings. To address these gaps, we first developed the Inpatient\nPathway Decision Support (IPDS) benchmark from the MIMIC-IV database,\nencompassing 51,274 cases across nine triage departments and 17 major disease\ncategories alongside 16 standardized treatment options. Then, we proposed the\nMulti-Agent Inpatient Pathways (MAP) framework to accomplish inpatient pathways\nwith three clinical agents, including a triage agent managing the patient\nadmission, a diagnosis agent serving as the primary decision maker at the\ndepartment, and a treatment agent providing treatment plans. Additionally, our\nMAP framework includes a chief agent overseeing the inpatient pathways to guide\nand promote these three clinician agents. Extensive experiments showed our MAP\nimproved the diagnosis accuracy by 25.10% compared to the state-of-the-art LLM\nHuatuoGPT2-13B. It is worth noting that our MAP demonstrated significant\nclinical compliance, outperforming three board-certified clinicians by 10%-12%,\nestablishing a foundation for inpatient pathways systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inpatient pathways demand complex clinical decision-making based on\ncomprehensive patient information, posing critical challenges for clinicians.\nDespite advancements in large language models (LLMs) in medical applications,\nlimited research focused on artificial intelligence (AI) inpatient pathways\nsystems, due to the lack of large-scale inpatient datasets. Moreover, existing\nmedical benchmarks typically concentrated on medical question-answering and\nexaminations, ignoring the multifaceted nature of clinical decision-making in\ninpatient settings. To address these gaps, we first developed the Inpatient\nPathway Decision Support (IPDS) benchmark from the MIMIC-IV database,\nencompassing 51,274 cases across nine triage departments and 17 major disease\ncategories alongside 16 standardized treatment options. Then, we proposed the\nMulti-Agent Inpatient Pathways (MAP) framework to accomplish inpatient pathways\nwith three clinical agents, including a triage agent managing the patient\nadmission, a diagnosis agent serving as the primary decision maker at the\ndepartment, and a treatment agent providing treatment plans. Additionally, our\nMAP framework includes a chief agent overseeing the inpatient pathways to guide\nand promote these three clinician agents. Extensive experiments showed our MAP\nimproved the diagnosis accuracy by 25.10% compared to the state-of-the-art LLM\nHuatuoGPT2-13B. It is worth noting that our MAP demonstrated significant\nclinical compliance, outperforming three board-certified clinicians by 10%-12%,\nestablishing a foundation for inpatient pathways systems."
                },
                "authors": [
                    {
                        "name": "Zhen Chen"
                    },
                    {
                        "name": "Zhihao Peng"
                    },
                    {
                        "name": "Xusheng Liang"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Peigan Liang"
                    },
                    {
                        "name": "Linsheng Zeng"
                    },
                    {
                        "name": "Minjie Ju"
                    },
                    {
                        "name": "Yixuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Yixuan Yuan"
                },
                "author": "Yixuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.08597v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.08597v2",
                "updated": "2025-03-17T14:12:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    12,
                    19,
                    0,
                    76,
                    0
                ],
                "published": "2023-08-16T18:00:02Z",
                "published_parsed": [
                    2023,
                    8,
                    16,
                    18,
                    0,
                    2,
                    2,
                    228,
                    0
                ],
                "title": "Scalable inference with Autoregressive Neural Ratio Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable inference with Autoregressive Neural Ratio Estimation"
                },
                "summary": "In recent years, there has been a remarkable development of simulation-based\ninference (SBI) algorithms, and they have now been applied across a wide range\nof astrophysical and cosmological analyses. There are a number of key\nadvantages to these methods, centred around the ability to perform scalable\nstatistical inference without an explicit likelihood. In this work, we propose\ntwo technical building blocks to a specific sequential SBI algorithm, truncated\nmarginal neural ratio estimation (TMNRE). In particular, first we develop\nautoregressive ratio estimation with the aim to robustly estimate correlated\nhigh-dimensional posteriors. Secondly, we propose a slice-based nested sampling\nalgorithm to efficiently draw both posterior samples and constrained prior\nsamples from ratio estimators, the latter being instrumental for sequential\ninference. To validate our implementation, we carry out inference tasks on\nthree concrete examples: a toy model of a multi-dimensional Gaussian, the\nanalysis of a stellar stream mock observation, and finally, a proof-of-concept\napplication to substructure searches in strong gravitational lensing. In\naddition, we publicly release the code for both the autoregressive ratio\nestimator and the slice sampler.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, there has been a remarkable development of simulation-based\ninference (SBI) algorithms, and they have now been applied across a wide range\nof astrophysical and cosmological analyses. There are a number of key\nadvantages to these methods, centred around the ability to perform scalable\nstatistical inference without an explicit likelihood. In this work, we propose\ntwo technical building blocks to a specific sequential SBI algorithm, truncated\nmarginal neural ratio estimation (TMNRE). In particular, first we develop\nautoregressive ratio estimation with the aim to robustly estimate correlated\nhigh-dimensional posteriors. Secondly, we propose a slice-based nested sampling\nalgorithm to efficiently draw both posterior samples and constrained prior\nsamples from ratio estimators, the latter being instrumental for sequential\ninference. To validate our implementation, we carry out inference tasks on\nthree concrete examples: a toy model of a multi-dimensional Gaussian, the\nanalysis of a stellar stream mock observation, and finally, a proof-of-concept\napplication to substructure searches in strong gravitational lensing. In\naddition, we publicly release the code for both the autoregressive ratio\nestimator and the slice sampler."
                },
                "authors": [
                    {
                        "name": "Noemi Anau Montel"
                    },
                    {
                        "name": "James Alvey"
                    },
                    {
                        "name": "Christoph Weniger"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Weniger"
                },
                "author": "Christoph Weniger",
                "arxiv_doi": "10.1093/mnras/stae1130",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/stae1130",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2308.08597v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.08597v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages. 6 figures. Codes: swyft is available at\n  https://github.com/undark-lab/swyft , torchns is available at\n  https://github.com/undark-lab/torchns - v2: version accepted by MNRAS",
                "arxiv_journal_ref": "Monthly Notices of the Royal Astronomical Society, Volume 530,\n  Issue 4, June 2024, Pages 4107-4124",
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13189v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13189v1",
                "updated": "2025-03-17T14:00:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    0,
                    2,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T14:00:02Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    0,
                    2,
                    0,
                    76,
                    0
                ],
                "title": "Causes of evolutionary divergence in prostate cancer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causes of evolutionary divergence in prostate cancer"
                },
                "summary": "Cancer progression involves the sequential accumulation of genetic\nalterations that cumulatively shape the tumour phenotype. In prostate cancer,\ntumours can follow divergent evolutionary trajectories that lead to distinct\nsubtypes, but the causes of this divergence remain unclear. While causal\ninference could elucidate the factors involved, conventional methods are\nunsuitable due to the possibility of unobserved confounders and ambiguity in\nthe direction of causality. Here, we propose a method that circumvents these\nissues and apply it to genomic data from 829 prostate cancer patients. We\nidentify several genetic alterations that drive divergence as well as others\nthat prevent this transition, locking tumours into one trajectory. Further\nanalysis reveals that these genetic alterations may cause each other, implying\na positive-feedback loop that accelerates divergence. Our findings provide\ninsights into how cancer subtypes emerge and offer a foundation for genomic\nsurveillance strategies aimed at monitoring the progression of prostate cancer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cancer progression involves the sequential accumulation of genetic\nalterations that cumulatively shape the tumour phenotype. In prostate cancer,\ntumours can follow divergent evolutionary trajectories that lead to distinct\nsubtypes, but the causes of this divergence remain unclear. While causal\ninference could elucidate the factors involved, conventional methods are\nunsuitable due to the possibility of unobserved confounders and ambiguity in\nthe direction of causality. Here, we propose a method that circumvents these\nissues and apply it to genomic data from 829 prostate cancer patients. We\nidentify several genetic alterations that drive divergence as well as others\nthat prevent this transition, locking tumours into one trajectory. Further\nanalysis reveals that these genetic alterations may cause each other, implying\na positive-feedback loop that accelerates divergence. Our findings provide\ninsights into how cancer subtypes emerge and offer a foundation for genomic\nsurveillance strategies aimed at monitoring the progression of prostate cancer."
                },
                "authors": [
                    {
                        "name": "Emre Esenturk"
                    },
                    {
                        "name": "Atef Sahli"
                    },
                    {
                        "name": "Valeriia Haberland"
                    },
                    {
                        "name": "Aleksandra Ziuboniewicz"
                    },
                    {
                        "name": "Christopher Wirth"
                    },
                    {
                        "name": "G. Steven Bova"
                    },
                    {
                        "name": "Robert G Bristow"
                    },
                    {
                        "name": "Mark N Brook"
                    },
                    {
                        "name": "Benedikt Brors"
                    },
                    {
                        "name": "Adam Butler"
                    },
                    {
                        "name": "Géraldine Cancel-Tassin"
                    },
                    {
                        "name": "Kevin CL Cheng"
                    },
                    {
                        "name": "Colin S Cooper"
                    },
                    {
                        "name": "Niall M Corcoran"
                    },
                    {
                        "name": "Olivier Cussenot"
                    },
                    {
                        "name": "Ros A Eeles"
                    },
                    {
                        "name": "Francesco Favero"
                    },
                    {
                        "name": "Clarissa Gerhauser"
                    },
                    {
                        "name": "Abraham Gihawi"
                    },
                    {
                        "name": "Etsehiwot G Girma"
                    },
                    {
                        "name": "Vincent J Gnanapragasam"
                    },
                    {
                        "name": "Andreas J Gruber"
                    },
                    {
                        "name": "Anis Hamid"
                    },
                    {
                        "name": "Vanessa M Hayes"
                    },
                    {
                        "name": "Housheng Hansen He"
                    },
                    {
                        "name": "Christopher M Hovens"
                    },
                    {
                        "name": "Eddie Luidy Imada"
                    },
                    {
                        "name": "G. Maria Jakobsdottir"
                    },
                    {
                        "name": "Chol-hee Jung"
                    },
                    {
                        "name": "Francesca Khani"
                    },
                    {
                        "name": "Zsofia Kote-Jarai"
                    },
                    {
                        "name": "Philippe Lamy"
                    },
                    {
                        "name": "Gregory Leeman"
                    },
                    {
                        "name": "Massimo Loda"
                    },
                    {
                        "name": "Pavlo Lutsik"
                    },
                    {
                        "name": "Luigi Marchionni"
                    },
                    {
                        "name": "Ramyar Molania"
                    },
                    {
                        "name": "Anthony T Papenfuss"
                    },
                    {
                        "name": "Diogo Pellegrina"
                    },
                    {
                        "name": "Bernard Pope"
                    },
                    {
                        "name": "Lucio R Queiroz"
                    },
                    {
                        "name": "Tobias Rausch"
                    },
                    {
                        "name": "Jüri Reimand"
                    },
                    {
                        "name": "Brian Robinson"
                    },
                    {
                        "name": "Thorsten Schlomm"
                    },
                    {
                        "name": "Karina D Sørensen"
                    },
                    {
                        "name": "Sebastian Uhrig"
                    },
                    {
                        "name": "Joachim Weischenfeldt"
                    },
                    {
                        "name": "Yaobo Xu"
                    },
                    {
                        "name": "Takafumi N Yamaguchi"
                    },
                    {
                        "name": "Claudio Zanettini"
                    },
                    {
                        "name": "Andy G Lynch"
                    },
                    {
                        "name": "David C Wedge"
                    },
                    {
                        "name": "Daniel S Brewer"
                    },
                    {
                        "name": "Dan J Woodcock"
                    }
                ],
                "author_detail": {
                    "name": "Dan J Woodcock"
                },
                "author": "Dan J Woodcock",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13189v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13189v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13183v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13183v1",
                "updated": "2025-03-17T13:56:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    13,
                    56,
                    45,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T13:56:45Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    13,
                    56,
                    45,
                    0,
                    76,
                    0
                ],
                "title": "OLÉ -- Online Learning Emulation in Cosmology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OLÉ -- Online Learning Emulation in Cosmology"
                },
                "summary": "In this work, we present OL\\'E, a new online learning emulator for use in\ncosmological inference. The emulator relies on Gaussian Processes and Principal\nComponent Analysis for efficient data compression and fast evaluation.\nMoreover, OL\\'E features an automatic error estimation for optimal active\nsampling and online learning. All training data is computed on-the-fly, making\nthe emulator applicable to any cosmological model or dataset. We illustrate the\nemulator's performance on an array of cosmological models and data sets,\nshowing significant improvements in efficiency over similar emulators without\ndegrading accuracy compared to standard theory codes. We find that OL\\'E is\nable to considerably speed up the inference process, increasing the efficiency\nby a factor of $30-350$, including data acquisition and training. Typically the\nruntime of the likelihood code becomes the computational bottleneck.\nFurthermore, OL\\'E emulators are differentiable; we demonstrate that, together\nwith the differentiable likelihoods available in the $\\texttt{candl}$ library,\nwe can construct a gradient-based sampling method which yields an additional\nimprovement factor of 4. OL\\'E can be easily interfaced with the popular\nsamplers $\\texttt{MontePython}$ and $\\texttt{Cobaya}$, and the\nEinstein-Boltzmann solvers $\\texttt{CLASS}$ and $\\texttt{CAMB}$. OL\\'E is\npublicly available at https://github.com/svenguenther/OLE .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we present OL\\'E, a new online learning emulator for use in\ncosmological inference. The emulator relies on Gaussian Processes and Principal\nComponent Analysis for efficient data compression and fast evaluation.\nMoreover, OL\\'E features an automatic error estimation for optimal active\nsampling and online learning. All training data is computed on-the-fly, making\nthe emulator applicable to any cosmological model or dataset. We illustrate the\nemulator's performance on an array of cosmological models and data sets,\nshowing significant improvements in efficiency over similar emulators without\ndegrading accuracy compared to standard theory codes. We find that OL\\'E is\nable to considerably speed up the inference process, increasing the efficiency\nby a factor of $30-350$, including data acquisition and training. Typically the\nruntime of the likelihood code becomes the computational bottleneck.\nFurthermore, OL\\'E emulators are differentiable; we demonstrate that, together\nwith the differentiable likelihoods available in the $\\texttt{candl}$ library,\nwe can construct a gradient-based sampling method which yields an additional\nimprovement factor of 4. OL\\'E can be easily interfaced with the popular\nsamplers $\\texttt{MontePython}$ and $\\texttt{Cobaya}$, and the\nEinstein-Boltzmann solvers $\\texttt{CLASS}$ and $\\texttt{CAMB}$. OL\\'E is\npublicly available at https://github.com/svenguenther/OLE ."
                },
                "authors": [
                    {
                        "name": "Sven Günther"
                    },
                    {
                        "name": "Lennart Balkenhol"
                    },
                    {
                        "name": "Christian Fidler"
                    },
                    {
                        "name": "Ali Rida Khalife"
                    },
                    {
                        "name": "Julien Lesgourgues"
                    },
                    {
                        "name": "Markus R. Mosbech"
                    },
                    {
                        "name": "Ravi Kumar Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Kumar Sharma"
                },
                "author": "Ravi Kumar Sharma",
                "arxiv_comment": "37 pages, 9 figures, code available at\n  https://github.com/svenguenther/OLE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13183v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13183v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13179v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13179v1",
                "updated": "2025-03-17T13:54:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    13,
                    54,
                    26,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T13:54:26Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    13,
                    54,
                    26,
                    0,
                    76,
                    0
                ],
                "title": "A super-resolution reconstruction method for lightweight building images\n  based on an expanding feature modulation network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A super-resolution reconstruction method for lightweight building images\n  based on an expanding feature modulation network"
                },
                "summary": "This study proposes a lightweight method for building image super-resolution\nusing a Dilated Contextual Feature Modulation Network (DCFMN). The process\nincludes obtaining high-resolution images, down-sampling them to\nlow-resolution, enhancing the low-resolution images, constructing and training\na lightweight network model, and generating super-resolution outputs. To\naddress challenges such as regular textures and long-range dependencies in\nbuilding images, the DCFMN integrates an expansion separable modulation unit\nand a local feature enhancement module. The former employs multiple expansion\nconvolutions equivalent to a large kernel to efficiently aggregate multi-scale\nfeatures while leveraging a simple attention mechanism for adaptivity. The\nlatter encodes local features, mixes channel information, and ensures no\nadditional computational burden during inference through reparameterization.\nThis approach effectively resolves the limitations of existing lightweight\nsuper-resolution networks in modeling long-range dependencies, achieving\naccurate and efficient global feature modeling without increasing computational\ncosts, and significantly improving both reconstruction quality and lightweight\nefficiency for building image super-resolution models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study proposes a lightweight method for building image super-resolution\nusing a Dilated Contextual Feature Modulation Network (DCFMN). The process\nincludes obtaining high-resolution images, down-sampling them to\nlow-resolution, enhancing the low-resolution images, constructing and training\na lightweight network model, and generating super-resolution outputs. To\naddress challenges such as regular textures and long-range dependencies in\nbuilding images, the DCFMN integrates an expansion separable modulation unit\nand a local feature enhancement module. The former employs multiple expansion\nconvolutions equivalent to a large kernel to efficiently aggregate multi-scale\nfeatures while leveraging a simple attention mechanism for adaptivity. The\nlatter encodes local features, mixes channel information, and ensures no\nadditional computational burden during inference through reparameterization.\nThis approach effectively resolves the limitations of existing lightweight\nsuper-resolution networks in modeling long-range dependencies, achieving\naccurate and efficient global feature modeling without increasing computational\ncosts, and significantly improving both reconstruction quality and lightweight\nefficiency for building image super-resolution models."
                },
                "authors": [
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Wenye Zhou"
                    },
                    {
                        "name": "Ruonan Lin"
                    }
                ],
                "author_detail": {
                    "name": "Ruonan Lin"
                },
                "author": "Ruonan Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13179v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.07207v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.07207v3",
                "updated": "2025-03-17T13:51:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    13,
                    51,
                    51,
                    0,
                    76,
                    0
                ],
                "published": "2023-06-12T16:11:10Z",
                "published_parsed": [
                    2023,
                    6,
                    12,
                    16,
                    11,
                    10,
                    0,
                    163,
                    0
                ],
                "title": "Valley: Video Assistant with Large Language model Enhanced abilitY",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Valley: Video Assistant with Large Language model Enhanced abilitY"
                },
                "summary": "Large Language Models (LLMs), with remarkable conversational capability, have\nemerged as AI assistants that can handle both visual and textual modalities.\nHowever, their effectiveness in joint video and language understanding has not\nbeen extensively explored. In the paper, we introduce Valley, a multi-modal\nfoundation model that is designed to enable enhanced video comprehension and\ninstruction-following capabilities. To this end, we construct two datasets,\nnamely Valley-702k and Valley-instruct-73k, to cover a diverse range of\nvideo-text alignment and video-based instruction tasks, such as multi-shot\ncaptions, long video descriptions, action recognition, causal inference, etc.\nThen, we adopt ViT-L/14 as the vision encoder and explore three different\ntemporal modeling modules to learn multifaceted features for enhanced video\nunderstanding. In addition, we implement a two-phase training approach for\nValley: the first phase focuses solely on training the projection module to\nfacilitate the LLM's capacity to understand visual input, and the second phase\njointly trains the projection module and the LLM to improve their instruction\nfollowing ability. Extensive experiments demonstrate that Valley has the\npotential to serve as an effective video assistant, simplifying complex\nvideo-understanding scenarios. Our code and data are published anonymously at\nhttps://github.com/valley-vl/Valley.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), with remarkable conversational capability, have\nemerged as AI assistants that can handle both visual and textual modalities.\nHowever, their effectiveness in joint video and language understanding has not\nbeen extensively explored. In the paper, we introduce Valley, a multi-modal\nfoundation model that is designed to enable enhanced video comprehension and\ninstruction-following capabilities. To this end, we construct two datasets,\nnamely Valley-702k and Valley-instruct-73k, to cover a diverse range of\nvideo-text alignment and video-based instruction tasks, such as multi-shot\ncaptions, long video descriptions, action recognition, causal inference, etc.\nThen, we adopt ViT-L/14 as the vision encoder and explore three different\ntemporal modeling modules to learn multifaceted features for enhanced video\nunderstanding. In addition, we implement a two-phase training approach for\nValley: the first phase focuses solely on training the projection module to\nfacilitate the LLM's capacity to understand visual input, and the second phase\njointly trains the projection module and the LLM to improve their instruction\nfollowing ability. Extensive experiments demonstrate that Valley has the\npotential to serve as an effective video assistant, simplifying complex\nvideo-understanding scenarios. Our code and data are published anonymously at\nhttps://github.com/valley-vl/Valley."
                },
                "authors": [
                    {
                        "name": "Ruipu Luo"
                    },
                    {
                        "name": "Ziwang Zhao"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Zheming Yang"
                    },
                    {
                        "name": "Minghui Qiu"
                    },
                    {
                        "name": "Tao Wang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    },
                    {
                        "name": "Yanhao Wang"
                    },
                    {
                        "name": "Cen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Cen Chen"
                },
                "author": "Cen Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.07207v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.07207v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21016v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21016v2",
                "updated": "2025-03-17T13:42:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    13,
                    42,
                    6,
                    0,
                    76,
                    0
                ],
                "published": "2024-12-30T15:33:34Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    34,
                    0,
                    365,
                    0
                ],
                "title": "Assessing the Robustness of LLM-based NLP Software via Automated Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Robustness of LLM-based NLP Software via Automated Testing"
                },
                "summary": "Benefiting from the advancements in LLMs, NLP software has undergone rapid\ndevelopment. Such software is widely employed in various safety-critical tasks,\nsuch as financial sentiment analysis, toxic content moderation, and log\ngeneration. Unlike traditional software, LLM-based NLP software relies on\nprompts and examples as inputs. Given the complexity of LLMs and the\nunpredictability of real-world inputs, quantitatively assessing the robustness\nof such software is crucial. However, to the best of our knowledge, no\nautomated robustness testing methods have been specifically designed to\nevaluate the overall inputs of LLM-based NLP software. To this end, this paper\nintroduces the first AutOmated Robustness Testing frAmework, AORTA, which\nreconceptualizes the testing process into a combinatorial optimization problem.\nExisting testing methods designed for DNN-based software can be applied to\nLLM-based software by AORTA, but their effectiveness is limited. To address\nthis, we propose a novel testing method for LLM-based software within AORTA\ncalled Adaptive Beam Search. ABS is tailored for the expansive feature space of\nLLMs and improves testing effectiveness through an adaptive beam width and the\ncapability for backtracking. We successfully embed 18 test methods in the\ndesigned framework AORTA and compared the test validity of ABS with three\ndatasets and five threat models. ABS facilitates a more comprehensive and\naccurate robustness assessment before software deployment, with an average test\nsuccess rate of 86.138%. Compared to the currently best-performing baseline\nPWWS, ABS significantly reduces the computational overhead by up to 3441.895\nseconds per successful test case and decreases the number of queries by 218.762\ntimes on average. Furthermore, test cases generated by ABS exhibit greater\nnaturalness and transferability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benefiting from the advancements in LLMs, NLP software has undergone rapid\ndevelopment. Such software is widely employed in various safety-critical tasks,\nsuch as financial sentiment analysis, toxic content moderation, and log\ngeneration. Unlike traditional software, LLM-based NLP software relies on\nprompts and examples as inputs. Given the complexity of LLMs and the\nunpredictability of real-world inputs, quantitatively assessing the robustness\nof such software is crucial. However, to the best of our knowledge, no\nautomated robustness testing methods have been specifically designed to\nevaluate the overall inputs of LLM-based NLP software. To this end, this paper\nintroduces the first AutOmated Robustness Testing frAmework, AORTA, which\nreconceptualizes the testing process into a combinatorial optimization problem.\nExisting testing methods designed for DNN-based software can be applied to\nLLM-based software by AORTA, but their effectiveness is limited. To address\nthis, we propose a novel testing method for LLM-based software within AORTA\ncalled Adaptive Beam Search. ABS is tailored for the expansive feature space of\nLLMs and improves testing effectiveness through an adaptive beam width and the\ncapability for backtracking. We successfully embed 18 test methods in the\ndesigned framework AORTA and compared the test validity of ABS with three\ndatasets and five threat models. ABS facilitates a more comprehensive and\naccurate robustness assessment before software deployment, with an average test\nsuccess rate of 86.138%. Compared to the currently best-performing baseline\nPWWS, ABS significantly reduces the computational overhead by up to 3441.895\nseconds per successful test case and decreases the number of queries by 218.762\ntimes on average. Furthermore, test cases generated by ABS exhibit greater\nnaturalness and transferability."
                },
                "authors": [
                    {
                        "name": "Mingxuan Xiao"
                    },
                    {
                        "name": "Yan Xiao"
                    },
                    {
                        "name": "Shunhui Ji"
                    },
                    {
                        "name": "Hanbo Cai"
                    },
                    {
                        "name": "Lei Xue"
                    },
                    {
                        "name": "Pengcheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Pengcheng Zhang"
                },
                "author": "Pengcheng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21016v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21016v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06027v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06027v2",
                "updated": "2025-03-17T13:37:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    13,
                    37,
                    33,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-08T02:59:51Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    2,
                    59,
                    51,
                    5,
                    67,
                    0
                ],
                "title": "Empowering Edge Intelligence: A Comprehensive Survey on On-Device AI\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering Edge Intelligence: A Comprehensive Survey on On-Device AI\n  Models"
                },
                "summary": "The rapid advancement of artificial intelligence (AI) technologies has led to\nan increasing deployment of AI models on edge and terminal devices, driven by\nthe proliferation of the Internet of Things (IoT) and the need for real-time\ndata processing. This survey comprehensively explores the current state,\ntechnical challenges, and future trends of on-device AI models. We define\non-device AI models as those designed to perform local data processing and\ninference, emphasizing their characteristics such as real-time performance,\nresource constraints, and enhanced data privacy. The survey is structured\naround key themes, including the fundamental concepts of AI models, application\nscenarios across various domains, and the technical challenges faced in edge\nenvironments. We also discuss optimization and implementation strategies, such\nas data preprocessing, model compression, and hardware acceleration, which are\nessential for effective deployment. Furthermore, we examine the impact of\nemerging technologies, including edge computing and foundation models, on the\nevolution of on-device AI models. By providing a structured overview of the\nchallenges, solutions, and future directions, this survey aims to facilitate\nfurther research and application of on-device AI, ultimately contributing to\nthe advancement of intelligent systems in everyday life.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of artificial intelligence (AI) technologies has led to\nan increasing deployment of AI models on edge and terminal devices, driven by\nthe proliferation of the Internet of Things (IoT) and the need for real-time\ndata processing. This survey comprehensively explores the current state,\ntechnical challenges, and future trends of on-device AI models. We define\non-device AI models as those designed to perform local data processing and\ninference, emphasizing their characteristics such as real-time performance,\nresource constraints, and enhanced data privacy. The survey is structured\naround key themes, including the fundamental concepts of AI models, application\nscenarios across various domains, and the technical challenges faced in edge\nenvironments. We also discuss optimization and implementation strategies, such\nas data preprocessing, model compression, and hardware acceleration, which are\nessential for effective deployment. Furthermore, we examine the impact of\nemerging technologies, including edge computing and foundation models, on the\nevolution of on-device AI models. By providing a structured overview of the\nchallenges, solutions, and future directions, this survey aims to facilitate\nfurther research and application of on-device AI, ultimately contributing to\nthe advancement of intelligent systems in everyday life."
                },
                "authors": [
                    {
                        "name": "Xubin Wang"
                    },
                    {
                        "name": "Zhiqing Tang"
                    },
                    {
                        "name": "Jianxiong Guo"
                    },
                    {
                        "name": "Tianhui Meng"
                    },
                    {
                        "name": "Chenhao Wang"
                    },
                    {
                        "name": "Tian Wang"
                    },
                    {
                        "name": "Weijia Jia"
                    }
                ],
                "author_detail": {
                    "name": "Weijia Jia"
                },
                "author": "Weijia Jia",
                "arxiv_doi": "10.1145/3724420",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3724420",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.06027v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06027v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This paper has been accepted by ACM Computing Surveys",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04927v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04927v3",
                "updated": "2025-03-17T13:34:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    13,
                    34,
                    7,
                    0,
                    76,
                    0
                ],
                "published": "2024-06-07T13:33:22Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    13,
                    33,
                    22,
                    4,
                    159,
                    0
                ],
                "title": "LLM-based speaker diarization correction: A generalizable approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based speaker diarization correction: A generalizable approach"
                },
                "summary": "Speaker diarization is necessary for interpreting conversations transcribed\nusing automated speech recognition (ASR) tools. Despite significant\ndevelopments in diarization methods, diarization accuracy remains an issue.\nHere, we investigate the use of large language models (LLMs) for diarization\ncorrection as a post-processing step. LLMs were fine-tuned using the Fisher\ncorpus, a large dataset of transcribed conversations. The ability of the models\nto improve diarization accuracy in a holdout dataset from the Fisher corpus as\nwell as an independent dataset was measured. We report that fine-tuned LLMs can\nmarkedly improve diarization accuracy. However, model performance is\nconstrained to transcripts produced using the same ASR tool as the transcripts\nused for fine-tuning, limiting generalizability. To address this constraint, an\nensemble model was developed by combining weights from three separate models,\neach fine-tuned using transcripts from a different ASR tool. The ensemble model\ndemonstrated better overall performance than each of the ASR-specific models,\nsuggesting that a generalizable and ASR-agnostic approach may be achievable. We\nhave made the weights of these models publicly available on HuggingFace at\nhttps://huggingface.co/bklynhlth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speaker diarization is necessary for interpreting conversations transcribed\nusing automated speech recognition (ASR) tools. Despite significant\ndevelopments in diarization methods, diarization accuracy remains an issue.\nHere, we investigate the use of large language models (LLMs) for diarization\ncorrection as a post-processing step. LLMs were fine-tuned using the Fisher\ncorpus, a large dataset of transcribed conversations. The ability of the models\nto improve diarization accuracy in a holdout dataset from the Fisher corpus as\nwell as an independent dataset was measured. We report that fine-tuned LLMs can\nmarkedly improve diarization accuracy. However, model performance is\nconstrained to transcripts produced using the same ASR tool as the transcripts\nused for fine-tuning, limiting generalizability. To address this constraint, an\nensemble model was developed by combining weights from three separate models,\neach fine-tuned using transcripts from a different ASR tool. The ensemble model\ndemonstrated better overall performance than each of the ASR-specific models,\nsuggesting that a generalizable and ASR-agnostic approach may be achievable. We\nhave made the weights of these models publicly available on HuggingFace at\nhttps://huggingface.co/bklynhlth."
                },
                "authors": [
                    {
                        "name": "Georgios Efstathiadis"
                    },
                    {
                        "name": "Vijay Yadav"
                    },
                    {
                        "name": "Anzar Abbas"
                    }
                ],
                "author_detail": {
                    "name": "Anzar Abbas"
                },
                "author": "Anzar Abbas",
                "arxiv_doi": "10.1016/j.specom.2025.103224",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.specom.2025.103224",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.04927v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04927v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Speech Communication, Volume 170, 2025, Page 103224",
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13160v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13160v1",
                "updated": "2025-03-17T13:31:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    13,
                    31,
                    19,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T13:31:19Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    13,
                    31,
                    19,
                    0,
                    76,
                    0
                ],
                "title": "Language-guided Open-world Video Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-guided Open-world Video Anomaly Detection"
                },
                "summary": "Video anomaly detection models aim to detect anomalies that deviate from what\nis expected. In open-world scenarios, the expected events may change as\nrequirements change. For example, not wearing a mask is considered abnormal\nduring a flu outbreak but normal otherwise. However, existing methods assume\nthat the definition of anomalies is invariable, and thus are not applicable to\nthe open world. To address this, we propose a novel open-world VAD paradigm\nwith variable definitions, allowing guided detection through user-provided\nnatural language at inference time. This paradigm necessitates establishing a\nrobust mapping from video and textual definition to anomaly score. Therefore,\nwe propose LaGoVAD (Language-guided Open-world VAD), a model that dynamically\nadapts anomaly definitions through two regularization strategies: diversifying\nthe relative durations of anomalies via dynamic video synthesis, and enhancing\nfeature robustness through contrastive learning with negative mining. Training\nsuch adaptable models requires diverse anomaly definitions, but existing\ndatasets typically provide given labels without semantic descriptions. To\nbridge this gap, we collect PreVAD (Pre-training Video Anomaly Dataset), the\nlargest and most diverse video anomaly dataset to date, featuring 35,279\nannotated videos with multi-level category labels and descriptions that\nexplicitly define anomalies. Zero-shot experiments on seven datasets\ndemonstrate SOTA performance. Data and code will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video anomaly detection models aim to detect anomalies that deviate from what\nis expected. In open-world scenarios, the expected events may change as\nrequirements change. For example, not wearing a mask is considered abnormal\nduring a flu outbreak but normal otherwise. However, existing methods assume\nthat the definition of anomalies is invariable, and thus are not applicable to\nthe open world. To address this, we propose a novel open-world VAD paradigm\nwith variable definitions, allowing guided detection through user-provided\nnatural language at inference time. This paradigm necessitates establishing a\nrobust mapping from video and textual definition to anomaly score. Therefore,\nwe propose LaGoVAD (Language-guided Open-world VAD), a model that dynamically\nadapts anomaly definitions through two regularization strategies: diversifying\nthe relative durations of anomalies via dynamic video synthesis, and enhancing\nfeature robustness through contrastive learning with negative mining. Training\nsuch adaptable models requires diverse anomaly definitions, but existing\ndatasets typically provide given labels without semantic descriptions. To\nbridge this gap, we collect PreVAD (Pre-training Video Anomaly Dataset), the\nlargest and most diverse video anomaly dataset to date, featuring 35,279\nannotated videos with multi-level category labels and descriptions that\nexplicitly define anomalies. Zero-shot experiments on seven datasets\ndemonstrate SOTA performance. Data and code will be released."
                },
                "authors": [
                    {
                        "name": "Zihao Liu"
                    },
                    {
                        "name": "Xiaoyu Wu"
                    },
                    {
                        "name": "Jianqin Wu"
                    },
                    {
                        "name": "Xuxu Wang"
                    },
                    {
                        "name": "Linlin Yang"
                    }
                ],
                "author_detail": {
                    "name": "Linlin Yang"
                },
                "author": "Linlin Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13160v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13160v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19371v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19371v2",
                "updated": "2025-03-17T13:23:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    13,
                    23,
                    33,
                    0,
                    76,
                    0
                ],
                "published": "2024-10-25T08:18:49Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    8,
                    18,
                    49,
                    4,
                    299,
                    0
                ],
                "title": "Noise-Aware Differentially Private Variational Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noise-Aware Differentially Private Variational Inference"
                },
                "summary": "Differential privacy (DP) provides robust privacy guarantees for statistical\ninference, but this can lead to unreliable results and biases in downstream\napplications. While several noise-aware approaches have been proposed which\nintegrate DP perturbation into the inference, they are limited to specific\ntypes of simple probabilistic models. In this work, we propose a novel method\nfor noise-aware approximate Bayesian inference based on stochastic gradient\nvariational inference which can also be applied to high-dimensional and\nnon-conjugate models. We also propose a more accurate evaluation method for\nnoise-aware posteriors. Empirically, our inference method has similar\nperformance to existing methods in the domain where they are applicable.\nOutside this domain, we obtain accurate coverages on high-dimensional Bayesian\nlinear regression and well-calibrated predictive probabilities on Bayesian\nlogistic regression with the UCI Adult dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differential privacy (DP) provides robust privacy guarantees for statistical\ninference, but this can lead to unreliable results and biases in downstream\napplications. While several noise-aware approaches have been proposed which\nintegrate DP perturbation into the inference, they are limited to specific\ntypes of simple probabilistic models. In this work, we propose a novel method\nfor noise-aware approximate Bayesian inference based on stochastic gradient\nvariational inference which can also be applied to high-dimensional and\nnon-conjugate models. We also propose a more accurate evaluation method for\nnoise-aware posteriors. Empirically, our inference method has similar\nperformance to existing methods in the domain where they are applicable.\nOutside this domain, we obtain accurate coverages on high-dimensional Bayesian\nlinear regression and well-calibrated predictive probabilities on Bayesian\nlogistic regression with the UCI Adult dataset."
                },
                "authors": [
                    {
                        "name": "Talal Alrawajfeh"
                    },
                    {
                        "name": "Joonas Jälkö"
                    },
                    {
                        "name": "Antti Honkela"
                    }
                ],
                "author_detail": {
                    "name": "Antti Honkela"
                },
                "author": "Antti Honkela",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19371v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19371v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13149v1",
                "updated": "2025-03-17T13:20:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    13,
                    20,
                    9,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T13:20:09Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    13,
                    20,
                    9,
                    0,
                    76,
                    0
                ],
                "title": "Are LLMs (Really) Ideological? An IRT-based Analysis and Alignment Tool\n  for Perceived Socio-Economic Bias in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are LLMs (Really) Ideological? An IRT-based Analysis and Alignment Tool\n  for Perceived Socio-Economic Bias in LLMs"
                },
                "summary": "We introduce an Item Response Theory (IRT)-based framework to detect and\nquantify socioeconomic bias in large language models (LLMs) without relying on\nsubjective human judgments. Unlike traditional methods, IRT accounts for item\ndifficulty, improving ideological bias estimation. We fine-tune two LLM\nfamilies (Meta-LLaMa 3.2-1B-Instruct and Chat- GPT 3.5) to represent distinct\nideological positions and introduce a two-stage approach: (1) modeling response\navoidance and (2) estimating perceived bias in answered responses. Our results\nshow that off-the-shelf LLMs often avoid ideological engagement rather than\nexhibit bias, challenging prior claims of partisanship. This empirically\nvalidated framework enhances AI alignment research and promotes fairer AI\ngovernance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an Item Response Theory (IRT)-based framework to detect and\nquantify socioeconomic bias in large language models (LLMs) without relying on\nsubjective human judgments. Unlike traditional methods, IRT accounts for item\ndifficulty, improving ideological bias estimation. We fine-tune two LLM\nfamilies (Meta-LLaMa 3.2-1B-Instruct and Chat- GPT 3.5) to represent distinct\nideological positions and introduce a two-stage approach: (1) modeling response\navoidance and (2) estimating perceived bias in answered responses. Our results\nshow that off-the-shelf LLMs often avoid ideological engagement rather than\nexhibit bias, challenging prior claims of partisanship. This empirically\nvalidated framework enhances AI alignment research and promotes fairer AI\ngovernance."
                },
                "authors": [
                    {
                        "name": "Jasmin Wachter"
                    },
                    {
                        "name": "Michael Radloff"
                    },
                    {
                        "name": "Maja Smolej"
                    },
                    {
                        "name": "Katharina Kinder-Kurlanda"
                    }
                ],
                "author_detail": {
                    "name": "Katharina Kinder-Kurlanda"
                },
                "author": "Katharina Kinder-Kurlanda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09151v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09151v2",
                "updated": "2025-03-17T13:01:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    13,
                    1,
                    59,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-12T08:26:15Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    8,
                    26,
                    15,
                    2,
                    71,
                    0
                ],
                "title": "Reangle-A-Video: 4D Video Generation as Video-to-Video Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reangle-A-Video: 4D Video Generation as Video-to-Video Translation"
                },
                "summary": "We introduce Reangle-A-Video, a unified framework for generating synchronized\nmulti-view videos from a single input video. Unlike mainstream approaches that\ntrain multi-view video diffusion models on large-scale 4D datasets, our method\nreframes the multi-view video generation task as video-to-videos translation,\nleveraging publicly available image and video diffusion priors. In essence,\nReangle-A-Video operates in two stages. (1) Multi-View Motion Learning: An\nimage-to-video diffusion transformer is synchronously fine-tuned in a\nself-supervised manner to distill view-invariant motion from a set of warped\nvideos. (2) Multi-View Consistent Image-to-Images Translation: The first frame\nof the input video is warped and inpainted into various camera perspectives\nunder an inference-time cross-view consistency guidance using DUSt3R,\ngenerating multi-view consistent starting images. Extensive experiments on\nstatic view transport and dynamic camera control show that Reangle-A-Video\nsurpasses existing methods, establishing a new solution for multi-view video\ngeneration. We will publicly release our code and data. Project page:\nhttps://hyeonho99.github.io/reangle-a-video/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Reangle-A-Video, a unified framework for generating synchronized\nmulti-view videos from a single input video. Unlike mainstream approaches that\ntrain multi-view video diffusion models on large-scale 4D datasets, our method\nreframes the multi-view video generation task as video-to-videos translation,\nleveraging publicly available image and video diffusion priors. In essence,\nReangle-A-Video operates in two stages. (1) Multi-View Motion Learning: An\nimage-to-video diffusion transformer is synchronously fine-tuned in a\nself-supervised manner to distill view-invariant motion from a set of warped\nvideos. (2) Multi-View Consistent Image-to-Images Translation: The first frame\nof the input video is warped and inpainted into various camera perspectives\nunder an inference-time cross-view consistency guidance using DUSt3R,\ngenerating multi-view consistent starting images. Extensive experiments on\nstatic view transport and dynamic camera control show that Reangle-A-Video\nsurpasses existing methods, establishing a new solution for multi-view video\ngeneration. We will publicly release our code and data. Project page:\nhttps://hyeonho99.github.io/reangle-a-video/"
                },
                "authors": [
                    {
                        "name": "Hyeonho Jeong"
                    },
                    {
                        "name": "Suhyeon Lee"
                    },
                    {
                        "name": "Jong Chul Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jong Chul Ye"
                },
                "author": "Jong Chul Ye",
                "arxiv_comment": "Project page: https://hyeonho99.github.io/reangle-a-video/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09151v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09151v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13123v1",
                "updated": "2025-03-17T12:48:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    48,
                    29,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T12:48:29Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    48,
                    29,
                    0,
                    76,
                    0
                ],
                "title": "MIXPINN: Mixed-Material Simulations by Physics-Informed Neural Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIXPINN: Mixed-Material Simulations by Physics-Informed Neural Network"
                },
                "summary": "Simulating the complex interactions between soft tissues and rigid anatomy is\ncritical for applications in surgical training, planning, and robotic-assisted\ninterventions. Traditional Finite Element Method (FEM)-based simulations, while\naccurate, are computationally expensive and impractical for real-time\nscenarios. Learning-based approaches have shown promise in accelerating\npredictions but have fallen short in modeling soft-rigid interactions\neffectively. We introduce MIXPINN, a physics-informed Graph Neural Network\n(GNN) framework for mixed-material simulations, explicitly capturing soft-rigid\ninteractions using graph-based augmentations. Our approach integrates Virtual\nNodes (VNs) and Virtual Edges (VEs) to enhance rigid body constraint\nsatisfaction while preserving computational efficiency. By leveraging a\ngraph-based representation of biomechanical structures, MIXPINN learns\nhigh-fidelity deformations from FEM-generated data and achieves real-time\ninference with sub-millimeter accuracy. We validate our method in a realistic\nclinical scenario, demonstrating superior performance compared to baseline GNN\nmodels and traditional FEM methods. Our results show that MIXPINN reduces\ncomputational cost by an order of magnitude while maintaining high physical\naccuracy, making it a viable solution for real-time surgical simulation and\nrobotic-assisted procedures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating the complex interactions between soft tissues and rigid anatomy is\ncritical for applications in surgical training, planning, and robotic-assisted\ninterventions. Traditional Finite Element Method (FEM)-based simulations, while\naccurate, are computationally expensive and impractical for real-time\nscenarios. Learning-based approaches have shown promise in accelerating\npredictions but have fallen short in modeling soft-rigid interactions\neffectively. We introduce MIXPINN, a physics-informed Graph Neural Network\n(GNN) framework for mixed-material simulations, explicitly capturing soft-rigid\ninteractions using graph-based augmentations. Our approach integrates Virtual\nNodes (VNs) and Virtual Edges (VEs) to enhance rigid body constraint\nsatisfaction while preserving computational efficiency. By leveraging a\ngraph-based representation of biomechanical structures, MIXPINN learns\nhigh-fidelity deformations from FEM-generated data and achieves real-time\ninference with sub-millimeter accuracy. We validate our method in a realistic\nclinical scenario, demonstrating superior performance compared to baseline GNN\nmodels and traditional FEM methods. Our results show that MIXPINN reduces\ncomputational cost by an order of magnitude while maintaining high physical\naccuracy, making it a viable solution for real-time surgical simulation and\nrobotic-assisted procedures."
                },
                "authors": [
                    {
                        "name": "Xintian Yuan"
                    },
                    {
                        "name": "Yunke Ao"
                    },
                    {
                        "name": "Boqi Chen"
                    },
                    {
                        "name": "Philipp Fuernstahl"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Fuernstahl"
                },
                "author": "Philipp Fuernstahl",
                "arxiv_comment": "This work has been submitted to the lEEE IROS 2025 for possible\n  publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19189v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19189v2",
                "updated": "2025-03-17T12:43:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    43,
                    52,
                    0,
                    76,
                    0
                ],
                "published": "2024-11-28T14:50:14Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    14,
                    50,
                    14,
                    3,
                    333,
                    0
                ],
                "title": "Video Depth without Video Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Depth without Video Models"
                },
                "summary": "Video depth estimation lifts monocular video clips to 3D by inferring dense\ndepth at every frame. Recent advances in single-image depth estimation, brought\nabout by the rise of large foundation models and the use of synthetic training\ndata, have fueled a renewed interest in video depth. However, naively applying\na single-image depth estimator to every frame of a video disregards temporal\ncontinuity, which not only leads to flickering but may also break when camera\nmotion causes sudden changes in depth range. An obvious and principled solution\nwould be to build on top of video foundation models, but these come with their\nown limitations; including expensive training and inference, imperfect 3D\nconsistency, and stitching routines for the fixed-length (short) outputs. We\ntake a step back and demonstrate how to turn a single-image latent diffusion\nmodel (LDM) into a state-of-the-art video depth estimator. Our model, which we\ncall RollingDepth, has two main ingredients: (i) a multi-frame depth estimator\nthat is derived from a single-image LDM and maps very short video snippets\n(typically frame triplets) to depth snippets. (ii) a robust, optimization-based\nregistration algorithm that optimally assembles depth snippets sampled at\nvarious different frame rates back into a consistent video. RollingDepth is\nable to efficiently handle long videos with hundreds of frames and delivers\nmore accurate depth videos than both dedicated video depth estimators and\nhigh-performing single-frame models. Project page: rollingdepth.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video depth estimation lifts monocular video clips to 3D by inferring dense\ndepth at every frame. Recent advances in single-image depth estimation, brought\nabout by the rise of large foundation models and the use of synthetic training\ndata, have fueled a renewed interest in video depth. However, naively applying\na single-image depth estimator to every frame of a video disregards temporal\ncontinuity, which not only leads to flickering but may also break when camera\nmotion causes sudden changes in depth range. An obvious and principled solution\nwould be to build on top of video foundation models, but these come with their\nown limitations; including expensive training and inference, imperfect 3D\nconsistency, and stitching routines for the fixed-length (short) outputs. We\ntake a step back and demonstrate how to turn a single-image latent diffusion\nmodel (LDM) into a state-of-the-art video depth estimator. Our model, which we\ncall RollingDepth, has two main ingredients: (i) a multi-frame depth estimator\nthat is derived from a single-image LDM and maps very short video snippets\n(typically frame triplets) to depth snippets. (ii) a robust, optimization-based\nregistration algorithm that optimally assembles depth snippets sampled at\nvarious different frame rates back into a consistent video. RollingDepth is\nable to efficiently handle long videos with hundreds of frames and delivers\nmore accurate depth videos than both dedicated video depth estimators and\nhigh-performing single-frame models. Project page: rollingdepth.github.io."
                },
                "authors": [
                    {
                        "name": "Bingxin Ke"
                    },
                    {
                        "name": "Dominik Narnhofer"
                    },
                    {
                        "name": "Shengyu Huang"
                    },
                    {
                        "name": "Lei Ke"
                    },
                    {
                        "name": "Torben Peters"
                    },
                    {
                        "name": "Katerina Fragkiadaki"
                    },
                    {
                        "name": "Anton Obukhov"
                    },
                    {
                        "name": "Konrad Schindler"
                    }
                ],
                "author_detail": {
                    "name": "Konrad Schindler"
                },
                "author": "Konrad Schindler",
                "arxiv_comment": "Project page: rollingdepth.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19189v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19189v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11367v2",
                "updated": "2025-03-17T12:39:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    39,
                    15,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-14T13:07:45Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    7,
                    45,
                    4,
                    73,
                    0
                ],
                "title": "Cornstarch: Distributed Multimodal Training Must Be Multimodality-Aware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cornstarch: Distributed Multimodal Training Must Be Multimodality-Aware"
                },
                "summary": "Multimodal large language models (MLLMs) extend the capabilities of large\nlanguage models (LLMs) by combining heterogeneous model architectures to handle\ndiverse modalities like images and audio. However, this inherent heterogeneity\nin MLLM model structure and data types makes makeshift extensions to existing\nLLM training frameworks unsuitable for efficient MLLM training.\n  In this paper, we present Cornstarch, the first general-purpose distributed\nMLLM training framework. Cornstarch facilitates modular MLLM construction,\nenables composable parallelization of constituent models, and introduces\nMLLM-specific optimizations to pipeline and context parallelism for efficient\ndistributed MLLM training. Our evaluation shows that Cornstarch outperforms\nstate-of-the-art solutions by up to $1.57\\times$ in terms of training\nthroughput.\n  Cornstarch is an open-source project available at\nhttps://github.com/cornstarch-org/Cornstarch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) extend the capabilities of large\nlanguage models (LLMs) by combining heterogeneous model architectures to handle\ndiverse modalities like images and audio. However, this inherent heterogeneity\nin MLLM model structure and data types makes makeshift extensions to existing\nLLM training frameworks unsuitable for efficient MLLM training.\n  In this paper, we present Cornstarch, the first general-purpose distributed\nMLLM training framework. Cornstarch facilitates modular MLLM construction,\nenables composable parallelization of constituent models, and introduces\nMLLM-specific optimizations to pipeline and context parallelism for efficient\ndistributed MLLM training. Our evaluation shows that Cornstarch outperforms\nstate-of-the-art solutions by up to $1.57\\times$ in terms of training\nthroughput.\n  Cornstarch is an open-source project available at\nhttps://github.com/cornstarch-org/Cornstarch."
                },
                "authors": [
                    {
                        "name": "Insu Jang"
                    },
                    {
                        "name": "Runyu Lu"
                    },
                    {
                        "name": "Nikhil Bansal"
                    },
                    {
                        "name": "Ang Chen"
                    },
                    {
                        "name": "Mosharaf Chowdhury"
                    }
                ],
                "author_detail": {
                    "name": "Mosharaf Chowdhury"
                },
                "author": "Mosharaf Chowdhury",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13116v1",
                "updated": "2025-03-17T12:38:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    38,
                    3,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T12:38:03Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    38,
                    3,
                    0,
                    76,
                    0
                ],
                "title": "VeriLeaky: Navigating IP Protection vs Utility in Fine-Tuning for\n  LLM-Driven Verilog Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeriLeaky: Navigating IP Protection vs Utility in Fine-Tuning for\n  LLM-Driven Verilog Coding"
                },
                "summary": "Large language models (LLMs) offer significant potential for coding, yet\nfine-tuning (FT) with curated data is essential for niche languages like\nVerilog. Using proprietary intellectual property (IP) for FT presents a serious\nrisk, as FT data can be leaked through LLM inference. This leads to a critical\ndilemma for design houses: seeking to build externally accessible LLMs offering\ncompetitive Verilog coding, how can they leverage in-house IP to enhance FT\nutility while ensuring IP protection?\n  For the first time in the literature, we study this dilemma. Using LLaMA\n3.1-8B, we conduct in-house FT on a baseline Verilog dataset (RTLCoder)\nsupplemented with our own in-house IP, which is validated through multiple\ntape-outs. To rigorously assess IP leakage, we quantify structural similarity\n(AST/Dolos) and functional equivalence (Synopsys Formality) between generated\ncodes and our in-house IP. We show that our IP can indeed be leaked, confirming\nthe threat. As defense, we evaluate logic locking of Verilog codes (ASSURE).\nThis offers some level of protection, yet reduces the IP's utility for FT and\ndegrades the LLM's performance. Our study shows the need for novel strategies\nthat are both effective and minimally disruptive to FT, an essential effort for\nenabling design houses to fully utilize their proprietary IP toward LLM-driven\nVerilog coding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) offer significant potential for coding, yet\nfine-tuning (FT) with curated data is essential for niche languages like\nVerilog. Using proprietary intellectual property (IP) for FT presents a serious\nrisk, as FT data can be leaked through LLM inference. This leads to a critical\ndilemma for design houses: seeking to build externally accessible LLMs offering\ncompetitive Verilog coding, how can they leverage in-house IP to enhance FT\nutility while ensuring IP protection?\n  For the first time in the literature, we study this dilemma. Using LLaMA\n3.1-8B, we conduct in-house FT on a baseline Verilog dataset (RTLCoder)\nsupplemented with our own in-house IP, which is validated through multiple\ntape-outs. To rigorously assess IP leakage, we quantify structural similarity\n(AST/Dolos) and functional equivalence (Synopsys Formality) between generated\ncodes and our in-house IP. We show that our IP can indeed be leaked, confirming\nthe threat. As defense, we evaluate logic locking of Verilog codes (ASSURE).\nThis offers some level of protection, yet reduces the IP's utility for FT and\ndegrades the LLM's performance. Our study shows the need for novel strategies\nthat are both effective and minimally disruptive to FT, an essential effort for\nenabling design houses to fully utilize their proprietary IP toward LLM-driven\nVerilog coding."
                },
                "authors": [
                    {
                        "name": "Zeng Wang"
                    },
                    {
                        "name": "Minghao Shao"
                    },
                    {
                        "name": "Mohammed Nabeel"
                    },
                    {
                        "name": "Prithwish Basu Roy"
                    },
                    {
                        "name": "Likhitha Mankali"
                    },
                    {
                        "name": "Jitendra Bhandari"
                    },
                    {
                        "name": "Ramesh Karri"
                    },
                    {
                        "name": "Ozgur Sinanoglu"
                    },
                    {
                        "name": "Muhammad Shafique"
                    },
                    {
                        "name": "Johann Knechtel"
                    }
                ],
                "author_detail": {
                    "name": "Johann Knechtel"
                },
                "author": "Johann Knechtel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13111v1",
                "updated": "2025-03-17T12:34:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    34,
                    22,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T12:34:22Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    34,
                    22,
                    0,
                    76,
                    0
                ],
                "title": "MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs"
                },
                "summary": "Multimodal large language models (MLLMs) excel at 2D visual understanding but\nremain limited in their ability to reason about 3D space. In this work, we\nleverage large-scale high-quality 3D scene data with open-set annotations to\nintroduce 1) a novel supervised fine-tuning dataset and 2) a new evaluation\nbenchmark, focused on indoor scenes. Our Cubify Anything VQA (CA-VQA) data\ncovers diverse spatial tasks including spatial relationship prediction, metric\nsize and distance estimation, and 3D grounding. We show that CA-VQA enables us\nto train MM-Spatial, a strong generalist MLLM that also achieves\nstate-of-the-art performance on 3D spatial understanding benchmarks, including\nour own. We show how incorporating metric depth and multi-view inputs (provided\nin CA-VQA) can further improve 3D understanding, and demonstrate that data\nalone allows our model to achieve depth perception capabilities comparable to\ndedicated monocular depth estimation models. We will publish our SFT dataset\nand benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) excel at 2D visual understanding but\nremain limited in their ability to reason about 3D space. In this work, we\nleverage large-scale high-quality 3D scene data with open-set annotations to\nintroduce 1) a novel supervised fine-tuning dataset and 2) a new evaluation\nbenchmark, focused on indoor scenes. Our Cubify Anything VQA (CA-VQA) data\ncovers diverse spatial tasks including spatial relationship prediction, metric\nsize and distance estimation, and 3D grounding. We show that CA-VQA enables us\nto train MM-Spatial, a strong generalist MLLM that also achieves\nstate-of-the-art performance on 3D spatial understanding benchmarks, including\nour own. We show how incorporating metric depth and multi-view inputs (provided\nin CA-VQA) can further improve 3D understanding, and demonstrate that data\nalone allows our model to achieve depth perception capabilities comparable to\ndedicated monocular depth estimation models. We will publish our SFT dataset\nand benchmark."
                },
                "authors": [
                    {
                        "name": "Erik Daxberger"
                    },
                    {
                        "name": "Nina Wenzel"
                    },
                    {
                        "name": "David Griffiths"
                    },
                    {
                        "name": "Haiming Gang"
                    },
                    {
                        "name": "Justin Lazarow"
                    },
                    {
                        "name": "Gefen Kohavi"
                    },
                    {
                        "name": "Kai Kang"
                    },
                    {
                        "name": "Marcin Eichner"
                    },
                    {
                        "name": "Yinfei Yang"
                    },
                    {
                        "name": "Afshin Dehghan"
                    },
                    {
                        "name": "Peter Grasch"
                    }
                ],
                "author_detail": {
                    "name": "Peter Grasch"
                },
                "author": "Peter Grasch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13108v1",
                "updated": "2025-03-17T12:31:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    31,
                    23,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T12:31:23Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    31,
                    23,
                    0,
                    76,
                    0
                ],
                "title": "Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways\n  to Faster Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways\n  to Faster Inference"
                },
                "summary": "Multimodal large language models (MLLMs) improve performance on\nvision-language tasks by integrating visual features from pre-trained vision\nencoders into large language models (LLMs). However, how MLLMs process and\nutilize visual information remains unclear. In this paper, a shift in the\ndominant flow of visual information is uncovered: (1) in shallow layers, strong\ninteractions are observed between image tokens and instruction tokens, where\nmost visual information is injected into instruction tokens to form cross-modal\nsemantic representations; (2) in deeper layers, image tokens primarily interact\nwith each other, aggregating the remaining visual information to optimize\nsemantic representations within visual modality. Based on these insights, we\npropose Hierarchical Modality-Aware Pruning (HiMAP), a plug-and-play inference\nacceleration method that dynamically prunes image tokens at specific layers,\nreducing computational costs by approximately 65% without sacrificing\nperformance. Our findings offer a new understanding of visual information\nprocessing in MLLMs and provide a state-of-the-art solution for efficient\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) improve performance on\nvision-language tasks by integrating visual features from pre-trained vision\nencoders into large language models (LLMs). However, how MLLMs process and\nutilize visual information remains unclear. In this paper, a shift in the\ndominant flow of visual information is uncovered: (1) in shallow layers, strong\ninteractions are observed between image tokens and instruction tokens, where\nmost visual information is injected into instruction tokens to form cross-modal\nsemantic representations; (2) in deeper layers, image tokens primarily interact\nwith each other, aggregating the remaining visual information to optimize\nsemantic representations within visual modality. Based on these insights, we\npropose Hierarchical Modality-Aware Pruning (HiMAP), a plug-and-play inference\nacceleration method that dynamically prunes image tokens at specific layers,\nreducing computational costs by approximately 65% without sacrificing\nperformance. Our findings offer a new understanding of visual information\nprocessing in MLLMs and provide a state-of-the-art solution for efficient\ninference."
                },
                "authors": [
                    {
                        "name": "Hao Yin"
                    },
                    {
                        "name": "Guangzong Si"
                    },
                    {
                        "name": "Zilei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zilei Wang"
                },
                "author": "Zilei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13107v1",
                "updated": "2025-03-17T12:30:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    30,
                    40,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T12:30:40Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    30,
                    40,
                    0,
                    76,
                    0
                ],
                "title": "ClearSight: Visual Signal Enhancement for Object Hallucination\n  Mitigation in Multimodal Large language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClearSight: Visual Signal Enhancement for Object Hallucination\n  Mitigation in Multimodal Large language Models"
                },
                "summary": "Contrastive decoding strategies are widely used to mitigate object\nhallucinations in multimodal large language models (MLLMs). By reducing\nover-reliance on language priors, these strategies ensure that generated\ncontent remains closely grounded in visual inputs, producing contextually\naccurate outputs. Since contrastive decoding requires no additional training or\nexternal tools, it offers both computational efficiency and versatility, making\nit highly attractive. However, these methods present two main limitations: (1)\nbluntly suppressing language priors can compromise coherence and accuracy of\ngenerated content, and (2) processing contrastive inputs adds computational\nload, significantly slowing inference speed. To address these challenges, we\npropose Visual Amplification Fusion (VAF), a plug-and-play technique that\nenhances attention to visual signals within the model's middle layers, where\nmodality fusion predominantly occurs. This approach enables more effective\ncapture of visual features, reducing the model's bias toward language modality.\nExperimental results demonstrate that VAF significantly reduces hallucinations\nacross various MLLMs without affecting inference speed, while maintaining\ncoherence and accuracy in generated outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive decoding strategies are widely used to mitigate object\nhallucinations in multimodal large language models (MLLMs). By reducing\nover-reliance on language priors, these strategies ensure that generated\ncontent remains closely grounded in visual inputs, producing contextually\naccurate outputs. Since contrastive decoding requires no additional training or\nexternal tools, it offers both computational efficiency and versatility, making\nit highly attractive. However, these methods present two main limitations: (1)\nbluntly suppressing language priors can compromise coherence and accuracy of\ngenerated content, and (2) processing contrastive inputs adds computational\nload, significantly slowing inference speed. To address these challenges, we\npropose Visual Amplification Fusion (VAF), a plug-and-play technique that\nenhances attention to visual signals within the model's middle layers, where\nmodality fusion predominantly occurs. This approach enables more effective\ncapture of visual features, reducing the model's bias toward language modality.\nExperimental results demonstrate that VAF significantly reduces hallucinations\nacross various MLLMs without affecting inference speed, while maintaining\ncoherence and accuracy in generated outputs."
                },
                "authors": [
                    {
                        "name": "Hao Yin"
                    },
                    {
                        "name": "Guangzong Si"
                    },
                    {
                        "name": "Zilei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zilei Wang"
                },
                "author": "Zilei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10880v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10880v2",
                "updated": "2025-03-17T12:29:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    29,
                    5,
                    0,
                    76,
                    0
                ],
                "published": "2024-10-09T15:36:42Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    36,
                    42,
                    2,
                    283,
                    0
                ],
                "title": "Fine-tuning can Help Detect Pretraining Data from Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning can Help Detect Pretraining Data from Large Language Models"
                },
                "summary": "In the era of large language models (LLMs), detecting pretraining data has\nbeen increasingly important due to concerns about fair evaluation and ethical\nrisks. Current methods differentiate members and non-members by designing\nscoring functions, like Perplexity and Min-k%. However, the diversity and\ncomplexity of training data magnifies the difficulty of distinguishing, leading\nto suboptimal performance in detecting pretraining data. In this paper, we\nfirst explore the benefits of unseen data, which can be easily collected after\nthe release of the LLM. We find that the perplexities of LLMs shift differently\nfor members and non-members, after fine-tuning with a small amount of\npreviously unseen data. In light of this, we introduce a novel and effective\nmethod termed Fine-tuned Score Deviation(FSD), which improves the performance\nof current scoring functions for pretraining data detection. In particular, we\npropose to measure the deviation distance of current scores after fine-tuning\non a small amount of unseen data within the same domain. In effect, using a few\nunseen data can largely decrease the scores of all non-members, leading to a\nlarger deviation distance than members. Extensive experiments demonstrate the\neffectiveness of our method, significantly improving the AUC score on common\nbenchmark datasets across various models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of large language models (LLMs), detecting pretraining data has\nbeen increasingly important due to concerns about fair evaluation and ethical\nrisks. Current methods differentiate members and non-members by designing\nscoring functions, like Perplexity and Min-k%. However, the diversity and\ncomplexity of training data magnifies the difficulty of distinguishing, leading\nto suboptimal performance in detecting pretraining data. In this paper, we\nfirst explore the benefits of unseen data, which can be easily collected after\nthe release of the LLM. We find that the perplexities of LLMs shift differently\nfor members and non-members, after fine-tuning with a small amount of\npreviously unseen data. In light of this, we introduce a novel and effective\nmethod termed Fine-tuned Score Deviation(FSD), which improves the performance\nof current scoring functions for pretraining data detection. In particular, we\npropose to measure the deviation distance of current scores after fine-tuning\non a small amount of unseen data within the same domain. In effect, using a few\nunseen data can largely decrease the scores of all non-members, leading to a\nlarger deviation distance than members. Extensive experiments demonstrate the\neffectiveness of our method, significantly improving the AUC score on common\nbenchmark datasets across various models."
                },
                "authors": [
                    {
                        "name": "Hengxiang Zhang"
                    },
                    {
                        "name": "Songxin Zhang"
                    },
                    {
                        "name": "Bingyi Jing"
                    },
                    {
                        "name": "Hongxin Wei"
                    }
                ],
                "author_detail": {
                    "name": "Hongxin Wei"
                },
                "author": "Hongxin Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10880v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10880v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13105v1",
                "updated": "2025-03-17T12:25:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    25,
                    42,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T12:25:42Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    25,
                    42,
                    0,
                    76,
                    0
                ],
                "title": "Managing Hybrid Solid-State Drives Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Managing Hybrid Solid-State Drives Using Large Language Models"
                },
                "summary": "Hybrid Solid-State Drives (SSDs), which integrate several types of flash\ncells (e.g., single-level cell (SLC) and multiple-level cell (MLC)) in a single\ndrive and enable them to convert between each other, are designed to deliver\nboth high performance and high storage capacity. However, compared to\ntraditional SSDs, hybrid SSDs also introduce a much larger design space,\nresulting in higher optimization complexity due to more design factors\ninvolved, including flash conversion timing and data migration between\ndifferent flash cells, etc. To address these challenges, large language models\n(LLMs) could be a promising technique, as they excel in handling complex,\nhigh-dimensional parameter space exploration by leveraging their advanced\ncapability to identify patterns and optimize solutions. Recent works have\nstarted exploring the use of LLMs to optimize computer systems. However, to the\nbest of our knowledge, no study has focused on optimizing SSDs with the\nassistance of LLMs.\n  In this work, we explore the potential of LLMs in understanding and\nefficiently managing hybrid SSD design space. Specifically, two important\nquestions are exploited and analyzed: 1) Can LLMs offer optimization potential\nfor Hybrid SSD management? 2) How to leverage LLMs for the performance and\nefficiency of hybrid SSD optimization? Based on the observations of\nexploration, we propose a comprehensive auto-tuning framework for hybrid SSDs,\nintegrating LLMs to recommend customized configurations using calibration\nprompts derived from hardware, system, and workload information. Experimental\nresults reveal a 62.35% improvement in throughput and a 57.99% decrease in\nwrite amplification compared to the default hybrid SSD configurations achieved\nwith the incorporation of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Solid-State Drives (SSDs), which integrate several types of flash\ncells (e.g., single-level cell (SLC) and multiple-level cell (MLC)) in a single\ndrive and enable them to convert between each other, are designed to deliver\nboth high performance and high storage capacity. However, compared to\ntraditional SSDs, hybrid SSDs also introduce a much larger design space,\nresulting in higher optimization complexity due to more design factors\ninvolved, including flash conversion timing and data migration between\ndifferent flash cells, etc. To address these challenges, large language models\n(LLMs) could be a promising technique, as they excel in handling complex,\nhigh-dimensional parameter space exploration by leveraging their advanced\ncapability to identify patterns and optimize solutions. Recent works have\nstarted exploring the use of LLMs to optimize computer systems. However, to the\nbest of our knowledge, no study has focused on optimizing SSDs with the\nassistance of LLMs.\n  In this work, we explore the potential of LLMs in understanding and\nefficiently managing hybrid SSD design space. Specifically, two important\nquestions are exploited and analyzed: 1) Can LLMs offer optimization potential\nfor Hybrid SSD management? 2) How to leverage LLMs for the performance and\nefficiency of hybrid SSD optimization? Based on the observations of\nexploration, we propose a comprehensive auto-tuning framework for hybrid SSDs,\nintegrating LLMs to recommend customized configurations using calibration\nprompts derived from hardware, system, and workload information. Experimental\nresults reveal a 62.35% improvement in throughput and a 57.99% decrease in\nwrite amplification compared to the default hybrid SSD configurations achieved\nwith the incorporation of LLMs."
                },
                "authors": [
                    {
                        "name": "Qian Wei"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Zehao Chen"
                    },
                    {
                        "name": "Zhaoyan Shen"
                    },
                    {
                        "name": "Dongxiao Yu"
                    },
                    {
                        "name": "Bingzhe Li"
                    }
                ],
                "author_detail": {
                    "name": "Bingzhe Li"
                },
                "author": "Bingzhe Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13102v1",
                "updated": "2025-03-17T12:15:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    15,
                    16,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T12:15:16Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    15,
                    16,
                    0,
                    76,
                    0
                ],
                "title": "REPA: Russian Error Types Annotation for Evaluating Text Generation and\n  Judgment Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REPA: Russian Error Types Annotation for Evaluating Text Generation and\n  Judgment Capabilities"
                },
                "summary": "Recent advances in large language models (LLMs) have introduced the novel\nparadigm of using LLMs as judges, where an LLM evaluates and scores the outputs\nof another LLM, which often correlates highly with human preferences. However,\nthe use of LLM-as-a-judge has been primarily studied in English. In this paper,\nwe evaluate this framework in Russian by introducing the Russian Error tyPes\nAnnotation dataset (REPA), a dataset of 1k user queries and 2k LLM-generated\nresponses. Human annotators labeled each response pair expressing their\npreferences across ten specific error types, as well as selecting an overall\npreference. We rank six generative LLMs across the error types using three\nrating systems based on human preferences. We also evaluate responses using\neight LLM judges in zero-shot and few-shot settings. We describe the results of\nanalyzing the judges and position and length biases. Our findings reveal a\nnotable gap between LLM judge performance in Russian and English. However,\nrankings based on human and LLM preferences show partial alignment, suggesting\nthat while current LLM judges struggle with fine-grained evaluation in Russian,\nthere is potential for improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have introduced the novel\nparadigm of using LLMs as judges, where an LLM evaluates and scores the outputs\nof another LLM, which often correlates highly with human preferences. However,\nthe use of LLM-as-a-judge has been primarily studied in English. In this paper,\nwe evaluate this framework in Russian by introducing the Russian Error tyPes\nAnnotation dataset (REPA), a dataset of 1k user queries and 2k LLM-generated\nresponses. Human annotators labeled each response pair expressing their\npreferences across ten specific error types, as well as selecting an overall\npreference. We rank six generative LLMs across the error types using three\nrating systems based on human preferences. We also evaluate responses using\neight LLM judges in zero-shot and few-shot settings. We describe the results of\nanalyzing the judges and position and length biases. Our findings reveal a\nnotable gap between LLM judge performance in Russian and English. However,\nrankings based on human and LLM preferences show partial alignment, suggesting\nthat while current LLM judges struggle with fine-grained evaluation in Russian,\nthere is potential for improvement."
                },
                "authors": [
                    {
                        "name": "Alexander Pugachev"
                    },
                    {
                        "name": "Alena Fenogenova"
                    },
                    {
                        "name": "Vladislav Mikhailov"
                    },
                    {
                        "name": "Ekaterina Artemova"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Artemova"
                },
                "author": "Ekaterina Artemova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13101v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13101v1",
                "updated": "2025-03-17T12:13:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    13,
                    37,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T12:13:37Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    13,
                    37,
                    0,
                    76,
                    0
                ],
                "title": "Who Wrote This? Identifying Machine vs Human-Generated Text in Hausa",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who Wrote This? Identifying Machine vs Human-Generated Text in Hausa"
                },
                "summary": "The advancement of large language models (LLMs) has allowed them to be\nproficient in various tasks, including content generation. However, their\nunregulated usage can lead to malicious activities such as plagiarism and\ngenerating and spreading fake news, especially for low-resource languages. Most\nexisting machine-generated text detectors are trained on high-resource\nlanguages like English, French, etc. In this study, we developed the first\nlarge-scale detector that can distinguish between human- and machine-generated\ncontent in Hausa. We scrapped seven Hausa-language media outlets for the\nhuman-generated text and the Gemini-2.0 flash model to automatically generate\nthe corresponding Hausa-language articles based on the human-generated article\nheadlines. We fine-tuned four pre-trained Afri-centric models (AfriTeVa,\nAfriBERTa, AfroXLMR, and AfroXLMR-76L) on the resulting dataset and assessed\ntheir performance using accuracy and F1-score metrics. AfroXLMR achieved the\nhighest performance with an accuracy of 99.23% and an F1 score of 99.21%,\ndemonstrating its effectiveness for Hausa text detection. Our dataset is made\npublicly available to enable further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of large language models (LLMs) has allowed them to be\nproficient in various tasks, including content generation. However, their\nunregulated usage can lead to malicious activities such as plagiarism and\ngenerating and spreading fake news, especially for low-resource languages. Most\nexisting machine-generated text detectors are trained on high-resource\nlanguages like English, French, etc. In this study, we developed the first\nlarge-scale detector that can distinguish between human- and machine-generated\ncontent in Hausa. We scrapped seven Hausa-language media outlets for the\nhuman-generated text and the Gemini-2.0 flash model to automatically generate\nthe corresponding Hausa-language articles based on the human-generated article\nheadlines. We fine-tuned four pre-trained Afri-centric models (AfriTeVa,\nAfriBERTa, AfroXLMR, and AfroXLMR-76L) on the resulting dataset and assessed\ntheir performance using accuracy and F1-score metrics. AfroXLMR achieved the\nhighest performance with an accuracy of 99.23% and an F1 score of 99.21%,\ndemonstrating its effectiveness for Hausa text detection. Our dataset is made\npublicly available to enable further research."
                },
                "authors": [
                    {
                        "name": "Babangida Sani"
                    },
                    {
                        "name": "Aakansha Soy"
                    },
                    {
                        "name": "Sukairaj Hafiz Imam"
                    },
                    {
                        "name": "Ahmad Mustapha"
                    },
                    {
                        "name": "Lukman Jibril Aliyu"
                    },
                    {
                        "name": "Idris Abdulmumin"
                    },
                    {
                        "name": "Ibrahim Said Ahmad"
                    },
                    {
                        "name": "Shamsuddeen Hassan Muhammad"
                    }
                ],
                "author_detail": {
                    "name": "Shamsuddeen Hassan Muhammad"
                },
                "author": "Shamsuddeen Hassan Muhammad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13101v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13101v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04032v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04032v3",
                "updated": "2025-03-17T12:05:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    5,
                    36,
                    0,
                    76,
                    0
                ],
                "published": "2024-11-06T16:31:28Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    16,
                    31,
                    28,
                    2,
                    311,
                    0
                ],
                "title": "Beemo: Benchmark of Expert-edited Machine-generated Outputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beemo: Benchmark of Expert-edited Machine-generated Outputs"
                },
                "summary": "The rapid proliferation of large language models (LLMs) has increased the\nvolume of machine-generated texts (MGTs) and blurred text authorship in various\ndomains. However, most existing MGT benchmarks include single-author texts\n(human-written and machine-generated). This conventional design fails to\ncapture more practical multi-author scenarios, where the user refines the LLM\nresponse for natural flow, coherence, and factual correctness. Our paper\nintroduces the Benchmark of Expert-edited Machine-generated Outputs (Beemo),\nwhich includes 6.5k texts written by humans, generated by ten\ninstruction-finetuned LLMs, and edited by experts for various use cases,\nranging from creative writing to summarization. Beemo additionally comprises\n13.1k machine-generated and LLM-edited texts, allowing for diverse MGT\ndetection evaluation across various edit types. We document Beemo's creation\nprotocol and present the results of benchmarking 33 configurations of MGT\ndetectors in different experimental setups. We find that expert-based editing\nevades MGT detection, while LLM-edited texts are unlikely to be recognized as\nhuman-written. Beemo and all materials are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of large language models (LLMs) has increased the\nvolume of machine-generated texts (MGTs) and blurred text authorship in various\ndomains. However, most existing MGT benchmarks include single-author texts\n(human-written and machine-generated). This conventional design fails to\ncapture more practical multi-author scenarios, where the user refines the LLM\nresponse for natural flow, coherence, and factual correctness. Our paper\nintroduces the Benchmark of Expert-edited Machine-generated Outputs (Beemo),\nwhich includes 6.5k texts written by humans, generated by ten\ninstruction-finetuned LLMs, and edited by experts for various use cases,\nranging from creative writing to summarization. Beemo additionally comprises\n13.1k machine-generated and LLM-edited texts, allowing for diverse MGT\ndetection evaluation across various edit types. We document Beemo's creation\nprotocol and present the results of benchmarking 33 configurations of MGT\ndetectors in different experimental setups. We find that expert-based editing\nevades MGT detection, while LLM-edited texts are unlikely to be recognized as\nhuman-written. Beemo and all materials are publicly available."
                },
                "authors": [
                    {
                        "name": "Ekaterina Artemova"
                    },
                    {
                        "name": "Jason Lucas"
                    },
                    {
                        "name": "Saranya Venkatraman"
                    },
                    {
                        "name": "Jooyoung Lee"
                    },
                    {
                        "name": "Sergei Tilga"
                    },
                    {
                        "name": "Adaku Uchendu"
                    },
                    {
                        "name": "Vladislav Mikhailov"
                    }
                ],
                "author_detail": {
                    "name": "Vladislav Mikhailov"
                },
                "author": "Vladislav Mikhailov",
                "arxiv_comment": "Accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04032v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04032v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13089v1",
                "updated": "2025-03-17T11:52:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    11,
                    52,
                    16,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T11:52:16Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    11,
                    52,
                    16,
                    0,
                    76,
                    0
                ],
                "title": "ClusComp: A Simple Paradigm for Model Compression and Efficient\n  Finetuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClusComp: A Simple Paradigm for Model Compression and Efficient\n  Finetuning"
                },
                "summary": "As large language models (LLMs) scale, model compression is crucial for edge\ndeployment and accessibility. Weight-only quantization reduces model size but\nsuffers from performance degradation at lower bit widths. Moreover, standard\nfinetuning is incompatible with quantized models, and alternative methods often\nfall short of full finetuning. In this paper, we propose ClusComp, a simple yet\neffective compression paradigm that clusters weight matrices into codebooks and\nfinetunes them block-by-block. ClusComp (1) achieves superior performance in\n2-4 bit quantization, (2) pushes compression to 1-bit while outperforming\nultra-low-bit methods with minimal finetuning, and (3) enables efficient\nfinetuning, even surpassing existing quantization-based approaches and rivaling\nfull FP16 finetuning. Notably, ClusComp supports compression and finetuning of\n70B LLMs on a single A6000-48GB GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) scale, model compression is crucial for edge\ndeployment and accessibility. Weight-only quantization reduces model size but\nsuffers from performance degradation at lower bit widths. Moreover, standard\nfinetuning is incompatible with quantized models, and alternative methods often\nfall short of full finetuning. In this paper, we propose ClusComp, a simple yet\neffective compression paradigm that clusters weight matrices into codebooks and\nfinetunes them block-by-block. ClusComp (1) achieves superior performance in\n2-4 bit quantization, (2) pushes compression to 1-bit while outperforming\nultra-low-bit methods with minimal finetuning, and (3) enables efficient\nfinetuning, even surpassing existing quantization-based approaches and rivaling\nfull FP16 finetuning. Notably, ClusComp supports compression and finetuning of\n70B LLMs on a single A6000-48GB GPU."
                },
                "authors": [
                    {
                        "name": "Baohao Liao"
                    },
                    {
                        "name": "Christian Herold"
                    },
                    {
                        "name": "Seyyed Hadi Hashemi"
                    },
                    {
                        "name": "Stefan Vasilev"
                    },
                    {
                        "name": "Shahram Khadivi"
                    },
                    {
                        "name": "Christof Monz"
                    }
                ],
                "author_detail": {
                    "name": "Christof Monz"
                },
                "author": "Christof Monz",
                "arxiv_comment": "26 pages, 11 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13081v1",
                "updated": "2025-03-17T11:39:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    11,
                    39,
                    44,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T11:39:44Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    11,
                    39,
                    44,
                    0,
                    76,
                    0
                ],
                "title": "A Framework to Assess Multilingual Vulnerabilities of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework to Assess Multilingual Vulnerabilities of LLMs"
                },
                "summary": "Large Language Models (LLMs) are acquiring a wider range of capabilities,\nincluding understanding and responding in multiple languages. While they\nundergo safety training to prevent them from answering illegal questions,\nimbalances in training data and human evaluation resources can make these\nmodels more susceptible to attacks in low-resource languages (LRL). This paper\nproposes a framework to automatically assess the multilingual vulnerabilities\nof commonly used LLMs. Using our framework, we evaluated six LLMs across eight\nlanguages representing varying levels of resource availability. We validated\nthe assessments generated by our automated framework through human evaluation\nin two languages, demonstrating that the framework's results align with human\njudgments in most cases. Our findings reveal vulnerabilities in LRL; however,\nthese may pose minimal risk as they often stem from the model's poor\nperformance, resulting in incoherent responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are acquiring a wider range of capabilities,\nincluding understanding and responding in multiple languages. While they\nundergo safety training to prevent them from answering illegal questions,\nimbalances in training data and human evaluation resources can make these\nmodels more susceptible to attacks in low-resource languages (LRL). This paper\nproposes a framework to automatically assess the multilingual vulnerabilities\nof commonly used LLMs. Using our framework, we evaluated six LLMs across eight\nlanguages representing varying levels of resource availability. We validated\nthe assessments generated by our automated framework through human evaluation\nin two languages, demonstrating that the framework's results align with human\njudgments in most cases. Our findings reveal vulnerabilities in LRL; however,\nthese may pose minimal risk as they often stem from the model's poor\nperformance, resulting in incoherent responses."
                },
                "authors": [
                    {
                        "name": "Likai Tang"
                    },
                    {
                        "name": "Niruth Bogahawatta"
                    },
                    {
                        "name": "Yasod Ginige"
                    },
                    {
                        "name": "Jiarui Xu"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Surangika Ranathunga"
                    },
                    {
                        "name": "Suranga Seneviratne"
                    }
                ],
                "author_detail": {
                    "name": "Suranga Seneviratne"
                },
                "author": "Suranga Seneviratne",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13066v1",
                "updated": "2025-03-17T11:11:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    11,
                    11,
                    18,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T11:11:18Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    11,
                    11,
                    18,
                    0,
                    76,
                    0
                ],
                "title": "A robust score test in g-computation for covariate adjustment in\n  randomized clinical trials leveraging different variance estimators via\n  influence functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A robust score test in g-computation for covariate adjustment in\n  randomized clinical trials leveraging different variance estimators via\n  influence functions"
                },
                "summary": "G-computation has become a widely used robust method for estimating\nunconditional (marginal) treatment effects with covariate adjustment in the\nanalysis of randomized clinical trials. Statistical inference in this context\ntypically relies on the Wald test or Wald interval, which can be easily\nimplemented using a consistent variance estimator. However, existing literature\nsuggests that when sample sizes are small or when parameters of interest are\nnear boundary values, Wald-based methods may be less reliable due to type I\nerror rate inflation and insufficient interval coverage. In this article, we\npropose a robust score test for g-computation estimators in the context of\ntwo-sample treatment comparisons. The proposed test is asymptotically valid\nunder simple and stratified (biased-coin) randomization schemes, even when\nregression models are misspecified. These test statistics can be conveniently\ncomputed using existing variance estimators, and the corresponding confidence\nintervals have closed-form expressions, making them convenient to implement.\nThrough extensive simulations, we demonstrate the superior finite-sample\nperformance of the proposed method. Finally, we apply the proposed method to\nreanalyze a completed randomized clinical trial. The new analysis using our\nproposed score test achieves statistical significance, whilst reducing the\nissue of type I error inflation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "G-computation has become a widely used robust method for estimating\nunconditional (marginal) treatment effects with covariate adjustment in the\nanalysis of randomized clinical trials. Statistical inference in this context\ntypically relies on the Wald test or Wald interval, which can be easily\nimplemented using a consistent variance estimator. However, existing literature\nsuggests that when sample sizes are small or when parameters of interest are\nnear boundary values, Wald-based methods may be less reliable due to type I\nerror rate inflation and insufficient interval coverage. In this article, we\npropose a robust score test for g-computation estimators in the context of\ntwo-sample treatment comparisons. The proposed test is asymptotically valid\nunder simple and stratified (biased-coin) randomization schemes, even when\nregression models are misspecified. These test statistics can be conveniently\ncomputed using existing variance estimators, and the corresponding confidence\nintervals have closed-form expressions, making them convenient to implement.\nThrough extensive simulations, we demonstrate the superior finite-sample\nperformance of the proposed method. Finally, we apply the proposed method to\nreanalyze a completed randomized clinical trial. The new analysis using our\nproposed score test achieves statistical significance, whilst reducing the\nissue of type I error inflation."
                },
                "authors": [
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Haitao Chu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Satrajit Roychoudhury"
                    }
                ],
                "author_detail": {
                    "name": "Satrajit Roychoudhury"
                },
                "author": "Satrajit Roychoudhury",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07435v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07435v2",
                "updated": "2025-03-17T11:06:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    11,
                    6,
                    8,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-10T15:18:10Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    18,
                    10,
                    0,
                    69,
                    0
                ],
                "title": "Open-Set Gait Recognition from Sparse mmWave Radar Point Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-Set Gait Recognition from Sparse mmWave Radar Point Clouds"
                },
                "summary": "The adoption of Millimeter-Wave (mmWave) radar devices for human sensing,\nparticularly gait recognition, has recently gathered significant attention due\nto their efficiency, resilience to environmental conditions, and\nprivacy-preserving nature. In this work, we tackle the challenging problem of\nOpen-set Gait Recognition (OSGR) from sparse mmWave radar point clouds. Unlike\nmost existing research, which assumes a closed-set scenario, our work considers\nthe more realistic open-set case, where unknown subjects might be present at\ninference time, and should be correctly recognized by the system. Point clouds\nare well-suited for edge computing applications with resource constraints, but\nare more significantly affected by noise and random fluctuations than other\nrepresentations, like the more common micro-Doppler signature. This is the\nfirst work addressing open-set gait recognition with sparse point cloud data.\nTo do so, we propose a novel neural network architecture that combines\nsupervised classification with unsupervised reconstruction of the point clouds,\ncreating a robust, rich, and highly regularized latent space of gait features.\nTo detect unknown subjects at inference time, we introduce a probabilistic\nnovelty detection algorithm that leverages the structured latent space and\noffers a tunable trade-off between inference speed and prediction accuracy.\nAlong with this paper, we release mmGait10, an original human gait dataset\nfeaturing over five hours of measurements from ten subjects, under varied\nwalking modalities. Extensive experimental results show that our solution\nattains F1-Score improvements by 24% over state-of-the-art methods, on average,\nand across multiple openness levels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adoption of Millimeter-Wave (mmWave) radar devices for human sensing,\nparticularly gait recognition, has recently gathered significant attention due\nto their efficiency, resilience to environmental conditions, and\nprivacy-preserving nature. In this work, we tackle the challenging problem of\nOpen-set Gait Recognition (OSGR) from sparse mmWave radar point clouds. Unlike\nmost existing research, which assumes a closed-set scenario, our work considers\nthe more realistic open-set case, where unknown subjects might be present at\ninference time, and should be correctly recognized by the system. Point clouds\nare well-suited for edge computing applications with resource constraints, but\nare more significantly affected by noise and random fluctuations than other\nrepresentations, like the more common micro-Doppler signature. This is the\nfirst work addressing open-set gait recognition with sparse point cloud data.\nTo do so, we propose a novel neural network architecture that combines\nsupervised classification with unsupervised reconstruction of the point clouds,\ncreating a robust, rich, and highly regularized latent space of gait features.\nTo detect unknown subjects at inference time, we introduce a probabilistic\nnovelty detection algorithm that leverages the structured latent space and\noffers a tunable trade-off between inference speed and prediction accuracy.\nAlong with this paper, we release mmGait10, an original human gait dataset\nfeaturing over five hours of measurements from ten subjects, under varied\nwalking modalities. Extensive experimental results show that our solution\nattains F1-Score improvements by 24% over state-of-the-art methods, on average,\nand across multiple openness levels."
                },
                "authors": [
                    {
                        "name": "Riccardo Mazzieri"
                    },
                    {
                        "name": "Jacopo Pegoraro"
                    },
                    {
                        "name": "Michele Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Michele Rossi"
                },
                "author": "Michele Rossi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07435v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07435v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13057v1",
                "updated": "2025-03-17T11:02:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    11,
                    2,
                    28,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T11:02:28Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    11,
                    2,
                    28,
                    0,
                    76,
                    0
                ],
                "title": "MaskSDM with Shapley values to improve flexibility, robustness, and\n  explainability in species distribution modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MaskSDM with Shapley values to improve flexibility, robustness, and\n  explainability in species distribution modeling"
                },
                "summary": "Species Distribution Models (SDMs) play a vital role in biodiversity\nresearch, conservation planning, and ecological niche modeling by predicting\nspecies distributions based on environmental conditions. The selection of\npredictors is crucial, strongly impacting both model accuracy and how well the\npredictions reflect ecological patterns. To ensure meaningful insights, input\nvariables must be carefully chosen to match the study objectives and the\necological requirements of the target species. However, existing SDMs,\nincluding both traditional and deep learning-based approaches, often lack key\ncapabilities for variable selection: (i) flexibility to choose relevant\npredictors at inference without retraining; (ii) robustness to handle missing\npredictor values without compromising accuracy; and (iii) explainability to\ninterpret and accurately quantify each predictor's contribution. To overcome\nthese limitations, we introduce MaskSDM, a novel deep learning-based SDM that\nenables flexible predictor selection by employing a masked training strategy.\nThis approach allows the model to make predictions with arbitrary subsets of\ninput variables while remaining robust to missing data. It also provides a\nclearer understanding of how adding or removing a given predictor affects model\nperformance and predictions. Additionally, MaskSDM leverages Shapley values for\nprecise predictor contribution assessments, improving upon traditional\napproximations. We evaluate MaskSDM on the global sPlotOpen dataset, modeling\nthe distributions of 12,738 plant species. Our results show that MaskSDM\noutperforms imputation-based methods and approximates models trained on\nspecific subsets of variables. These findings underscore MaskSDM's potential to\nincrease the applicability and adoption of SDMs, laying the groundwork for\ndeveloping foundation models in SDMs that can be readily applied to diverse\necological applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Species Distribution Models (SDMs) play a vital role in biodiversity\nresearch, conservation planning, and ecological niche modeling by predicting\nspecies distributions based on environmental conditions. The selection of\npredictors is crucial, strongly impacting both model accuracy and how well the\npredictions reflect ecological patterns. To ensure meaningful insights, input\nvariables must be carefully chosen to match the study objectives and the\necological requirements of the target species. However, existing SDMs,\nincluding both traditional and deep learning-based approaches, often lack key\ncapabilities for variable selection: (i) flexibility to choose relevant\npredictors at inference without retraining; (ii) robustness to handle missing\npredictor values without compromising accuracy; and (iii) explainability to\ninterpret and accurately quantify each predictor's contribution. To overcome\nthese limitations, we introduce MaskSDM, a novel deep learning-based SDM that\nenables flexible predictor selection by employing a masked training strategy.\nThis approach allows the model to make predictions with arbitrary subsets of\ninput variables while remaining robust to missing data. It also provides a\nclearer understanding of how adding or removing a given predictor affects model\nperformance and predictions. Additionally, MaskSDM leverages Shapley values for\nprecise predictor contribution assessments, improving upon traditional\napproximations. We evaluate MaskSDM on the global sPlotOpen dataset, modeling\nthe distributions of 12,738 plant species. Our results show that MaskSDM\noutperforms imputation-based methods and approximates models trained on\nspecific subsets of variables. These findings underscore MaskSDM's potential to\nincrease the applicability and adoption of SDMs, laying the groundwork for\ndeveloping foundation models in SDMs that can be readily applied to diverse\necological applications."
                },
                "authors": [
                    {
                        "name": "Robin Zbinden"
                    },
                    {
                        "name": "Nina van Tiel"
                    },
                    {
                        "name": "Gencer Sumbul"
                    },
                    {
                        "name": "Chiara Vanalli"
                    },
                    {
                        "name": "Benjamin Kellenberger"
                    },
                    {
                        "name": "Devis Tuia"
                    }
                ],
                "author_detail": {
                    "name": "Devis Tuia"
                },
                "author": "Devis Tuia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08010v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08010v2",
                "updated": "2025-03-17T10:47:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    47,
                    22,
                    0,
                    76,
                    0
                ],
                "published": "2024-08-15T08:19:08Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    8,
                    19,
                    8,
                    3,
                    228,
                    0
                ],
                "title": "Perturbing a quantum black hole",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perturbing a quantum black hole"
                },
                "summary": "We analyze the analytic structure of correlators in the field theory dual to\nthe quantum Ba\\~{n}ados-Teitelboim-Zanelli (qBTZ) black hole, a braneworld\nmodel incorporating exact backreaction from quantum conformal matter. We first\ncompute the quasi-normal mode (QNM) spectrum of operators with dimension\n$\\Delta$ and spin $s=0,\\pm 1/2$. The leading QNMs and their overtones display\nqualitatively different behavior depending on the branch of qBTZ solution,\nwhich corresponds to distinct CFT states: branch 1 is a conical singularity\ndressed with a horizon while branch 2 is a quantum-corrected BTZ black hole.\nConsequently, the relaxation of probe matter effectively differentiates the CFT\nstates and identifies the corresponding bulk descriptions. We then turn to\npole-skipping locations where Green's functions are not unique. At these\npoints, frequency is proportional to temperature, but momentum exhibits complex\ntemperature dependence due to quantum effects. Under the assumption that the\npole-skipping point closest to the origin reflects quantum chaos, we infer the\nlikely behavior of the quantum Lyapunov exponent and butterfly velocity in the\ndual theory. Finally, we examine pole collisions in complex momentum space,\nshowing that quantum corrections imprint a unique signature on the analytic\nstructure of the poles in retarded Green's functions, resulting in\nlevel-crossing phenomena that differ notably from the level-touching phenomena\nin the uncorrected BTZ geometry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We analyze the analytic structure of correlators in the field theory dual to\nthe quantum Ba\\~{n}ados-Teitelboim-Zanelli (qBTZ) black hole, a braneworld\nmodel incorporating exact backreaction from quantum conformal matter. We first\ncompute the quasi-normal mode (QNM) spectrum of operators with dimension\n$\\Delta$ and spin $s=0,\\pm 1/2$. The leading QNMs and their overtones display\nqualitatively different behavior depending on the branch of qBTZ solution,\nwhich corresponds to distinct CFT states: branch 1 is a conical singularity\ndressed with a horizon while branch 2 is a quantum-corrected BTZ black hole.\nConsequently, the relaxation of probe matter effectively differentiates the CFT\nstates and identifies the corresponding bulk descriptions. We then turn to\npole-skipping locations where Green's functions are not unique. At these\npoints, frequency is proportional to temperature, but momentum exhibits complex\ntemperature dependence due to quantum effects. Under the assumption that the\npole-skipping point closest to the origin reflects quantum chaos, we infer the\nlikely behavior of the quantum Lyapunov exponent and butterfly velocity in the\ndual theory. Finally, we examine pole collisions in complex momentum space,\nshowing that quantum corrections imprint a unique signature on the analytic\nstructure of the poles in retarded Green's functions, resulting in\nlevel-crossing phenomena that differ notably from the level-touching phenomena\nin the uncorrected BTZ geometry."
                },
                "authors": [
                    {
                        "name": "Casey Cartwright"
                    },
                    {
                        "name": "Umut Gürsoy"
                    },
                    {
                        "name": "Juan F. Pedraza"
                    },
                    {
                        "name": "Guim Planella Planas"
                    }
                ],
                "author_detail": {
                    "name": "Guim Planella Planas"
                },
                "author": "Guim Planella Planas",
                "arxiv_doi": "10.1007/JHEP03(2025)039",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/JHEP03(2025)039",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.08010v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08010v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "V2 - Published version. 28 images. 70 pgs. Restructured to focus main\n  document on results",
                "arxiv_primary_category": {
                    "term": "hep-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12972v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12972v2",
                "updated": "2025-03-17T10:45:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    45,
                    15,
                    0,
                    76,
                    0
                ],
                "published": "2024-10-16T19:07:37Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    19,
                    7,
                    37,
                    2,
                    290,
                    0
                ],
                "title": "Evaluating the Instruction-following Abilities of Language Models using\n  Knowledge Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Instruction-following Abilities of Language Models using\n  Knowledge Tasks"
                },
                "summary": "LLM evaluation benchmarks have traditionally separated the testing of\nknowledge/reasoning capabilities from instruction following. In this work, we\nstudy the interaction between knowledge and instruction following, and observe\nthat LLMs struggle to follow simple answer modifying instructions, and are also\ndistracted by instructions that should have no bearing on the original\nknowledge task answer. We leverage existing multiple-choice answer based\nknowledge benchmarks and apply a set of simple instructions which include\nmanipulating text (eg.: change case), numeric quantities (eg.: increase value,\nchange formatting), operate on lists (eg.: sort answer candidates) and\ndistractor instructions (eg.: change case of numeric answers).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM evaluation benchmarks have traditionally separated the testing of\nknowledge/reasoning capabilities from instruction following. In this work, we\nstudy the interaction between knowledge and instruction following, and observe\nthat LLMs struggle to follow simple answer modifying instructions, and are also\ndistracted by instructions that should have no bearing on the original\nknowledge task answer. We leverage existing multiple-choice answer based\nknowledge benchmarks and apply a set of simple instructions which include\nmanipulating text (eg.: change case), numeric quantities (eg.: increase value,\nchange formatting), operate on lists (eg.: sort answer candidates) and\ndistractor instructions (eg.: change case of numeric answers)."
                },
                "authors": [
                    {
                        "name": "Rudra Murthy"
                    },
                    {
                        "name": "Praveen Venkateswaran"
                    },
                    {
                        "name": "Prince Kumar"
                    },
                    {
                        "name": "Danish Contractor"
                    }
                ],
                "author_detail": {
                    "name": "Danish Contractor"
                },
                "author": "Danish Contractor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12972v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12972v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12782v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12782v2",
                "updated": "2025-03-17T10:43:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    43,
                    54,
                    0,
                    76,
                    0
                ],
                "published": "2024-10-16T17:56:49Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    56,
                    49,
                    2,
                    290,
                    0
                ],
                "title": "In-Context Learning Enables Robot Action Prediction in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning Enables Robot Action Prediction in LLMs"
                },
                "summary": "Recently, Large Language Models (LLMs) have achieved remarkable success using\nin-context learning (ICL) in the language domain. However, leveraging the ICL\ncapabilities within LLMs to directly predict robot actions remains largely\nunexplored. In this paper, we introduce RoboPrompt, a framework that enables\noff-the-shelf text-only LLMs to directly predict robot actions through ICL\nwithout training. Our approach first heuristically identifies keyframes that\ncapture important moments from an episode. Next, we extract end-effector\nactions from these keyframes as well as the estimated initial object poses, and\nboth are converted into textual descriptions. Finally, we construct a\nstructured template to form ICL demonstrations from these textual descriptions\nand a task instruction. This enables an LLM to directly predict robot actions\nat test time. Through extensive experiments and analysis, RoboPrompt shows\nstronger performance over zero-shot and ICL baselines in simulated and\nreal-world settings. Our project page is available at\nhttps://davidyyd.github.io/roboprompt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) have achieved remarkable success using\nin-context learning (ICL) in the language domain. However, leveraging the ICL\ncapabilities within LLMs to directly predict robot actions remains largely\nunexplored. In this paper, we introduce RoboPrompt, a framework that enables\noff-the-shelf text-only LLMs to directly predict robot actions through ICL\nwithout training. Our approach first heuristically identifies keyframes that\ncapture important moments from an episode. Next, we extract end-effector\nactions from these keyframes as well as the estimated initial object poses, and\nboth are converted into textual descriptions. Finally, we construct a\nstructured template to form ICL demonstrations from these textual descriptions\nand a task instruction. This enables an LLM to directly predict robot actions\nat test time. Through extensive experiments and analysis, RoboPrompt shows\nstronger performance over zero-shot and ICL baselines in simulated and\nreal-world settings. Our project page is available at\nhttps://davidyyd.github.io/roboprompt."
                },
                "authors": [
                    {
                        "name": "Yida Yin"
                    },
                    {
                        "name": "Zekai Wang"
                    },
                    {
                        "name": "Yuvan Sharma"
                    },
                    {
                        "name": "Dantong Niu"
                    },
                    {
                        "name": "Trevor Darrell"
                    },
                    {
                        "name": "Roei Herzig"
                    }
                ],
                "author_detail": {
                    "name": "Roei Herzig"
                },
                "author": "Roei Herzig",
                "arxiv_comment": "Published in ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12782v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12782v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13038v1",
                "updated": "2025-03-17T10:42:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    42,
                    34,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T10:42:34Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    42,
                    34,
                    0,
                    76,
                    0
                ],
                "title": "Overview of the NTCIR-18 Automatic Evaluation of LLMs (AEOLLM) Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Overview of the NTCIR-18 Automatic Evaluation of LLMs (AEOLLM) Task"
                },
                "summary": "In this paper, we provide an overview of the NTCIR-18 Automatic Evaluation of\nLLMs (AEOLLM) task. As large language models (LLMs) grow popular in both\nacademia and industry, how to effectively evaluate the capacity of LLMs becomes\nan increasingly critical but still challenging issue. Existing methods can be\ndivided into two types: manual evaluation, which is expensive, and automatic\nevaluation, which faces many limitations including task format (the majority\nbelong to multiple-choice questions) and evaluation criteria (occupied by\nreference-based metrics). To advance the innovation of automatic evaluation, we\npropose the AEOLLM task which focuses on generative tasks and encourages\nreference-free methods. Besides, we set up diverse subtasks such as dialogue\ngeneration, text expansion, summary generation and non-factoid question\nanswering to comprehensively test different methods. This year, we received 48\nruns from 4 teams in total. This paper will describe the background of the\ntask, the data set, the evaluation measures and the evaluation results,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we provide an overview of the NTCIR-18 Automatic Evaluation of\nLLMs (AEOLLM) task. As large language models (LLMs) grow popular in both\nacademia and industry, how to effectively evaluate the capacity of LLMs becomes\nan increasingly critical but still challenging issue. Existing methods can be\ndivided into two types: manual evaluation, which is expensive, and automatic\nevaluation, which faces many limitations including task format (the majority\nbelong to multiple-choice questions) and evaluation criteria (occupied by\nreference-based metrics). To advance the innovation of automatic evaluation, we\npropose the AEOLLM task which focuses on generative tasks and encourages\nreference-free methods. Besides, we set up diverse subtasks such as dialogue\ngeneration, text expansion, summary generation and non-factoid question\nanswering to comprehensively test different methods. This year, we received 48\nruns from 4 teams in total. This paper will describe the background of the\ntask, the data set, the evaluation measures and the evaluation results,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Haitao Li"
                    },
                    {
                        "name": "Zhumin Chu"
                    },
                    {
                        "name": "Yiqun Liu"
                    },
                    {
                        "name": "Qingyao Ai"
                    }
                ],
                "author_detail": {
                    "name": "Qingyao Ai"
                },
                "author": "Qingyao Ai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13037v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13037v1",
                "updated": "2025-03-17T10:41:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    41,
                    31,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T10:41:31Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    41,
                    31,
                    0,
                    76,
                    0
                ],
                "title": "H-AddiVortes: Heteroscedastic (Bayesian) Additive Voronoi Tessellations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "H-AddiVortes: Heteroscedastic (Bayesian) Additive Voronoi Tessellations"
                },
                "summary": "This paper introduces the Heteroscedastic AddiVortes model, a Bayesian\nnon-parametric regression framework that simultaneously models the conditional\nmean and variance of a response variable using adaptive Voronoi tessellations.\nBy employing a sum-of-tessellations approach for the mean and a\nproduct-of-tessellations approach for the variance, the model provides a\nflexible and interpretable means to capture complex, predictor-dependent\nrelationships and heteroscedastic patterns in data. This dual-layer\nrepresentation enables precise inference, even in high-dimensional settings,\nwhile maintaining computational feasibility through efficient Markov Chain\nMonte Carlo (MCMC) sampling and conjugate prior structures. We illustrate the\nmodel's capability through both simulated and real-world datasets,\ndemonstrating its ability to capture nuanced variance structures, provide\nreliable predictive uncertainty quantification, and highlight key predictors\ninfluencing both the mean response and its variability. Empirical results show\nthat the Heteroscedastic AddiVortes model offers a substantial improvement in\ncapturing distributional properties compared to both homoscedastic and\nheteroscedastic alternatives, making it a robust tool for complex regression\nproblems in various applied settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces the Heteroscedastic AddiVortes model, a Bayesian\nnon-parametric regression framework that simultaneously models the conditional\nmean and variance of a response variable using adaptive Voronoi tessellations.\nBy employing a sum-of-tessellations approach for the mean and a\nproduct-of-tessellations approach for the variance, the model provides a\nflexible and interpretable means to capture complex, predictor-dependent\nrelationships and heteroscedastic patterns in data. This dual-layer\nrepresentation enables precise inference, even in high-dimensional settings,\nwhile maintaining computational feasibility through efficient Markov Chain\nMonte Carlo (MCMC) sampling and conjugate prior structures. We illustrate the\nmodel's capability through both simulated and real-world datasets,\ndemonstrating its ability to capture nuanced variance structures, provide\nreliable predictive uncertainty quantification, and highlight key predictors\ninfluencing both the mean response and its variability. Empirical results show\nthat the Heteroscedastic AddiVortes model offers a substantial improvement in\ncapturing distributional properties compared to both homoscedastic and\nheteroscedastic alternatives, making it a robust tool for complex regression\nproblems in various applied settings."
                },
                "authors": [
                    {
                        "name": "Adam J. Stone"
                    },
                    {
                        "name": "John Paul Gosling"
                    }
                ],
                "author_detail": {
                    "name": "John Paul Gosling"
                },
                "author": "John Paul Gosling",
                "arxiv_comment": "27 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13037v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13037v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09483v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09483v2",
                "updated": "2025-03-17T10:38:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    38,
                    39,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-12T15:38:11Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    15,
                    38,
                    11,
                    2,
                    71,
                    0
                ],
                "title": "Learning Spatially Adaptive $\\ell_1$-Norms Weights for Convolutional\n  Synthesis Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Spatially Adaptive $\\ell_1$-Norms Weights for Convolutional\n  Synthesis Regularization"
                },
                "summary": "We propose an unrolled algorithm approach for learning spatially adaptive\nparameter maps in the framework of convolutional synthesis-based $\\ell_1$\nregularization. More precisely, we consider a family of pre-trained\nconvolutional filters and estimate deeply parametrized spatially varying\nparameters applied to the sparse feature maps by means of unrolling a FISTA\nalgorithm to solve the underlying sparse estimation problem. The proposed\napproach is evaluated for image reconstruction of low-field MRI and compared to\nspatially adaptive and non-adaptive analysis-type procedures relying on Total\nVariation regularization and to a well-established model-based deep learning\napproach. We show that the proposed approach produces visually and\nquantitatively comparable results with the latter approaches and at the same\ntime remains highly interpretable. In particular, the inferred parameter maps\nquantify\n  the local contribution of each filter in the reconstruction, which provides\nvaluable insight into the algorithm mechanism and could potentially be used to\ndiscard unsuited filters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an unrolled algorithm approach for learning spatially adaptive\nparameter maps in the framework of convolutional synthesis-based $\\ell_1$\nregularization. More precisely, we consider a family of pre-trained\nconvolutional filters and estimate deeply parametrized spatially varying\nparameters applied to the sparse feature maps by means of unrolling a FISTA\nalgorithm to solve the underlying sparse estimation problem. The proposed\napproach is evaluated for image reconstruction of low-field MRI and compared to\nspatially adaptive and non-adaptive analysis-type procedures relying on Total\nVariation regularization and to a well-established model-based deep learning\napproach. We show that the proposed approach produces visually and\nquantitatively comparable results with the latter approaches and at the same\ntime remains highly interpretable. In particular, the inferred parameter maps\nquantify\n  the local contribution of each filter in the reconstruction, which provides\nvaluable insight into the algorithm mechanism and could potentially be used to\ndiscard unsuited filters."
                },
                "authors": [
                    {
                        "name": "Andreas Kofler"
                    },
                    {
                        "name": "Luca Calatroni"
                    },
                    {
                        "name": "Christoph Kolbitsch"
                    },
                    {
                        "name": "Kostas Papafitsoros"
                    }
                ],
                "author_detail": {
                    "name": "Kostas Papafitsoros"
                },
                "author": "Kostas Papafitsoros",
                "arxiv_comment": "To be submitted to the EUSIPCO 2025 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09483v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09483v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04872v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04872v2",
                "updated": "2025-03-17T10:36:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    36,
                    30,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-06T16:25:53Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    25,
                    53,
                    3,
                    65,
                    0
                ],
                "title": "TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation"
                },
                "summary": "The challenge of reducing the size of Large Language Models (LLMs) while\nmaintaining their performance has gained significant attention. However,\nexisting methods, such as model distillation and transfer learning, often fail\nto achieve high accuracy. To address this limitation, we introduce the\nBranch-Merge distillation approach, which enhances model compression through\ntwo phases: (1) the Branch Phase, where knowledge from a large teacher model is\n\\textit{selectively distilled} into specialized student models via\ndomain-specific supervised fine-tuning (SFT); And (2) the Merge Phase, where\nthese student models are merged to enable cross-domain knowledge transfer and\nimprove generalization. We validate our distillation approach using DeepSeek-R1\nas the teacher and DeepSeek-R1-Distill-Qwen-32B as the student. The resulting\nmerged model, TinyR1-32B-Preview, outperforms its counterpart\nDeepSeek-R1-Distill-Qwen-32B across multiple benchmarks, including Mathematics\n(+5.5 points), Coding (+4.4 points) and Science (+2.9 points), while achieving\nnear-equal performance to DeepSeek-R1 on AIME 2024. The Branch-Merge\ndistillation approach provides a scalable solution for creating smaller,\nhigh-performing LLMs with reduced computational cost and time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The challenge of reducing the size of Large Language Models (LLMs) while\nmaintaining their performance has gained significant attention. However,\nexisting methods, such as model distillation and transfer learning, often fail\nto achieve high accuracy. To address this limitation, we introduce the\nBranch-Merge distillation approach, which enhances model compression through\ntwo phases: (1) the Branch Phase, where knowledge from a large teacher model is\n\\textit{selectively distilled} into specialized student models via\ndomain-specific supervised fine-tuning (SFT); And (2) the Merge Phase, where\nthese student models are merged to enable cross-domain knowledge transfer and\nimprove generalization. We validate our distillation approach using DeepSeek-R1\nas the teacher and DeepSeek-R1-Distill-Qwen-32B as the student. The resulting\nmerged model, TinyR1-32B-Preview, outperforms its counterpart\nDeepSeek-R1-Distill-Qwen-32B across multiple benchmarks, including Mathematics\n(+5.5 points), Coding (+4.4 points) and Science (+2.9 points), while achieving\nnear-equal performance to DeepSeek-R1 on AIME 2024. The Branch-Merge\ndistillation approach provides a scalable solution for creating smaller,\nhigh-performing LLMs with reduced computational cost and time."
                },
                "authors": [
                    {
                        "name": "Lin Sun"
                    },
                    {
                        "name": "Guangxiang Zhao"
                    },
                    {
                        "name": "Xiaoqi Jian"
                    },
                    {
                        "name": "Yuhan Wu"
                    },
                    {
                        "name": "Weihong Lin"
                    },
                    {
                        "name": "Yongfu Zhu"
                    },
                    {
                        "name": "Change Jia"
                    },
                    {
                        "name": "Linglin Zhang"
                    },
                    {
                        "name": "Jinzhu Wu"
                    },
                    {
                        "name": "Junfeng Ran"
                    },
                    {
                        "name": "Sai-er Hu"
                    },
                    {
                        "name": "Zihan Jiang"
                    },
                    {
                        "name": "Junting Zhou"
                    },
                    {
                        "name": "Wenrui Liu"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Tong Yang"
                    },
                    {
                        "name": "Xiangzheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangzheng Zhang"
                },
                "author": "Xiangzheng Zhang",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04872v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04872v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.05787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.05787v2",
                "updated": "2025-03-17T10:35:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    35,
                    11,
                    0,
                    76,
                    0
                ],
                "published": "2024-01-11T09:49:15Z",
                "published_parsed": [
                    2024,
                    1,
                    11,
                    9,
                    49,
                    15,
                    3,
                    11,
                    0
                ],
                "title": "Chain of Evidences and Evidence to Generate: Prompting for Context\n  Grounded and Retrieval Augmented Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Evidences and Evidence to Generate: Prompting for Context\n  Grounded and Retrieval Augmented Reasoning"
                },
                "summary": "While chain-of-thoughts (CoT) prompting has revolutionized how LLMs perform\nreasoning tasks, its current methods and variations (e.g, Self-consistency,\nReACT, Reflexion, Tree-of-Thoughts (ToT), Cumulative Reasoning (CR) etc.,)\nsuffer from limitations like limited context grounding,\nhallucination/inconsistent output generation, and iterative sluggishness. To\novercome these challenges, we introduce a novel mono/dual-step zero-shot\nprompting framework built upon two unique strategies Chain of Evidences (CoE)}\nand Evidence to Generate (E2G). Instead of unverified reasoning claims, our\ninnovative approaches leverage the power of \"evidence for decision making\" by\nfirst focusing exclusively on the thought sequences explicitly mentioned in the\ncontext which then serve as extracted evidence, guiding the LLM's output\ngeneration process with greater precision and efficiency. This simple yet\npotent approach unlocks the full potential of chain-of-thoughts prompting,\nfacilitating faster, more reliable, and contextually aware reasoning in LLMs.\nOur framework consistently achieves remarkable results across various\nknowledge-intensive reasoning and generation tasks, surpassing baseline\napproaches with state-of-the-art LLMs. For instance, (i) on the LogiQA\nbenchmark using GPT-4, CoE achieves a new state-of-the-art accuracy of 53.8%,\nsurpassing CoT by 18%, ToT by 11%, and CR by 9%; (ii) CoE with PaLM-2\noutperforms the variable-shot performance of Gemini Ultra by 0.9 F1 points,\nachieving an F1 score of 83.3 on DROP. We release our prompts and outputs on\nthese benchmarks as a new instruction tuning dataset for future research at\nhttps://huggingface.co/datasets/kagnlp/Chain-of-Evidences/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While chain-of-thoughts (CoT) prompting has revolutionized how LLMs perform\nreasoning tasks, its current methods and variations (e.g, Self-consistency,\nReACT, Reflexion, Tree-of-Thoughts (ToT), Cumulative Reasoning (CR) etc.,)\nsuffer from limitations like limited context grounding,\nhallucination/inconsistent output generation, and iterative sluggishness. To\novercome these challenges, we introduce a novel mono/dual-step zero-shot\nprompting framework built upon two unique strategies Chain of Evidences (CoE)}\nand Evidence to Generate (E2G). Instead of unverified reasoning claims, our\ninnovative approaches leverage the power of \"evidence for decision making\" by\nfirst focusing exclusively on the thought sequences explicitly mentioned in the\ncontext which then serve as extracted evidence, guiding the LLM's output\ngeneration process with greater precision and efficiency. This simple yet\npotent approach unlocks the full potential of chain-of-thoughts prompting,\nfacilitating faster, more reliable, and contextually aware reasoning in LLMs.\nOur framework consistently achieves remarkable results across various\nknowledge-intensive reasoning and generation tasks, surpassing baseline\napproaches with state-of-the-art LLMs. For instance, (i) on the LogiQA\nbenchmark using GPT-4, CoE achieves a new state-of-the-art accuracy of 53.8%,\nsurpassing CoT by 18%, ToT by 11%, and CR by 9%; (ii) CoE with PaLM-2\noutperforms the variable-shot performance of Gemini Ultra by 0.9 F1 points,\nachieving an F1 score of 83.3 on DROP. We release our prompts and outputs on\nthese benchmarks as a new instruction tuning dataset for future research at\nhttps://huggingface.co/datasets/kagnlp/Chain-of-Evidences/."
                },
                "authors": [
                    {
                        "name": "Md Rizwan Parvez"
                    }
                ],
                "author_detail": {
                    "name": "Md Rizwan Parvez"
                },
                "author": "Md Rizwan Parvez",
                "arxiv_comment": "Accepted at NAACL KnowledgeNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.05787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.05787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13026v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13026v1",
                "updated": "2025-03-17T10:29:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    29,
                    8,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T10:29:08Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    29,
                    8,
                    0,
                    76,
                    0
                ],
                "title": "HiMTok: Learning Hierarchical Mask Tokens for Image Segmentation with\n  Large Multimodal Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiMTok: Learning Hierarchical Mask Tokens for Image Segmentation with\n  Large Multimodal Model"
                },
                "summary": "The remarkable performance of large multimodal models (LMMs) has attracted\nsignificant interest from the image segmentation community. To align with the\nnext-token-prediction paradigm, current LMM-driven segmentation methods either\nuse object boundary points to represent masks or introduce special segmentation\ntokens, whose hidden states are decoded by a segmentation model requiring the\noriginal image as input. However, these approaches often suffer from inadequate\nmask representation and complex architectures, limiting the potential of LMMs.\nIn this work, we propose the Hierarchical Mask Tokenizer (HiMTok), which\nrepresents segmentation masks with up to 32 tokens and eliminates the need for\nthe original image during mask de-tokenization. HiMTok allows for compact and\ncoarse-to-fine mask representations, aligning well with the LLM\nnext-token-prediction paradigm and facilitating the direct acquisition of\nsegmentation capabilities. We develop a 3-stage training recipe for progressive\nlearning of segmentation and visual capabilities, featuring a hierarchical mask\nloss for effective coarse-to-fine learning. Additionally, we enable\nbidirectional information flow, allowing conversion between bounding boxes and\nmask tokens to fully leverage multi-task training potential. Extensive\nexperiments demonstrate that our method achieves state-of-the-art performance\nacross various segmentation tasks,while also enhancing visual grounding and\nmaintaining overall visual understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable performance of large multimodal models (LMMs) has attracted\nsignificant interest from the image segmentation community. To align with the\nnext-token-prediction paradigm, current LMM-driven segmentation methods either\nuse object boundary points to represent masks or introduce special segmentation\ntokens, whose hidden states are decoded by a segmentation model requiring the\noriginal image as input. However, these approaches often suffer from inadequate\nmask representation and complex architectures, limiting the potential of LMMs.\nIn this work, we propose the Hierarchical Mask Tokenizer (HiMTok), which\nrepresents segmentation masks with up to 32 tokens and eliminates the need for\nthe original image during mask de-tokenization. HiMTok allows for compact and\ncoarse-to-fine mask representations, aligning well with the LLM\nnext-token-prediction paradigm and facilitating the direct acquisition of\nsegmentation capabilities. We develop a 3-stage training recipe for progressive\nlearning of segmentation and visual capabilities, featuring a hierarchical mask\nloss for effective coarse-to-fine learning. Additionally, we enable\nbidirectional information flow, allowing conversion between bounding boxes and\nmask tokens to fully leverage multi-task training potential. Extensive\nexperiments demonstrate that our method achieves state-of-the-art performance\nacross various segmentation tasks,while also enhancing visual grounding and\nmaintaining overall visual understanding."
                },
                "authors": [
                    {
                        "name": "Tao Wang"
                    },
                    {
                        "name": "Changxu Cheng"
                    },
                    {
                        "name": "Lingfeng Wang"
                    },
                    {
                        "name": "Senda Chen"
                    },
                    {
                        "name": "Wuyue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wuyue Zhao"
                },
                "author": "Wuyue Zhao",
                "arxiv_comment": "technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13026v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13026v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13021v1",
                "updated": "2025-03-17T10:24:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    24,
                    27,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T10:24:27Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    24,
                    27,
                    0,
                    76,
                    0
                ],
                "title": "Dynamic Relation Inference via Verb Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Relation Inference via Verb Embeddings"
                },
                "summary": "CLIP has demonstrated exceptional image-text matching capabilities due to its\ntraining on contrastive learning tasks. Past research has suggested that\nwhereas CLIP effectively matches text to images when the matching can be\nachieved just by matching the text with the objects in the image, CLIP\nstruggles when the matching depends on representing the relationship among the\nobjects in the images (i.e., inferring relations). Previous attempts to address\nthis limitation by training CLIP on relation detection datasets with only\nlinguistic supervision have met with limited success. In this paper, we offer\ninsights and practical methods to advance the field of relation inference from\nimages. This paper approaches the task of creating a model that effectively\ndetects relations among the objects in images by producing text and image\nembeddings that capture relationships through linguistic supervision. To this\nend, we propose Dynamic Relation Inference via Verb Embeddings (DRIVE), which\naugments the COCO dataset, fine-tunes CLIP with hard negatives\nsubject-relation-object triples and corresponding images, and introduces a\nnovel loss function to improve relation detection. Evaluated on multiple\nCLIP-based models, our method significantly improves zero-shot relation\ninference accuracy in both frozen and fine-tuned settings, significantly\noutperforming CLIP and state-of-the-art models while generalizing well on\nunseen data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLIP has demonstrated exceptional image-text matching capabilities due to its\ntraining on contrastive learning tasks. Past research has suggested that\nwhereas CLIP effectively matches text to images when the matching can be\nachieved just by matching the text with the objects in the image, CLIP\nstruggles when the matching depends on representing the relationship among the\nobjects in the images (i.e., inferring relations). Previous attempts to address\nthis limitation by training CLIP on relation detection datasets with only\nlinguistic supervision have met with limited success. In this paper, we offer\ninsights and practical methods to advance the field of relation inference from\nimages. This paper approaches the task of creating a model that effectively\ndetects relations among the objects in images by producing text and image\nembeddings that capture relationships through linguistic supervision. To this\nend, we propose Dynamic Relation Inference via Verb Embeddings (DRIVE), which\naugments the COCO dataset, fine-tunes CLIP with hard negatives\nsubject-relation-object triples and corresponding images, and introduces a\nnovel loss function to improve relation detection. Evaluated on multiple\nCLIP-based models, our method significantly improves zero-shot relation\ninference accuracy in both frozen and fine-tuned settings, significantly\noutperforming CLIP and state-of-the-art models while generalizing well on\nunseen data."
                },
                "authors": [
                    {
                        "name": "Omri Suissa"
                    },
                    {
                        "name": "Muhiim Ali"
                    },
                    {
                        "name": "Ariana Azarbal"
                    },
                    {
                        "name": "Hui Shen"
                    },
                    {
                        "name": "Shekhar Pradhan"
                    }
                ],
                "author_detail": {
                    "name": "Shekhar Pradhan"
                },
                "author": "Shekhar Pradhan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13017v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13017v1",
                "updated": "2025-03-17T10:21:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    21,
                    43,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T10:21:43Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    21,
                    43,
                    0,
                    76,
                    0
                ],
                "title": "The nebular spectra of SN 2023ixf: A lower mass, partially stripped\n  progenitor may be the result of binary interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The nebular spectra of SN 2023ixf: A lower mass, partially stripped\n  progenitor may be the result of binary interaction"
                },
                "summary": "SN 2023ixf is one of the brightest Core Collapse Supernovae of the 21st\ncentury and offers a rare opportunity to investigate the late stage of a\nSupernova through nebular phase spectroscopy. We present four nebular phase\nspectra from day +291 to +413 after explosion. This is supplemented with high\ncadence early phase spectroscopic observations and photometry covering the\nfirst 500 days to investigate explosion parameters. The narrow and blue-shifted\nnebular Oxygen emission lines are used to infer an ejected Oxygen mass of\n$<0.65M_\\odot$, consistent with models of a relatively low mass\n($M_{ZAMS}<15M_\\odot$) progenitor. An energy of 0.3 to $1.4 \\times10^{51}$ erg\nand a light curve powered by an initial $^{56}$Ni mass of $0.049 \\pm 0.005\nM_\\odot$ appear consistent with a relatively standard Type II explosion, while\nan incomplete $\\gamma$-ray trapping (with timescale of $240\\pm4$ days) suggests\na lower ejecta mass. Assuming a typical explosion, the broad Hydrogen and\nCalcium profiles suggest a common origin within a lower mass, partially\nstripped envelope. Hydrogen emission broadens with time, indicating\ncontribution from an additional power source at an extended distance; while the\nemergence of high velocity ($\\sim$6,000 km s$^{-1}$) Hydrogen emission features\n(beginning around day +200) may be explained by Shock Interaction with a dense\nHydrogen-rich region located at $\\sim1.5 \\times 10^{16}$cm. Such envelope mass\nloss for a low mass progenitor may be explained through theoretical models of\nBinary interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SN 2023ixf is one of the brightest Core Collapse Supernovae of the 21st\ncentury and offers a rare opportunity to investigate the late stage of a\nSupernova through nebular phase spectroscopy. We present four nebular phase\nspectra from day +291 to +413 after explosion. This is supplemented with high\ncadence early phase spectroscopic observations and photometry covering the\nfirst 500 days to investigate explosion parameters. The narrow and blue-shifted\nnebular Oxygen emission lines are used to infer an ejected Oxygen mass of\n$<0.65M_\\odot$, consistent with models of a relatively low mass\n($M_{ZAMS}<15M_\\odot$) progenitor. An energy of 0.3 to $1.4 \\times10^{51}$ erg\nand a light curve powered by an initial $^{56}$Ni mass of $0.049 \\pm 0.005\nM_\\odot$ appear consistent with a relatively standard Type II explosion, while\nan incomplete $\\gamma$-ray trapping (with timescale of $240\\pm4$ days) suggests\na lower ejecta mass. Assuming a typical explosion, the broad Hydrogen and\nCalcium profiles suggest a common origin within a lower mass, partially\nstripped envelope. Hydrogen emission broadens with time, indicating\ncontribution from an additional power source at an extended distance; while the\nemergence of high velocity ($\\sim$6,000 km s$^{-1}$) Hydrogen emission features\n(beginning around day +200) may be explained by Shock Interaction with a dense\nHydrogen-rich region located at $\\sim1.5 \\times 10^{16}$cm. Such envelope mass\nloss for a low mass progenitor may be explained through theoretical models of\nBinary interaction."
                },
                "authors": [
                    {
                        "name": "Philip D. Michel"
                    },
                    {
                        "name": "Paolo A. Mazzali"
                    },
                    {
                        "name": "Daniel A. Perley"
                    },
                    {
                        "name": "K-Ryan Hinds"
                    },
                    {
                        "name": "Jacob L. Wise"
                    }
                ],
                "author_detail": {
                    "name": "Jacob L. Wise"
                },
                "author": "Jacob L. Wise",
                "arxiv_doi": "10.1093/mnras/staf443",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/staf443",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.13017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13017v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 Pages, 12 Figures, 5 Tables",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13016v1",
                "updated": "2025-03-17T10:20:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    20,
                    5,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T10:20:05Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    20,
                    5,
                    0,
                    76,
                    0
                ],
                "title": "Efficient Motion-Aware Video MLLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Motion-Aware Video MLLM"
                },
                "summary": "Most current video MLLMs rely on uniform frame sampling and image-level\nencoders, resulting in inefficient data processing and limited motion\nawareness. To address these challenges, we introduce EMA, an Efficient\nMotion-Aware video MLLM that utilizes compressed video structures as inputs. We\npropose a motion-aware GOP (Group of Pictures) encoder that fuses spatial and\nmotion information within a GOP unit in the compressed video stream, generating\ncompact, informative visual tokens. By integrating fewer but denser RGB frames\nwith more but sparser motion vectors in this native slow-fast input\narchitecture, our approach reduces redundancy and enhances motion\nrepresentation. Additionally, we introduce MotionBench, a benchmark for\nevaluating motion understanding across four motion types: linear, curved,\nrotational, and contact-based. Experimental results show that EMA achieves\nstate-of-the-art performance on both MotionBench and popular video question\nanswering benchmarks, while reducing inference costs. Moreover, EMA\ndemonstrates strong scalability, as evidenced by its competitive performance on\nlong video understanding benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most current video MLLMs rely on uniform frame sampling and image-level\nencoders, resulting in inefficient data processing and limited motion\nawareness. To address these challenges, we introduce EMA, an Efficient\nMotion-Aware video MLLM that utilizes compressed video structures as inputs. We\npropose a motion-aware GOP (Group of Pictures) encoder that fuses spatial and\nmotion information within a GOP unit in the compressed video stream, generating\ncompact, informative visual tokens. By integrating fewer but denser RGB frames\nwith more but sparser motion vectors in this native slow-fast input\narchitecture, our approach reduces redundancy and enhances motion\nrepresentation. Additionally, we introduce MotionBench, a benchmark for\nevaluating motion understanding across four motion types: linear, curved,\nrotational, and contact-based. Experimental results show that EMA achieves\nstate-of-the-art performance on both MotionBench and popular video question\nanswering benchmarks, while reducing inference costs. Moreover, EMA\ndemonstrates strong scalability, as evidenced by its competitive performance on\nlong video understanding benchmarks."
                },
                "authors": [
                    {
                        "name": "Zijia Zhao"
                    },
                    {
                        "name": "Yuqi Huo"
                    },
                    {
                        "name": "Tongtian Yue"
                    },
                    {
                        "name": "Longteng Guo"
                    },
                    {
                        "name": "Haoyu Lu"
                    },
                    {
                        "name": "Bingning Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Jing Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jing Liu"
                },
                "author": "Jing Liu",
                "arxiv_comment": "CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09297v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09297v2",
                "updated": "2025-03-17T10:16:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    16,
                    46,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-12T11:46:29Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    11,
                    46,
                    29,
                    2,
                    71,
                    0
                ],
                "title": "Gravitational form factors and mechanical properties of the nucleon in a\n  meson dominance approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational form factors and mechanical properties of the nucleon in a\n  meson dominance approach"
                },
                "summary": "We analyze the gravitational form factors and mechanical properties of the\nnucleon, focusing on both some general issues as well as on modeling with meson\ndominance. We show that the lattice QCD results for the nucleon gravitational\nform factors at $m_\\pi=170$~MeV, available for space-like momentum transfer\nsquared up to 2GeV, are explained in a natural way within the meson dominance\napproach. We carry out the proper Raman spin decomposition of the\nenergy-momentum tensor and in each spin channel use a minimum number of\nresonances consistent with the perturbative QCD short-distance constraints.\nThese constraints are related to the super-convergence sum rules, following\nfrom the asymptotic perturbative QCD fall-off of the form factors. The value of\nthe nucleon $D$-term following from the fits is -3.0(4). Next, we obtain the\ntwo-dimensional transverse gravitational densities of the nucleon in the\ntransverse coordinate $b$. With the super-convergence sum rules, we derive new\nsum rules for these densities at the origin and for their derivatives,\ninvolving logarithmic weighting in the corresponding spectral density\nintegrals. From analysis of the threshold behavior in the time-like region and\nthe properties of the $\\pi\\pi \\to N\\bar{N}$ reaction, we infer the behavior of\nthe transverse densities at asymptotically large coordinates. We also carry out\nthe meson dominance analysis of the two- and three-dimensional mechanical\nproperties of the nucleon (the pressure and stress) and explore their\nconnection to the spectral densities via dispersion relations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We analyze the gravitational form factors and mechanical properties of the\nnucleon, focusing on both some general issues as well as on modeling with meson\ndominance. We show that the lattice QCD results for the nucleon gravitational\nform factors at $m_\\pi=170$~MeV, available for space-like momentum transfer\nsquared up to 2GeV, are explained in a natural way within the meson dominance\napproach. We carry out the proper Raman spin decomposition of the\nenergy-momentum tensor and in each spin channel use a minimum number of\nresonances consistent with the perturbative QCD short-distance constraints.\nThese constraints are related to the super-convergence sum rules, following\nfrom the asymptotic perturbative QCD fall-off of the form factors. The value of\nthe nucleon $D$-term following from the fits is -3.0(4). Next, we obtain the\ntwo-dimensional transverse gravitational densities of the nucleon in the\ntransverse coordinate $b$. With the super-convergence sum rules, we derive new\nsum rules for these densities at the origin and for their derivatives,\ninvolving logarithmic weighting in the corresponding spectral density\nintegrals. From analysis of the threshold behavior in the time-like region and\nthe properties of the $\\pi\\pi \\to N\\bar{N}$ reaction, we infer the behavior of\nthe transverse densities at asymptotically large coordinates. We also carry out\nthe meson dominance analysis of the two- and three-dimensional mechanical\nproperties of the nucleon (the pressure and stress) and explore their\nconnection to the spectral densities via dispersion relations."
                },
                "authors": [
                    {
                        "name": "Wojciech Broniowski"
                    },
                    {
                        "name": "Enrique Ruiz Arriola"
                    }
                ],
                "author_detail": {
                    "name": "Enrique Ruiz Arriola"
                },
                "author": "Enrique Ruiz Arriola",
                "arxiv_comment": "21 pages, 12 figures. References, discussion, and a figure added",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09297v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09297v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-lat",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.19597v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.19597v3",
                "updated": "2025-03-17T10:09:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    9,
                    29,
                    0,
                    76,
                    0
                ],
                "published": "2024-04-30T14:43:57Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    14,
                    43,
                    57,
                    1,
                    121,
                    0
                ],
                "title": "TuBA: Cross-Lingual Transferability of Backdoor Attacks in LLMs with\n  Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TuBA: Cross-Lingual Transferability of Backdoor Attacks in LLMs with\n  Instruction Tuning"
                },
                "summary": "The implications of backdoor attacks on English-centric large language models\n(LLMs) have been widely examined - such attacks can be achieved by embedding\nmalicious behaviors during training and activated under specific conditions\nthat trigger malicious outputs. Despite the increasing support for multilingual\ncapabilities in open-source and proprietary LLMs, the impact of backdoor\nattacks on these systems remains largely under-explored. Our research focuses\non cross-lingual backdoor attacks against multilingual LLMs, particularly\ninvestigating how poisoning the instruction-tuning data for one or two\nlanguages can affect the outputs for languages whose instruction-tuning data\nwere not poisoned. Despite its simplicity, our empirical analysis reveals that\nour method exhibits remarkable efficacy in models like mT5 and GPT-4o, with\nhigh attack success rates, surpassing 90% in more than 7 out of 12 languages\nacross various scenarios. Our findings also indicate that more powerful models\nshow increased susceptibility to transferable cross-lingual backdoor attacks,\nwhich also applies to LLMs predominantly pre-trained on English data, such as\nLlama2, Llama3, and Gemma. Moreover, our experiments demonstrate 1) High\nTransferability: the backdoor mechanism operates successfully in cross-lingual\nresponse scenarios across 26 languages, achieving an average attack success\nrate of 99%, and 2) Robustness: the proposed attack remains effective even\nafter defenses are applied. These findings expose critical security\nvulnerabilities in multilingual LLMs and highlight the urgent need for more\nrobust, targeted defense strategies to address the unique challenges posed by\ncross-lingual backdoor transfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The implications of backdoor attacks on English-centric large language models\n(LLMs) have been widely examined - such attacks can be achieved by embedding\nmalicious behaviors during training and activated under specific conditions\nthat trigger malicious outputs. Despite the increasing support for multilingual\ncapabilities in open-source and proprietary LLMs, the impact of backdoor\nattacks on these systems remains largely under-explored. Our research focuses\non cross-lingual backdoor attacks against multilingual LLMs, particularly\ninvestigating how poisoning the instruction-tuning data for one or two\nlanguages can affect the outputs for languages whose instruction-tuning data\nwere not poisoned. Despite its simplicity, our empirical analysis reveals that\nour method exhibits remarkable efficacy in models like mT5 and GPT-4o, with\nhigh attack success rates, surpassing 90% in more than 7 out of 12 languages\nacross various scenarios. Our findings also indicate that more powerful models\nshow increased susceptibility to transferable cross-lingual backdoor attacks,\nwhich also applies to LLMs predominantly pre-trained on English data, such as\nLlama2, Llama3, and Gemma. Moreover, our experiments demonstrate 1) High\nTransferability: the backdoor mechanism operates successfully in cross-lingual\nresponse scenarios across 26 languages, achieving an average attack success\nrate of 99%, and 2) Robustness: the proposed attack remains effective even\nafter defenses are applied. These findings expose critical security\nvulnerabilities in multilingual LLMs and highlight the urgent need for more\nrobust, targeted defense strategies to address the unique challenges posed by\ncross-lingual backdoor transfer."
                },
                "authors": [
                    {
                        "name": "Xuanli He"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Qiongkai Xu"
                    },
                    {
                        "name": "Pasquale Minervini"
                    },
                    {
                        "name": "Pontus Stenetorp"
                    },
                    {
                        "name": "Benjamin I. P. Rubinstein"
                    },
                    {
                        "name": "Trevor Cohn"
                    }
                ],
                "author_detail": {
                    "name": "Trevor Cohn"
                },
                "author": "Trevor Cohn",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.19597v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.19597v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13008v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13008v1",
                "updated": "2025-03-17T10:07:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    7,
                    50,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T10:07:50Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    7,
                    50,
                    0,
                    76,
                    0
                ],
                "title": "Knowledge Distillation: Enhancing Neural Network Compression with\n  Integrated Gradients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Distillation: Enhancing Neural Network Compression with\n  Integrated Gradients"
                },
                "summary": "Efficient deployment of deep neural networks on resource-constrained devices\ndemands advanced compression techniques that preserve accuracy and\ninteroperability. This paper proposes a machine learning framework that\naugments Knowledge Distillation (KD) with Integrated Gradients (IG), an\nattribution method, to optimise the compression of convolutional neural\nnetworks. We introduce a novel data augmentation strategy where IG maps,\nprecomputed from a teacher model, are overlaid onto training images to guide a\ncompact student model toward critical feature representations. This approach\nleverages the teacher's decision-making insights, enhancing the student's\nability to replicate complex patterns with reduced parameters. Experiments on\nCIFAR-10 demonstrate the efficacy of our method: a student model, compressed\n4.1-fold from the MobileNet-V2 teacher, achieves 92.5% classification accuracy,\nsurpassing the baseline student's 91.4% and traditional KD approaches, while\nreducing inference latency from 140 ms to 13 ms--a tenfold speedup. We perform\nhyperparameter optimisation for efficient learning. Comprehensive ablation\nstudies dissect the contributions of KD and IG, revealing synergistic effects\nthat boost both performance and model explainability. Our method's emphasis on\nfeature-level guidance via IG distinguishes it from conventional KD, offering a\ndata-driven solution for mining transferable knowledge in neural architectures.\nThis work contributes to machine learning by providing a scalable,\ninterpretable compression technique, ideal for edge computing applications\nwhere efficiency and transparency are paramount.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient deployment of deep neural networks on resource-constrained devices\ndemands advanced compression techniques that preserve accuracy and\ninteroperability. This paper proposes a machine learning framework that\naugments Knowledge Distillation (KD) with Integrated Gradients (IG), an\nattribution method, to optimise the compression of convolutional neural\nnetworks. We introduce a novel data augmentation strategy where IG maps,\nprecomputed from a teacher model, are overlaid onto training images to guide a\ncompact student model toward critical feature representations. This approach\nleverages the teacher's decision-making insights, enhancing the student's\nability to replicate complex patterns with reduced parameters. Experiments on\nCIFAR-10 demonstrate the efficacy of our method: a student model, compressed\n4.1-fold from the MobileNet-V2 teacher, achieves 92.5% classification accuracy,\nsurpassing the baseline student's 91.4% and traditional KD approaches, while\nreducing inference latency from 140 ms to 13 ms--a tenfold speedup. We perform\nhyperparameter optimisation for efficient learning. Comprehensive ablation\nstudies dissect the contributions of KD and IG, revealing synergistic effects\nthat boost both performance and model explainability. Our method's emphasis on\nfeature-level guidance via IG distinguishes it from conventional KD, offering a\ndata-driven solution for mining transferable knowledge in neural architectures.\nThis work contributes to machine learning by providing a scalable,\ninterpretable compression technique, ideal for edge computing applications\nwhere efficiency and transparency are paramount."
                },
                "authors": [
                    {
                        "name": "David E. Hernandez"
                    },
                    {
                        "name": "Jose Ramon Chang"
                    },
                    {
                        "name": "Torbjörn E. M. Nordling"
                    }
                ],
                "author_detail": {
                    "name": "Torbjörn E. M. Nordling"
                },
                "author": "Torbjörn E. M. Nordling",
                "arxiv_comment": "15 pages, 3 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13008v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13008v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05, 68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.4.2; I.4.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01262v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01262v2",
                "updated": "2025-03-17T10:01:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    1,
                    21,
                    0,
                    76,
                    0
                ],
                "published": "2024-12-02T08:30:22Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    30,
                    22,
                    0,
                    337,
                    0
                ],
                "title": "Exploring ReAct Prompting for Task-Oriented Dialogue: Insights and\n  Shortcomings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring ReAct Prompting for Task-Oriented Dialogue: Insights and\n  Shortcomings"
                },
                "summary": "Large language models (LLMs) gained immense popularity due to their\nimpressive capabilities in unstructured conversations. Empowering LLMs with\nadvanced prompting strategies such as reasoning and acting (ReAct) (Yao et al.,\n2022) has shown promise in solving complex tasks traditionally requiring\nreinforcement learning. In this work, we apply the ReAct strategy to guide LLMs\nperforming task-oriented dialogue (TOD). We evaluate ReAct-based LLMs\n(ReAct-LLMs) both in simulation and with real users. While ReAct-LLMs severely\nunderperform state-of-the-art approaches on success rate in simulation, this\ndifference becomes less pronounced in human evaluation. Moreover, compared to\nthe baseline, humans report higher subjective satisfaction with ReAct-LLM\ndespite its lower success rate, most likely thanks to its natural and\nconfidently phrased responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) gained immense popularity due to their\nimpressive capabilities in unstructured conversations. Empowering LLMs with\nadvanced prompting strategies such as reasoning and acting (ReAct) (Yao et al.,\n2022) has shown promise in solving complex tasks traditionally requiring\nreinforcement learning. In this work, we apply the ReAct strategy to guide LLMs\nperforming task-oriented dialogue (TOD). We evaluate ReAct-based LLMs\n(ReAct-LLMs) both in simulation and with real users. While ReAct-LLMs severely\nunderperform state-of-the-art approaches on success rate in simulation, this\ndifference becomes less pronounced in human evaluation. Moreover, compared to\nthe baseline, humans report higher subjective satisfaction with ReAct-LLM\ndespite its lower success rate, most likely thanks to its natural and\nconfidently phrased responses."
                },
                "authors": [
                    {
                        "name": "Michelle Elizabeth"
                    },
                    {
                        "name": "Morgan Veyret"
                    },
                    {
                        "name": "Miguel Couceiro"
                    },
                    {
                        "name": "Ondrej Dusek"
                    },
                    {
                        "name": "Lina M. Rojas-Barahona"
                    }
                ],
                "author_detail": {
                    "name": "Lina M. Rojas-Barahona"
                },
                "author": "Lina M. Rojas-Barahona",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01262v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01262v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13004v1",
                "updated": "2025-03-17T10:00:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    0,
                    14,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T10:00:14Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    0,
                    14,
                    0,
                    76,
                    0
                ],
                "title": "TFDM: Time-Variant Frequency-Based Point Cloud Diffusion with Mamba",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TFDM: Time-Variant Frequency-Based Point Cloud Diffusion with Mamba"
                },
                "summary": "Diffusion models currently demonstrate impressive performance over various\ngenerative tasks. Recent work on image diffusion highlights the strong\ncapabilities of Mamba (state space models) due to its efficient handling of\nlong-range dependencies and sequential data modeling. Unfortunately, joint\nconsideration of state space models with 3D point cloud generation remains\nlimited. To harness the powerful capabilities of the Mamba model for 3D point\ncloud generation, we propose a novel diffusion framework containing dual latent\nMamba block (DM-Block) and a time-variant frequency encoder (TF-Encoder). The\nDM-Block apply a space-filling curve to reorder points into sequences suitable\nfor Mamba state-space modeling, while operating in a latent space to mitigate\nthe computational overhead that arises from direct 3D data processing.\nMeanwhile, the TF-Encoder takes advantage of the ability of the diffusion model\nto refine fine details in later recovery stages by prioritizing key points\nwithin the U-Net architecture. This frequency-based mechanism ensures enhanced\ndetail quality in the final stages of generation. Experimental results on the\nShapeNet-v2 dataset demonstrate that our method achieves state-of-the-art\nperformance (ShapeNet-v2: 0.14\\% on 1-NNA-Abs50 EMD and 57.90\\% on COV EMD) on\ncertain metrics for specific categories while reducing computational parameters\nand inference time by up to 10$\\times$ and 9$\\times$, respectively. Source code\nis available in Supplementary Materials and will be released upon accpetance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models currently demonstrate impressive performance over various\ngenerative tasks. Recent work on image diffusion highlights the strong\ncapabilities of Mamba (state space models) due to its efficient handling of\nlong-range dependencies and sequential data modeling. Unfortunately, joint\nconsideration of state space models with 3D point cloud generation remains\nlimited. To harness the powerful capabilities of the Mamba model for 3D point\ncloud generation, we propose a novel diffusion framework containing dual latent\nMamba block (DM-Block) and a time-variant frequency encoder (TF-Encoder). The\nDM-Block apply a space-filling curve to reorder points into sequences suitable\nfor Mamba state-space modeling, while operating in a latent space to mitigate\nthe computational overhead that arises from direct 3D data processing.\nMeanwhile, the TF-Encoder takes advantage of the ability of the diffusion model\nto refine fine details in later recovery stages by prioritizing key points\nwithin the U-Net architecture. This frequency-based mechanism ensures enhanced\ndetail quality in the final stages of generation. Experimental results on the\nShapeNet-v2 dataset demonstrate that our method achieves state-of-the-art\nperformance (ShapeNet-v2: 0.14\\% on 1-NNA-Abs50 EMD and 57.90\\% on COV EMD) on\ncertain metrics for specific categories while reducing computational parameters\nand inference time by up to 10$\\times$ and 9$\\times$, respectively. Source code\nis available in Supplementary Materials and will be released upon accpetance."
                },
                "authors": [
                    {
                        "name": "Jiaxu Liu"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Hubert P. H. Shum"
                    },
                    {
                        "name": "Toby P. Breckon"
                    }
                ],
                "author_detail": {
                    "name": "Toby P. Breckon"
                },
                "author": "Toby P. Breckon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.13447v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13447v1",
                "updated": "2025-03-17T17:59:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    59,
                    54,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T17:59:54Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    59,
                    54,
                    0,
                    76,
                    0
                ],
                "title": "MetaScale: Test-Time Scaling with Evolving Meta-Thoughts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaScale: Test-Time Scaling with Evolving Meta-Thoughts"
                },
                "summary": "One critical challenge for large language models (LLMs) for making complex\nreasoning is their reliance on matching reasoning patterns from training data,\ninstead of proactively selecting the most appropriate cognitive strategy to\nsolve a given task. Existing approaches impose fixed cognitive structures that\nenhance performance in specific tasks but lack adaptability across diverse\nscenarios. To address this limitation, we introduce METASCALE, a test-time\nscaling framework based on meta-thoughts -- adaptive thinking strategies\ntailored to each task. METASCALE initializes a pool of candidate meta-thoughts,\nthen iteratively selects and evaluates them using a multi-armed bandit\nalgorithm with upper confidence bound selection, guided by a reward model. To\nfurther enhance adaptability, a genetic algorithm evolves high-reward\nmeta-thoughts, refining and extending the strategy pool over time. By\ndynamically proposing and optimizing meta-thoughts at inference time, METASCALE\nimproves both accuracy and generalization across a wide range of tasks.\nExperimental results demonstrate that MetaScale consistently outperforms\nstandard inference approaches, achieving an 11% performance gain in win rate on\nArena-Hard for GPT-4o, surpassing o1-mini by 0.9% under style control. Notably,\nMETASCALE scales more effectively with increasing sampling budgets and produces\nmore structured, expert-level responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One critical challenge for large language models (LLMs) for making complex\nreasoning is their reliance on matching reasoning patterns from training data,\ninstead of proactively selecting the most appropriate cognitive strategy to\nsolve a given task. Existing approaches impose fixed cognitive structures that\nenhance performance in specific tasks but lack adaptability across diverse\nscenarios. To address this limitation, we introduce METASCALE, a test-time\nscaling framework based on meta-thoughts -- adaptive thinking strategies\ntailored to each task. METASCALE initializes a pool of candidate meta-thoughts,\nthen iteratively selects and evaluates them using a multi-armed bandit\nalgorithm with upper confidence bound selection, guided by a reward model. To\nfurther enhance adaptability, a genetic algorithm evolves high-reward\nmeta-thoughts, refining and extending the strategy pool over time. By\ndynamically proposing and optimizing meta-thoughts at inference time, METASCALE\nimproves both accuracy and generalization across a wide range of tasks.\nExperimental results demonstrate that MetaScale consistently outperforms\nstandard inference approaches, achieving an 11% performance gain in win rate on\nArena-Hard for GPT-4o, surpassing o1-mini by 0.9% under style control. Notably,\nMETASCALE scales more effectively with increasing sampling budgets and produces\nmore structured, expert-level responses."
                },
                "authors": [
                    {
                        "name": "Qin Liu"
                    },
                    {
                        "name": "Wenxuan Zhou"
                    },
                    {
                        "name": "Nan Xu"
                    },
                    {
                        "name": "James Y. Huang"
                    },
                    {
                        "name": "Fei Wang"
                    },
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Hoifung Poon"
                    },
                    {
                        "name": "Muhao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Muhao Chen"
                },
                "author": "Muhao Chen",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13447v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13447v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13446v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13446v1",
                "updated": "2025-03-17T17:59:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    59,
                    52,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T17:59:52Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    59,
                    52,
                    0,
                    76,
                    0
                ],
                "title": "MoManipVLA: Transferring Vision-language-action Models for General\n  Mobile Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoManipVLA: Transferring Vision-language-action Models for General\n  Mobile Manipulation"
                },
                "summary": "Mobile manipulation is the fundamental challenge for robotics to assist\nhumans with diverse tasks and environments in everyday life. However,\nconventional mobile manipulation approaches often struggle to generalize across\ndifferent tasks and environments because of the lack of large-scale training.\nIn contrast, recent advances in vision-language-action (VLA) models have shown\nimpressive generalization capabilities, but these foundation models are\ndeveloped for fixed-base manipulation tasks. Therefore, we propose an efficient\npolicy adaptation framework named MoManipVLA to transfer pre-trained VLA models\nof fix-base manipulation to mobile manipulation, so that high generalization\nability across tasks and environments can be achieved in mobile manipulation\npolicy. Specifically, we utilize pre-trained VLA models to generate waypoints\nof the end-effector with high generalization ability. We design motion planning\nobjectives for the mobile base and the robot arm, which aim at maximizing the\nphysical feasibility of the trajectory. Finally, we present an efficient\nbi-level objective optimization framework for trajectory generation, where the\nupper-level optimization predicts waypoints for base movement to enhance the\nmanipulator policy space, and the lower-level optimization selects the optimal\nend-effector trajectory to complete the manipulation task. In this way,\nMoManipVLA can adjust the position of the robot base in a zero-shot manner,\nthus making the waypoints predicted from the fixed-base VLA models feasible.\nExtensive experimental results on OVMM and the real world demonstrate that\nMoManipVLA achieves a 4.2% higher success rate than the state-of-the-art mobile\nmanipulation, and only requires 50 training cost for real world deployment due\nto the strong generalization ability in the pre-trained VLA models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile manipulation is the fundamental challenge for robotics to assist\nhumans with diverse tasks and environments in everyday life. However,\nconventional mobile manipulation approaches often struggle to generalize across\ndifferent tasks and environments because of the lack of large-scale training.\nIn contrast, recent advances in vision-language-action (VLA) models have shown\nimpressive generalization capabilities, but these foundation models are\ndeveloped for fixed-base manipulation tasks. Therefore, we propose an efficient\npolicy adaptation framework named MoManipVLA to transfer pre-trained VLA models\nof fix-base manipulation to mobile manipulation, so that high generalization\nability across tasks and environments can be achieved in mobile manipulation\npolicy. Specifically, we utilize pre-trained VLA models to generate waypoints\nof the end-effector with high generalization ability. We design motion planning\nobjectives for the mobile base and the robot arm, which aim at maximizing the\nphysical feasibility of the trajectory. Finally, we present an efficient\nbi-level objective optimization framework for trajectory generation, where the\nupper-level optimization predicts waypoints for base movement to enhance the\nmanipulator policy space, and the lower-level optimization selects the optimal\nend-effector trajectory to complete the manipulation task. In this way,\nMoManipVLA can adjust the position of the robot base in a zero-shot manner,\nthus making the waypoints predicted from the fixed-base VLA models feasible.\nExtensive experimental results on OVMM and the real world demonstrate that\nMoManipVLA achieves a 4.2% higher success rate than the state-of-the-art mobile\nmanipulation, and only requires 50 training cost for real world deployment due\nto the strong generalization ability in the pre-trained VLA models."
                },
                "authors": [
                    {
                        "name": "Zhenyu Wu"
                    },
                    {
                        "name": "Yuheng Zhou"
                    },
                    {
                        "name": "Xiuwei Xu"
                    },
                    {
                        "name": "Ziwei Wang"
                    },
                    {
                        "name": "Haibin Yan"
                    }
                ],
                "author_detail": {
                    "name": "Haibin Yan"
                },
                "author": "Haibin Yan",
                "arxiv_comment": "Accepted to CVPR 2025. Project Page:\n  https://gary3410.github.io/momanipVLA/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13446v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13445v1",
                "updated": "2025-03-17T17:59:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    59,
                    39,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T17:59:39Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    59,
                    39,
                    0,
                    76,
                    0
                ],
                "title": "Faithfulness of LLM Self-Explanations for Commonsense Tasks: Larger Is\n  Better, and Instruction-Tuning Allows Trade-Offs but Not Pareto Dominance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faithfulness of LLM Self-Explanations for Commonsense Tasks: Larger Is\n  Better, and Instruction-Tuning Allows Trade-Offs but Not Pareto Dominance"
                },
                "summary": "As large language models (LLMs) become increasingly capable, ensuring that\ntheir self-generated explanations are faithful to their internal\ndecision-making process is critical for safety and oversight. In this work, we\nconduct a comprehensive counterfactual faithfulness analysis across 62 models\nfrom 8 families, encompassing both pretrained and instruction-tuned variants\nand significantly extending prior studies of counterfactual tests. We introduce\nphi-CCT, a simplified variant of the Correlational Counterfactual Test, which\navoids the need for token probabilities while explaining most of the variance\nof the original test. Our findings reveal clear scaling trends: larger models\nare consistently more faithful on our metrics. However, when comparing\ninstruction-tuned and human-imitated explanations, we find that observed\ndifferences in faithfulness can often be attributed to explanation verbosity,\nleading to shifts along the true-positive/false-positive Pareto frontier. While\ninstruction-tuning and prompting can influence this trade-off, we find limited\nevidence that they fundamentally expand the frontier of explanatory\nfaithfulness beyond what is achievable with pretrained models of comparable\nsize. Our analysis highlights the nuanced relationship between\ninstruction-tuning, verbosity, and the faithful representation of model\ndecision processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become increasingly capable, ensuring that\ntheir self-generated explanations are faithful to their internal\ndecision-making process is critical for safety and oversight. In this work, we\nconduct a comprehensive counterfactual faithfulness analysis across 62 models\nfrom 8 families, encompassing both pretrained and instruction-tuned variants\nand significantly extending prior studies of counterfactual tests. We introduce\nphi-CCT, a simplified variant of the Correlational Counterfactual Test, which\navoids the need for token probabilities while explaining most of the variance\nof the original test. Our findings reveal clear scaling trends: larger models\nare consistently more faithful on our metrics. However, when comparing\ninstruction-tuned and human-imitated explanations, we find that observed\ndifferences in faithfulness can often be attributed to explanation verbosity,\nleading to shifts along the true-positive/false-positive Pareto frontier. While\ninstruction-tuning and prompting can influence this trade-off, we find limited\nevidence that they fundamentally expand the frontier of explanatory\nfaithfulness beyond what is achievable with pretrained models of comparable\nsize. Our analysis highlights the nuanced relationship between\ninstruction-tuning, verbosity, and the faithful representation of model\ndecision processes."
                },
                "authors": [
                    {
                        "name": "Noah Y. Siegel"
                    },
                    {
                        "name": "Nicolas Heess"
                    },
                    {
                        "name": "Maria Perez-Ortiz"
                    },
                    {
                        "name": "Oana-Maria Camburu"
                    }
                ],
                "author_detail": {
                    "name": "Oana-Maria Camburu"
                },
                "author": "Oana-Maria Camburu",
                "arxiv_comment": "38 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13436v1",
                "updated": "2025-03-17T17:58:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    58,
                    30,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T17:58:30Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    58,
                    30,
                    0,
                    76,
                    0
                ],
                "title": "Unified Autoregressive Visual Generation and Understanding with\n  Continuous Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Autoregressive Visual Generation and Understanding with\n  Continuous Tokens"
                },
                "summary": "We present UniFluid, a unified autoregressive framework for joint visual\ngeneration and understanding leveraging continuous visual tokens. Our unified\nautoregressive architecture processes multimodal image and text inputs,\ngenerating discrete tokens for text and continuous tokens for image. We find\nthough there is an inherent trade-off between the image generation and\nunderstanding task, a carefully tuned training recipe enables them to improve\neach other. By selecting an appropriate loss balance weight, the unified model\nachieves results comparable to or exceeding those of single-task baselines on\nboth tasks. Furthermore, we demonstrate that employing stronger pre-trained\nLLMs and random-order generation during training is important to achieve\nhigh-fidelity image generation within this unified framework. Built upon the\nGemma model series, UniFluid exhibits competitive performance across both image\ngeneration and understanding, demonstrating strong transferability to various\ndownstream tasks, including image editing for generation, as well as visual\ncaptioning and question answering for understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present UniFluid, a unified autoregressive framework for joint visual\ngeneration and understanding leveraging continuous visual tokens. Our unified\nautoregressive architecture processes multimodal image and text inputs,\ngenerating discrete tokens for text and continuous tokens for image. We find\nthough there is an inherent trade-off between the image generation and\nunderstanding task, a carefully tuned training recipe enables them to improve\neach other. By selecting an appropriate loss balance weight, the unified model\nachieves results comparable to or exceeding those of single-task baselines on\nboth tasks. Furthermore, we demonstrate that employing stronger pre-trained\nLLMs and random-order generation during training is important to achieve\nhigh-fidelity image generation within this unified framework. Built upon the\nGemma model series, UniFluid exhibits competitive performance across both image\ngeneration and understanding, demonstrating strong transferability to various\ndownstream tasks, including image editing for generation, as well as visual\ncaptioning and question answering for understanding."
                },
                "authors": [
                    {
                        "name": "Lijie Fan"
                    },
                    {
                        "name": "Luming Tang"
                    },
                    {
                        "name": "Siyang Qin"
                    },
                    {
                        "name": "Tianhong Li"
                    },
                    {
                        "name": "Xuan Yang"
                    },
                    {
                        "name": "Siyuan Qiao"
                    },
                    {
                        "name": "Andreas Steiner"
                    },
                    {
                        "name": "Chen Sun"
                    },
                    {
                        "name": "Yuanzhen Li"
                    },
                    {
                        "name": "Tao Zhu"
                    },
                    {
                        "name": "Michael Rubinstein"
                    },
                    {
                        "name": "Michalis Raptis"
                    },
                    {
                        "name": "Deqing Sun"
                    },
                    {
                        "name": "Radu Soricut"
                    }
                ],
                "author_detail": {
                    "name": "Radu Soricut"
                },
                "author": "Radu Soricut",
                "arxiv_comment": "Tech report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18573v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18573v2",
                "updated": "2025-03-17T17:58:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    58,
                    13,
                    0,
                    76,
                    0
                ],
                "published": "2024-12-24T17:56:08Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    17,
                    56,
                    8,
                    1,
                    359,
                    0
                ],
                "title": "Top General Performance = Top Domain Performance? DomainCodeBench: A\n  Multi-domain Code Generation Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Top General Performance = Top Domain Performance? DomainCodeBench: A\n  Multi-domain Code Generation Benchmark"
                },
                "summary": "With the rapid advancement of large language models (LLMs), extensive\nresearch has been conducted to investigate the code generation capabilities of\nLLMs. However, existing efforts primarily focus on general-domain tasks,\nleaving LLMs' code generation performance in real-world application domains\nunderexplored. This raises a critical question: can a model's general-domain\ncoding ability reliably represent its ability in specialized domains? In this\npaper, we introduce DomainCodeBench, a multi-domain code generation benchmark\ndesigned to systematically evaluate LLMs across 12 software application domains\nand 15 programming languages. DomainCodeBench contains 2,400 manually verified\ntasks with ground truth, human-annotated docstrings, and fine-grained\ndependency information to ensure more coverage of domain-specific challenges.\nSpecifically, we first identify the most popular application domains by topic\nmining. Then, we curate coding tasks based on commonly used frameworks and\nplatforms in each domain. We obtain several findings through extensive\nexperiments on DomainCodeBench with ten mainstream LLMs. (1) Performance\ndecoupling: experiments reveal that top general-domain models do not\nconsistently excel in specific application domains; (2) Domain-specific\nweaknesses: LLMs often fail due to domain knowledge gaps and third-party\nlibrary misusage; (3) Contextual enhancement: we show that augmenting prompts\nwith domain-specific knowledge improves performance by around 38.17%, providing\nactionable insights for performance optimization. Our replication package,\nincluding the benchmark, source code, and experimental results, is available at\nhttps://github.com/DeepSoftwareAnalytics/DomainCodeBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of large language models (LLMs), extensive\nresearch has been conducted to investigate the code generation capabilities of\nLLMs. However, existing efforts primarily focus on general-domain tasks,\nleaving LLMs' code generation performance in real-world application domains\nunderexplored. This raises a critical question: can a model's general-domain\ncoding ability reliably represent its ability in specialized domains? In this\npaper, we introduce DomainCodeBench, a multi-domain code generation benchmark\ndesigned to systematically evaluate LLMs across 12 software application domains\nand 15 programming languages. DomainCodeBench contains 2,400 manually verified\ntasks with ground truth, human-annotated docstrings, and fine-grained\ndependency information to ensure more coverage of domain-specific challenges.\nSpecifically, we first identify the most popular application domains by topic\nmining. Then, we curate coding tasks based on commonly used frameworks and\nplatforms in each domain. We obtain several findings through extensive\nexperiments on DomainCodeBench with ten mainstream LLMs. (1) Performance\ndecoupling: experiments reveal that top general-domain models do not\nconsistently excel in specific application domains; (2) Domain-specific\nweaknesses: LLMs often fail due to domain knowledge gaps and third-party\nlibrary misusage; (3) Contextual enhancement: we show that augmenting prompts\nwith domain-specific knowledge improves performance by around 38.17%, providing\nactionable insights for performance optimization. Our replication package,\nincluding the benchmark, source code, and experimental results, is available at\nhttps://github.com/DeepSoftwareAnalytics/DomainCodeBench."
                },
                "authors": [
                    {
                        "name": "Dewu Zheng"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Ensheng Shi"
                    },
                    {
                        "name": "Xilin Liu"
                    },
                    {
                        "name": "Yuchi Ma"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18573v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13427v1",
                "updated": "2025-03-17T17:54:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    54,
                    55,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T17:54:55Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    54,
                    55,
                    0,
                    76,
                    0
                ],
                "title": "xLSTM 7B: A Recurrent LLM for Fast and Efficient Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xLSTM 7B: A Recurrent LLM for Fast and Efficient Inference"
                },
                "summary": "Recent breakthroughs in solving reasoning, math and coding problems with\nLarge Language Models (LLMs) have been enabled by investing substantial\ncomputation budgets at inference time. Therefore, inference speed is one of the\nmost critical properties of LLM architectures, and there is a growing need for\nLLMs that are efficient and fast at inference. Recently, LLMs built on the\nxLSTM architecture have emerged as a powerful alternative to Transformers,\noffering linear compute scaling with sequence length and constant memory usage,\nboth highly desirable properties for efficient inference. However, such\nxLSTM-based LLMs have yet to be scaled to larger models and assessed and\ncompared with respect to inference speed and efficiency. In this work, we\nintroduce xLSTM 7B, a 7-billion-parameter LLM that combines xLSTM's\narchitectural benefits with targeted optimizations for fast and efficient\ninference. Our experiments demonstrate that xLSTM 7B achieves performance on\ndownstream tasks comparable to other similar-sized LLMs, while providing\nsignificantly faster inference speeds and greater efficiency compared to Llama-\nand Mamba-based LLMs. These results establish xLSTM 7B as the fastest and most\nefficient 7B LLM, offering a solution for tasks that require large amounts of\ntest-time computation. Our work highlights xLSTM's potential as a foundational\narchitecture for methods building on heavy use of LLM inference. Our model\nweights, model code and training code are open-source.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent breakthroughs in solving reasoning, math and coding problems with\nLarge Language Models (LLMs) have been enabled by investing substantial\ncomputation budgets at inference time. Therefore, inference speed is one of the\nmost critical properties of LLM architectures, and there is a growing need for\nLLMs that are efficient and fast at inference. Recently, LLMs built on the\nxLSTM architecture have emerged as a powerful alternative to Transformers,\noffering linear compute scaling with sequence length and constant memory usage,\nboth highly desirable properties for efficient inference. However, such\nxLSTM-based LLMs have yet to be scaled to larger models and assessed and\ncompared with respect to inference speed and efficiency. In this work, we\nintroduce xLSTM 7B, a 7-billion-parameter LLM that combines xLSTM's\narchitectural benefits with targeted optimizations for fast and efficient\ninference. Our experiments demonstrate that xLSTM 7B achieves performance on\ndownstream tasks comparable to other similar-sized LLMs, while providing\nsignificantly faster inference speeds and greater efficiency compared to Llama-\nand Mamba-based LLMs. These results establish xLSTM 7B as the fastest and most\nefficient 7B LLM, offering a solution for tasks that require large amounts of\ntest-time computation. Our work highlights xLSTM's potential as a foundational\narchitecture for methods building on heavy use of LLM inference. Our model\nweights, model code and training code are open-source."
                },
                "authors": [
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Korbinian Pöppel"
                    },
                    {
                        "name": "Phillip Lippe"
                    },
                    {
                        "name": "Richard Kurle"
                    },
                    {
                        "name": "Patrick M. Blies"
                    },
                    {
                        "name": "Günter Klambauer"
                    },
                    {
                        "name": "Sebastian Böck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "arxiv_comment": "Code available at: https://github.com/NX-AI/xlstm and\n  https://github.com/NX-AI/xlstm-jax",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13415v1",
                "updated": "2025-03-17T17:45:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    45,
                    46,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T17:45:46Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    45,
                    46,
                    0,
                    76,
                    0
                ],
                "title": "A Comprehensive Survey on Multi-Agent Cooperative Decision-Making:\n  Scenarios, Approaches, Challenges and Perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey on Multi-Agent Cooperative Decision-Making:\n  Scenarios, Approaches, Challenges and Perspectives"
                },
                "summary": "With the rapid development of artificial intelligence, intelligent\ndecision-making techniques have gradually surpassed human levels in various\nhuman-machine competitions, especially in complex multi-agent cooperative task\nscenarios. Multi-agent cooperative decision-making involves multiple agents\nworking together to complete established tasks and achieve specific objectives.\nThese techniques are widely applicable in real-world scenarios such as\nautonomous driving, drone navigation, disaster rescue, and simulated military\nconfrontations. This paper begins with a comprehensive survey of the leading\nsimulation environments and platforms used for multi-agent cooperative\ndecision-making. Specifically, we provide an in-depth analysis for these\nsimulation environments from various perspectives, including task formats,\nreward allocation, and the underlying technologies employed. Subsequently, we\nprovide a comprehensive overview of the mainstream intelligent decision-making\napproaches, algorithms and models for multi-agent systems (MAS).\nTheseapproaches can be broadly categorized into five types: rule-based\n(primarily fuzzy logic), game theory-based, evolutionary algorithms-based, deep\nmulti-agent reinforcement learning (MARL)-based, and large language\nmodels(LLMs)reasoning-based. Given the significant advantages of MARL\nandLLMs-baseddecision-making methods over the traditional rule, game theory,\nand evolutionary algorithms, this paper focuses on these multi-agent methods\nutilizing MARL and LLMs-based techniques. We provide an in-depth discussion of\nthese approaches, highlighting their methodology taxonomies, advantages, and\ndrawbacks. Further, several prominent research directions in the future and\npotential challenges of multi-agent cooperative decision-making are also\ndetailed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of artificial intelligence, intelligent\ndecision-making techniques have gradually surpassed human levels in various\nhuman-machine competitions, especially in complex multi-agent cooperative task\nscenarios. Multi-agent cooperative decision-making involves multiple agents\nworking together to complete established tasks and achieve specific objectives.\nThese techniques are widely applicable in real-world scenarios such as\nautonomous driving, drone navigation, disaster rescue, and simulated military\nconfrontations. This paper begins with a comprehensive survey of the leading\nsimulation environments and platforms used for multi-agent cooperative\ndecision-making. Specifically, we provide an in-depth analysis for these\nsimulation environments from various perspectives, including task formats,\nreward allocation, and the underlying technologies employed. Subsequently, we\nprovide a comprehensive overview of the mainstream intelligent decision-making\napproaches, algorithms and models for multi-agent systems (MAS).\nTheseapproaches can be broadly categorized into five types: rule-based\n(primarily fuzzy logic), game theory-based, evolutionary algorithms-based, deep\nmulti-agent reinforcement learning (MARL)-based, and large language\nmodels(LLMs)reasoning-based. Given the significant advantages of MARL\nandLLMs-baseddecision-making methods over the traditional rule, game theory,\nand evolutionary algorithms, this paper focuses on these multi-agent methods\nutilizing MARL and LLMs-based techniques. We provide an in-depth discussion of\nthese approaches, highlighting their methodology taxonomies, advantages, and\ndrawbacks. Further, several prominent research directions in the future and\npotential challenges of multi-agent cooperative decision-making are also\ndetailed."
                },
                "authors": [
                    {
                        "name": "Weiqiang Jin"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Biao Zhao"
                    },
                    {
                        "name": "Xingwu Tian"
                    },
                    {
                        "name": "Bohang Shi"
                    },
                    {
                        "name": "Guang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guang Yang"
                },
                "author": "Guang Yang",
                "arxiv_comment": "54 pages, 24 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13413v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13413v2",
                "updated": "2025-03-18T04:41:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    4,
                    41,
                    37,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-17T17:42:51Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    42,
                    51,
                    0,
                    76,
                    0
                ],
                "title": "DLPO: Towards a Robust, Efficient, and Generalizable Prompt Optimization\n  Framework from a Deep-Learning Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DLPO: Towards a Robust, Efficient, and Generalizable Prompt Optimization\n  Framework from a Deep-Learning Perspective"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across diverse\ntasks, largely driven by well-designed prompts. However, crafting and selecting\nsuch prompts often requires considerable human effort, significantly limiting\nits scalability. To mitigate this, recent studies have explored automated\nprompt optimization as a promising solution. Despite these efforts, existing\nmethods still face critical challenges in robustness, efficiency, and\ngeneralization. To systematically address these challenges, we first conduct an\nempirical analysis to identify the limitations of current reflection-based\nprompt optimization paradigm. Building on these insights, we propose 7\ninnovative approaches inspired by traditional deep learning paradigms for\nprompt optimization (DLPO), seamlessly integrating these concepts into\ntext-based gradient optimization. Through these advancements, we progressively\ntackle the aforementioned challenges and validate our methods through extensive\nexperimentation. We hope our study not only provides valuable guidance for\nfuture research but also offers a comprehensive understanding of the challenges\nand potential solutions in prompt optimization. Our code is available at\nhttps://github.com/sfasfaffa/DLPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across diverse\ntasks, largely driven by well-designed prompts. However, crafting and selecting\nsuch prompts often requires considerable human effort, significantly limiting\nits scalability. To mitigate this, recent studies have explored automated\nprompt optimization as a promising solution. Despite these efforts, existing\nmethods still face critical challenges in robustness, efficiency, and\ngeneralization. To systematically address these challenges, we first conduct an\nempirical analysis to identify the limitations of current reflection-based\nprompt optimization paradigm. Building on these insights, we propose 7\ninnovative approaches inspired by traditional deep learning paradigms for\nprompt optimization (DLPO), seamlessly integrating these concepts into\ntext-based gradient optimization. Through these advancements, we progressively\ntackle the aforementioned challenges and validate our methods through extensive\nexperimentation. We hope our study not only provides valuable guidance for\nfuture research but also offers a comprehensive understanding of the challenges\nand potential solutions in prompt optimization. Our code is available at\nhttps://github.com/sfasfaffa/DLPO."
                },
                "authors": [
                    {
                        "name": "Dengyun Peng"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Jinhao Liu"
                    },
                    {
                        "name": "Jingjing Chen"
                    },
                    {
                        "name": "Libo Qin"
                    }
                ],
                "author_detail": {
                    "name": "Libo Qin"
                },
                "author": "Libo Qin",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13413v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13413v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13402v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13402v1",
                "updated": "2025-03-17T17:34:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    34,
                    4,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T17:34:04Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    34,
                    4,
                    0,
                    76,
                    0
                ],
                "title": "Toward Generative 6G Simulation: An Experimental Multi-Agent LLM and\n  ns-3 Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Generative 6G Simulation: An Experimental Multi-Agent LLM and\n  ns-3 Integration"
                },
                "summary": "The move toward open Sixth-Generation (6G) networks necessitates a novel\napproach to full-stack simulation environments for evaluating complex\ntechnology developments before prototyping and real-world implementation. This\npaper introduces an innovative approach\\footnote{A lightweight, mock version of\nthe code is available on GitHub at that combines a multi-agent framework with\nthe Network Simulator 3 (ns-3) to automate and optimize the generation,\ndebugging, execution, and analysis of complex 5G network scenarios. Our\nframework orchestrates a suite of specialized agents -- namely, the Simulation\nGeneration Agent, Test Designer Agent, Test Executor Agent, and Result\nInterpretation Agent -- using advanced LangChain coordination. The Simulation\nGeneration Agent employs a structured chain-of-thought (CoT) reasoning process,\nleveraging LLMs and retrieval-augmented generation (RAG) to translate natural\nlanguage simulation specifications into precise ns-3 scripts. Concurrently, the\nTest Designer Agent generates comprehensive automated test suites by\nintegrating knowledge retrieval techniques with dynamic test case synthesis.\nThe Test Executor Agent dynamically deploys and runs simulations, managing\ndependencies and parsing detailed performance metrics. At the same time, the\nResult Interpretation Agent utilizes LLM-driven analysis to extract actionable\ninsights from the simulation outputs. By integrating external resources such as\nlibrary documentation and ns-3 testing frameworks, our experimental approach\ncan enhance simulation accuracy and adaptability, reducing reliance on\nextensive programming expertise. A detailed case study using the ns-3 5G-LENA\nmodule validates the effectiveness of the proposed approach. The code\ngeneration process converges in an average of 1.8 iterations, has a syntax\nerror rate of 17.0%, a mean response time of 7.3 seconds, and receives a human\nevaluation score of 7.5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The move toward open Sixth-Generation (6G) networks necessitates a novel\napproach to full-stack simulation environments for evaluating complex\ntechnology developments before prototyping and real-world implementation. This\npaper introduces an innovative approach\\footnote{A lightweight, mock version of\nthe code is available on GitHub at that combines a multi-agent framework with\nthe Network Simulator 3 (ns-3) to automate and optimize the generation,\ndebugging, execution, and analysis of complex 5G network scenarios. Our\nframework orchestrates a suite of specialized agents -- namely, the Simulation\nGeneration Agent, Test Designer Agent, Test Executor Agent, and Result\nInterpretation Agent -- using advanced LangChain coordination. The Simulation\nGeneration Agent employs a structured chain-of-thought (CoT) reasoning process,\nleveraging LLMs and retrieval-augmented generation (RAG) to translate natural\nlanguage simulation specifications into precise ns-3 scripts. Concurrently, the\nTest Designer Agent generates comprehensive automated test suites by\nintegrating knowledge retrieval techniques with dynamic test case synthesis.\nThe Test Executor Agent dynamically deploys and runs simulations, managing\ndependencies and parsing detailed performance metrics. At the same time, the\nResult Interpretation Agent utilizes LLM-driven analysis to extract actionable\ninsights from the simulation outputs. By integrating external resources such as\nlibrary documentation and ns-3 testing frameworks, our experimental approach\ncan enhance simulation accuracy and adaptability, reducing reliance on\nextensive programming expertise. A detailed case study using the ns-3 5G-LENA\nmodule validates the effectiveness of the proposed approach. The code\ngeneration process converges in an average of 1.8 iterations, has a syntax\nerror rate of 17.0%, a mean response time of 7.3 seconds, and receives a human\nevaluation score of 7.5."
                },
                "authors": [
                    {
                        "name": "Farhad Rezazadeh"
                    },
                    {
                        "name": "Amir Ashtari Gargari"
                    },
                    {
                        "name": "Sandra Lagen"
                    },
                    {
                        "name": "Houbing Song"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Lingjia Liu"
                    }
                ],
                "author_detail": {
                    "name": "Lingjia Liu"
                },
                "author": "Lingjia Liu",
                "arxiv_comment": "6 pages, 4 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13402v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13402v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13399v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13399v1",
                "updated": "2025-03-17T17:33:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    33,
                    10,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T17:33:10Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    33,
                    10,
                    0,
                    76,
                    0
                ],
                "title": "MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based\n  Scientific Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based\n  Scientific Research"
                },
                "summary": "Scientific research demands sophisticated reasoning over multimodal data, a\nchallenge especially prevalent in biology. Despite recent advances in\nmultimodal large language models (MLLMs) for AI-assisted research, existing\nmultimodal reasoning benchmarks only target up to college-level difficulty,\nwhile research-level benchmarks emphasize lower-level perception, falling short\nof the complex multimodal reasoning needed for scientific discovery. To bridge\nthis gap, we introduce MicroVQA, a visual-question answering (VQA) benchmark\ndesigned to assess three reasoning capabilities vital in research workflows:\nexpert image understanding, hypothesis generation, and experiment proposal.\nMicroVQA consists of 1,042 multiple-choice questions (MCQs) curated by biology\nexperts across diverse microscopy modalities, ensuring VQA samples represent\nreal scientific practice. In constructing the benchmark, we find that standard\nMCQ generation methods induce language shortcuts, motivating a new two-stage\npipeline: an optimized LLM prompt structures question-answer pairs into MCQs;\nthen, an agent-based `RefineBot' updates them to remove shortcuts. Benchmarking\non state-of-the-art MLLMs reveal a peak performance of 53\\%; models with\nsmaller LLMs only slightly underperform top models, suggesting that\nlanguage-based reasoning is less challenging than multimodal reasoning; and\ntuning with scientific articles enhances performance. Expert analysis of\nchain-of-thought responses shows that perception errors are the most frequent,\nfollowed by knowledge errors and then overgeneralization errors. These insights\nhighlight the challenges in multimodal scientific reasoning, showing MicroVQA\nis a valuable resource advancing AI-driven biomedical research. MicroVQA is\navailable at https://huggingface.co/datasets/jmhb/microvqa, and project page at\nhttps://jmhb0.github.io/microvqa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific research demands sophisticated reasoning over multimodal data, a\nchallenge especially prevalent in biology. Despite recent advances in\nmultimodal large language models (MLLMs) for AI-assisted research, existing\nmultimodal reasoning benchmarks only target up to college-level difficulty,\nwhile research-level benchmarks emphasize lower-level perception, falling short\nof the complex multimodal reasoning needed for scientific discovery. To bridge\nthis gap, we introduce MicroVQA, a visual-question answering (VQA) benchmark\ndesigned to assess three reasoning capabilities vital in research workflows:\nexpert image understanding, hypothesis generation, and experiment proposal.\nMicroVQA consists of 1,042 multiple-choice questions (MCQs) curated by biology\nexperts across diverse microscopy modalities, ensuring VQA samples represent\nreal scientific practice. In constructing the benchmark, we find that standard\nMCQ generation methods induce language shortcuts, motivating a new two-stage\npipeline: an optimized LLM prompt structures question-answer pairs into MCQs;\nthen, an agent-based `RefineBot' updates them to remove shortcuts. Benchmarking\non state-of-the-art MLLMs reveal a peak performance of 53\\%; models with\nsmaller LLMs only slightly underperform top models, suggesting that\nlanguage-based reasoning is less challenging than multimodal reasoning; and\ntuning with scientific articles enhances performance. Expert analysis of\nchain-of-thought responses shows that perception errors are the most frequent,\nfollowed by knowledge errors and then overgeneralization errors. These insights\nhighlight the challenges in multimodal scientific reasoning, showing MicroVQA\nis a valuable resource advancing AI-driven biomedical research. MicroVQA is\navailable at https://huggingface.co/datasets/jmhb/microvqa, and project page at\nhttps://jmhb0.github.io/microvqa."
                },
                "authors": [
                    {
                        "name": "James Burgess"
                    },
                    {
                        "name": "Jeffrey J Nirschl"
                    },
                    {
                        "name": "Laura Bravo-Sánchez"
                    },
                    {
                        "name": "Alejandro Lozano"
                    },
                    {
                        "name": "Sanket Rajan Gupte"
                    },
                    {
                        "name": "Jesus G. Galaz-Montoya"
                    },
                    {
                        "name": "Yuhui Zhang"
                    },
                    {
                        "name": "Yuchang Su"
                    },
                    {
                        "name": "Disha Bhowmik"
                    },
                    {
                        "name": "Zachary Coman"
                    },
                    {
                        "name": "Sarina M. Hasan"
                    },
                    {
                        "name": "Alexandra Johannesson"
                    },
                    {
                        "name": "William D. Leineweber"
                    },
                    {
                        "name": "Malvika G Nair"
                    },
                    {
                        "name": "Ridhi Yarlagadda"
                    },
                    {
                        "name": "Connor Zuraski"
                    },
                    {
                        "name": "Wah Chiu"
                    },
                    {
                        "name": "Sarah Cohen"
                    },
                    {
                        "name": "Jan N. Hansen"
                    },
                    {
                        "name": "Manuel D Leonetti"
                    },
                    {
                        "name": "Chad Liu"
                    },
                    {
                        "name": "Emma Lundberg"
                    },
                    {
                        "name": "Serena Yeung-Levy"
                    }
                ],
                "author_detail": {
                    "name": "Serena Yeung-Levy"
                },
                "author": "Serena Yeung-Levy",
                "arxiv_comment": "CVPR 2025 (Conference on Computer Vision and Pattern Recognition)\n  Project page at https://jmhb0.github.io/microvqa Benchmark at\n  https://huggingface.co/datasets/jmhb/microvqa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13399v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13399v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.CB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13383v1",
                "updated": "2025-03-17T17:11:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    11,
                    22,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T17:11:22Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    11,
                    22,
                    0,
                    76,
                    0
                ],
                "title": "Cream of the Crop: Harvesting Rich, Scalable and Transferable\n  Multi-Modal Data for Instruction Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cream of the Crop: Harvesting Rich, Scalable and Transferable\n  Multi-Modal Data for Instruction Fine-Tuning"
                },
                "summary": "The hypothesis that pretrained large language models (LLMs) necessitate only\nminimal supervision during the fine-tuning (SFT) stage (Zhou et al., 2024) has\nbeen substantiated by recent advancements in data curation and selection\nresearch. However, their stability and generalizability are compromised due to\nthe vulnerability to experimental setups and validation protocols, falling\nshort of surpassing random sampling (Diddee & Ippolito, 2024; Xia et al.,\n2024b). Built upon LLMs, multi-modal LLMs (MLLMs), combined with the sheer\ntoken volume and heightened heterogeneity of data sources, amplify both the\nsignificance and complexity of data selection.\n  To harvest multi-modal instructional data in a robust and efficient manner,\nwe re-define the granularity of the quality metric by decomposing it into 14\nvision-language-related capabilities, and introduce multi-modal rich scorers to\nevaluate the capabilities of each data candidate. To promote diversity, in\nlight of the inherent objective of the alignment stage, we take interaction\nstyle as diversity indicator and use a multi-modal rich styler to identify data\ninstruction patterns. In doing so, our multi-modal rich scorers and styler\n(mmSSR) guarantee that high-scoring information is conveyed to users in\ndiversified forms. Free from embedding-based clustering or greedy sampling,\nmmSSR efficiently scales to millions of data with varying budget constraints,\nsupports customization for general or specific capability acquisition, and\nfacilitates training-free generalization to new domains for curation. Across\n10+ experimental settings, validated by 14 multi-modal benchmarks, we\ndemonstrate consistent improvements over random sampling, baseline strategies\nand state-of-the-art selection methods, achieving 99.1% of full performance\nwith only 30% of the 2.6M data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The hypothesis that pretrained large language models (LLMs) necessitate only\nminimal supervision during the fine-tuning (SFT) stage (Zhou et al., 2024) has\nbeen substantiated by recent advancements in data curation and selection\nresearch. However, their stability and generalizability are compromised due to\nthe vulnerability to experimental setups and validation protocols, falling\nshort of surpassing random sampling (Diddee & Ippolito, 2024; Xia et al.,\n2024b). Built upon LLMs, multi-modal LLMs (MLLMs), combined with the sheer\ntoken volume and heightened heterogeneity of data sources, amplify both the\nsignificance and complexity of data selection.\n  To harvest multi-modal instructional data in a robust and efficient manner,\nwe re-define the granularity of the quality metric by decomposing it into 14\nvision-language-related capabilities, and introduce multi-modal rich scorers to\nevaluate the capabilities of each data candidate. To promote diversity, in\nlight of the inherent objective of the alignment stage, we take interaction\nstyle as diversity indicator and use a multi-modal rich styler to identify data\ninstruction patterns. In doing so, our multi-modal rich scorers and styler\n(mmSSR) guarantee that high-scoring information is conveyed to users in\ndiversified forms. Free from embedding-based clustering or greedy sampling,\nmmSSR efficiently scales to millions of data with varying budget constraints,\nsupports customization for general or specific capability acquisition, and\nfacilitates training-free generalization to new domains for curation. Across\n10+ experimental settings, validated by 14 multi-modal benchmarks, we\ndemonstrate consistent improvements over random sampling, baseline strategies\nand state-of-the-art selection methods, achieving 99.1% of full performance\nwith only 30% of the 2.6M data."
                },
                "authors": [
                    {
                        "name": "Mengyao Lyu"
                    },
                    {
                        "name": "Yan Li"
                    },
                    {
                        "name": "Huasong Zhong"
                    },
                    {
                        "name": "Wenhao Yang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    },
                    {
                        "name": "Zhenheng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zhenheng Yang"
                },
                "author": "Zhenheng Yang",
                "arxiv_comment": "update comparison with sota and analysis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08154v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08154v2",
                "updated": "2025-03-17T16:50:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    50,
                    29,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-11T08:10:03Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    8,
                    10,
                    3,
                    1,
                    70,
                    0
                ],
                "title": "Structure-Activation Synergy: A Dual Efficiency Framework for\n  Parameter-Memory Optimized Transfer Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structure-Activation Synergy: A Dual Efficiency Framework for\n  Parameter-Memory Optimized Transfer Learning"
                },
                "summary": "While parameter-efficient transfer learning (PETL) successfully reduces\ntrainable parameters for adapting large pre-trained models, conventional\nmethods exhibit limited effectiveness in decreasing activation memory\nconsumption - a critical bottleneck for deployment on resource-constrained\ndevices. We present Structure-Activation Synergy (S2A), an innovative framework\nachieving dual optimization of parameters and memory through two synergistic\nmechanisms: (1) Structural activation modules (bias/prompt/side adaptations)\nthat strategically minimize both parametric complexity and intermediate feature\nstorage requirements, and (2) Derivative-aware 4-bit quantization for\nnon-parametric operators that maintains model fidelity through\ngradient-informed precision allocation. Extensive evaluations across multiple\narchitectures (ViT, Swin, ResNet) and datasets (ImageNet-1K, CIFAR, DomainNet)\ndemonstrate S2A's superior efficiency, reducing GPU memory consumption by 75\\%\n(4.2 average reduction) while maintaining 98.7\\% of full fine-tuning accuracy\nwith only 0.9\\% tunable parameters. This hardware-aware paradigm establishes\nnew state-of-the-art in efficient model adaptation, offering practical\ndeployment advantages through simultaneous parameter and memory optimization\nwithout compromising model capability",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While parameter-efficient transfer learning (PETL) successfully reduces\ntrainable parameters for adapting large pre-trained models, conventional\nmethods exhibit limited effectiveness in decreasing activation memory\nconsumption - a critical bottleneck for deployment on resource-constrained\ndevices. We present Structure-Activation Synergy (S2A), an innovative framework\nachieving dual optimization of parameters and memory through two synergistic\nmechanisms: (1) Structural activation modules (bias/prompt/side adaptations)\nthat strategically minimize both parametric complexity and intermediate feature\nstorage requirements, and (2) Derivative-aware 4-bit quantization for\nnon-parametric operators that maintains model fidelity through\ngradient-informed precision allocation. Extensive evaluations across multiple\narchitectures (ViT, Swin, ResNet) and datasets (ImageNet-1K, CIFAR, DomainNet)\ndemonstrate S2A's superior efficiency, reducing GPU memory consumption by 75\\%\n(4.2 average reduction) while maintaining 98.7\\% of full fine-tuning accuracy\nwith only 0.9\\% tunable parameters. This hardware-aware paradigm establishes\nnew state-of-the-art in efficient model adaptation, offering practical\ndeployment advantages through simultaneous parameter and memory optimization\nwithout compromising model capability"
                },
                "authors": [
                    {
                        "name": "Tian Jin"
                    },
                    {
                        "name": "Enjun Du"
                    },
                    {
                        "name": "Changwei Wang"
                    },
                    {
                        "name": "Wenhao Xu"
                    },
                    {
                        "name": "Ding Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ding Luo"
                },
                "author": "Ding Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08154v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08154v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13360v1",
                "updated": "2025-03-17T16:45:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    45,
                    12,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T16:45:12Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    45,
                    12,
                    0,
                    76,
                    0
                ],
                "title": "Mitigating Visual Forgetting via Take-along Visual Conditioning for\n  Multi-modal Long CoT Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Visual Forgetting via Take-along Visual Conditioning for\n  Multi-modal Long CoT Reasoning"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nenhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting\nto advanced, product-oriented solutions like OpenAI o1. During our\nre-implementation of this model, we noticed that in multimodal tasks requiring\nvisual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to\nmaintain focus on the visual information, in other words, MLLMs suffer from a\ngradual decline in attention to visual information as reasoning progresses,\ncausing text-over-relied outputs. To investigate this, we ablate image inputs\nduring long-chain reasoning. Concretely, we truncate the reasoning process\nmidway, then re-complete the reasoning process with the input image removed. We\nobserve only a ~2% accuracy drop on MathVista's test-hard subset, revealing the\nmodel's textual outputs dominate the following reasoning process. Motivated by\nthis, we propose Take-along Visual Conditioning (TVC), a strategy that shifts\nimage input to critical reasoning stages and compresses redundant visual tokens\nvia dynamic pruning. This methodology helps the model retain attention to the\nvisual components throughout the reasoning. Our approach achieves\nstate-of-the-art performance on average across five mathematical reasoning\nbenchmarks (+3.4% vs previous sota), demonstrating the effectiveness of TVC in\nenhancing multimodal reasoning systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have demonstrated\nenhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting\nto advanced, product-oriented solutions like OpenAI o1. During our\nre-implementation of this model, we noticed that in multimodal tasks requiring\nvisual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to\nmaintain focus on the visual information, in other words, MLLMs suffer from a\ngradual decline in attention to visual information as reasoning progresses,\ncausing text-over-relied outputs. To investigate this, we ablate image inputs\nduring long-chain reasoning. Concretely, we truncate the reasoning process\nmidway, then re-complete the reasoning process with the input image removed. We\nobserve only a ~2% accuracy drop on MathVista's test-hard subset, revealing the\nmodel's textual outputs dominate the following reasoning process. Motivated by\nthis, we propose Take-along Visual Conditioning (TVC), a strategy that shifts\nimage input to critical reasoning stages and compresses redundant visual tokens\nvia dynamic pruning. This methodology helps the model retain attention to the\nvisual components throughout the reasoning. Our approach achieves\nstate-of-the-art performance on average across five mathematical reasoning\nbenchmarks (+3.4% vs previous sota), demonstrating the effectiveness of TVC in\nenhancing multimodal reasoning systems."
                },
                "authors": [
                    {
                        "name": "Hai-Long Sun"
                    },
                    {
                        "name": "Zhun Sun"
                    },
                    {
                        "name": "Houwen Peng"
                    },
                    {
                        "name": "Han-Jia Ye"
                    }
                ],
                "author_detail": {
                    "name": "Han-Jia Ye"
                },
                "author": "Han-Jia Ye",
                "arxiv_comment": "The project page is available at\n  https://sun-hailong.github.io/projects/TVC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13356v1",
                "updated": "2025-03-17T16:42:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    42,
                    34,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T16:42:34Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    42,
                    34,
                    0,
                    76,
                    0
                ],
                "title": "Agents Play Thousands of 3D Video Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents Play Thousands of 3D Video Games"
                },
                "summary": "We present PORTAL, a novel framework for developing artificial intelligence\nagents capable of playing thousands of 3D video games through language-guided\npolicy generation. By transforming decision-making problems into language\nmodeling tasks, our approach leverages large language models (LLMs) to generate\nbehavior trees represented in domain-specific language (DSL). This method\neliminates the computational burden associated with traditional reinforcement\nlearning approaches while preserving strategic depth and rapid adaptability.\nOur framework introduces a hybrid policy structure that combines rule-based\nnodes with neural network components, enabling both high-level strategic\nreasoning and precise low-level control. A dual-feedback mechanism\nincorporating quantitative game metrics and vision-language model analysis\nfacilitates iterative policy improvement at both tactical and strategic levels.\nThe resulting policies are instantaneously deployable, human-interpretable, and\ncapable of generalizing across diverse gaming environments. Experimental\nresults demonstrate PORTAL's effectiveness across thousands of first-person\nshooter (FPS) games, showcasing significant improvements in development\nefficiency, policy generalization, and behavior diversity compared to\ntraditional approaches. PORTAL represents a significant advancement in game AI\ndevelopment, offering a practical solution for creating sophisticated agents\nthat can operate across thousands of commercial video games with minimal\ndevelopment overhead. Experiment results on the 3D video games are best viewed\non https://zhongwen.one/projects/portal .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present PORTAL, a novel framework for developing artificial intelligence\nagents capable of playing thousands of 3D video games through language-guided\npolicy generation. By transforming decision-making problems into language\nmodeling tasks, our approach leverages large language models (LLMs) to generate\nbehavior trees represented in domain-specific language (DSL). This method\neliminates the computational burden associated with traditional reinforcement\nlearning approaches while preserving strategic depth and rapid adaptability.\nOur framework introduces a hybrid policy structure that combines rule-based\nnodes with neural network components, enabling both high-level strategic\nreasoning and precise low-level control. A dual-feedback mechanism\nincorporating quantitative game metrics and vision-language model analysis\nfacilitates iterative policy improvement at both tactical and strategic levels.\nThe resulting policies are instantaneously deployable, human-interpretable, and\ncapable of generalizing across diverse gaming environments. Experimental\nresults demonstrate PORTAL's effectiveness across thousands of first-person\nshooter (FPS) games, showcasing significant improvements in development\nefficiency, policy generalization, and behavior diversity compared to\ntraditional approaches. PORTAL represents a significant advancement in game AI\ndevelopment, offering a practical solution for creating sophisticated agents\nthat can operate across thousands of commercial video games with minimal\ndevelopment overhead. Experiment results on the 3D video games are best viewed\non https://zhongwen.one/projects/portal ."
                },
                "authors": [
                    {
                        "name": "Zhongwen Xu"
                    },
                    {
                        "name": "Xianliang Wang"
                    },
                    {
                        "name": "Siyi Li"
                    },
                    {
                        "name": "Tao Yu"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Qiang Fu"
                    },
                    {
                        "name": "Wei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Yang"
                },
                "author": "Wei Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01162v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01162v2",
                "updated": "2025-03-17T16:42:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    42,
                    33,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-03T04:23:24Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    4,
                    23,
                    24,
                    0,
                    62,
                    0
                ],
                "title": "CogSys: Efficient and Scalable Neurosymbolic Cognition System via\n  Algorithm-Hardware Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CogSys: Efficient and Scalable Neurosymbolic Cognition System via\n  Algorithm-Hardware Co-Design"
                },
                "summary": "Neurosymbolic AI is an emerging compositional paradigm that fuses neural\nlearning with symbolic reasoning to enhance the transparency, interpretability,\nand trustworthiness of AI. It also exhibits higher data efficiency making it\npromising for edge deployments. Despite the algorithmic promises and\ndemonstrations, unfortunately executing neurosymbolic workloads on current\nhardware (CPU/GPU/TPU) is challenging due to higher memory intensity, greater\ncompute heterogeneity and access pattern irregularity, leading to severe\nhardware underutilization.\n  This work proposes CogSys, a characterization and co-design framework\ndedicated to neurosymbolic AI system acceleration, aiming to win both reasoning\nefficiency and scalability. On the algorithm side, CogSys proposes an efficient\nfactorization technique to alleviate compute and memory overhead. On the\nhardware side, CogSys proposes a scalable neurosymbolic architecture with\nreconfigurable neuro/symbolic processing elements (nsPE) and bubble streaming\n(BS) dataflow with spatial-temporal (ST) mapping for highly parallel and\nefficient neurosymbolic computation. On the system side, CogSys features an\nadaptive workload-aware scheduler (adSCH) to orchestrate heterogeneous kernels\nand enhance resource utilization. Evaluated across cognitive workloads, CogSys\nenables reconfigurable support for neural and symbolic kernels and exhibits\n>75x speedup over TPU-like systolic array with only <5% area overhead, as\nbenchmarked under the TSMC 28nm technology node. CogSys achieves 4x-96x speedup\ncompared to desktop and edge GPUs. For the first time, CogSys enables real-time\nabduction reasoning towards human fluid intelligence, requiring only 0.3 s per\nreasoning task with 4 mm2 area and 1.48 W power consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neurosymbolic AI is an emerging compositional paradigm that fuses neural\nlearning with symbolic reasoning to enhance the transparency, interpretability,\nand trustworthiness of AI. It also exhibits higher data efficiency making it\npromising for edge deployments. Despite the algorithmic promises and\ndemonstrations, unfortunately executing neurosymbolic workloads on current\nhardware (CPU/GPU/TPU) is challenging due to higher memory intensity, greater\ncompute heterogeneity and access pattern irregularity, leading to severe\nhardware underutilization.\n  This work proposes CogSys, a characterization and co-design framework\ndedicated to neurosymbolic AI system acceleration, aiming to win both reasoning\nefficiency and scalability. On the algorithm side, CogSys proposes an efficient\nfactorization technique to alleviate compute and memory overhead. On the\nhardware side, CogSys proposes a scalable neurosymbolic architecture with\nreconfigurable neuro/symbolic processing elements (nsPE) and bubble streaming\n(BS) dataflow with spatial-temporal (ST) mapping for highly parallel and\nefficient neurosymbolic computation. On the system side, CogSys features an\nadaptive workload-aware scheduler (adSCH) to orchestrate heterogeneous kernels\nand enhance resource utilization. Evaluated across cognitive workloads, CogSys\nenables reconfigurable support for neural and symbolic kernels and exhibits\n>75x speedup over TPU-like systolic array with only <5% area overhead, as\nbenchmarked under the TSMC 28nm technology node. CogSys achieves 4x-96x speedup\ncompared to desktop and edge GPUs. For the first time, CogSys enables real-time\nabduction reasoning towards human fluid intelligence, requiring only 0.3 s per\nreasoning task with 4 mm2 area and 1.48 W power consumption."
                },
                "authors": [
                    {
                        "name": "Zishen Wan"
                    },
                    {
                        "name": "Hanchen Yang"
                    },
                    {
                        "name": "Ritik Raj"
                    },
                    {
                        "name": "Che-Kai Liu"
                    },
                    {
                        "name": "Ananda Samajdar"
                    },
                    {
                        "name": "Arijit Raychowdhury"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "arxiv_comment": "2025 IEEE International Symposium on High-Performance Computer\n  Architecture (HPCA), 15 pages, 19 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01162v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01162v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13340v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13340v1",
                "updated": "2025-03-17T16:18:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    18,
                    23,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T16:18:23Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    18,
                    23,
                    0,
                    76,
                    0
                ],
                "title": "LearnMate: Enhancing Online Education with LLM-Powered Personalized\n  Learning Plans and Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LearnMate: Enhancing Online Education with LLM-Powered Personalized\n  Learning Plans and Support"
                },
                "summary": "With the increasing prevalence of online learning, adapting education to\ndiverse learner needs remains a persistent challenge. Recent advancements in\nartificial intelligence (AI), particularly large language models (LLMs),\npromise powerful tools and capabilities to enhance personalized learning in\nonline educational environments. In this work, we explore how LLMs can improve\npersonalized learning experiences by catering to individual user needs toward\nenhancing the overall quality of online education. We designed personalization\nguidelines based on the growing literature on personalized learning to ground\nLLMs in generating tailored learning plans. To operationalize these guidelines,\nwe implemented LearnMate, an LLM-based system that generates personalized\nlearning plans and provides users with real-time learning support. We discuss\nthe implications and future directions of this work, aiming to move beyond the\ntraditional one-size-fits-all approach by integrating LLM-based personalized\nsupport into online learning environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing prevalence of online learning, adapting education to\ndiverse learner needs remains a persistent challenge. Recent advancements in\nartificial intelligence (AI), particularly large language models (LLMs),\npromise powerful tools and capabilities to enhance personalized learning in\nonline educational environments. In this work, we explore how LLMs can improve\npersonalized learning experiences by catering to individual user needs toward\nenhancing the overall quality of online education. We designed personalization\nguidelines based on the growing literature on personalized learning to ground\nLLMs in generating tailored learning plans. To operationalize these guidelines,\nwe implemented LearnMate, an LLM-based system that generates personalized\nlearning plans and provides users with real-time learning support. We discuss\nthe implications and future directions of this work, aiming to move beyond the\ntraditional one-size-fits-all approach by integrating LLM-based personalized\nsupport into online learning environments."
                },
                "authors": [
                    {
                        "name": "Xinyu Jessica Wang"
                    },
                    {
                        "name": "Christine Lee"
                    },
                    {
                        "name": "Bilge Mutlu"
                    }
                ],
                "author_detail": {
                    "name": "Bilge Mutlu"
                },
                "author": "Bilge Mutlu",
                "arxiv_doi": "10.1145/3706599.3719857",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706599.3719857",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.13340v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13340v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Extended Abstracts of the CHI Conference on Human Factors in\n  Computing Systems (CHI EA '25), April 26-May 1, 2025, Yokohama, Japan",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13335v1",
                "updated": "2025-03-17T16:15:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    15,
                    2,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T16:15:02Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    15,
                    2,
                    0,
                    76,
                    0
                ],
                "title": "Reliable and Efficient Amortized Model-based Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable and Efficient Amortized Model-based Evaluation"
                },
                "summary": "Comprehensive evaluations of language models (LM) during both development and\ndeployment phases are necessary because these models possess numerous\ncapabilities (e.g., mathematical reasoning, legal support, or medical\ndiagnostic) as well as safety risks (e.g., racial bias, toxicity, or\nmisinformation). The average score across a wide range of benchmarks provides a\nsignal that helps guide the use of these LMs in practice. Currently, holistic\nevaluations are costly due to the large volume of benchmark questions, making\nfrequent evaluations impractical. A popular attempt to lower the cost is to\ncompute the average score on a subset of the benchmark. This approach,\nunfortunately, often renders an unreliable measure of LM performance because\nthe average score is often confounded with the difficulty of the questions in\nthe benchmark subset. Item response theory (IRT) was designed to address this\nchallenge, providing a reliable measurement by careful controlling for question\ndifficulty. Unfortunately, question difficulty is expensive to estimate. Facing\nthis challenge, we train a model that predicts question difficulty from its\ncontent, enabling a reliable measurement at a fraction of the cost. In\naddition, we leverage this difficulty predictor to further improve the\nevaluation efficiency through training a question generator given a difficulty\nlevel. This question generator is essential in adaptive testing, where, instead\nof using a random subset of the benchmark questions, informative questions are\nadaptively chosen based on the current estimation of LLM performance.\nExperiments on 22 common natural language benchmarks and 172 LMs show that this\napproach is more reliable and efficient compared to current common practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehensive evaluations of language models (LM) during both development and\ndeployment phases are necessary because these models possess numerous\ncapabilities (e.g., mathematical reasoning, legal support, or medical\ndiagnostic) as well as safety risks (e.g., racial bias, toxicity, or\nmisinformation). The average score across a wide range of benchmarks provides a\nsignal that helps guide the use of these LMs in practice. Currently, holistic\nevaluations are costly due to the large volume of benchmark questions, making\nfrequent evaluations impractical. A popular attempt to lower the cost is to\ncompute the average score on a subset of the benchmark. This approach,\nunfortunately, often renders an unreliable measure of LM performance because\nthe average score is often confounded with the difficulty of the questions in\nthe benchmark subset. Item response theory (IRT) was designed to address this\nchallenge, providing a reliable measurement by careful controlling for question\ndifficulty. Unfortunately, question difficulty is expensive to estimate. Facing\nthis challenge, we train a model that predicts question difficulty from its\ncontent, enabling a reliable measurement at a fraction of the cost. In\naddition, we leverage this difficulty predictor to further improve the\nevaluation efficiency through training a question generator given a difficulty\nlevel. This question generator is essential in adaptive testing, where, instead\nof using a random subset of the benchmark questions, informative questions are\nadaptively chosen based on the current estimation of LLM performance.\nExperiments on 22 common natural language benchmarks and 172 LMs show that this\napproach is more reliable and efficient compared to current common practice."
                },
                "authors": [
                    {
                        "name": "Sang Truong"
                    },
                    {
                        "name": "Yuheng Tu"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Sanmi Koyejo"
                    }
                ],
                "author_detail": {
                    "name": "Sanmi Koyejo"
                },
                "author": "Sanmi Koyejo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13330v1",
                "updated": "2025-03-17T16:09:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    9,
                    22,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T16:09:22Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    9,
                    22,
                    0,
                    76,
                    0
                ],
                "title": "LEAVS: An LLM-based Labeler for Abdominal CT Supervision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEAVS: An LLM-based Labeler for Abdominal CT Supervision"
                },
                "summary": "Extracting structured labels from radiology reports has been employed to\ncreate vision models to simultaneously detect several types of abnormalities.\nHowever, existing works focus mainly on the chest region. Few works have been\ninvestigated on abdominal radiology reports due to more complex anatomy and a\nwider range of pathologies in the abdomen. We propose LEAVS (Large language\nmodel Extractor for Abdominal Vision Supervision). This labeler can annotate\nthe certainty of presence and the urgency of seven types of abnormalities for\nnine abdominal organs on CT radiology reports. To ensure broad coverage, we\nchose abnormalities that encompass most of the finding types from CT reports.\nOur approach employs a specialized chain-of-thought prompting strategy for a\nlocally-run LLM using sentence extraction and multiple-choice questions in a\ntree-based decision system. We demonstrate that the LLM can extract several\nabnormality types across abdominal organs with an average F1 score of 0.89,\nsignificantly outperforming competing labelers and humans. Additionally, we\nshow that extraction of urgency labels achieved performance comparable to human\nannotations. Finally, we demonstrate that the abnormality labels contain\nvaluable information for training a single vision model that classifies several\norgans as normal or abnormal. We release our code and structured annotations\nfor a public CT dataset containing over 1,000 CT volumes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting structured labels from radiology reports has been employed to\ncreate vision models to simultaneously detect several types of abnormalities.\nHowever, existing works focus mainly on the chest region. Few works have been\ninvestigated on abdominal radiology reports due to more complex anatomy and a\nwider range of pathologies in the abdomen. We propose LEAVS (Large language\nmodel Extractor for Abdominal Vision Supervision). This labeler can annotate\nthe certainty of presence and the urgency of seven types of abnormalities for\nnine abdominal organs on CT radiology reports. To ensure broad coverage, we\nchose abnormalities that encompass most of the finding types from CT reports.\nOur approach employs a specialized chain-of-thought prompting strategy for a\nlocally-run LLM using sentence extraction and multiple-choice questions in a\ntree-based decision system. We demonstrate that the LLM can extract several\nabnormality types across abdominal organs with an average F1 score of 0.89,\nsignificantly outperforming competing labelers and humans. Additionally, we\nshow that extraction of urgency labels achieved performance comparable to human\nannotations. Finally, we demonstrate that the abnormality labels contain\nvaluable information for training a single vision model that classifies several\norgans as normal or abnormal. We release our code and structured annotations\nfor a public CT dataset containing over 1,000 CT volumes."
                },
                "authors": [
                    {
                        "name": "Ricardo Bigolin Lanfredi"
                    },
                    {
                        "name": "Yan Zhuang"
                    },
                    {
                        "name": "Mark Finkelstein"
                    },
                    {
                        "name": "Praveen Thoppey Srinivasan Balamuralikrishna"
                    },
                    {
                        "name": "Luke Krembs"
                    },
                    {
                        "name": "Brandon Khoury"
                    },
                    {
                        "name": "Arthi Reddy"
                    },
                    {
                        "name": "Pritam Mukherjee"
                    },
                    {
                        "name": "Neil M. Rofsky"
                    },
                    {
                        "name": "Ronald M. Summers"
                    }
                ],
                "author_detail": {
                    "name": "Ronald M. Summers"
                },
                "author": "Ronald M. Summers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13817v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13817v3",
                "updated": "2025-03-17T16:05:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    5,
                    34,
                    0,
                    76,
                    0
                ],
                "published": "2024-12-18T13:04:30Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    4,
                    30,
                    2,
                    353,
                    0
                ],
                "title": "Nullu: Mitigating Object Hallucinations in Large Vision-Language Models\n  via HalluSpace Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nullu: Mitigating Object Hallucinations in Large Vision-Language Models\n  via HalluSpace Projection"
                },
                "summary": "Recent studies have shown that large vision-language models (LVLMs) often\nsuffer from the issue of object hallucinations (OH). To mitigate this issue, we\nintroduce an efficient method that edits the model weights based on an unsafe\nsubspace, which we call HalluSpace in this paper. With truthful and\nhallucinated text prompts accompanying the visual content as inputs, the\nHalluSpace can be identified by extracting the hallucinated embedding features\nand removing the truthful representations in LVLMs. By orthogonalizing the\nmodel weights, input features will be projected into the Null space of the\nHalluSpace to reduce OH, based on which we name our method Nullu. We reveal\nthat HalluSpaces generally contain prior information in the large language\nmodels (LLMs) applied to build LVLMs, which have been shown as essential causes\nof OH in previous studies. Therefore, null space projection suppresses the\nLLMs' priors to filter out the hallucinated features, resulting in contextually\naccurate outputs. Experiments show that our method can effectively mitigate OH\nacross different LVLM families without extra inference costs and also show\nstrong performance in general LVLM benchmarks. Code is released at\nhttps://github.com/Ziwei-Zheng/Nullu.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have shown that large vision-language models (LVLMs) often\nsuffer from the issue of object hallucinations (OH). To mitigate this issue, we\nintroduce an efficient method that edits the model weights based on an unsafe\nsubspace, which we call HalluSpace in this paper. With truthful and\nhallucinated text prompts accompanying the visual content as inputs, the\nHalluSpace can be identified by extracting the hallucinated embedding features\nand removing the truthful representations in LVLMs. By orthogonalizing the\nmodel weights, input features will be projected into the Null space of the\nHalluSpace to reduce OH, based on which we name our method Nullu. We reveal\nthat HalluSpaces generally contain prior information in the large language\nmodels (LLMs) applied to build LVLMs, which have been shown as essential causes\nof OH in previous studies. Therefore, null space projection suppresses the\nLLMs' priors to filter out the hallucinated features, resulting in contextually\naccurate outputs. Experiments show that our method can effectively mitigate OH\nacross different LVLM families without extra inference costs and also show\nstrong performance in general LVLM benchmarks. Code is released at\nhttps://github.com/Ziwei-Zheng/Nullu."
                },
                "authors": [
                    {
                        "name": "Le Yang"
                    },
                    {
                        "name": "Ziwei Zheng"
                    },
                    {
                        "name": "Boxu Chen"
                    },
                    {
                        "name": "Zhengyu Zhao"
                    },
                    {
                        "name": "Chenhao Lin"
                    },
                    {
                        "name": "Chao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chao Shen"
                },
                "author": "Chao Shen",
                "arxiv_comment": "CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13817v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13817v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13322v1",
                "updated": "2025-03-17T15:59:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    59,
                    20,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T15:59:20Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    59,
                    20,
                    0,
                    76,
                    0
                ],
                "title": "SMPR: A structure-enhanced multimodal drug-disease prediction model for\n  drug repositioning and cold start",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMPR: A structure-enhanced multimodal drug-disease prediction model for\n  drug repositioning and cold start"
                },
                "summary": "Repositioning drug-disease relationships has always been a hot field of\nresearch. However, actual cases of biologically validated drug relocation\nremain very limited, and existing models have not yet fully utilized the\nstructural information of the drug. Furthermore, most repositioning models are\nonly used to complete the relationship matrix, and their practicality is poor\nwhen dealing with drug cold start problems. This paper proposes a\nstructure-enhanced multimodal relationship prediction model (SMRP). SMPR is\nbased on the SMILE structure of the drug, using the Mol2VEC method to generate\ndrug embedded representations, and learn disease embedded representations\nthrough heterogeneous network graph neural networks. Ultimately, a drug-disease\nrelationship matrix is constructed. In addition, to reduce the difficulty of\nusers' use, SMPR also provides a cold start interface based on structural\nsimilarity based on reposition results to simply and quickly predict\ndrug-related diseases. The repositioning ability and cold start capability of\nthe model are verified from multiple perspectives. While the AUC and ACUPR\nscores of repositioning reach 99% and 61% respectively, the AUC of cold start\nachieve 80%. In particular, the cold start Recall indicator can reach more than\n70%, which means that SMPR is more sensitive to positive samples. Finally, case\nanalysis is used to verify the practical value of the model and visual analysis\ndirectly demonstrates the improvement of the structure to the model. For quick\nuse, we also provide local deployment of the model and package it into an\nexecutable program.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repositioning drug-disease relationships has always been a hot field of\nresearch. However, actual cases of biologically validated drug relocation\nremain very limited, and existing models have not yet fully utilized the\nstructural information of the drug. Furthermore, most repositioning models are\nonly used to complete the relationship matrix, and their practicality is poor\nwhen dealing with drug cold start problems. This paper proposes a\nstructure-enhanced multimodal relationship prediction model (SMRP). SMPR is\nbased on the SMILE structure of the drug, using the Mol2VEC method to generate\ndrug embedded representations, and learn disease embedded representations\nthrough heterogeneous network graph neural networks. Ultimately, a drug-disease\nrelationship matrix is constructed. In addition, to reduce the difficulty of\nusers' use, SMPR also provides a cold start interface based on structural\nsimilarity based on reposition results to simply and quickly predict\ndrug-related diseases. The repositioning ability and cold start capability of\nthe model are verified from multiple perspectives. While the AUC and ACUPR\nscores of repositioning reach 99% and 61% respectively, the AUC of cold start\nachieve 80%. In particular, the cold start Recall indicator can reach more than\n70%, which means that SMPR is more sensitive to positive samples. Finally, case\nanalysis is used to verify the practical value of the model and visual analysis\ndirectly demonstrates the improvement of the structure to the model. For quick\nuse, we also provide local deployment of the model and package it into an\nexecutable program."
                },
                "authors": [
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Rui Miao"
                    },
                    {
                        "name": "Suyan Zhang"
                    },
                    {
                        "name": "Shuaibing Jia"
                    },
                    {
                        "name": "Leifeng Zhang"
                    },
                    {
                        "name": "Yong Liang"
                    },
                    {
                        "name": "Jianhua Zhang"
                    },
                    {
                        "name": "Yi Zhun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zhun Zhu"
                },
                "author": "Yi Zhun Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.04928v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.04928v3",
                "updated": "2025-03-17T15:50:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    50,
                    13,
                    0,
                    76,
                    0
                ],
                "published": "2023-11-03T18:27:21Z",
                "published_parsed": [
                    2023,
                    11,
                    3,
                    18,
                    27,
                    21,
                    4,
                    307,
                    0
                ],
                "title": "Leveraging Large Language Models for Collective Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Collective Decision-Making"
                },
                "summary": "In various work contexts, such as meeting scheduling, collaborating, and\nproject planning, collective decision-making is essential but often challenging\ndue to diverse individual preferences, varying work focuses, and power dynamics\namong members. To address this, we propose a system leveraging Large Language\nModels (LLMs) to facilitate group decision-making by managing conversations and\nbalancing preferences among individuals. Our system aims to extract individual\npreferences from each member's conversation with the system and suggest options\nthat satisfy the preferences of the members. We specifically apply this system\nto corporate meeting scheduling. We create synthetic employee profiles and\nsimulate conversations at scale, leveraging LLMs to evaluate the system\nperformance as a novel approach to conducting a user study. Our results\nindicate efficient coordination with reduced interactions between the members\nand the LLM-based system. The system refines and improves its proposed options\nover time, ensuring that many of the members' individual preferences are\nsatisfied in an equitable way. Finally, we conduct a survey study involving\nhuman participants to assess our system's ability to aggregate preferences and\nreasoning about them. Our findings show that the system exhibits strong\nperformance in both dimensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In various work contexts, such as meeting scheduling, collaborating, and\nproject planning, collective decision-making is essential but often challenging\ndue to diverse individual preferences, varying work focuses, and power dynamics\namong members. To address this, we propose a system leveraging Large Language\nModels (LLMs) to facilitate group decision-making by managing conversations and\nbalancing preferences among individuals. Our system aims to extract individual\npreferences from each member's conversation with the system and suggest options\nthat satisfy the preferences of the members. We specifically apply this system\nto corporate meeting scheduling. We create synthetic employee profiles and\nsimulate conversations at scale, leveraging LLMs to evaluate the system\nperformance as a novel approach to conducting a user study. Our results\nindicate efficient coordination with reduced interactions between the members\nand the LLM-based system. The system refines and improves its proposed options\nover time, ensuring that many of the members' individual preferences are\nsatisfied in an equitable way. Finally, we conduct a survey study involving\nhuman participants to assess our system's ability to aggregate preferences and\nreasoning about them. Our findings show that the system exhibits strong\nperformance in both dimensions."
                },
                "authors": [
                    {
                        "name": "Marios Papachristou"
                    },
                    {
                        "name": "Longqi Yang"
                    },
                    {
                        "name": "Chin-Chia Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Chin-Chia Hsu"
                },
                "author": "Chin-Chia Hsu",
                "arxiv_comment": "To appear at ACM CSCW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.04928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.04928v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13305v1",
                "updated": "2025-03-17T15:47:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    47,
                    37,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T15:47:37Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    47,
                    37,
                    0,
                    76,
                    0
                ],
                "title": "Computation Mechanism Behind LLM Position Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computation Mechanism Behind LLM Position Generalization"
                },
                "summary": "Most written natural languages are composed of sequences of words and\nsentences. Similar to humans, large language models (LLMs) exhibit flexibility\nin handling textual positions - a phenomenon we term position generalization.\nThey can understand texts with position perturbations and generalize to longer\ntexts than those encountered during training with the latest techniques. These\nphenomena suggest that LLMs handle positions tolerantly, but how LLMs\ncomputationally process positional relevance remains largely unexplored. This\nwork connects the linguistic phenomenon with LLMs' computational mechanisms. We\nshow how LLMs enforce certain computational mechanisms for the aforementioned\ntolerance in position perturbations. Despite the complex design of the\nself-attention mechanism, this work reveals that LLMs learn a counterintuitive\ndisentanglement of attention logits. Their values show a 0.959 linear\ncorrelation with an approximation of the arithmetic sum of positional relevance\nand semantic importance. Furthermore, we identify a prevalent pattern in\nintermediate features, which we prove theoretically enables this effect. The\npattern, which is different from how randomly initialized parameters would\nbehave, suggests that it is a learned behavior rather than a natural result of\nthe model architecture. Based on these findings, we provide computational\nexplanations and criteria for LLMs' position flexibilities. This work takes a\npioneering step in linking position generalization with modern LLMs' internal\nmechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most written natural languages are composed of sequences of words and\nsentences. Similar to humans, large language models (LLMs) exhibit flexibility\nin handling textual positions - a phenomenon we term position generalization.\nThey can understand texts with position perturbations and generalize to longer\ntexts than those encountered during training with the latest techniques. These\nphenomena suggest that LLMs handle positions tolerantly, but how LLMs\ncomputationally process positional relevance remains largely unexplored. This\nwork connects the linguistic phenomenon with LLMs' computational mechanisms. We\nshow how LLMs enforce certain computational mechanisms for the aforementioned\ntolerance in position perturbations. Despite the complex design of the\nself-attention mechanism, this work reveals that LLMs learn a counterintuitive\ndisentanglement of attention logits. Their values show a 0.959 linear\ncorrelation with an approximation of the arithmetic sum of positional relevance\nand semantic importance. Furthermore, we identify a prevalent pattern in\nintermediate features, which we prove theoretically enables this effect. The\npattern, which is different from how randomly initialized parameters would\nbehave, suggests that it is a learned behavior rather than a natural result of\nthe model architecture. Based on these findings, we provide computational\nexplanations and criteria for LLMs' position flexibilities. This work takes a\npioneering step in linking position generalization with modern LLMs' internal\nmechanisms."
                },
                "authors": [
                    {
                        "name": "Chi Han"
                    },
                    {
                        "name": "Heng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Heng Ji"
                },
                "author": "Heng Ji",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13301v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13301v1",
                "updated": "2025-03-17T15:45:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    45,
                    17,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T15:45:17Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    45,
                    17,
                    0,
                    76,
                    0
                ],
                "title": "LIMCA: LLM for Automating Analog In-Memory Computing Architecture Design\n  Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LIMCA: LLM for Automating Analog In-Memory Computing Architecture Design\n  Exploration"
                },
                "summary": "Resistive crossbars enabling analog In-Memory Computing (IMC) have emerged as\na promising architecture for Deep Neural Network (DNN) acceleration, offering\nhigh memory bandwidth and in-situ computation. However, the manual,\nknowledge-intensive design process and the lack of high-quality circuit\nnetlists have significantly constrained design space exploration and\noptimization to behavioral system-level tools. In this work, we introduce\nLIMCA, a novel fine-tune-free Large Language Model (LLM)-driven framework for\nautomating the design and evaluation of IMC crossbar architectures. Unlike\ntraditional approaches, LIMCA employs a No-Human-In-Loop (NHIL) automated\npipeline to generate and validate circuit netlists for SPICE simulations,\neliminating manual intervention. LIMCA systematically explores the IMC design\nspace by leveraging a structured dataset and LLM-based performance evaluation.\nOur experimental results on MNIST classification demonstrate that LIMCA\nsuccessfully generates crossbar designs achieving $\\geq$96% accuracy while\nmaintaining a power consumption $\\leq$3W, making this the first work in\nLLM-assisted IMC design space exploration. Compared to existing frameworks,\nLIMCA provides an automated, scalable, and hardware-aware solution, reducing\ndesign exploration time while ensuring user-constrained performance trade-offs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resistive crossbars enabling analog In-Memory Computing (IMC) have emerged as\na promising architecture for Deep Neural Network (DNN) acceleration, offering\nhigh memory bandwidth and in-situ computation. However, the manual,\nknowledge-intensive design process and the lack of high-quality circuit\nnetlists have significantly constrained design space exploration and\noptimization to behavioral system-level tools. In this work, we introduce\nLIMCA, a novel fine-tune-free Large Language Model (LLM)-driven framework for\nautomating the design and evaluation of IMC crossbar architectures. Unlike\ntraditional approaches, LIMCA employs a No-Human-In-Loop (NHIL) automated\npipeline to generate and validate circuit netlists for SPICE simulations,\neliminating manual intervention. LIMCA systematically explores the IMC design\nspace by leveraging a structured dataset and LLM-based performance evaluation.\nOur experimental results on MNIST classification demonstrate that LIMCA\nsuccessfully generates crossbar designs achieving $\\geq$96% accuracy while\nmaintaining a power consumption $\\leq$3W, making this the first work in\nLLM-assisted IMC design space exploration. Compared to existing frameworks,\nLIMCA provides an automated, scalable, and hardware-aware solution, reducing\ndesign exploration time while ensuring user-constrained performance trade-offs."
                },
                "authors": [
                    {
                        "name": "Deepak Vungarala"
                    },
                    {
                        "name": "Md Hasibul Amin"
                    },
                    {
                        "name": "Pietro Mercati"
                    },
                    {
                        "name": "Arnob Ghosh"
                    },
                    {
                        "name": "Arman Roohi"
                    },
                    {
                        "name": "Ramtin Zand"
                    },
                    {
                        "name": "Shaahin Angizi"
                    }
                ],
                "author_detail": {
                    "name": "Shaahin Angizi"
                },
                "author": "Shaahin Angizi",
                "arxiv_comment": "4 Figures, 5 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13301v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13299v1",
                "updated": "2025-03-17T15:44:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    44,
                    9,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T15:44:09Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    44,
                    9,
                    0,
                    76,
                    0
                ],
                "title": "A Survey on Transformer Context Extension: Approaches and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Transformer Context Extension: Approaches and Evaluation"
                },
                "summary": "Large language models (LLMs) based on Transformer have been widely applied in\nthe filed of natural language processing (NLP), demonstrating strong\nperformance, particularly in handling short text tasks. However, when it comes\nto long context scenarios, the performance of LLMs degrades due to some\nchallenges. To alleviate this phenomenon, there is a number of work proposed\nrecently. In this survey, we first list the challenges of applying pre-trained\nLLMs to process long contexts. Then systematically review the approaches\nrelated to long context and propose our taxonomy categorizing them into four\nmain types: positional encoding, context compression, retrieval augmented, and\nattention pattern. In addition to the approaches, we focus on the evaluation of\nlong context, organizing relevant data, tasks, and metrics based on existing\nlong context benchmarks. Finally, we summarize unresolved issues in the long\ncontext domain and put forward our views on future developments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) based on Transformer have been widely applied in\nthe filed of natural language processing (NLP), demonstrating strong\nperformance, particularly in handling short text tasks. However, when it comes\nto long context scenarios, the performance of LLMs degrades due to some\nchallenges. To alleviate this phenomenon, there is a number of work proposed\nrecently. In this survey, we first list the challenges of applying pre-trained\nLLMs to process long contexts. Then systematically review the approaches\nrelated to long context and propose our taxonomy categorizing them into four\nmain types: positional encoding, context compression, retrieval augmented, and\nattention pattern. In addition to the approaches, we focus on the evaluation of\nlong context, organizing relevant data, tasks, and metrics based on existing\nlong context benchmarks. Finally, we summarize unresolved issues in the long\ncontext domain and put forward our views on future developments."
                },
                "authors": [
                    {
                        "name": "Yijun Liu"
                    },
                    {
                        "name": "Jinzheng Yu"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Zhongyang Li"
                    },
                    {
                        "name": "Qingfu Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Qingfu Zhu"
                },
                "author": "Qingfu Zhu",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13288v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13288v1",
                "updated": "2025-03-17T15:38:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    38,
                    33,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T15:38:33Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    38,
                    33,
                    0,
                    76,
                    0
                ],
                "title": "$φ$-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time\n  Exploration and Exploitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$φ$-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time\n  Exploration and Exploitation"
                },
                "summary": "Inference-time optimization scales computation to derive deliberate reasoning\nsteps for effective performance. While previous search-based strategies address\nthe short-sightedness of auto-regressive generation, the vast search space\nleads to excessive exploration and insufficient exploitation. To strike an\nefficient balance to derive the optimal step, we frame the decoding strategy as\nforesight sampling, leveraging simulated future steps to obtain globally\noptimal step estimation. Built on it, we propose a novel decoding strategy,\nnamed $\\phi$-Decoding. To provide a precise and expressive estimation of step\nvalue, $\\phi$-Decoding approximates two distributions via foresight and\nclustering. Sampling from the joint distribution, the optimal steps can be\nselected for exploitation. To support adaptive computation allocation, we\npropose in-width and in-depth pruning strategies, featuring a light-weight\nsolution to achieve inference efficiency. Extensive experiments across seven\nbenchmarks show $\\phi$-Decoding outperforms strong baselines in both\nperformance and efficiency. Additional analysis demonstrates its generalization\nacross various LLMs and scalability across a wide range of computing budgets.\nThe code will be released at https://github.com/xufangzhi/phi-Decoding, and the\nopen-source PyPI package is coming soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-time optimization scales computation to derive deliberate reasoning\nsteps for effective performance. While previous search-based strategies address\nthe short-sightedness of auto-regressive generation, the vast search space\nleads to excessive exploration and insufficient exploitation. To strike an\nefficient balance to derive the optimal step, we frame the decoding strategy as\nforesight sampling, leveraging simulated future steps to obtain globally\noptimal step estimation. Built on it, we propose a novel decoding strategy,\nnamed $\\phi$-Decoding. To provide a precise and expressive estimation of step\nvalue, $\\phi$-Decoding approximates two distributions via foresight and\nclustering. Sampling from the joint distribution, the optimal steps can be\nselected for exploitation. To support adaptive computation allocation, we\npropose in-width and in-depth pruning strategies, featuring a light-weight\nsolution to achieve inference efficiency. Extensive experiments across seven\nbenchmarks show $\\phi$-Decoding outperforms strong baselines in both\nperformance and efficiency. Additional analysis demonstrates its generalization\nacross various LLMs and scalability across a wide range of computing budgets.\nThe code will be released at https://github.com/xufangzhi/phi-Decoding, and the\nopen-source PyPI package is coming soon."
                },
                "authors": [
                    {
                        "name": "Fangzhi Xu"
                    },
                    {
                        "name": "Hang Yan"
                    },
                    {
                        "name": "Chang Ma"
                    },
                    {
                        "name": "Haiteng Zhao"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Qika Lin"
                    },
                    {
                        "name": "Zhiyong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Wu"
                },
                "author": "Zhiyong Wu",
                "arxiv_comment": "13 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13288v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13288v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13281v1",
                "updated": "2025-03-17T15:31:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    31,
                    55,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T15:31:55Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    31,
                    55,
                    0,
                    76,
                    0
                ],
                "title": "LLM-Match: An Open-Sourced Patient Matching Model Based on Large\n  Language Models and Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Match: An Open-Sourced Patient Matching Model Based on Large\n  Language Models and Retrieval-Augmented Generation"
                },
                "summary": "Patient matching is the process of linking patients to appropriate clinical\ntrials by accurately identifying and matching their medical records with trial\neligibility criteria. We propose LLM-Match, a novel framework for patient\nmatching leveraging fine-tuned open-source large language models. Our approach\nconsists of four key components. First, a retrieval-augmented generation (RAG)\nmodule extracts relevant patient context from a vast pool of electronic health\nrecords (EHRs). Second, a prompt generation module constructs input prompts by\nintegrating trial eligibility criteria (both inclusion and exclusion criteria),\npatient context, and system instructions. Third, a fine-tuning module with a\nclassification head optimizes the model parameters using structured prompts and\nground-truth labels. Fourth, an evaluation module assesses the fine-tuned\nmodel's performance on the testing datasets. We evaluated LLM-Match on four\nopen datasets, n2c2, SIGIR, TREC 2021, and TREC 2022, using open-source models,\ncomparing it against TrialGPT, Zero-Shot, and GPT-4-based closed models.\nLLM-Match outperformed all baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patient matching is the process of linking patients to appropriate clinical\ntrials by accurately identifying and matching their medical records with trial\neligibility criteria. We propose LLM-Match, a novel framework for patient\nmatching leveraging fine-tuned open-source large language models. Our approach\nconsists of four key components. First, a retrieval-augmented generation (RAG)\nmodule extracts relevant patient context from a vast pool of electronic health\nrecords (EHRs). Second, a prompt generation module constructs input prompts by\nintegrating trial eligibility criteria (both inclusion and exclusion criteria),\npatient context, and system instructions. Third, a fine-tuning module with a\nclassification head optimizes the model parameters using structured prompts and\nground-truth labels. Fourth, an evaluation module assesses the fine-tuned\nmodel's performance on the testing datasets. We evaluated LLM-Match on four\nopen datasets, n2c2, SIGIR, TREC 2021, and TREC 2022, using open-source models,\ncomparing it against TrialGPT, Zero-Shot, and GPT-4-based closed models.\nLLM-Match outperformed all baselines."
                },
                "authors": [
                    {
                        "name": "Xiaodi Li"
                    },
                    {
                        "name": "Shaika Chowdhury"
                    },
                    {
                        "name": "Chung Il Wi"
                    },
                    {
                        "name": "Maria Vassilaki"
                    },
                    {
                        "name": "Ken Liu"
                    },
                    {
                        "name": "Terence T Sio"
                    },
                    {
                        "name": "Owen Garrick"
                    },
                    {
                        "name": "Young J Juhn"
                    },
                    {
                        "name": "James R Cerhan"
                    },
                    {
                        "name": "Cui Tao"
                    },
                    {
                        "name": "Nansu Zong"
                    }
                ],
                "author_detail": {
                    "name": "Nansu Zong"
                },
                "author": "Nansu Zong",
                "arxiv_comment": "10 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13279v1",
                "updated": "2025-03-17T15:31:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    31,
                    20,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T15:31:20Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    31,
                    20,
                    0,
                    76,
                    0
                ],
                "title": "Goal2Story: A Multi-Agent Fleet based on Privately Enabled sLLMs for\n  Impacting Mapping on Requirements Elicitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Goal2Story: A Multi-Agent Fleet based on Privately Enabled sLLMs for\n  Impacting Mapping on Requirements Elicitation"
                },
                "summary": "As requirements drift with rapid iterations, agile development becomes the\ndominant paradigm. Goal-driven Requirements Elicitation (RE) is a pivotal yet\nchallenging task in agile project development due to its heavy tangling with\nadaptive planning and efficient collaboration. Recently, AI agents have shown\npromising ability in supporting requirements analysis by saving significant\ntime and effort for stakeholders. However, current research mainly focuses on\nfunctional RE, and research works have not been reported bridging the long\njourney from goal to user stories. Moreover, considering the cost of LLM\nfacilities and the need for data and idea protection, privately hosted\nsmall-sized LLM should be further utilized in RE. To address these challenges,\nwe propose Goal2Story, a multi-agent fleet that adopts the Impact Mapping (IM)\nframework while merely using cost-effective sLLMs for goal-driven RE. Moreover,\nwe introduce a StorySeek dataset that contains over 1,000 user stories (USs)\nwith corresponding goals and project context information, as well as the\nsemi-automatic dataset construction method. For evaluation, we proposed two\nmetrics: Factuality Hit Rate (FHR) to measure consistency between the generated\nUSs with the dataset and Quality And Consistency Evaluation (QuACE) to evaluate\nthe quality of the generated USs. Experimental results demonstrate that\nGoal2Story outperforms the baseline performance of the Super-Agent adopting\npowerful LLMs, while also showcasing the performance improvements in key\nmetrics brought by CoT and Agent Profile to Goal2Story, as well as its\nexploration in identifying latent needs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As requirements drift with rapid iterations, agile development becomes the\ndominant paradigm. Goal-driven Requirements Elicitation (RE) is a pivotal yet\nchallenging task in agile project development due to its heavy tangling with\nadaptive planning and efficient collaboration. Recently, AI agents have shown\npromising ability in supporting requirements analysis by saving significant\ntime and effort for stakeholders. However, current research mainly focuses on\nfunctional RE, and research works have not been reported bridging the long\njourney from goal to user stories. Moreover, considering the cost of LLM\nfacilities and the need for data and idea protection, privately hosted\nsmall-sized LLM should be further utilized in RE. To address these challenges,\nwe propose Goal2Story, a multi-agent fleet that adopts the Impact Mapping (IM)\nframework while merely using cost-effective sLLMs for goal-driven RE. Moreover,\nwe introduce a StorySeek dataset that contains over 1,000 user stories (USs)\nwith corresponding goals and project context information, as well as the\nsemi-automatic dataset construction method. For evaluation, we proposed two\nmetrics: Factuality Hit Rate (FHR) to measure consistency between the generated\nUSs with the dataset and Quality And Consistency Evaluation (QuACE) to evaluate\nthe quality of the generated USs. Experimental results demonstrate that\nGoal2Story outperforms the baseline performance of the Super-Agent adopting\npowerful LLMs, while also showcasing the performance improvements in key\nmetrics brought by CoT and Agent Profile to Goal2Story, as well as its\nexploration in identifying latent needs."
                },
                "authors": [
                    {
                        "name": "Xinkai Zou"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Xiongbo Shi"
                    },
                    {
                        "name": "Chen Yang"
                    }
                ],
                "author_detail": {
                    "name": "Chen Yang"
                },
                "author": "Chen Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13275v1",
                "updated": "2025-03-17T15:27:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T15:27:02Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "title": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems"
                },
                "summary": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability."
                },
                "authors": [
                    {
                        "name": "Seyoung Song"
                    }
                ],
                "author_detail": {
                    "name": "Seyoung Song"
                },
                "author": "Seyoung Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7; I.2.11; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14299v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14299v4",
                "updated": "2025-03-17T15:22:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    22,
                    28,
                    0,
                    76,
                    0
                ],
                "published": "2024-11-21T16:50:11Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    50,
                    11,
                    3,
                    326,
                    0
                ],
                "title": "Masala-CHAI: A Large-Scale SPICE Netlist Dataset for Analog Circuits by\n  Harnessing AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masala-CHAI: A Large-Scale SPICE Netlist Dataset for Analog Circuits by\n  Harnessing AI"
                },
                "summary": "Masala-CHAI is the first fully automated framework leveraging large language\nmodels (LLMs) to generate Simulation Programs with Integrated Circuit Emphasis\n(SPICE) netlists. It addresses a long-standing challenge in automating netlist\ngeneration for analog circuits within circuit design automation. Automating\nthis workflow could accelerate the creation of finetuned LLMs for analog\ncircuit design and verification. We identify key challenges in this automation\nand evaluate the multi-modal capabilities of state-of-the-art LLMs,\nparticularly GPT-4, to address these issues. We propose a three-step workflow\nto overcome current limitations: labeling analog circuits, prompt tuning, and\nnetlist verification. This approach aims to create an end-to-end SPICE netlist\ngenerator from circuit schematic images, tackling the long-standing hurdle of\naccurate netlist generation. Our framework demonstrates significant performance\nimprovements, tested on approximately 2,100 schematics of varying complexity.\nWe open-source this solution for community-driven development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masala-CHAI is the first fully automated framework leveraging large language\nmodels (LLMs) to generate Simulation Programs with Integrated Circuit Emphasis\n(SPICE) netlists. It addresses a long-standing challenge in automating netlist\ngeneration for analog circuits within circuit design automation. Automating\nthis workflow could accelerate the creation of finetuned LLMs for analog\ncircuit design and verification. We identify key challenges in this automation\nand evaluate the multi-modal capabilities of state-of-the-art LLMs,\nparticularly GPT-4, to address these issues. We propose a three-step workflow\nto overcome current limitations: labeling analog circuits, prompt tuning, and\nnetlist verification. This approach aims to create an end-to-end SPICE netlist\ngenerator from circuit schematic images, tackling the long-standing hurdle of\naccurate netlist generation. Our framework demonstrates significant performance\nimprovements, tested on approximately 2,100 schematics of varying complexity.\nWe open-source this solution for community-driven development."
                },
                "authors": [
                    {
                        "name": "Jitendra Bhandari"
                    },
                    {
                        "name": "Vineet Bhat"
                    },
                    {
                        "name": "Yuheng He"
                    },
                    {
                        "name": "Hamed Rahmani"
                    },
                    {
                        "name": "Siddharth Garg"
                    },
                    {
                        "name": "Ramesh Karri"
                    }
                ],
                "author_detail": {
                    "name": "Ramesh Karri"
                },
                "author": "Ramesh Karri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14299v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14299v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13269v1",
                "updated": "2025-03-17T15:22:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    22,
                    19,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T15:22:19Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    22,
                    19,
                    0,
                    76,
                    0
                ],
                "title": "DAgent: A Relational Database-Driven Data Analysis Report Generation\n  Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAgent: A Relational Database-Driven Data Analysis Report Generation\n  Agent"
                },
                "summary": "Relational database-driven data analysis (RDB-DA) report generation, which\naims to generate data analysis reports after querying relational databases, has\nbeen widely applied in fields such as finance and healthcare. Typically, these\ntasks are manually completed by data scientists, making the process very\nlabor-intensive and showing a clear need for automation. Although existing\nmethods (e.g., Table QA or Text-to-SQL) have been proposed to reduce human\ndependency, they cannot handle complex analytical tasks that require multi-step\nreasoning, cross-table associations, and synthesizing insights into reports.\nMoreover, there is no dataset available for developing automatic RDB-DA report\ngeneration. To fill this gap, this paper proposes an LLM agent system for\nRDB-DA report generation tasks, dubbed DAgent; moreover, we construct a\nbenchmark for automatic data analysis report generation, which includes a new\ndataset DA-Dataset and evaluation metrics. DAgent integrates planning, tools,\nand memory modules to decompose natural language questions into logically\nindependent sub-queries, accurately retrieve key information from relational\ndatabases, and generate analytical reports that meet the requirements of\ncompleteness, correctness, and conciseness through multi-step reasoning and\neffective data integration. Experimental analysis on the DA-Dataset\ndemonstrates that DAgent's superiority in retrieval performance and analysis\nreport generation quality, showcasing its strong potential for tackling complex\ndatabase analysis report generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relational database-driven data analysis (RDB-DA) report generation, which\naims to generate data analysis reports after querying relational databases, has\nbeen widely applied in fields such as finance and healthcare. Typically, these\ntasks are manually completed by data scientists, making the process very\nlabor-intensive and showing a clear need for automation. Although existing\nmethods (e.g., Table QA or Text-to-SQL) have been proposed to reduce human\ndependency, they cannot handle complex analytical tasks that require multi-step\nreasoning, cross-table associations, and synthesizing insights into reports.\nMoreover, there is no dataset available for developing automatic RDB-DA report\ngeneration. To fill this gap, this paper proposes an LLM agent system for\nRDB-DA report generation tasks, dubbed DAgent; moreover, we construct a\nbenchmark for automatic data analysis report generation, which includes a new\ndataset DA-Dataset and evaluation metrics. DAgent integrates planning, tools,\nand memory modules to decompose natural language questions into logically\nindependent sub-queries, accurately retrieve key information from relational\ndatabases, and generate analytical reports that meet the requirements of\ncompleteness, correctness, and conciseness through multi-step reasoning and\neffective data integration. Experimental analysis on the DA-Dataset\ndemonstrates that DAgent's superiority in retrieval performance and analysis\nreport generation quality, showcasing its strong potential for tackling complex\ndatabase analysis report generation tasks."
                },
                "authors": [
                    {
                        "name": "Wenyi Xu"
                    },
                    {
                        "name": "Yuren Mao"
                    },
                    {
                        "name": "Xiaolu Zhang"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Xuemei Dong"
                    },
                    {
                        "name": "Mengfei Zhang"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Yunjun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yunjun Gao"
                },
                "author": "Yunjun Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13268v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13268v1",
                "updated": "2025-03-17T15:20:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    20,
                    49,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T15:20:49Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    20,
                    49,
                    0,
                    76,
                    0
                ],
                "title": "Channel Estimation for Pinching-Antenna Systems (PASS)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Channel Estimation for Pinching-Antenna Systems (PASS)"
                },
                "summary": "Pinching Antennas (PAs) represent a revolutionary flexible antenna technology\nthat leverages dielectric waveguides and electromagnetic coupling to mitigate\nlarge-scale path loss. This letter is the first to explore channel estimation\nfor Pinching-Antenna SyStems (PASS), addressing their uniquely ill-conditioned\nand underdetermined channel characteristics. In particular, two efficient deep\nlearning-based channel estimators are proposed. 1) PAMoE: This estimator\nincorporates dynamic padding, feature embedding, fusion, and mixture of experts\n(MoE) modules, which effectively leverage the positional information of PAs and\nexploit expert diversity. 2) PAformer: This Transformer-style estimator employs\nthe self-attention mechanism to predict channel coefficients in a per-antenna\nmanner, which offers more flexibility to adaptively deal with dynamic numbers\nof PAs in practical deployment. Numerical results demonstrate that 1) the\nproposed deep learning-based channel estimators outperform conventional methods\nand exhibit excellent zero-shot learning capabilities, and 2) PAMoE delivers\nhigher channel estimation accuracy via MoE specialization, while PAformer\nnatively handles an arbitrary number of PAs, trading self-attention complexity\nfor superior scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pinching Antennas (PAs) represent a revolutionary flexible antenna technology\nthat leverages dielectric waveguides and electromagnetic coupling to mitigate\nlarge-scale path loss. This letter is the first to explore channel estimation\nfor Pinching-Antenna SyStems (PASS), addressing their uniquely ill-conditioned\nand underdetermined channel characteristics. In particular, two efficient deep\nlearning-based channel estimators are proposed. 1) PAMoE: This estimator\nincorporates dynamic padding, feature embedding, fusion, and mixture of experts\n(MoE) modules, which effectively leverage the positional information of PAs and\nexploit expert diversity. 2) PAformer: This Transformer-style estimator employs\nthe self-attention mechanism to predict channel coefficients in a per-antenna\nmanner, which offers more flexibility to adaptively deal with dynamic numbers\nof PAs in practical deployment. Numerical results demonstrate that 1) the\nproposed deep learning-based channel estimators outperform conventional methods\nand exhibit excellent zero-shot learning capabilities, and 2) PAMoE delivers\nhigher channel estimation accuracy via MoE specialization, while PAformer\nnatively handles an arbitrary number of PAs, trading self-attention complexity\nfor superior scalability."
                },
                "authors": [
                    {
                        "name": "Jian Xiao"
                    },
                    {
                        "name": "Ji Wang"
                    },
                    {
                        "name": "Yuanwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuanwei Liu"
                },
                "author": "Yuanwei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13268v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13268v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03834v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03834v2",
                "updated": "2025-03-17T15:08:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    8,
                    47,
                    0,
                    76,
                    0
                ],
                "published": "2024-10-04T18:02:48Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    18,
                    2,
                    48,
                    4,
                    278,
                    0
                ],
                "title": "GraphRouter: A Graph-based Router for LLM Selections",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphRouter: A Graph-based Router for LLM Selections"
                },
                "summary": "The rapidly growing number and variety of Large Language Models (LLMs)\npresent significant challenges in efficiently selecting the appropriate LLM for\na given query, especially considering the trade-offs between performance and\ncomputational cost. Current LLM selection methods often struggle to generalize\nacross new LLMs and different tasks because of their limited ability to\nleverage contextual interactions among tasks, queries, and LLMs, as well as\ntheir dependence on a transductive learning framework. To address these\nshortcomings, we introduce a novel inductive graph framework, named as\nGraphRouter, which fully utilizes the contextual information among tasks,\nqueries, and LLMs to enhance the LLM selection process. GraphRouter constructs\na heterogeneous graph comprising task, query, and LLM nodes, with interactions\nrepresented as edges, which efficiently captures the contextual information\nbetween the query's requirements and the LLM's capabilities. Through an\ninnovative edge prediction mechanism, GraphRouter is able to predict attributes\n(the effect and cost of LLM response) of potential edges, allowing for\noptimized recommendations that adapt to both existing and newly introduced LLMs\nwithout requiring retraining. Comprehensive experiments across three distinct\neffect-cost weight scenarios have shown that GraphRouter substantially\nsurpasses existing routers, delivering a minimum performance improvement of\n12.3%. In addition, it achieves enhanced generalization across new LLMs\nsettings and supports diverse tasks with at least a 9.5% boost in effect and a\nsignificant reduction in computational demands. This work endeavors to apply a\ngraph-based approach for the contextual and adaptive selection of LLMs,\noffering insights for real-world applications. Our codes for GraphRouter is\nreleased at https://github.com/ulab-uiuc/GraphRouter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapidly growing number and variety of Large Language Models (LLMs)\npresent significant challenges in efficiently selecting the appropriate LLM for\na given query, especially considering the trade-offs between performance and\ncomputational cost. Current LLM selection methods often struggle to generalize\nacross new LLMs and different tasks because of their limited ability to\nleverage contextual interactions among tasks, queries, and LLMs, as well as\ntheir dependence on a transductive learning framework. To address these\nshortcomings, we introduce a novel inductive graph framework, named as\nGraphRouter, which fully utilizes the contextual information among tasks,\nqueries, and LLMs to enhance the LLM selection process. GraphRouter constructs\na heterogeneous graph comprising task, query, and LLM nodes, with interactions\nrepresented as edges, which efficiently captures the contextual information\nbetween the query's requirements and the LLM's capabilities. Through an\ninnovative edge prediction mechanism, GraphRouter is able to predict attributes\n(the effect and cost of LLM response) of potential edges, allowing for\noptimized recommendations that adapt to both existing and newly introduced LLMs\nwithout requiring retraining. Comprehensive experiments across three distinct\neffect-cost weight scenarios have shown that GraphRouter substantially\nsurpasses existing routers, delivering a minimum performance improvement of\n12.3%. In addition, it achieves enhanced generalization across new LLMs\nsettings and supports diverse tasks with at least a 9.5% boost in effect and a\nsignificant reduction in computational demands. This work endeavors to apply a\ngraph-based approach for the contextual and adaptive selection of LLMs,\noffering insights for real-world applications. Our codes for GraphRouter is\nreleased at https://github.com/ulab-uiuc/GraphRouter."
                },
                "authors": [
                    {
                        "name": "Tao Feng"
                    },
                    {
                        "name": "Yanzhen Shen"
                    },
                    {
                        "name": "Jiaxuan You"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxuan You"
                },
                "author": "Jiaxuan You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03834v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03834v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13250v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13250v1",
                "updated": "2025-03-17T15:06:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    6,
                    14,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T15:06:14Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    6,
                    14,
                    0,
                    76,
                    0
                ],
                "title": "MindEye-OmniAssist: A Gaze-Driven LLM-Enhanced Assistive Robot System\n  for Implicit Intention Recognition and Task Execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MindEye-OmniAssist: A Gaze-Driven LLM-Enhanced Assistive Robot System\n  for Implicit Intention Recognition and Task Execution"
                },
                "summary": "A promising effective human-robot interaction in assistive robotic systems is\ngaze-based control. However, current gaze-based assistive systems mainly help\nusers with basic grasping actions, offering limited support. Moreover, the\nrestricted intent recognition capability constrains the assistive system's\nability to provide diverse assistance functions. In this paper, we propose an\nopen implicit intention recognition framework powered by Large Language Model\n(LLM) and Vision Foundation Model (VFM), which can process gaze input and\nrecognize user intents that are not confined to predefined or specific\nscenarios. Furthermore, we implement a gaze-driven LLM-enhanced assistive robot\nsystem (MindEye-OmniAssist) that recognizes user's intentions through gaze and\nassists in completing task. To achieve this, the system utilizes open\nvocabulary object detector, intention recognition network and LLM to infer\ntheir full intentions. By integrating eye movement feedback and LLM, it\ngenerates action sequences to assist the user in completing tasks. Real-world\nexperiments have been conducted for assistive tasks, and the system achieved an\noverall success rate of 41/55 across various undefined tasks. Preliminary\nresults show that the proposed method holds the potential to provide a more\nuser-friendly human-computer interaction interface and significantly enhance\nthe versatility and effectiveness of assistive systems by supporting more\ncomplex and diverse task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A promising effective human-robot interaction in assistive robotic systems is\ngaze-based control. However, current gaze-based assistive systems mainly help\nusers with basic grasping actions, offering limited support. Moreover, the\nrestricted intent recognition capability constrains the assistive system's\nability to provide diverse assistance functions. In this paper, we propose an\nopen implicit intention recognition framework powered by Large Language Model\n(LLM) and Vision Foundation Model (VFM), which can process gaze input and\nrecognize user intents that are not confined to predefined or specific\nscenarios. Furthermore, we implement a gaze-driven LLM-enhanced assistive robot\nsystem (MindEye-OmniAssist) that recognizes user's intentions through gaze and\nassists in completing task. To achieve this, the system utilizes open\nvocabulary object detector, intention recognition network and LLM to infer\ntheir full intentions. By integrating eye movement feedback and LLM, it\ngenerates action sequences to assist the user in completing tasks. Real-world\nexperiments have been conducted for assistive tasks, and the system achieved an\noverall success rate of 41/55 across various undefined tasks. Preliminary\nresults show that the proposed method holds the potential to provide a more\nuser-friendly human-computer interaction interface and significantly enhance\nthe versatility and effectiveness of assistive systems by supporting more\ncomplex and diverse task."
                },
                "authors": [
                    {
                        "name": "Zejia Zhang"
                    },
                    {
                        "name": "Bo Yang"
                    },
                    {
                        "name": "Xinxing Chen"
                    },
                    {
                        "name": "Weizhuang Shi"
                    },
                    {
                        "name": "Haoyuan Wang"
                    },
                    {
                        "name": "Wei Luo"
                    },
                    {
                        "name": "Jian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Huang"
                },
                "author": "Jian Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13250v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13250v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08597v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08597v2",
                "updated": "2025-03-17T15:04:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    4,
                    6,
                    0,
                    76,
                    0
                ],
                "published": "2024-07-11T15:25:02Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    15,
                    25,
                    2,
                    3,
                    193,
                    0
                ],
                "title": "Learning Program Behavioral Models from Synthesized Input-Output Pairs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Program Behavioral Models from Synthesized Input-Output Pairs"
                },
                "summary": "We introduce Modelizer - a novel framework that, given a black-box program,\nlearns a model from its input/output behavior using neural machine translation\nalgorithms. The resulting model mocks the original program: Given an input, the\nmodel predicts the output that would have been produced by the program.\nHowever, the model is also reversible - that is, the model can predict the\ninput that would have produced a given output. Finally, the model is\ndifferentiable and can be efficiently restricted to predict only a certain\naspect of the program behavior. Modelizer uses grammars to synthesize and\ninputs and unsupervised tokenizers to decompose the resulting outputs, allowing\nit to learn sequence-to-sequence associations between token streams. Other than\ninput grammars, Modelizer only requires the ability to execute the program. The\nresulting models are small, requiring fewer than 6.3 million parameters for\nlanguages such as Markdown or HTML; and they are accurate, achieving up to\n95.4% accuracy and a BLEU score of 0.98 with standard error 0.04 in mocking\nreal-world applications. As it learns from and predicts executions rather than\ncode, Modelizer departs from the LLM-centric research trend, opening new\nopportunities for program-specific models that are fully tuned towards\nindividual programs. Indeed, we foresee several applications of these models,\nespecially as the output of the program can be any aspect of program behavior.\nBeyond mocking and predicting program behavior, the models can also synthesize\ninputs that are likely to produce a particular behavior, such as failures or\ncoverage, thus assisting in program understanding and maintenance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Modelizer - a novel framework that, given a black-box program,\nlearns a model from its input/output behavior using neural machine translation\nalgorithms. The resulting model mocks the original program: Given an input, the\nmodel predicts the output that would have been produced by the program.\nHowever, the model is also reversible - that is, the model can predict the\ninput that would have produced a given output. Finally, the model is\ndifferentiable and can be efficiently restricted to predict only a certain\naspect of the program behavior. Modelizer uses grammars to synthesize and\ninputs and unsupervised tokenizers to decompose the resulting outputs, allowing\nit to learn sequence-to-sequence associations between token streams. Other than\ninput grammars, Modelizer only requires the ability to execute the program. The\nresulting models are small, requiring fewer than 6.3 million parameters for\nlanguages such as Markdown or HTML; and they are accurate, achieving up to\n95.4% accuracy and a BLEU score of 0.98 with standard error 0.04 in mocking\nreal-world applications. As it learns from and predicts executions rather than\ncode, Modelizer departs from the LLM-centric research trend, opening new\nopportunities for program-specific models that are fully tuned towards\nindividual programs. Indeed, we foresee several applications of these models,\nespecially as the output of the program can be any aspect of program behavior.\nBeyond mocking and predicting program behavior, the models can also synthesize\ninputs that are likely to produce a particular behavior, such as failures or\ncoverage, thus assisting in program understanding and maintenance."
                },
                "authors": [
                    {
                        "name": "Tural Mammadov"
                    },
                    {
                        "name": "Dietrich Klakow"
                    },
                    {
                        "name": "Alexander Koller"
                    },
                    {
                        "name": "Andreas Zeller"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Zeller"
                },
                "author": "Andreas Zeller",
                "arxiv_comment": "42 pages, 9 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08597v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08597v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07 (Primary), 68N30 (Secondary), 68Q42",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.5; D.2.7; I.2.6; F.1.1; F.4.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11015v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11015v2",
                "updated": "2025-03-17T14:46:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    46,
                    24,
                    0,
                    76,
                    0
                ],
                "published": "2025-02-16T06:47:40Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    6,
                    47,
                    40,
                    6,
                    47,
                    0
                ],
                "title": "Wireless charging and readout via textile coil for continuous full-body\n  wearable computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless charging and readout via textile coil for continuous full-body\n  wearable computing"
                },
                "summary": "The growing use of wearable devices for activity tracking, healthcare, and\nhaptics faces challenges due to the bulkiness and short lifespan of batteries.\nIntegration of a textile-based wireless charging and readout system into\neveryday clothing can enable seamless power supply and data collection around\nthe body. However, expanding such system to cover the entire body is\nchallenging, as it increases electromagnetic interference with the body,\ndegrading the performance of wireless system. This article introduces a\nmeandered textile coil designed for body-scale, efficient wireless charging and\nreadout. The meander coil can confine a strong inductive field near the body\nsurface, ensuring W-class safe charging and sensitive readout with uW-class low\npower. Moreover, its zigzag design is simple enough for mass production on\nindustrial knitting machines. Therefore, the body-scale meander coil can\ncontinuously operate battery-free wearable devices across the body, leading to\nubiquitous deployment of continuous full-body wearable computing into everyday\nclothing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing use of wearable devices for activity tracking, healthcare, and\nhaptics faces challenges due to the bulkiness and short lifespan of batteries.\nIntegration of a textile-based wireless charging and readout system into\neveryday clothing can enable seamless power supply and data collection around\nthe body. However, expanding such system to cover the entire body is\nchallenging, as it increases electromagnetic interference with the body,\ndegrading the performance of wireless system. This article introduces a\nmeandered textile coil designed for body-scale, efficient wireless charging and\nreadout. The meander coil can confine a strong inductive field near the body\nsurface, ensuring W-class safe charging and sensitive readout with uW-class low\npower. Moreover, its zigzag design is simple enough for mass production on\nindustrial knitting machines. Therefore, the body-scale meander coil can\ncontinuously operate battery-free wearable devices across the body, leading to\nubiquitous deployment of continuous full-body wearable computing into everyday\nclothing."
                },
                "authors": [
                    {
                        "name": "Ryo Takahashi"
                    },
                    {
                        "name": "Yoshihiro Kawahara"
                    }
                ],
                "author_detail": {
                    "name": "Yoshihiro Kawahara"
                },
                "author": "Yoshihiro Kawahara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11015v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11015v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13223v1",
                "updated": "2025-03-17T14:36:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    36,
                    8,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T14:36:08Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    36,
                    8,
                    0,
                    76,
                    0
                ],
                "title": "Robust Decision-Making Via Free Energy Minimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Decision-Making Via Free Energy Minimization"
                },
                "summary": "Despite their groundbreaking performance, state-of-the-art autonomous agents\ncan misbehave when training and environmental conditions become inconsistent,\nwith minor mismatches leading to undesirable behaviors or even catastrophic\nfailures. Robustness towards these training/environment ambiguities is a core\nrequirement for intelligent agents and its fulfillment is a long-standing\nchallenge when deploying agents in the real world. Here, departing from\nmainstream views seeking robustness through training, we introduce DR-FREE, a\nfree energy model that installs this core property by design. It directly wires\nrobustness into the agent decision-making mechanisms via free energy\nminimization. By combining a robust extension of the free energy principle with\na novel resolution engine, DR-FREE returns a policy that is optimal-yet-robust\nagainst ambiguity. Moreover, for the first time, it reveals the mechanistic\nrole of ambiguity on optimal decisions and requisite Bayesian belief updating.\nWe evaluate DR-FREE on an experimental testbed involving real rovers navigating\nan ambiguous environment filled with obstacles. Across all the experiments,\nDR-FREE enables robots to successfully navigate towards their goal even when,\nin contrast, standard free energy minimizing agents that do not use DR-FREE\nfail. In short, DR-FREE can tackle scenarios that elude previous methods: this\nmilestone may inspire both deployment in multi-agent settings and, at a perhaps\ndeeper level, the quest for a biologically plausible explanation of how natural\nagents - with little or no training - survive in capricious environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their groundbreaking performance, state-of-the-art autonomous agents\ncan misbehave when training and environmental conditions become inconsistent,\nwith minor mismatches leading to undesirable behaviors or even catastrophic\nfailures. Robustness towards these training/environment ambiguities is a core\nrequirement for intelligent agents and its fulfillment is a long-standing\nchallenge when deploying agents in the real world. Here, departing from\nmainstream views seeking robustness through training, we introduce DR-FREE, a\nfree energy model that installs this core property by design. It directly wires\nrobustness into the agent decision-making mechanisms via free energy\nminimization. By combining a robust extension of the free energy principle with\na novel resolution engine, DR-FREE returns a policy that is optimal-yet-robust\nagainst ambiguity. Moreover, for the first time, it reveals the mechanistic\nrole of ambiguity on optimal decisions and requisite Bayesian belief updating.\nWe evaluate DR-FREE on an experimental testbed involving real rovers navigating\nan ambiguous environment filled with obstacles. Across all the experiments,\nDR-FREE enables robots to successfully navigate towards their goal even when,\nin contrast, standard free energy minimizing agents that do not use DR-FREE\nfail. In short, DR-FREE can tackle scenarios that elude previous methods: this\nmilestone may inspire both deployment in multi-agent settings and, at a perhaps\ndeeper level, the quest for a biologically plausible explanation of how natural\nagents - with little or no training - survive in capricious environments."
                },
                "authors": [
                    {
                        "name": "Allahkaram Shafiei"
                    },
                    {
                        "name": "Hozefa Jesawada"
                    },
                    {
                        "name": "Karl Friston"
                    },
                    {
                        "name": "Giovanni Russo"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Russo"
                },
                "author": "Giovanni Russo",
                "arxiv_comment": "Contains main text and supplementary information",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14892v2",
                "updated": "2025-03-17T14:32:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    32,
                    8,
                    0,
                    76,
                    0
                ],
                "published": "2025-01-24T19:31:06Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    19,
                    31,
                    6,
                    4,
                    24,
                    0
                ],
                "title": "Causal Graphs Meet Thoughts: Enhancing Complex Reasoning in\n  Graph-Augmented LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Graphs Meet Thoughts: Enhancing Complex Reasoning in\n  Graph-Augmented LLMs"
                },
                "summary": "In knowledge-intensive tasks, especially in high-stakes domains like medicine\nand law, it is critical not only to retrieve relevant information but also to\nprovide causal reasoning and explainability. Large language models (LLMs) have\nachieved remarkable performance in natural language understanding and\ngeneration tasks. However, they often suffer from limitations such as\ndifficulty in incorporating new knowledge, generating hallucinations, and\nexplaining their reasoning process. To address these challenges, integrating\nknowledge graphs with Graph Retrieval-Augmented Generation (Graph RAG) has\nemerged as an effective solution. Traditional Graph RAG methods often rely on\nsimple graph traversal or semantic similarity, which do not capture causal\nrelationships or align well with the model's internal reasoning steps. This\npaper proposes a novel pipeline that filters large knowledge graphs to\nemphasize cause-effect edges, aligns the retrieval process with the model's\nchain-of-thought (CoT), and enhances reasoning through multi-stage path\nimprovements. Experiments on medical question-answering tasks show consistent\ngains, with up to a 10\\% absolute improvement across multiple large language\nmodels (LLMs). This approach demonstrates the value of combining causal\nreasoning with stepwise retrieval, leading to more interpretable and logically\ngrounded solutions for complex queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In knowledge-intensive tasks, especially in high-stakes domains like medicine\nand law, it is critical not only to retrieve relevant information but also to\nprovide causal reasoning and explainability. Large language models (LLMs) have\nachieved remarkable performance in natural language understanding and\ngeneration tasks. However, they often suffer from limitations such as\ndifficulty in incorporating new knowledge, generating hallucinations, and\nexplaining their reasoning process. To address these challenges, integrating\nknowledge graphs with Graph Retrieval-Augmented Generation (Graph RAG) has\nemerged as an effective solution. Traditional Graph RAG methods often rely on\nsimple graph traversal or semantic similarity, which do not capture causal\nrelationships or align well with the model's internal reasoning steps. This\npaper proposes a novel pipeline that filters large knowledge graphs to\nemphasize cause-effect edges, aligns the retrieval process with the model's\nchain-of-thought (CoT), and enhances reasoning through multi-stage path\nimprovements. Experiments on medical question-answering tasks show consistent\ngains, with up to a 10\\% absolute improvement across multiple large language\nmodels (LLMs). This approach demonstrates the value of combining causal\nreasoning with stepwise retrieval, leading to more interpretable and logically\ngrounded solutions for complex queries."
                },
                "authors": [
                    {
                        "name": "Hang Luo"
                    },
                    {
                        "name": "Jian Zhang"
                    },
                    {
                        "name": "Chujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Chujun Li"
                },
                "author": "Chujun Li",
                "arxiv_comment": "18 pages, 3 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13222v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13222v1",
                "updated": "2025-03-17T14:31:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    31,
                    37,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T14:31:37Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    31,
                    37,
                    0,
                    76,
                    0
                ],
                "title": "Can Language Models Follow Multiple Turns of Entangled Instructions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Language Models Follow Multiple Turns of Entangled Instructions?"
                },
                "summary": "Despite significant achievements in improving the instruction-following\ncapabilities of large language models (LLMs), the ability to process multiple\npotentially entangled or conflicting instructions remains a considerable\nchallenge. Real-world scenarios often require consistency across multiple\ninstructions over time, such as secret privacy, personal preferences, and\nprioritization, which demand sophisticated abilities to integrate multiple\nturns and carefully balance competing objectives when instructions intersect or\nconflict. This work presents a systematic investigation of LLMs' capabilities\nin handling multiple turns of instructions, covering three levels of\ndifficulty: (1) retrieving information from instructions, (2) tracking and\nreasoning across turns, and (3) resolving conflicts among instructions. We\nconstruct MultiTurnInstruct with around 1.1K high-quality multi-turn\nconversations through the human-in-the-loop approach and result in nine\ncapability categories, including statics and dynamics, reasoning, and\nmultitasking. Our finding reveals an intriguing trade-off between different\ncapabilities. While GPT models demonstrate superior memorization, they show\nreduced effectiveness in privacy-protection tasks requiring selective\ninformation withholding. Larger models exhibit stronger reasoning capabilities\nbut still struggle with resolving conflicting instructions. Importantly, these\nperformance gaps cannot be attributed solely to information loss, as models\ndemonstrate strong BLEU scores on memorization tasks but their attention\nmechanisms fail to integrate multiple related instructions effectively. These\nfindings highlight critical areas for improvement in complex real-world tasks\ninvolving multi-turn instructions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant achievements in improving the instruction-following\ncapabilities of large language models (LLMs), the ability to process multiple\npotentially entangled or conflicting instructions remains a considerable\nchallenge. Real-world scenarios often require consistency across multiple\ninstructions over time, such as secret privacy, personal preferences, and\nprioritization, which demand sophisticated abilities to integrate multiple\nturns and carefully balance competing objectives when instructions intersect or\nconflict. This work presents a systematic investigation of LLMs' capabilities\nin handling multiple turns of instructions, covering three levels of\ndifficulty: (1) retrieving information from instructions, (2) tracking and\nreasoning across turns, and (3) resolving conflicts among instructions. We\nconstruct MultiTurnInstruct with around 1.1K high-quality multi-turn\nconversations through the human-in-the-loop approach and result in nine\ncapability categories, including statics and dynamics, reasoning, and\nmultitasking. Our finding reveals an intriguing trade-off between different\ncapabilities. While GPT models demonstrate superior memorization, they show\nreduced effectiveness in privacy-protection tasks requiring selective\ninformation withholding. Larger models exhibit stronger reasoning capabilities\nbut still struggle with resolving conflicting instructions. Importantly, these\nperformance gaps cannot be attributed solely to information loss, as models\ndemonstrate strong BLEU scores on memorization tasks but their attention\nmechanisms fail to integrate multiple related instructions effectively. These\nfindings highlight critical areas for improvement in complex real-world tasks\ninvolving multi-turn instructions."
                },
                "authors": [
                    {
                        "name": "Chi Han"
                    }
                ],
                "author_detail": {
                    "name": "Chi Han"
                },
                "author": "Chi Han",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13222v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13222v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13208v1",
                "updated": "2025-03-17T14:20:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    20,
                    48,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T14:20:48Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    20,
                    48,
                    0,
                    76,
                    0
                ],
                "title": "Improving Complex Reasoning with Dynamic Prompt Corruption: A soft\n  prompt Optimization Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Complex Reasoning with Dynamic Prompt Corruption: A soft\n  prompt Optimization Approach"
                },
                "summary": "Prompt-tuning (PT) for large language models (LLMs) can facilitate the\nperformance on various conventional NLP tasks with significantly fewer\ntrainable parameters. However, our investigation reveals that PT provides\nlimited improvement and may even degrade the primitive performance of LLMs on\ncomplex reasoning tasks. Such a phenomenon suggests that soft prompts can\npositively impact certain instances while negatively affecting others,\nparticularly during the later phases of reasoning. To address these challenges,\nWe first identify an information accumulation within the soft prompts. Through\ndetailed analysis, we demonstrate that this phenomenon is often accompanied by\nerroneous information flow patterns in the deeper layers of the model, which\nultimately lead to incorrect reasoning outcomes. we propose a novel method\ncalled \\textbf{D}ynamic \\textbf{P}rompt \\textbf{C}orruption (DPC) to take\nbetter advantage of soft prompts in complex reasoning tasks, which dynamically\nadjusts the influence of soft prompts based on their impact on the reasoning\nprocess. Specifically, DPC consists of two stages: Dynamic Trigger and Dynamic\nCorruption. First, Dynamic Trigger measures the impact of soft prompts,\nidentifying whether beneficial or detrimental. Then, Dynamic Corruption\nmitigates the negative effects of soft prompts by selectively masking key\ntokens that interfere with the reasoning process. We validate the proposed\napproach through extensive experiments on various LLMs and reasoning tasks,\nincluding GSM8K, MATH, and AQuA. Experimental results demonstrate that DPC can\nconsistently enhance the performance of PT, achieving 4\\%-8\\% accuracy gains\ncompared to vanilla prompt tuning, highlighting the effectiveness of our\napproach and its potential to enhance complex reasoning in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-tuning (PT) for large language models (LLMs) can facilitate the\nperformance on various conventional NLP tasks with significantly fewer\ntrainable parameters. However, our investigation reveals that PT provides\nlimited improvement and may even degrade the primitive performance of LLMs on\ncomplex reasoning tasks. Such a phenomenon suggests that soft prompts can\npositively impact certain instances while negatively affecting others,\nparticularly during the later phases of reasoning. To address these challenges,\nWe first identify an information accumulation within the soft prompts. Through\ndetailed analysis, we demonstrate that this phenomenon is often accompanied by\nerroneous information flow patterns in the deeper layers of the model, which\nultimately lead to incorrect reasoning outcomes. we propose a novel method\ncalled \\textbf{D}ynamic \\textbf{P}rompt \\textbf{C}orruption (DPC) to take\nbetter advantage of soft prompts in complex reasoning tasks, which dynamically\nadjusts the influence of soft prompts based on their impact on the reasoning\nprocess. Specifically, DPC consists of two stages: Dynamic Trigger and Dynamic\nCorruption. First, Dynamic Trigger measures the impact of soft prompts,\nidentifying whether beneficial or detrimental. Then, Dynamic Corruption\nmitigates the negative effects of soft prompts by selectively masking key\ntokens that interfere with the reasoning process. We validate the proposed\napproach through extensive experiments on various LLMs and reasoning tasks,\nincluding GSM8K, MATH, and AQuA. Experimental results demonstrate that DPC can\nconsistently enhance the performance of PT, achieving 4\\%-8\\% accuracy gains\ncompared to vanilla prompt tuning, highlighting the effectiveness of our\napproach and its potential to enhance complex reasoning in LLMs."
                },
                "authors": [
                    {
                        "name": "Sinan Fan"
                    },
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Chen Shen"
                    },
                    {
                        "name": "Ge Teng"
                    },
                    {
                        "name": "Xiaosong Yuan"
                    },
                    {
                        "name": "Xiaofeng Zhang"
                    },
                    {
                        "name": "Chenxi Huang"
                    },
                    {
                        "name": "Wenxiao Wang"
                    },
                    {
                        "name": "Xiaofei He"
                    },
                    {
                        "name": "Jieping Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jieping Ye"
                },
                "author": "Jieping Ye",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02500v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02500v3",
                "updated": "2025-03-17T14:18:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    18,
                    42,
                    0,
                    76,
                    0
                ],
                "published": "2024-06-04T17:18:40Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    18,
                    40,
                    1,
                    156,
                    0
                ],
                "title": "Towards Efficient Mixture of Experts: A Holistic Study of Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Efficient Mixture of Experts: A Holistic Study of Compression\n  Techniques"
                },
                "summary": "Scaling large language models has driven remarkable advancements across\nvarious domains, yet the continual increase in model size presents significant\nchallenges for real-world deployment. The Mixture of Experts (MoE) architecture\noffers a promising solution by dynamically selecting and activating only a\nsubset of experts during inference, thus substantially reducing computational\ncosts while preserving high performance. Despite these benefits, MoE introduces\nnew inefficiencies, such as excessive parameters and communication overhead. In\nthis work, we present a holistic study of compression techniques for Mixture of\nExperts to enhance both efficiency and scalability. While recent efforts have\nfocused on Expert Trimming, which reduces the number of experts, these\napproaches still suffer from considerable communication and computational\ncosts. To address this, we propose more aggressive strategies, such as Layer\nDrop, which removes entire MoE layers, and Block Drop, which eliminates\ntransformer blocks. Surprisingly, these aggressive pruning techniques not only\npreserve model performance but also substantially improve computation and\nmemory efficiency. Furthermore, beyond Expert Trimming, we also introduce\nExpert Slimming, which compresses individual experts to further boost\nperformance and can be seamlessly integrated with Expert Trimming. Extensive\nexperimental results demonstrate the effectiveness of our proposed\nmethods-Layer Drop and Block Drop-along with the comprehensive recipe that\nintegrates Expert Slimming and Expert Trimming, achieving a 6.05x speedup with\n77.1% reduced memory usage while maintaining over 92% of performance on\nMixtral-8x7B. Our code is released at\nhttps://github.com/CASE-Lab-UMD/Unified-MoE-Compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling large language models has driven remarkable advancements across\nvarious domains, yet the continual increase in model size presents significant\nchallenges for real-world deployment. The Mixture of Experts (MoE) architecture\noffers a promising solution by dynamically selecting and activating only a\nsubset of experts during inference, thus substantially reducing computational\ncosts while preserving high performance. Despite these benefits, MoE introduces\nnew inefficiencies, such as excessive parameters and communication overhead. In\nthis work, we present a holistic study of compression techniques for Mixture of\nExperts to enhance both efficiency and scalability. While recent efforts have\nfocused on Expert Trimming, which reduces the number of experts, these\napproaches still suffer from considerable communication and computational\ncosts. To address this, we propose more aggressive strategies, such as Layer\nDrop, which removes entire MoE layers, and Block Drop, which eliminates\ntransformer blocks. Surprisingly, these aggressive pruning techniques not only\npreserve model performance but also substantially improve computation and\nmemory efficiency. Furthermore, beyond Expert Trimming, we also introduce\nExpert Slimming, which compresses individual experts to further boost\nperformance and can be seamlessly integrated with Expert Trimming. Extensive\nexperimental results demonstrate the effectiveness of our proposed\nmethods-Layer Drop and Block Drop-along with the comprehensive recipe that\nintegrates Expert Slimming and Expert Trimming, achieving a 6.05x speedup with\n77.1% reduced memory usage while maintaining over 92% of performance on\nMixtral-8x7B. Our code is released at\nhttps://github.com/CASE-Lab-UMD/Unified-MoE-Compression."
                },
                "authors": [
                    {
                        "name": "Shwai He"
                    },
                    {
                        "name": "Daize Dong"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Ang Li"
                    }
                ],
                "author_detail": {
                    "name": "Ang Li"
                },
                "author": "Ang Li",
                "arxiv_comment": "Transactions on Machine Learning Research (TMLR)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02500v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02500v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13205v1",
                "updated": "2025-03-17T14:14:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    14,
                    28,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T14:14:28Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    14,
                    14,
                    28,
                    0,
                    76,
                    0
                ],
                "title": "MAP: Evaluation and Multi-Agent Enhancement of Large Language Models for\n  Inpatient Pathways",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAP: Evaluation and Multi-Agent Enhancement of Large Language Models for\n  Inpatient Pathways"
                },
                "summary": "Inpatient pathways demand complex clinical decision-making based on\ncomprehensive patient information, posing critical challenges for clinicians.\nDespite advancements in large language models (LLMs) in medical applications,\nlimited research focused on artificial intelligence (AI) inpatient pathways\nsystems, due to the lack of large-scale inpatient datasets. Moreover, existing\nmedical benchmarks typically concentrated on medical question-answering and\nexaminations, ignoring the multifaceted nature of clinical decision-making in\ninpatient settings. To address these gaps, we first developed the Inpatient\nPathway Decision Support (IPDS) benchmark from the MIMIC-IV database,\nencompassing 51,274 cases across nine triage departments and 17 major disease\ncategories alongside 16 standardized treatment options. Then, we proposed the\nMulti-Agent Inpatient Pathways (MAP) framework to accomplish inpatient pathways\nwith three clinical agents, including a triage agent managing the patient\nadmission, a diagnosis agent serving as the primary decision maker at the\ndepartment, and a treatment agent providing treatment plans. Additionally, our\nMAP framework includes a chief agent overseeing the inpatient pathways to guide\nand promote these three clinician agents. Extensive experiments showed our MAP\nimproved the diagnosis accuracy by 25.10% compared to the state-of-the-art LLM\nHuatuoGPT2-13B. It is worth noting that our MAP demonstrated significant\nclinical compliance, outperforming three board-certified clinicians by 10%-12%,\nestablishing a foundation for inpatient pathways systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inpatient pathways demand complex clinical decision-making based on\ncomprehensive patient information, posing critical challenges for clinicians.\nDespite advancements in large language models (LLMs) in medical applications,\nlimited research focused on artificial intelligence (AI) inpatient pathways\nsystems, due to the lack of large-scale inpatient datasets. Moreover, existing\nmedical benchmarks typically concentrated on medical question-answering and\nexaminations, ignoring the multifaceted nature of clinical decision-making in\ninpatient settings. To address these gaps, we first developed the Inpatient\nPathway Decision Support (IPDS) benchmark from the MIMIC-IV database,\nencompassing 51,274 cases across nine triage departments and 17 major disease\ncategories alongside 16 standardized treatment options. Then, we proposed the\nMulti-Agent Inpatient Pathways (MAP) framework to accomplish inpatient pathways\nwith three clinical agents, including a triage agent managing the patient\nadmission, a diagnosis agent serving as the primary decision maker at the\ndepartment, and a treatment agent providing treatment plans. Additionally, our\nMAP framework includes a chief agent overseeing the inpatient pathways to guide\nand promote these three clinician agents. Extensive experiments showed our MAP\nimproved the diagnosis accuracy by 25.10% compared to the state-of-the-art LLM\nHuatuoGPT2-13B. It is worth noting that our MAP demonstrated significant\nclinical compliance, outperforming three board-certified clinicians by 10%-12%,\nestablishing a foundation for inpatient pathways systems."
                },
                "authors": [
                    {
                        "name": "Zhen Chen"
                    },
                    {
                        "name": "Zhihao Peng"
                    },
                    {
                        "name": "Xusheng Liang"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Peigan Liang"
                    },
                    {
                        "name": "Linsheng Zeng"
                    },
                    {
                        "name": "Minjie Ju"
                    },
                    {
                        "name": "Yixuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Yixuan Yuan"
                },
                "author": "Yixuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.07207v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.07207v3",
                "updated": "2025-03-17T13:51:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    13,
                    51,
                    51,
                    0,
                    76,
                    0
                ],
                "published": "2023-06-12T16:11:10Z",
                "published_parsed": [
                    2023,
                    6,
                    12,
                    16,
                    11,
                    10,
                    0,
                    163,
                    0
                ],
                "title": "Valley: Video Assistant with Large Language model Enhanced abilitY",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Valley: Video Assistant with Large Language model Enhanced abilitY"
                },
                "summary": "Large Language Models (LLMs), with remarkable conversational capability, have\nemerged as AI assistants that can handle both visual and textual modalities.\nHowever, their effectiveness in joint video and language understanding has not\nbeen extensively explored. In the paper, we introduce Valley, a multi-modal\nfoundation model that is designed to enable enhanced video comprehension and\ninstruction-following capabilities. To this end, we construct two datasets,\nnamely Valley-702k and Valley-instruct-73k, to cover a diverse range of\nvideo-text alignment and video-based instruction tasks, such as multi-shot\ncaptions, long video descriptions, action recognition, causal inference, etc.\nThen, we adopt ViT-L/14 as the vision encoder and explore three different\ntemporal modeling modules to learn multifaceted features for enhanced video\nunderstanding. In addition, we implement a two-phase training approach for\nValley: the first phase focuses solely on training the projection module to\nfacilitate the LLM's capacity to understand visual input, and the second phase\njointly trains the projection module and the LLM to improve their instruction\nfollowing ability. Extensive experiments demonstrate that Valley has the\npotential to serve as an effective video assistant, simplifying complex\nvideo-understanding scenarios. Our code and data are published anonymously at\nhttps://github.com/valley-vl/Valley.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), with remarkable conversational capability, have\nemerged as AI assistants that can handle both visual and textual modalities.\nHowever, their effectiveness in joint video and language understanding has not\nbeen extensively explored. In the paper, we introduce Valley, a multi-modal\nfoundation model that is designed to enable enhanced video comprehension and\ninstruction-following capabilities. To this end, we construct two datasets,\nnamely Valley-702k and Valley-instruct-73k, to cover a diverse range of\nvideo-text alignment and video-based instruction tasks, such as multi-shot\ncaptions, long video descriptions, action recognition, causal inference, etc.\nThen, we adopt ViT-L/14 as the vision encoder and explore three different\ntemporal modeling modules to learn multifaceted features for enhanced video\nunderstanding. In addition, we implement a two-phase training approach for\nValley: the first phase focuses solely on training the projection module to\nfacilitate the LLM's capacity to understand visual input, and the second phase\njointly trains the projection module and the LLM to improve their instruction\nfollowing ability. Extensive experiments demonstrate that Valley has the\npotential to serve as an effective video assistant, simplifying complex\nvideo-understanding scenarios. Our code and data are published anonymously at\nhttps://github.com/valley-vl/Valley."
                },
                "authors": [
                    {
                        "name": "Ruipu Luo"
                    },
                    {
                        "name": "Ziwang Zhao"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Zheming Yang"
                    },
                    {
                        "name": "Minghui Qiu"
                    },
                    {
                        "name": "Tao Wang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    },
                    {
                        "name": "Yanhao Wang"
                    },
                    {
                        "name": "Cen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Cen Chen"
                },
                "author": "Cen Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.07207v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.07207v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21016v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21016v2",
                "updated": "2025-03-17T13:42:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    13,
                    42,
                    6,
                    0,
                    76,
                    0
                ],
                "published": "2024-12-30T15:33:34Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    34,
                    0,
                    365,
                    0
                ],
                "title": "Assessing the Robustness of LLM-based NLP Software via Automated Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Robustness of LLM-based NLP Software via Automated Testing"
                },
                "summary": "Benefiting from the advancements in LLMs, NLP software has undergone rapid\ndevelopment. Such software is widely employed in various safety-critical tasks,\nsuch as financial sentiment analysis, toxic content moderation, and log\ngeneration. Unlike traditional software, LLM-based NLP software relies on\nprompts and examples as inputs. Given the complexity of LLMs and the\nunpredictability of real-world inputs, quantitatively assessing the robustness\nof such software is crucial. However, to the best of our knowledge, no\nautomated robustness testing methods have been specifically designed to\nevaluate the overall inputs of LLM-based NLP software. To this end, this paper\nintroduces the first AutOmated Robustness Testing frAmework, AORTA, which\nreconceptualizes the testing process into a combinatorial optimization problem.\nExisting testing methods designed for DNN-based software can be applied to\nLLM-based software by AORTA, but their effectiveness is limited. To address\nthis, we propose a novel testing method for LLM-based software within AORTA\ncalled Adaptive Beam Search. ABS is tailored for the expansive feature space of\nLLMs and improves testing effectiveness through an adaptive beam width and the\ncapability for backtracking. We successfully embed 18 test methods in the\ndesigned framework AORTA and compared the test validity of ABS with three\ndatasets and five threat models. ABS facilitates a more comprehensive and\naccurate robustness assessment before software deployment, with an average test\nsuccess rate of 86.138%. Compared to the currently best-performing baseline\nPWWS, ABS significantly reduces the computational overhead by up to 3441.895\nseconds per successful test case and decreases the number of queries by 218.762\ntimes on average. Furthermore, test cases generated by ABS exhibit greater\nnaturalness and transferability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benefiting from the advancements in LLMs, NLP software has undergone rapid\ndevelopment. Such software is widely employed in various safety-critical tasks,\nsuch as financial sentiment analysis, toxic content moderation, and log\ngeneration. Unlike traditional software, LLM-based NLP software relies on\nprompts and examples as inputs. Given the complexity of LLMs and the\nunpredictability of real-world inputs, quantitatively assessing the robustness\nof such software is crucial. However, to the best of our knowledge, no\nautomated robustness testing methods have been specifically designed to\nevaluate the overall inputs of LLM-based NLP software. To this end, this paper\nintroduces the first AutOmated Robustness Testing frAmework, AORTA, which\nreconceptualizes the testing process into a combinatorial optimization problem.\nExisting testing methods designed for DNN-based software can be applied to\nLLM-based software by AORTA, but their effectiveness is limited. To address\nthis, we propose a novel testing method for LLM-based software within AORTA\ncalled Adaptive Beam Search. ABS is tailored for the expansive feature space of\nLLMs and improves testing effectiveness through an adaptive beam width and the\ncapability for backtracking. We successfully embed 18 test methods in the\ndesigned framework AORTA and compared the test validity of ABS with three\ndatasets and five threat models. ABS facilitates a more comprehensive and\naccurate robustness assessment before software deployment, with an average test\nsuccess rate of 86.138%. Compared to the currently best-performing baseline\nPWWS, ABS significantly reduces the computational overhead by up to 3441.895\nseconds per successful test case and decreases the number of queries by 218.762\ntimes on average. Furthermore, test cases generated by ABS exhibit greater\nnaturalness and transferability."
                },
                "authors": [
                    {
                        "name": "Mingxuan Xiao"
                    },
                    {
                        "name": "Yan Xiao"
                    },
                    {
                        "name": "Shunhui Ji"
                    },
                    {
                        "name": "Hanbo Cai"
                    },
                    {
                        "name": "Lei Xue"
                    },
                    {
                        "name": "Pengcheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Pengcheng Zhang"
                },
                "author": "Pengcheng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21016v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21016v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06027v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06027v2",
                "updated": "2025-03-17T13:37:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    13,
                    37,
                    33,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-08T02:59:51Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    2,
                    59,
                    51,
                    5,
                    67,
                    0
                ],
                "title": "Empowering Edge Intelligence: A Comprehensive Survey on On-Device AI\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering Edge Intelligence: A Comprehensive Survey on On-Device AI\n  Models"
                },
                "summary": "The rapid advancement of artificial intelligence (AI) technologies has led to\nan increasing deployment of AI models on edge and terminal devices, driven by\nthe proliferation of the Internet of Things (IoT) and the need for real-time\ndata processing. This survey comprehensively explores the current state,\ntechnical challenges, and future trends of on-device AI models. We define\non-device AI models as those designed to perform local data processing and\ninference, emphasizing their characteristics such as real-time performance,\nresource constraints, and enhanced data privacy. The survey is structured\naround key themes, including the fundamental concepts of AI models, application\nscenarios across various domains, and the technical challenges faced in edge\nenvironments. We also discuss optimization and implementation strategies, such\nas data preprocessing, model compression, and hardware acceleration, which are\nessential for effective deployment. Furthermore, we examine the impact of\nemerging technologies, including edge computing and foundation models, on the\nevolution of on-device AI models. By providing a structured overview of the\nchallenges, solutions, and future directions, this survey aims to facilitate\nfurther research and application of on-device AI, ultimately contributing to\nthe advancement of intelligent systems in everyday life.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of artificial intelligence (AI) technologies has led to\nan increasing deployment of AI models on edge and terminal devices, driven by\nthe proliferation of the Internet of Things (IoT) and the need for real-time\ndata processing. This survey comprehensively explores the current state,\ntechnical challenges, and future trends of on-device AI models. We define\non-device AI models as those designed to perform local data processing and\ninference, emphasizing their characteristics such as real-time performance,\nresource constraints, and enhanced data privacy. The survey is structured\naround key themes, including the fundamental concepts of AI models, application\nscenarios across various domains, and the technical challenges faced in edge\nenvironments. We also discuss optimization and implementation strategies, such\nas data preprocessing, model compression, and hardware acceleration, which are\nessential for effective deployment. Furthermore, we examine the impact of\nemerging technologies, including edge computing and foundation models, on the\nevolution of on-device AI models. By providing a structured overview of the\nchallenges, solutions, and future directions, this survey aims to facilitate\nfurther research and application of on-device AI, ultimately contributing to\nthe advancement of intelligent systems in everyday life."
                },
                "authors": [
                    {
                        "name": "Xubin Wang"
                    },
                    {
                        "name": "Zhiqing Tang"
                    },
                    {
                        "name": "Jianxiong Guo"
                    },
                    {
                        "name": "Tianhui Meng"
                    },
                    {
                        "name": "Chenhao Wang"
                    },
                    {
                        "name": "Tian Wang"
                    },
                    {
                        "name": "Weijia Jia"
                    }
                ],
                "author_detail": {
                    "name": "Weijia Jia"
                },
                "author": "Weijia Jia",
                "arxiv_doi": "10.1145/3724420",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3724420",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.06027v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06027v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This paper has been accepted by ACM Computing Surveys",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04927v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04927v3",
                "updated": "2025-03-17T13:34:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    13,
                    34,
                    7,
                    0,
                    76,
                    0
                ],
                "published": "2024-06-07T13:33:22Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    13,
                    33,
                    22,
                    4,
                    159,
                    0
                ],
                "title": "LLM-based speaker diarization correction: A generalizable approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based speaker diarization correction: A generalizable approach"
                },
                "summary": "Speaker diarization is necessary for interpreting conversations transcribed\nusing automated speech recognition (ASR) tools. Despite significant\ndevelopments in diarization methods, diarization accuracy remains an issue.\nHere, we investigate the use of large language models (LLMs) for diarization\ncorrection as a post-processing step. LLMs were fine-tuned using the Fisher\ncorpus, a large dataset of transcribed conversations. The ability of the models\nto improve diarization accuracy in a holdout dataset from the Fisher corpus as\nwell as an independent dataset was measured. We report that fine-tuned LLMs can\nmarkedly improve diarization accuracy. However, model performance is\nconstrained to transcripts produced using the same ASR tool as the transcripts\nused for fine-tuning, limiting generalizability. To address this constraint, an\nensemble model was developed by combining weights from three separate models,\neach fine-tuned using transcripts from a different ASR tool. The ensemble model\ndemonstrated better overall performance than each of the ASR-specific models,\nsuggesting that a generalizable and ASR-agnostic approach may be achievable. We\nhave made the weights of these models publicly available on HuggingFace at\nhttps://huggingface.co/bklynhlth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speaker diarization is necessary for interpreting conversations transcribed\nusing automated speech recognition (ASR) tools. Despite significant\ndevelopments in diarization methods, diarization accuracy remains an issue.\nHere, we investigate the use of large language models (LLMs) for diarization\ncorrection as a post-processing step. LLMs were fine-tuned using the Fisher\ncorpus, a large dataset of transcribed conversations. The ability of the models\nto improve diarization accuracy in a holdout dataset from the Fisher corpus as\nwell as an independent dataset was measured. We report that fine-tuned LLMs can\nmarkedly improve diarization accuracy. However, model performance is\nconstrained to transcripts produced using the same ASR tool as the transcripts\nused for fine-tuning, limiting generalizability. To address this constraint, an\nensemble model was developed by combining weights from three separate models,\neach fine-tuned using transcripts from a different ASR tool. The ensemble model\ndemonstrated better overall performance than each of the ASR-specific models,\nsuggesting that a generalizable and ASR-agnostic approach may be achievable. We\nhave made the weights of these models publicly available on HuggingFace at\nhttps://huggingface.co/bklynhlth."
                },
                "authors": [
                    {
                        "name": "Georgios Efstathiadis"
                    },
                    {
                        "name": "Vijay Yadav"
                    },
                    {
                        "name": "Anzar Abbas"
                    }
                ],
                "author_detail": {
                    "name": "Anzar Abbas"
                },
                "author": "Anzar Abbas",
                "arxiv_doi": "10.1016/j.specom.2025.103224",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.specom.2025.103224",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.04927v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04927v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Speech Communication, Volume 170, 2025, Page 103224",
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13149v1",
                "updated": "2025-03-17T13:20:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    13,
                    20,
                    9,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T13:20:09Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    13,
                    20,
                    9,
                    0,
                    76,
                    0
                ],
                "title": "Are LLMs (Really) Ideological? An IRT-based Analysis and Alignment Tool\n  for Perceived Socio-Economic Bias in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are LLMs (Really) Ideological? An IRT-based Analysis and Alignment Tool\n  for Perceived Socio-Economic Bias in LLMs"
                },
                "summary": "We introduce an Item Response Theory (IRT)-based framework to detect and\nquantify socioeconomic bias in large language models (LLMs) without relying on\nsubjective human judgments. Unlike traditional methods, IRT accounts for item\ndifficulty, improving ideological bias estimation. We fine-tune two LLM\nfamilies (Meta-LLaMa 3.2-1B-Instruct and Chat- GPT 3.5) to represent distinct\nideological positions and introduce a two-stage approach: (1) modeling response\navoidance and (2) estimating perceived bias in answered responses. Our results\nshow that off-the-shelf LLMs often avoid ideological engagement rather than\nexhibit bias, challenging prior claims of partisanship. This empirically\nvalidated framework enhances AI alignment research and promotes fairer AI\ngovernance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an Item Response Theory (IRT)-based framework to detect and\nquantify socioeconomic bias in large language models (LLMs) without relying on\nsubjective human judgments. Unlike traditional methods, IRT accounts for item\ndifficulty, improving ideological bias estimation. We fine-tune two LLM\nfamilies (Meta-LLaMa 3.2-1B-Instruct and Chat- GPT 3.5) to represent distinct\nideological positions and introduce a two-stage approach: (1) modeling response\navoidance and (2) estimating perceived bias in answered responses. Our results\nshow that off-the-shelf LLMs often avoid ideological engagement rather than\nexhibit bias, challenging prior claims of partisanship. This empirically\nvalidated framework enhances AI alignment research and promotes fairer AI\ngovernance."
                },
                "authors": [
                    {
                        "name": "Jasmin Wachter"
                    },
                    {
                        "name": "Michael Radloff"
                    },
                    {
                        "name": "Maja Smolej"
                    },
                    {
                        "name": "Katharina Kinder-Kurlanda"
                    }
                ],
                "author_detail": {
                    "name": "Katharina Kinder-Kurlanda"
                },
                "author": "Katharina Kinder-Kurlanda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11367v2",
                "updated": "2025-03-17T12:39:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    39,
                    15,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-14T13:07:45Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    13,
                    7,
                    45,
                    4,
                    73,
                    0
                ],
                "title": "Cornstarch: Distributed Multimodal Training Must Be Multimodality-Aware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cornstarch: Distributed Multimodal Training Must Be Multimodality-Aware"
                },
                "summary": "Multimodal large language models (MLLMs) extend the capabilities of large\nlanguage models (LLMs) by combining heterogeneous model architectures to handle\ndiverse modalities like images and audio. However, this inherent heterogeneity\nin MLLM model structure and data types makes makeshift extensions to existing\nLLM training frameworks unsuitable for efficient MLLM training.\n  In this paper, we present Cornstarch, the first general-purpose distributed\nMLLM training framework. Cornstarch facilitates modular MLLM construction,\nenables composable parallelization of constituent models, and introduces\nMLLM-specific optimizations to pipeline and context parallelism for efficient\ndistributed MLLM training. Our evaluation shows that Cornstarch outperforms\nstate-of-the-art solutions by up to $1.57\\times$ in terms of training\nthroughput.\n  Cornstarch is an open-source project available at\nhttps://github.com/cornstarch-org/Cornstarch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) extend the capabilities of large\nlanguage models (LLMs) by combining heterogeneous model architectures to handle\ndiverse modalities like images and audio. However, this inherent heterogeneity\nin MLLM model structure and data types makes makeshift extensions to existing\nLLM training frameworks unsuitable for efficient MLLM training.\n  In this paper, we present Cornstarch, the first general-purpose distributed\nMLLM training framework. Cornstarch facilitates modular MLLM construction,\nenables composable parallelization of constituent models, and introduces\nMLLM-specific optimizations to pipeline and context parallelism for efficient\ndistributed MLLM training. Our evaluation shows that Cornstarch outperforms\nstate-of-the-art solutions by up to $1.57\\times$ in terms of training\nthroughput.\n  Cornstarch is an open-source project available at\nhttps://github.com/cornstarch-org/Cornstarch."
                },
                "authors": [
                    {
                        "name": "Insu Jang"
                    },
                    {
                        "name": "Runyu Lu"
                    },
                    {
                        "name": "Nikhil Bansal"
                    },
                    {
                        "name": "Ang Chen"
                    },
                    {
                        "name": "Mosharaf Chowdhury"
                    }
                ],
                "author_detail": {
                    "name": "Mosharaf Chowdhury"
                },
                "author": "Mosharaf Chowdhury",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13116v1",
                "updated": "2025-03-17T12:38:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    38,
                    3,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T12:38:03Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    38,
                    3,
                    0,
                    76,
                    0
                ],
                "title": "VeriLeaky: Navigating IP Protection vs Utility in Fine-Tuning for\n  LLM-Driven Verilog Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeriLeaky: Navigating IP Protection vs Utility in Fine-Tuning for\n  LLM-Driven Verilog Coding"
                },
                "summary": "Large language models (LLMs) offer significant potential for coding, yet\nfine-tuning (FT) with curated data is essential for niche languages like\nVerilog. Using proprietary intellectual property (IP) for FT presents a serious\nrisk, as FT data can be leaked through LLM inference. This leads to a critical\ndilemma for design houses: seeking to build externally accessible LLMs offering\ncompetitive Verilog coding, how can they leverage in-house IP to enhance FT\nutility while ensuring IP protection?\n  For the first time in the literature, we study this dilemma. Using LLaMA\n3.1-8B, we conduct in-house FT on a baseline Verilog dataset (RTLCoder)\nsupplemented with our own in-house IP, which is validated through multiple\ntape-outs. To rigorously assess IP leakage, we quantify structural similarity\n(AST/Dolos) and functional equivalence (Synopsys Formality) between generated\ncodes and our in-house IP. We show that our IP can indeed be leaked, confirming\nthe threat. As defense, we evaluate logic locking of Verilog codes (ASSURE).\nThis offers some level of protection, yet reduces the IP's utility for FT and\ndegrades the LLM's performance. Our study shows the need for novel strategies\nthat are both effective and minimally disruptive to FT, an essential effort for\nenabling design houses to fully utilize their proprietary IP toward LLM-driven\nVerilog coding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) offer significant potential for coding, yet\nfine-tuning (FT) with curated data is essential for niche languages like\nVerilog. Using proprietary intellectual property (IP) for FT presents a serious\nrisk, as FT data can be leaked through LLM inference. This leads to a critical\ndilemma for design houses: seeking to build externally accessible LLMs offering\ncompetitive Verilog coding, how can they leverage in-house IP to enhance FT\nutility while ensuring IP protection?\n  For the first time in the literature, we study this dilemma. Using LLaMA\n3.1-8B, we conduct in-house FT on a baseline Verilog dataset (RTLCoder)\nsupplemented with our own in-house IP, which is validated through multiple\ntape-outs. To rigorously assess IP leakage, we quantify structural similarity\n(AST/Dolos) and functional equivalence (Synopsys Formality) between generated\ncodes and our in-house IP. We show that our IP can indeed be leaked, confirming\nthe threat. As defense, we evaluate logic locking of Verilog codes (ASSURE).\nThis offers some level of protection, yet reduces the IP's utility for FT and\ndegrades the LLM's performance. Our study shows the need for novel strategies\nthat are both effective and minimally disruptive to FT, an essential effort for\nenabling design houses to fully utilize their proprietary IP toward LLM-driven\nVerilog coding."
                },
                "authors": [
                    {
                        "name": "Zeng Wang"
                    },
                    {
                        "name": "Minghao Shao"
                    },
                    {
                        "name": "Mohammed Nabeel"
                    },
                    {
                        "name": "Prithwish Basu Roy"
                    },
                    {
                        "name": "Likhitha Mankali"
                    },
                    {
                        "name": "Jitendra Bhandari"
                    },
                    {
                        "name": "Ramesh Karri"
                    },
                    {
                        "name": "Ozgur Sinanoglu"
                    },
                    {
                        "name": "Muhammad Shafique"
                    },
                    {
                        "name": "Johann Knechtel"
                    }
                ],
                "author_detail": {
                    "name": "Johann Knechtel"
                },
                "author": "Johann Knechtel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13111v1",
                "updated": "2025-03-17T12:34:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    34,
                    22,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T12:34:22Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    34,
                    22,
                    0,
                    76,
                    0
                ],
                "title": "MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs"
                },
                "summary": "Multimodal large language models (MLLMs) excel at 2D visual understanding but\nremain limited in their ability to reason about 3D space. In this work, we\nleverage large-scale high-quality 3D scene data with open-set annotations to\nintroduce 1) a novel supervised fine-tuning dataset and 2) a new evaluation\nbenchmark, focused on indoor scenes. Our Cubify Anything VQA (CA-VQA) data\ncovers diverse spatial tasks including spatial relationship prediction, metric\nsize and distance estimation, and 3D grounding. We show that CA-VQA enables us\nto train MM-Spatial, a strong generalist MLLM that also achieves\nstate-of-the-art performance on 3D spatial understanding benchmarks, including\nour own. We show how incorporating metric depth and multi-view inputs (provided\nin CA-VQA) can further improve 3D understanding, and demonstrate that data\nalone allows our model to achieve depth perception capabilities comparable to\ndedicated monocular depth estimation models. We will publish our SFT dataset\nand benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) excel at 2D visual understanding but\nremain limited in their ability to reason about 3D space. In this work, we\nleverage large-scale high-quality 3D scene data with open-set annotations to\nintroduce 1) a novel supervised fine-tuning dataset and 2) a new evaluation\nbenchmark, focused on indoor scenes. Our Cubify Anything VQA (CA-VQA) data\ncovers diverse spatial tasks including spatial relationship prediction, metric\nsize and distance estimation, and 3D grounding. We show that CA-VQA enables us\nto train MM-Spatial, a strong generalist MLLM that also achieves\nstate-of-the-art performance on 3D spatial understanding benchmarks, including\nour own. We show how incorporating metric depth and multi-view inputs (provided\nin CA-VQA) can further improve 3D understanding, and demonstrate that data\nalone allows our model to achieve depth perception capabilities comparable to\ndedicated monocular depth estimation models. We will publish our SFT dataset\nand benchmark."
                },
                "authors": [
                    {
                        "name": "Erik Daxberger"
                    },
                    {
                        "name": "Nina Wenzel"
                    },
                    {
                        "name": "David Griffiths"
                    },
                    {
                        "name": "Haiming Gang"
                    },
                    {
                        "name": "Justin Lazarow"
                    },
                    {
                        "name": "Gefen Kohavi"
                    },
                    {
                        "name": "Kai Kang"
                    },
                    {
                        "name": "Marcin Eichner"
                    },
                    {
                        "name": "Yinfei Yang"
                    },
                    {
                        "name": "Afshin Dehghan"
                    },
                    {
                        "name": "Peter Grasch"
                    }
                ],
                "author_detail": {
                    "name": "Peter Grasch"
                },
                "author": "Peter Grasch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13108v1",
                "updated": "2025-03-17T12:31:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    31,
                    23,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T12:31:23Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    31,
                    23,
                    0,
                    76,
                    0
                ],
                "title": "Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways\n  to Faster Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways\n  to Faster Inference"
                },
                "summary": "Multimodal large language models (MLLMs) improve performance on\nvision-language tasks by integrating visual features from pre-trained vision\nencoders into large language models (LLMs). However, how MLLMs process and\nutilize visual information remains unclear. In this paper, a shift in the\ndominant flow of visual information is uncovered: (1) in shallow layers, strong\ninteractions are observed between image tokens and instruction tokens, where\nmost visual information is injected into instruction tokens to form cross-modal\nsemantic representations; (2) in deeper layers, image tokens primarily interact\nwith each other, aggregating the remaining visual information to optimize\nsemantic representations within visual modality. Based on these insights, we\npropose Hierarchical Modality-Aware Pruning (HiMAP), a plug-and-play inference\nacceleration method that dynamically prunes image tokens at specific layers,\nreducing computational costs by approximately 65% without sacrificing\nperformance. Our findings offer a new understanding of visual information\nprocessing in MLLMs and provide a state-of-the-art solution for efficient\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) improve performance on\nvision-language tasks by integrating visual features from pre-trained vision\nencoders into large language models (LLMs). However, how MLLMs process and\nutilize visual information remains unclear. In this paper, a shift in the\ndominant flow of visual information is uncovered: (1) in shallow layers, strong\ninteractions are observed between image tokens and instruction tokens, where\nmost visual information is injected into instruction tokens to form cross-modal\nsemantic representations; (2) in deeper layers, image tokens primarily interact\nwith each other, aggregating the remaining visual information to optimize\nsemantic representations within visual modality. Based on these insights, we\npropose Hierarchical Modality-Aware Pruning (HiMAP), a plug-and-play inference\nacceleration method that dynamically prunes image tokens at specific layers,\nreducing computational costs by approximately 65% without sacrificing\nperformance. Our findings offer a new understanding of visual information\nprocessing in MLLMs and provide a state-of-the-art solution for efficient\ninference."
                },
                "authors": [
                    {
                        "name": "Hao Yin"
                    },
                    {
                        "name": "Guangzong Si"
                    },
                    {
                        "name": "Zilei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zilei Wang"
                },
                "author": "Zilei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10880v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10880v2",
                "updated": "2025-03-17T12:29:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    29,
                    5,
                    0,
                    76,
                    0
                ],
                "published": "2024-10-09T15:36:42Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    36,
                    42,
                    2,
                    283,
                    0
                ],
                "title": "Fine-tuning can Help Detect Pretraining Data from Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning can Help Detect Pretraining Data from Large Language Models"
                },
                "summary": "In the era of large language models (LLMs), detecting pretraining data has\nbeen increasingly important due to concerns about fair evaluation and ethical\nrisks. Current methods differentiate members and non-members by designing\nscoring functions, like Perplexity and Min-k%. However, the diversity and\ncomplexity of training data magnifies the difficulty of distinguishing, leading\nto suboptimal performance in detecting pretraining data. In this paper, we\nfirst explore the benefits of unseen data, which can be easily collected after\nthe release of the LLM. We find that the perplexities of LLMs shift differently\nfor members and non-members, after fine-tuning with a small amount of\npreviously unseen data. In light of this, we introduce a novel and effective\nmethod termed Fine-tuned Score Deviation(FSD), which improves the performance\nof current scoring functions for pretraining data detection. In particular, we\npropose to measure the deviation distance of current scores after fine-tuning\non a small amount of unseen data within the same domain. In effect, using a few\nunseen data can largely decrease the scores of all non-members, leading to a\nlarger deviation distance than members. Extensive experiments demonstrate the\neffectiveness of our method, significantly improving the AUC score on common\nbenchmark datasets across various models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of large language models (LLMs), detecting pretraining data has\nbeen increasingly important due to concerns about fair evaluation and ethical\nrisks. Current methods differentiate members and non-members by designing\nscoring functions, like Perplexity and Min-k%. However, the diversity and\ncomplexity of training data magnifies the difficulty of distinguishing, leading\nto suboptimal performance in detecting pretraining data. In this paper, we\nfirst explore the benefits of unseen data, which can be easily collected after\nthe release of the LLM. We find that the perplexities of LLMs shift differently\nfor members and non-members, after fine-tuning with a small amount of\npreviously unseen data. In light of this, we introduce a novel and effective\nmethod termed Fine-tuned Score Deviation(FSD), which improves the performance\nof current scoring functions for pretraining data detection. In particular, we\npropose to measure the deviation distance of current scores after fine-tuning\non a small amount of unseen data within the same domain. In effect, using a few\nunseen data can largely decrease the scores of all non-members, leading to a\nlarger deviation distance than members. Extensive experiments demonstrate the\neffectiveness of our method, significantly improving the AUC score on common\nbenchmark datasets across various models."
                },
                "authors": [
                    {
                        "name": "Hengxiang Zhang"
                    },
                    {
                        "name": "Songxin Zhang"
                    },
                    {
                        "name": "Bingyi Jing"
                    },
                    {
                        "name": "Hongxin Wei"
                    }
                ],
                "author_detail": {
                    "name": "Hongxin Wei"
                },
                "author": "Hongxin Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10880v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10880v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13105v1",
                "updated": "2025-03-17T12:25:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    25,
                    42,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T12:25:42Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    25,
                    42,
                    0,
                    76,
                    0
                ],
                "title": "Managing Hybrid Solid-State Drives Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Managing Hybrid Solid-State Drives Using Large Language Models"
                },
                "summary": "Hybrid Solid-State Drives (SSDs), which integrate several types of flash\ncells (e.g., single-level cell (SLC) and multiple-level cell (MLC)) in a single\ndrive and enable them to convert between each other, are designed to deliver\nboth high performance and high storage capacity. However, compared to\ntraditional SSDs, hybrid SSDs also introduce a much larger design space,\nresulting in higher optimization complexity due to more design factors\ninvolved, including flash conversion timing and data migration between\ndifferent flash cells, etc. To address these challenges, large language models\n(LLMs) could be a promising technique, as they excel in handling complex,\nhigh-dimensional parameter space exploration by leveraging their advanced\ncapability to identify patterns and optimize solutions. Recent works have\nstarted exploring the use of LLMs to optimize computer systems. However, to the\nbest of our knowledge, no study has focused on optimizing SSDs with the\nassistance of LLMs.\n  In this work, we explore the potential of LLMs in understanding and\nefficiently managing hybrid SSD design space. Specifically, two important\nquestions are exploited and analyzed: 1) Can LLMs offer optimization potential\nfor Hybrid SSD management? 2) How to leverage LLMs for the performance and\nefficiency of hybrid SSD optimization? Based on the observations of\nexploration, we propose a comprehensive auto-tuning framework for hybrid SSDs,\nintegrating LLMs to recommend customized configurations using calibration\nprompts derived from hardware, system, and workload information. Experimental\nresults reveal a 62.35% improvement in throughput and a 57.99% decrease in\nwrite amplification compared to the default hybrid SSD configurations achieved\nwith the incorporation of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Solid-State Drives (SSDs), which integrate several types of flash\ncells (e.g., single-level cell (SLC) and multiple-level cell (MLC)) in a single\ndrive and enable them to convert between each other, are designed to deliver\nboth high performance and high storage capacity. However, compared to\ntraditional SSDs, hybrid SSDs also introduce a much larger design space,\nresulting in higher optimization complexity due to more design factors\ninvolved, including flash conversion timing and data migration between\ndifferent flash cells, etc. To address these challenges, large language models\n(LLMs) could be a promising technique, as they excel in handling complex,\nhigh-dimensional parameter space exploration by leveraging their advanced\ncapability to identify patterns and optimize solutions. Recent works have\nstarted exploring the use of LLMs to optimize computer systems. However, to the\nbest of our knowledge, no study has focused on optimizing SSDs with the\nassistance of LLMs.\n  In this work, we explore the potential of LLMs in understanding and\nefficiently managing hybrid SSD design space. Specifically, two important\nquestions are exploited and analyzed: 1) Can LLMs offer optimization potential\nfor Hybrid SSD management? 2) How to leverage LLMs for the performance and\nefficiency of hybrid SSD optimization? Based on the observations of\nexploration, we propose a comprehensive auto-tuning framework for hybrid SSDs,\nintegrating LLMs to recommend customized configurations using calibration\nprompts derived from hardware, system, and workload information. Experimental\nresults reveal a 62.35% improvement in throughput and a 57.99% decrease in\nwrite amplification compared to the default hybrid SSD configurations achieved\nwith the incorporation of LLMs."
                },
                "authors": [
                    {
                        "name": "Qian Wei"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Zehao Chen"
                    },
                    {
                        "name": "Zhaoyan Shen"
                    },
                    {
                        "name": "Dongxiao Yu"
                    },
                    {
                        "name": "Bingzhe Li"
                    }
                ],
                "author_detail": {
                    "name": "Bingzhe Li"
                },
                "author": "Bingzhe Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13102v1",
                "updated": "2025-03-17T12:15:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    15,
                    16,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T12:15:16Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    15,
                    16,
                    0,
                    76,
                    0
                ],
                "title": "REPA: Russian Error Types Annotation for Evaluating Text Generation and\n  Judgment Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REPA: Russian Error Types Annotation for Evaluating Text Generation and\n  Judgment Capabilities"
                },
                "summary": "Recent advances in large language models (LLMs) have introduced the novel\nparadigm of using LLMs as judges, where an LLM evaluates and scores the outputs\nof another LLM, which often correlates highly with human preferences. However,\nthe use of LLM-as-a-judge has been primarily studied in English. In this paper,\nwe evaluate this framework in Russian by introducing the Russian Error tyPes\nAnnotation dataset (REPA), a dataset of 1k user queries and 2k LLM-generated\nresponses. Human annotators labeled each response pair expressing their\npreferences across ten specific error types, as well as selecting an overall\npreference. We rank six generative LLMs across the error types using three\nrating systems based on human preferences. We also evaluate responses using\neight LLM judges in zero-shot and few-shot settings. We describe the results of\nanalyzing the judges and position and length biases. Our findings reveal a\nnotable gap between LLM judge performance in Russian and English. However,\nrankings based on human and LLM preferences show partial alignment, suggesting\nthat while current LLM judges struggle with fine-grained evaluation in Russian,\nthere is potential for improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have introduced the novel\nparadigm of using LLMs as judges, where an LLM evaluates and scores the outputs\nof another LLM, which often correlates highly with human preferences. However,\nthe use of LLM-as-a-judge has been primarily studied in English. In this paper,\nwe evaluate this framework in Russian by introducing the Russian Error tyPes\nAnnotation dataset (REPA), a dataset of 1k user queries and 2k LLM-generated\nresponses. Human annotators labeled each response pair expressing their\npreferences across ten specific error types, as well as selecting an overall\npreference. We rank six generative LLMs across the error types using three\nrating systems based on human preferences. We also evaluate responses using\neight LLM judges in zero-shot and few-shot settings. We describe the results of\nanalyzing the judges and position and length biases. Our findings reveal a\nnotable gap between LLM judge performance in Russian and English. However,\nrankings based on human and LLM preferences show partial alignment, suggesting\nthat while current LLM judges struggle with fine-grained evaluation in Russian,\nthere is potential for improvement."
                },
                "authors": [
                    {
                        "name": "Alexander Pugachev"
                    },
                    {
                        "name": "Alena Fenogenova"
                    },
                    {
                        "name": "Vladislav Mikhailov"
                    },
                    {
                        "name": "Ekaterina Artemova"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Artemova"
                },
                "author": "Ekaterina Artemova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13101v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13101v1",
                "updated": "2025-03-17T12:13:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    13,
                    37,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T12:13:37Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    13,
                    37,
                    0,
                    76,
                    0
                ],
                "title": "Who Wrote This? Identifying Machine vs Human-Generated Text in Hausa",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who Wrote This? Identifying Machine vs Human-Generated Text in Hausa"
                },
                "summary": "The advancement of large language models (LLMs) has allowed them to be\nproficient in various tasks, including content generation. However, their\nunregulated usage can lead to malicious activities such as plagiarism and\ngenerating and spreading fake news, especially for low-resource languages. Most\nexisting machine-generated text detectors are trained on high-resource\nlanguages like English, French, etc. In this study, we developed the first\nlarge-scale detector that can distinguish between human- and machine-generated\ncontent in Hausa. We scrapped seven Hausa-language media outlets for the\nhuman-generated text and the Gemini-2.0 flash model to automatically generate\nthe corresponding Hausa-language articles based on the human-generated article\nheadlines. We fine-tuned four pre-trained Afri-centric models (AfriTeVa,\nAfriBERTa, AfroXLMR, and AfroXLMR-76L) on the resulting dataset and assessed\ntheir performance using accuracy and F1-score metrics. AfroXLMR achieved the\nhighest performance with an accuracy of 99.23% and an F1 score of 99.21%,\ndemonstrating its effectiveness for Hausa text detection. Our dataset is made\npublicly available to enable further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of large language models (LLMs) has allowed them to be\nproficient in various tasks, including content generation. However, their\nunregulated usage can lead to malicious activities such as plagiarism and\ngenerating and spreading fake news, especially for low-resource languages. Most\nexisting machine-generated text detectors are trained on high-resource\nlanguages like English, French, etc. In this study, we developed the first\nlarge-scale detector that can distinguish between human- and machine-generated\ncontent in Hausa. We scrapped seven Hausa-language media outlets for the\nhuman-generated text and the Gemini-2.0 flash model to automatically generate\nthe corresponding Hausa-language articles based on the human-generated article\nheadlines. We fine-tuned four pre-trained Afri-centric models (AfriTeVa,\nAfriBERTa, AfroXLMR, and AfroXLMR-76L) on the resulting dataset and assessed\ntheir performance using accuracy and F1-score metrics. AfroXLMR achieved the\nhighest performance with an accuracy of 99.23% and an F1 score of 99.21%,\ndemonstrating its effectiveness for Hausa text detection. Our dataset is made\npublicly available to enable further research."
                },
                "authors": [
                    {
                        "name": "Babangida Sani"
                    },
                    {
                        "name": "Aakansha Soy"
                    },
                    {
                        "name": "Sukairaj Hafiz Imam"
                    },
                    {
                        "name": "Ahmad Mustapha"
                    },
                    {
                        "name": "Lukman Jibril Aliyu"
                    },
                    {
                        "name": "Idris Abdulmumin"
                    },
                    {
                        "name": "Ibrahim Said Ahmad"
                    },
                    {
                        "name": "Shamsuddeen Hassan Muhammad"
                    }
                ],
                "author_detail": {
                    "name": "Shamsuddeen Hassan Muhammad"
                },
                "author": "Shamsuddeen Hassan Muhammad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13101v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13101v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04032v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04032v3",
                "updated": "2025-03-17T12:05:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    5,
                    36,
                    0,
                    76,
                    0
                ],
                "published": "2024-11-06T16:31:28Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    16,
                    31,
                    28,
                    2,
                    311,
                    0
                ],
                "title": "Beemo: Benchmark of Expert-edited Machine-generated Outputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beemo: Benchmark of Expert-edited Machine-generated Outputs"
                },
                "summary": "The rapid proliferation of large language models (LLMs) has increased the\nvolume of machine-generated texts (MGTs) and blurred text authorship in various\ndomains. However, most existing MGT benchmarks include single-author texts\n(human-written and machine-generated). This conventional design fails to\ncapture more practical multi-author scenarios, where the user refines the LLM\nresponse for natural flow, coherence, and factual correctness. Our paper\nintroduces the Benchmark of Expert-edited Machine-generated Outputs (Beemo),\nwhich includes 6.5k texts written by humans, generated by ten\ninstruction-finetuned LLMs, and edited by experts for various use cases,\nranging from creative writing to summarization. Beemo additionally comprises\n13.1k machine-generated and LLM-edited texts, allowing for diverse MGT\ndetection evaluation across various edit types. We document Beemo's creation\nprotocol and present the results of benchmarking 33 configurations of MGT\ndetectors in different experimental setups. We find that expert-based editing\nevades MGT detection, while LLM-edited texts are unlikely to be recognized as\nhuman-written. Beemo and all materials are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of large language models (LLMs) has increased the\nvolume of machine-generated texts (MGTs) and blurred text authorship in various\ndomains. However, most existing MGT benchmarks include single-author texts\n(human-written and machine-generated). This conventional design fails to\ncapture more practical multi-author scenarios, where the user refines the LLM\nresponse for natural flow, coherence, and factual correctness. Our paper\nintroduces the Benchmark of Expert-edited Machine-generated Outputs (Beemo),\nwhich includes 6.5k texts written by humans, generated by ten\ninstruction-finetuned LLMs, and edited by experts for various use cases,\nranging from creative writing to summarization. Beemo additionally comprises\n13.1k machine-generated and LLM-edited texts, allowing for diverse MGT\ndetection evaluation across various edit types. We document Beemo's creation\nprotocol and present the results of benchmarking 33 configurations of MGT\ndetectors in different experimental setups. We find that expert-based editing\nevades MGT detection, while LLM-edited texts are unlikely to be recognized as\nhuman-written. Beemo and all materials are publicly available."
                },
                "authors": [
                    {
                        "name": "Ekaterina Artemova"
                    },
                    {
                        "name": "Jason Lucas"
                    },
                    {
                        "name": "Saranya Venkatraman"
                    },
                    {
                        "name": "Jooyoung Lee"
                    },
                    {
                        "name": "Sergei Tilga"
                    },
                    {
                        "name": "Adaku Uchendu"
                    },
                    {
                        "name": "Vladislav Mikhailov"
                    }
                ],
                "author_detail": {
                    "name": "Vladislav Mikhailov"
                },
                "author": "Vladislav Mikhailov",
                "arxiv_comment": "Accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04032v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04032v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13089v1",
                "updated": "2025-03-17T11:52:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    11,
                    52,
                    16,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T11:52:16Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    11,
                    52,
                    16,
                    0,
                    76,
                    0
                ],
                "title": "ClusComp: A Simple Paradigm for Model Compression and Efficient\n  Finetuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClusComp: A Simple Paradigm for Model Compression and Efficient\n  Finetuning"
                },
                "summary": "As large language models (LLMs) scale, model compression is crucial for edge\ndeployment and accessibility. Weight-only quantization reduces model size but\nsuffers from performance degradation at lower bit widths. Moreover, standard\nfinetuning is incompatible with quantized models, and alternative methods often\nfall short of full finetuning. In this paper, we propose ClusComp, a simple yet\neffective compression paradigm that clusters weight matrices into codebooks and\nfinetunes them block-by-block. ClusComp (1) achieves superior performance in\n2-4 bit quantization, (2) pushes compression to 1-bit while outperforming\nultra-low-bit methods with minimal finetuning, and (3) enables efficient\nfinetuning, even surpassing existing quantization-based approaches and rivaling\nfull FP16 finetuning. Notably, ClusComp supports compression and finetuning of\n70B LLMs on a single A6000-48GB GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) scale, model compression is crucial for edge\ndeployment and accessibility. Weight-only quantization reduces model size but\nsuffers from performance degradation at lower bit widths. Moreover, standard\nfinetuning is incompatible with quantized models, and alternative methods often\nfall short of full finetuning. In this paper, we propose ClusComp, a simple yet\neffective compression paradigm that clusters weight matrices into codebooks and\nfinetunes them block-by-block. ClusComp (1) achieves superior performance in\n2-4 bit quantization, (2) pushes compression to 1-bit while outperforming\nultra-low-bit methods with minimal finetuning, and (3) enables efficient\nfinetuning, even surpassing existing quantization-based approaches and rivaling\nfull FP16 finetuning. Notably, ClusComp supports compression and finetuning of\n70B LLMs on a single A6000-48GB GPU."
                },
                "authors": [
                    {
                        "name": "Baohao Liao"
                    },
                    {
                        "name": "Christian Herold"
                    },
                    {
                        "name": "Seyyed Hadi Hashemi"
                    },
                    {
                        "name": "Stefan Vasilev"
                    },
                    {
                        "name": "Shahram Khadivi"
                    },
                    {
                        "name": "Christof Monz"
                    }
                ],
                "author_detail": {
                    "name": "Christof Monz"
                },
                "author": "Christof Monz",
                "arxiv_comment": "26 pages, 11 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13081v1",
                "updated": "2025-03-17T11:39:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    11,
                    39,
                    44,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T11:39:44Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    11,
                    39,
                    44,
                    0,
                    76,
                    0
                ],
                "title": "A Framework to Assess Multilingual Vulnerabilities of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework to Assess Multilingual Vulnerabilities of LLMs"
                },
                "summary": "Large Language Models (LLMs) are acquiring a wider range of capabilities,\nincluding understanding and responding in multiple languages. While they\nundergo safety training to prevent them from answering illegal questions,\nimbalances in training data and human evaluation resources can make these\nmodels more susceptible to attacks in low-resource languages (LRL). This paper\nproposes a framework to automatically assess the multilingual vulnerabilities\nof commonly used LLMs. Using our framework, we evaluated six LLMs across eight\nlanguages representing varying levels of resource availability. We validated\nthe assessments generated by our automated framework through human evaluation\nin two languages, demonstrating that the framework's results align with human\njudgments in most cases. Our findings reveal vulnerabilities in LRL; however,\nthese may pose minimal risk as they often stem from the model's poor\nperformance, resulting in incoherent responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are acquiring a wider range of capabilities,\nincluding understanding and responding in multiple languages. While they\nundergo safety training to prevent them from answering illegal questions,\nimbalances in training data and human evaluation resources can make these\nmodels more susceptible to attacks in low-resource languages (LRL). This paper\nproposes a framework to automatically assess the multilingual vulnerabilities\nof commonly used LLMs. Using our framework, we evaluated six LLMs across eight\nlanguages representing varying levels of resource availability. We validated\nthe assessments generated by our automated framework through human evaluation\nin two languages, demonstrating that the framework's results align with human\njudgments in most cases. Our findings reveal vulnerabilities in LRL; however,\nthese may pose minimal risk as they often stem from the model's poor\nperformance, resulting in incoherent responses."
                },
                "authors": [
                    {
                        "name": "Likai Tang"
                    },
                    {
                        "name": "Niruth Bogahawatta"
                    },
                    {
                        "name": "Yasod Ginige"
                    },
                    {
                        "name": "Jiarui Xu"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Surangika Ranathunga"
                    },
                    {
                        "name": "Suranga Seneviratne"
                    }
                ],
                "author_detail": {
                    "name": "Suranga Seneviratne"
                },
                "author": "Suranga Seneviratne",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12972v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12972v2",
                "updated": "2025-03-17T10:45:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    45,
                    15,
                    0,
                    76,
                    0
                ],
                "published": "2024-10-16T19:07:37Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    19,
                    7,
                    37,
                    2,
                    290,
                    0
                ],
                "title": "Evaluating the Instruction-following Abilities of Language Models using\n  Knowledge Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Instruction-following Abilities of Language Models using\n  Knowledge Tasks"
                },
                "summary": "LLM evaluation benchmarks have traditionally separated the testing of\nknowledge/reasoning capabilities from instruction following. In this work, we\nstudy the interaction between knowledge and instruction following, and observe\nthat LLMs struggle to follow simple answer modifying instructions, and are also\ndistracted by instructions that should have no bearing on the original\nknowledge task answer. We leverage existing multiple-choice answer based\nknowledge benchmarks and apply a set of simple instructions which include\nmanipulating text (eg.: change case), numeric quantities (eg.: increase value,\nchange formatting), operate on lists (eg.: sort answer candidates) and\ndistractor instructions (eg.: change case of numeric answers).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM evaluation benchmarks have traditionally separated the testing of\nknowledge/reasoning capabilities from instruction following. In this work, we\nstudy the interaction between knowledge and instruction following, and observe\nthat LLMs struggle to follow simple answer modifying instructions, and are also\ndistracted by instructions that should have no bearing on the original\nknowledge task answer. We leverage existing multiple-choice answer based\nknowledge benchmarks and apply a set of simple instructions which include\nmanipulating text (eg.: change case), numeric quantities (eg.: increase value,\nchange formatting), operate on lists (eg.: sort answer candidates) and\ndistractor instructions (eg.: change case of numeric answers)."
                },
                "authors": [
                    {
                        "name": "Rudra Murthy"
                    },
                    {
                        "name": "Praveen Venkateswaran"
                    },
                    {
                        "name": "Prince Kumar"
                    },
                    {
                        "name": "Danish Contractor"
                    }
                ],
                "author_detail": {
                    "name": "Danish Contractor"
                },
                "author": "Danish Contractor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12972v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12972v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12782v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12782v2",
                "updated": "2025-03-17T10:43:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    43,
                    54,
                    0,
                    76,
                    0
                ],
                "published": "2024-10-16T17:56:49Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    56,
                    49,
                    2,
                    290,
                    0
                ],
                "title": "In-Context Learning Enables Robot Action Prediction in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning Enables Robot Action Prediction in LLMs"
                },
                "summary": "Recently, Large Language Models (LLMs) have achieved remarkable success using\nin-context learning (ICL) in the language domain. However, leveraging the ICL\ncapabilities within LLMs to directly predict robot actions remains largely\nunexplored. In this paper, we introduce RoboPrompt, a framework that enables\noff-the-shelf text-only LLMs to directly predict robot actions through ICL\nwithout training. Our approach first heuristically identifies keyframes that\ncapture important moments from an episode. Next, we extract end-effector\nactions from these keyframes as well as the estimated initial object poses, and\nboth are converted into textual descriptions. Finally, we construct a\nstructured template to form ICL demonstrations from these textual descriptions\nand a task instruction. This enables an LLM to directly predict robot actions\nat test time. Through extensive experiments and analysis, RoboPrompt shows\nstronger performance over zero-shot and ICL baselines in simulated and\nreal-world settings. Our project page is available at\nhttps://davidyyd.github.io/roboprompt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) have achieved remarkable success using\nin-context learning (ICL) in the language domain. However, leveraging the ICL\ncapabilities within LLMs to directly predict robot actions remains largely\nunexplored. In this paper, we introduce RoboPrompt, a framework that enables\noff-the-shelf text-only LLMs to directly predict robot actions through ICL\nwithout training. Our approach first heuristically identifies keyframes that\ncapture important moments from an episode. Next, we extract end-effector\nactions from these keyframes as well as the estimated initial object poses, and\nboth are converted into textual descriptions. Finally, we construct a\nstructured template to form ICL demonstrations from these textual descriptions\nand a task instruction. This enables an LLM to directly predict robot actions\nat test time. Through extensive experiments and analysis, RoboPrompt shows\nstronger performance over zero-shot and ICL baselines in simulated and\nreal-world settings. Our project page is available at\nhttps://davidyyd.github.io/roboprompt."
                },
                "authors": [
                    {
                        "name": "Yida Yin"
                    },
                    {
                        "name": "Zekai Wang"
                    },
                    {
                        "name": "Yuvan Sharma"
                    },
                    {
                        "name": "Dantong Niu"
                    },
                    {
                        "name": "Trevor Darrell"
                    },
                    {
                        "name": "Roei Herzig"
                    }
                ],
                "author_detail": {
                    "name": "Roei Herzig"
                },
                "author": "Roei Herzig",
                "arxiv_comment": "Published in ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12782v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12782v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13038v1",
                "updated": "2025-03-17T10:42:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    42,
                    34,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T10:42:34Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    42,
                    34,
                    0,
                    76,
                    0
                ],
                "title": "Overview of the NTCIR-18 Automatic Evaluation of LLMs (AEOLLM) Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Overview of the NTCIR-18 Automatic Evaluation of LLMs (AEOLLM) Task"
                },
                "summary": "In this paper, we provide an overview of the NTCIR-18 Automatic Evaluation of\nLLMs (AEOLLM) task. As large language models (LLMs) grow popular in both\nacademia and industry, how to effectively evaluate the capacity of LLMs becomes\nan increasingly critical but still challenging issue. Existing methods can be\ndivided into two types: manual evaluation, which is expensive, and automatic\nevaluation, which faces many limitations including task format (the majority\nbelong to multiple-choice questions) and evaluation criteria (occupied by\nreference-based metrics). To advance the innovation of automatic evaluation, we\npropose the AEOLLM task which focuses on generative tasks and encourages\nreference-free methods. Besides, we set up diverse subtasks such as dialogue\ngeneration, text expansion, summary generation and non-factoid question\nanswering to comprehensively test different methods. This year, we received 48\nruns from 4 teams in total. This paper will describe the background of the\ntask, the data set, the evaluation measures and the evaluation results,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we provide an overview of the NTCIR-18 Automatic Evaluation of\nLLMs (AEOLLM) task. As large language models (LLMs) grow popular in both\nacademia and industry, how to effectively evaluate the capacity of LLMs becomes\nan increasingly critical but still challenging issue. Existing methods can be\ndivided into two types: manual evaluation, which is expensive, and automatic\nevaluation, which faces many limitations including task format (the majority\nbelong to multiple-choice questions) and evaluation criteria (occupied by\nreference-based metrics). To advance the innovation of automatic evaluation, we\npropose the AEOLLM task which focuses on generative tasks and encourages\nreference-free methods. Besides, we set up diverse subtasks such as dialogue\ngeneration, text expansion, summary generation and non-factoid question\nanswering to comprehensively test different methods. This year, we received 48\nruns from 4 teams in total. This paper will describe the background of the\ntask, the data set, the evaluation measures and the evaluation results,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Haitao Li"
                    },
                    {
                        "name": "Zhumin Chu"
                    },
                    {
                        "name": "Yiqun Liu"
                    },
                    {
                        "name": "Qingyao Ai"
                    }
                ],
                "author_detail": {
                    "name": "Qingyao Ai"
                },
                "author": "Qingyao Ai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04872v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04872v2",
                "updated": "2025-03-17T10:36:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    36,
                    30,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-06T16:25:53Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    25,
                    53,
                    3,
                    65,
                    0
                ],
                "title": "TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation"
                },
                "summary": "The challenge of reducing the size of Large Language Models (LLMs) while\nmaintaining their performance has gained significant attention. However,\nexisting methods, such as model distillation and transfer learning, often fail\nto achieve high accuracy. To address this limitation, we introduce the\nBranch-Merge distillation approach, which enhances model compression through\ntwo phases: (1) the Branch Phase, where knowledge from a large teacher model is\n\\textit{selectively distilled} into specialized student models via\ndomain-specific supervised fine-tuning (SFT); And (2) the Merge Phase, where\nthese student models are merged to enable cross-domain knowledge transfer and\nimprove generalization. We validate our distillation approach using DeepSeek-R1\nas the teacher and DeepSeek-R1-Distill-Qwen-32B as the student. The resulting\nmerged model, TinyR1-32B-Preview, outperforms its counterpart\nDeepSeek-R1-Distill-Qwen-32B across multiple benchmarks, including Mathematics\n(+5.5 points), Coding (+4.4 points) and Science (+2.9 points), while achieving\nnear-equal performance to DeepSeek-R1 on AIME 2024. The Branch-Merge\ndistillation approach provides a scalable solution for creating smaller,\nhigh-performing LLMs with reduced computational cost and time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The challenge of reducing the size of Large Language Models (LLMs) while\nmaintaining their performance has gained significant attention. However,\nexisting methods, such as model distillation and transfer learning, often fail\nto achieve high accuracy. To address this limitation, we introduce the\nBranch-Merge distillation approach, which enhances model compression through\ntwo phases: (1) the Branch Phase, where knowledge from a large teacher model is\n\\textit{selectively distilled} into specialized student models via\ndomain-specific supervised fine-tuning (SFT); And (2) the Merge Phase, where\nthese student models are merged to enable cross-domain knowledge transfer and\nimprove generalization. We validate our distillation approach using DeepSeek-R1\nas the teacher and DeepSeek-R1-Distill-Qwen-32B as the student. The resulting\nmerged model, TinyR1-32B-Preview, outperforms its counterpart\nDeepSeek-R1-Distill-Qwen-32B across multiple benchmarks, including Mathematics\n(+5.5 points), Coding (+4.4 points) and Science (+2.9 points), while achieving\nnear-equal performance to DeepSeek-R1 on AIME 2024. The Branch-Merge\ndistillation approach provides a scalable solution for creating smaller,\nhigh-performing LLMs with reduced computational cost and time."
                },
                "authors": [
                    {
                        "name": "Lin Sun"
                    },
                    {
                        "name": "Guangxiang Zhao"
                    },
                    {
                        "name": "Xiaoqi Jian"
                    },
                    {
                        "name": "Yuhan Wu"
                    },
                    {
                        "name": "Weihong Lin"
                    },
                    {
                        "name": "Yongfu Zhu"
                    },
                    {
                        "name": "Change Jia"
                    },
                    {
                        "name": "Linglin Zhang"
                    },
                    {
                        "name": "Jinzhu Wu"
                    },
                    {
                        "name": "Junfeng Ran"
                    },
                    {
                        "name": "Sai-er Hu"
                    },
                    {
                        "name": "Zihan Jiang"
                    },
                    {
                        "name": "Junting Zhou"
                    },
                    {
                        "name": "Wenrui Liu"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Tong Yang"
                    },
                    {
                        "name": "Xiangzheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangzheng Zhang"
                },
                "author": "Xiangzheng Zhang",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04872v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04872v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.05787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.05787v2",
                "updated": "2025-03-17T10:35:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    35,
                    11,
                    0,
                    76,
                    0
                ],
                "published": "2024-01-11T09:49:15Z",
                "published_parsed": [
                    2024,
                    1,
                    11,
                    9,
                    49,
                    15,
                    3,
                    11,
                    0
                ],
                "title": "Chain of Evidences and Evidence to Generate: Prompting for Context\n  Grounded and Retrieval Augmented Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Evidences and Evidence to Generate: Prompting for Context\n  Grounded and Retrieval Augmented Reasoning"
                },
                "summary": "While chain-of-thoughts (CoT) prompting has revolutionized how LLMs perform\nreasoning tasks, its current methods and variations (e.g, Self-consistency,\nReACT, Reflexion, Tree-of-Thoughts (ToT), Cumulative Reasoning (CR) etc.,)\nsuffer from limitations like limited context grounding,\nhallucination/inconsistent output generation, and iterative sluggishness. To\novercome these challenges, we introduce a novel mono/dual-step zero-shot\nprompting framework built upon two unique strategies Chain of Evidences (CoE)}\nand Evidence to Generate (E2G). Instead of unverified reasoning claims, our\ninnovative approaches leverage the power of \"evidence for decision making\" by\nfirst focusing exclusively on the thought sequences explicitly mentioned in the\ncontext which then serve as extracted evidence, guiding the LLM's output\ngeneration process with greater precision and efficiency. This simple yet\npotent approach unlocks the full potential of chain-of-thoughts prompting,\nfacilitating faster, more reliable, and contextually aware reasoning in LLMs.\nOur framework consistently achieves remarkable results across various\nknowledge-intensive reasoning and generation tasks, surpassing baseline\napproaches with state-of-the-art LLMs. For instance, (i) on the LogiQA\nbenchmark using GPT-4, CoE achieves a new state-of-the-art accuracy of 53.8%,\nsurpassing CoT by 18%, ToT by 11%, and CR by 9%; (ii) CoE with PaLM-2\noutperforms the variable-shot performance of Gemini Ultra by 0.9 F1 points,\nachieving an F1 score of 83.3 on DROP. We release our prompts and outputs on\nthese benchmarks as a new instruction tuning dataset for future research at\nhttps://huggingface.co/datasets/kagnlp/Chain-of-Evidences/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While chain-of-thoughts (CoT) prompting has revolutionized how LLMs perform\nreasoning tasks, its current methods and variations (e.g, Self-consistency,\nReACT, Reflexion, Tree-of-Thoughts (ToT), Cumulative Reasoning (CR) etc.,)\nsuffer from limitations like limited context grounding,\nhallucination/inconsistent output generation, and iterative sluggishness. To\novercome these challenges, we introduce a novel mono/dual-step zero-shot\nprompting framework built upon two unique strategies Chain of Evidences (CoE)}\nand Evidence to Generate (E2G). Instead of unverified reasoning claims, our\ninnovative approaches leverage the power of \"evidence for decision making\" by\nfirst focusing exclusively on the thought sequences explicitly mentioned in the\ncontext which then serve as extracted evidence, guiding the LLM's output\ngeneration process with greater precision and efficiency. This simple yet\npotent approach unlocks the full potential of chain-of-thoughts prompting,\nfacilitating faster, more reliable, and contextually aware reasoning in LLMs.\nOur framework consistently achieves remarkable results across various\nknowledge-intensive reasoning and generation tasks, surpassing baseline\napproaches with state-of-the-art LLMs. For instance, (i) on the LogiQA\nbenchmark using GPT-4, CoE achieves a new state-of-the-art accuracy of 53.8%,\nsurpassing CoT by 18%, ToT by 11%, and CR by 9%; (ii) CoE with PaLM-2\noutperforms the variable-shot performance of Gemini Ultra by 0.9 F1 points,\nachieving an F1 score of 83.3 on DROP. We release our prompts and outputs on\nthese benchmarks as a new instruction tuning dataset for future research at\nhttps://huggingface.co/datasets/kagnlp/Chain-of-Evidences/."
                },
                "authors": [
                    {
                        "name": "Md Rizwan Parvez"
                    }
                ],
                "author_detail": {
                    "name": "Md Rizwan Parvez"
                },
                "author": "Md Rizwan Parvez",
                "arxiv_comment": "Accepted at NAACL KnowledgeNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.05787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.05787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13026v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13026v1",
                "updated": "2025-03-17T10:29:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    29,
                    8,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T10:29:08Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    29,
                    8,
                    0,
                    76,
                    0
                ],
                "title": "HiMTok: Learning Hierarchical Mask Tokens for Image Segmentation with\n  Large Multimodal Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiMTok: Learning Hierarchical Mask Tokens for Image Segmentation with\n  Large Multimodal Model"
                },
                "summary": "The remarkable performance of large multimodal models (LMMs) has attracted\nsignificant interest from the image segmentation community. To align with the\nnext-token-prediction paradigm, current LMM-driven segmentation methods either\nuse object boundary points to represent masks or introduce special segmentation\ntokens, whose hidden states are decoded by a segmentation model requiring the\noriginal image as input. However, these approaches often suffer from inadequate\nmask representation and complex architectures, limiting the potential of LMMs.\nIn this work, we propose the Hierarchical Mask Tokenizer (HiMTok), which\nrepresents segmentation masks with up to 32 tokens and eliminates the need for\nthe original image during mask de-tokenization. HiMTok allows for compact and\ncoarse-to-fine mask representations, aligning well with the LLM\nnext-token-prediction paradigm and facilitating the direct acquisition of\nsegmentation capabilities. We develop a 3-stage training recipe for progressive\nlearning of segmentation and visual capabilities, featuring a hierarchical mask\nloss for effective coarse-to-fine learning. Additionally, we enable\nbidirectional information flow, allowing conversion between bounding boxes and\nmask tokens to fully leverage multi-task training potential. Extensive\nexperiments demonstrate that our method achieves state-of-the-art performance\nacross various segmentation tasks,while also enhancing visual grounding and\nmaintaining overall visual understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable performance of large multimodal models (LMMs) has attracted\nsignificant interest from the image segmentation community. To align with the\nnext-token-prediction paradigm, current LMM-driven segmentation methods either\nuse object boundary points to represent masks or introduce special segmentation\ntokens, whose hidden states are decoded by a segmentation model requiring the\noriginal image as input. However, these approaches often suffer from inadequate\nmask representation and complex architectures, limiting the potential of LMMs.\nIn this work, we propose the Hierarchical Mask Tokenizer (HiMTok), which\nrepresents segmentation masks with up to 32 tokens and eliminates the need for\nthe original image during mask de-tokenization. HiMTok allows for compact and\ncoarse-to-fine mask representations, aligning well with the LLM\nnext-token-prediction paradigm and facilitating the direct acquisition of\nsegmentation capabilities. We develop a 3-stage training recipe for progressive\nlearning of segmentation and visual capabilities, featuring a hierarchical mask\nloss for effective coarse-to-fine learning. Additionally, we enable\nbidirectional information flow, allowing conversion between bounding boxes and\nmask tokens to fully leverage multi-task training potential. Extensive\nexperiments demonstrate that our method achieves state-of-the-art performance\nacross various segmentation tasks,while also enhancing visual grounding and\nmaintaining overall visual understanding."
                },
                "authors": [
                    {
                        "name": "Tao Wang"
                    },
                    {
                        "name": "Changxu Cheng"
                    },
                    {
                        "name": "Lingfeng Wang"
                    },
                    {
                        "name": "Senda Chen"
                    },
                    {
                        "name": "Wuyue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wuyue Zhao"
                },
                "author": "Wuyue Zhao",
                "arxiv_comment": "technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13026v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13026v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13012v1",
                "updated": "2025-03-17T10:11:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    11,
                    11,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T10:11:11Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    11,
                    11,
                    0,
                    76,
                    0
                ],
                "title": "Test-Time Domain Generalization via Universe Learning: A Multi-Graph\n  Matching Approach for Medical Image Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Domain Generalization via Universe Learning: A Multi-Graph\n  Matching Approach for Medical Image Segmentation"
                },
                "summary": "Despite domain generalization (DG) has significantly addressed the\nperformance degradation of pre-trained models caused by domain shifts, it often\nfalls short in real-world deployment. Test-time adaptation (TTA), which adjusts\na learned model using unlabeled test data, presents a promising solution.\nHowever, most existing TTA methods struggle to deliver strong performance in\nmedical image segmentation, primarily because they overlook the crucial prior\nknowledge inherent to medical images. To address this challenge, we incorporate\nmorphological information and propose a framework based on multi-graph\nmatching. Specifically, we introduce learnable universe embeddings that\nintegrate morphological priors during multi-source training, along with novel\nunsupervised test-time paradigms for domain adaptation. This approach\nguarantees cycle-consistency in multi-matching while enabling the model to more\neffectively capture the invariant priors of unseen data, significantly\nmitigating the effects of domain shifts. Extensive experiments demonstrate that\nour method outperforms other state-of-the-art approaches on two medical image\nsegmentation benchmarks for both multi-source and single-source domain\ngeneralization tasks. The source code is available at\nhttps://github.com/Yore0/TTDG-MGM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite domain generalization (DG) has significantly addressed the\nperformance degradation of pre-trained models caused by domain shifts, it often\nfalls short in real-world deployment. Test-time adaptation (TTA), which adjusts\na learned model using unlabeled test data, presents a promising solution.\nHowever, most existing TTA methods struggle to deliver strong performance in\nmedical image segmentation, primarily because they overlook the crucial prior\nknowledge inherent to medical images. To address this challenge, we incorporate\nmorphological information and propose a framework based on multi-graph\nmatching. Specifically, we introduce learnable universe embeddings that\nintegrate morphological priors during multi-source training, along with novel\nunsupervised test-time paradigms for domain adaptation. This approach\nguarantees cycle-consistency in multi-matching while enabling the model to more\neffectively capture the invariant priors of unseen data, significantly\nmitigating the effects of domain shifts. Extensive experiments demonstrate that\nour method outperforms other state-of-the-art approaches on two medical image\nsegmentation benchmarks for both multi-source and single-source domain\ngeneralization tasks. The source code is available at\nhttps://github.com/Yore0/TTDG-MGM."
                },
                "authors": [
                    {
                        "name": "Xingguo Lv"
                    },
                    {
                        "name": "Xingbo Dong"
                    },
                    {
                        "name": "Liwen Wang"
                    },
                    {
                        "name": "Jiewen Yang"
                    },
                    {
                        "name": "Lei Zhao"
                    },
                    {
                        "name": "Bin Pu"
                    },
                    {
                        "name": "Zhe Jin"
                    },
                    {
                        "name": "Xuejun Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuejun Li"
                },
                "author": "Xuejun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.19597v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.19597v3",
                "updated": "2025-03-17T10:09:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    9,
                    29,
                    0,
                    76,
                    0
                ],
                "published": "2024-04-30T14:43:57Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    14,
                    43,
                    57,
                    1,
                    121,
                    0
                ],
                "title": "TuBA: Cross-Lingual Transferability of Backdoor Attacks in LLMs with\n  Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TuBA: Cross-Lingual Transferability of Backdoor Attacks in LLMs with\n  Instruction Tuning"
                },
                "summary": "The implications of backdoor attacks on English-centric large language models\n(LLMs) have been widely examined - such attacks can be achieved by embedding\nmalicious behaviors during training and activated under specific conditions\nthat trigger malicious outputs. Despite the increasing support for multilingual\ncapabilities in open-source and proprietary LLMs, the impact of backdoor\nattacks on these systems remains largely under-explored. Our research focuses\non cross-lingual backdoor attacks against multilingual LLMs, particularly\ninvestigating how poisoning the instruction-tuning data for one or two\nlanguages can affect the outputs for languages whose instruction-tuning data\nwere not poisoned. Despite its simplicity, our empirical analysis reveals that\nour method exhibits remarkable efficacy in models like mT5 and GPT-4o, with\nhigh attack success rates, surpassing 90% in more than 7 out of 12 languages\nacross various scenarios. Our findings also indicate that more powerful models\nshow increased susceptibility to transferable cross-lingual backdoor attacks,\nwhich also applies to LLMs predominantly pre-trained on English data, such as\nLlama2, Llama3, and Gemma. Moreover, our experiments demonstrate 1) High\nTransferability: the backdoor mechanism operates successfully in cross-lingual\nresponse scenarios across 26 languages, achieving an average attack success\nrate of 99%, and 2) Robustness: the proposed attack remains effective even\nafter defenses are applied. These findings expose critical security\nvulnerabilities in multilingual LLMs and highlight the urgent need for more\nrobust, targeted defense strategies to address the unique challenges posed by\ncross-lingual backdoor transfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The implications of backdoor attacks on English-centric large language models\n(LLMs) have been widely examined - such attacks can be achieved by embedding\nmalicious behaviors during training and activated under specific conditions\nthat trigger malicious outputs. Despite the increasing support for multilingual\ncapabilities in open-source and proprietary LLMs, the impact of backdoor\nattacks on these systems remains largely under-explored. Our research focuses\non cross-lingual backdoor attacks against multilingual LLMs, particularly\ninvestigating how poisoning the instruction-tuning data for one or two\nlanguages can affect the outputs for languages whose instruction-tuning data\nwere not poisoned. Despite its simplicity, our empirical analysis reveals that\nour method exhibits remarkable efficacy in models like mT5 and GPT-4o, with\nhigh attack success rates, surpassing 90% in more than 7 out of 12 languages\nacross various scenarios. Our findings also indicate that more powerful models\nshow increased susceptibility to transferable cross-lingual backdoor attacks,\nwhich also applies to LLMs predominantly pre-trained on English data, such as\nLlama2, Llama3, and Gemma. Moreover, our experiments demonstrate 1) High\nTransferability: the backdoor mechanism operates successfully in cross-lingual\nresponse scenarios across 26 languages, achieving an average attack success\nrate of 99%, and 2) Robustness: the proposed attack remains effective even\nafter defenses are applied. These findings expose critical security\nvulnerabilities in multilingual LLMs and highlight the urgent need for more\nrobust, targeted defense strategies to address the unique challenges posed by\ncross-lingual backdoor transfer."
                },
                "authors": [
                    {
                        "name": "Xuanli He"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Qiongkai Xu"
                    },
                    {
                        "name": "Pasquale Minervini"
                    },
                    {
                        "name": "Pontus Stenetorp"
                    },
                    {
                        "name": "Benjamin I. P. Rubinstein"
                    },
                    {
                        "name": "Trevor Cohn"
                    }
                ],
                "author_detail": {
                    "name": "Trevor Cohn"
                },
                "author": "Trevor Cohn",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.19597v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.19597v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13008v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13008v1",
                "updated": "2025-03-17T10:07:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    7,
                    50,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T10:07:50Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    7,
                    50,
                    0,
                    76,
                    0
                ],
                "title": "Knowledge Distillation: Enhancing Neural Network Compression with\n  Integrated Gradients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Distillation: Enhancing Neural Network Compression with\n  Integrated Gradients"
                },
                "summary": "Efficient deployment of deep neural networks on resource-constrained devices\ndemands advanced compression techniques that preserve accuracy and\ninteroperability. This paper proposes a machine learning framework that\naugments Knowledge Distillation (KD) with Integrated Gradients (IG), an\nattribution method, to optimise the compression of convolutional neural\nnetworks. We introduce a novel data augmentation strategy where IG maps,\nprecomputed from a teacher model, are overlaid onto training images to guide a\ncompact student model toward critical feature representations. This approach\nleverages the teacher's decision-making insights, enhancing the student's\nability to replicate complex patterns with reduced parameters. Experiments on\nCIFAR-10 demonstrate the efficacy of our method: a student model, compressed\n4.1-fold from the MobileNet-V2 teacher, achieves 92.5% classification accuracy,\nsurpassing the baseline student's 91.4% and traditional KD approaches, while\nreducing inference latency from 140 ms to 13 ms--a tenfold speedup. We perform\nhyperparameter optimisation for efficient learning. Comprehensive ablation\nstudies dissect the contributions of KD and IG, revealing synergistic effects\nthat boost both performance and model explainability. Our method's emphasis on\nfeature-level guidance via IG distinguishes it from conventional KD, offering a\ndata-driven solution for mining transferable knowledge in neural architectures.\nThis work contributes to machine learning by providing a scalable,\ninterpretable compression technique, ideal for edge computing applications\nwhere efficiency and transparency are paramount.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient deployment of deep neural networks on resource-constrained devices\ndemands advanced compression techniques that preserve accuracy and\ninteroperability. This paper proposes a machine learning framework that\naugments Knowledge Distillation (KD) with Integrated Gradients (IG), an\nattribution method, to optimise the compression of convolutional neural\nnetworks. We introduce a novel data augmentation strategy where IG maps,\nprecomputed from a teacher model, are overlaid onto training images to guide a\ncompact student model toward critical feature representations. This approach\nleverages the teacher's decision-making insights, enhancing the student's\nability to replicate complex patterns with reduced parameters. Experiments on\nCIFAR-10 demonstrate the efficacy of our method: a student model, compressed\n4.1-fold from the MobileNet-V2 teacher, achieves 92.5% classification accuracy,\nsurpassing the baseline student's 91.4% and traditional KD approaches, while\nreducing inference latency from 140 ms to 13 ms--a tenfold speedup. We perform\nhyperparameter optimisation for efficient learning. Comprehensive ablation\nstudies dissect the contributions of KD and IG, revealing synergistic effects\nthat boost both performance and model explainability. Our method's emphasis on\nfeature-level guidance via IG distinguishes it from conventional KD, offering a\ndata-driven solution for mining transferable knowledge in neural architectures.\nThis work contributes to machine learning by providing a scalable,\ninterpretable compression technique, ideal for edge computing applications\nwhere efficiency and transparency are paramount."
                },
                "authors": [
                    {
                        "name": "David E. Hernandez"
                    },
                    {
                        "name": "Jose Ramon Chang"
                    },
                    {
                        "name": "Torbjörn E. M. Nordling"
                    }
                ],
                "author_detail": {
                    "name": "Torbjörn E. M. Nordling"
                },
                "author": "Torbjörn E. M. Nordling",
                "arxiv_comment": "15 pages, 3 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13008v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13008v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05, 68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.4.2; I.4.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01262v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01262v2",
                "updated": "2025-03-17T10:01:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    1,
                    21,
                    0,
                    76,
                    0
                ],
                "published": "2024-12-02T08:30:22Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    30,
                    22,
                    0,
                    337,
                    0
                ],
                "title": "Exploring ReAct Prompting for Task-Oriented Dialogue: Insights and\n  Shortcomings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring ReAct Prompting for Task-Oriented Dialogue: Insights and\n  Shortcomings"
                },
                "summary": "Large language models (LLMs) gained immense popularity due to their\nimpressive capabilities in unstructured conversations. Empowering LLMs with\nadvanced prompting strategies such as reasoning and acting (ReAct) (Yao et al.,\n2022) has shown promise in solving complex tasks traditionally requiring\nreinforcement learning. In this work, we apply the ReAct strategy to guide LLMs\nperforming task-oriented dialogue (TOD). We evaluate ReAct-based LLMs\n(ReAct-LLMs) both in simulation and with real users. While ReAct-LLMs severely\nunderperform state-of-the-art approaches on success rate in simulation, this\ndifference becomes less pronounced in human evaluation. Moreover, compared to\nthe baseline, humans report higher subjective satisfaction with ReAct-LLM\ndespite its lower success rate, most likely thanks to its natural and\nconfidently phrased responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) gained immense popularity due to their\nimpressive capabilities in unstructured conversations. Empowering LLMs with\nadvanced prompting strategies such as reasoning and acting (ReAct) (Yao et al.,\n2022) has shown promise in solving complex tasks traditionally requiring\nreinforcement learning. In this work, we apply the ReAct strategy to guide LLMs\nperforming task-oriented dialogue (TOD). We evaluate ReAct-based LLMs\n(ReAct-LLMs) both in simulation and with real users. While ReAct-LLMs severely\nunderperform state-of-the-art approaches on success rate in simulation, this\ndifference becomes less pronounced in human evaluation. Moreover, compared to\nthe baseline, humans report higher subjective satisfaction with ReAct-LLM\ndespite its lower success rate, most likely thanks to its natural and\nconfidently phrased responses."
                },
                "authors": [
                    {
                        "name": "Michelle Elizabeth"
                    },
                    {
                        "name": "Morgan Veyret"
                    },
                    {
                        "name": "Miguel Couceiro"
                    },
                    {
                        "name": "Ondrej Dusek"
                    },
                    {
                        "name": "Lina M. Rojas-Barahona"
                    }
                ],
                "author_detail": {
                    "name": "Lina M. Rojas-Barahona"
                },
                "author": "Lina M. Rojas-Barahona",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01262v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01262v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12989v1",
                "updated": "2025-03-17T09:44:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    44,
                    50,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T09:44:50Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    44,
                    50,
                    0,
                    76,
                    0
                ],
                "title": "A Multi-Stage Framework with Taxonomy-Guided Reasoning for Occupation\n  Classification Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Stage Framework with Taxonomy-Guided Reasoning for Occupation\n  Classification Using Large Language Models"
                },
                "summary": "Automatically annotating job data with standardized occupations from\ntaxonomies, known as occupation classification, is crucial for labor market\nanalysis. However, this task is often hindered by data scarcity and the\nchallenges of manual annotations. While large language models (LLMs) hold\npromise due to their extensive world knowledge and in-context learning\ncapabilities, their effectiveness depends on their knowledge of occupational\ntaxonomies, which remains unclear. In this study, we assess the ability of LLMs\nto generate precise taxonomic entities from taxonomy, highlighting their\nlimitations. To address these challenges, we propose a multi-stage framework\nconsisting of inference, retrieval, and reranking stages, which integrates\ntaxonomy-guided reasoning examples to enhance performance by aligning outputs\nwith taxonomic knowledge. Evaluations on a large-scale dataset show significant\nimprovements in classification accuracy. Furthermore, we demonstrate the\nframework's adaptability for multi-label skill classification. Our results\nindicate that the framework outperforms existing LLM-based methods, offering a\npractical and scalable solution for occupation classification and related tasks\nacross LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically annotating job data with standardized occupations from\ntaxonomies, known as occupation classification, is crucial for labor market\nanalysis. However, this task is often hindered by data scarcity and the\nchallenges of manual annotations. While large language models (LLMs) hold\npromise due to their extensive world knowledge and in-context learning\ncapabilities, their effectiveness depends on their knowledge of occupational\ntaxonomies, which remains unclear. In this study, we assess the ability of LLMs\nto generate precise taxonomic entities from taxonomy, highlighting their\nlimitations. To address these challenges, we propose a multi-stage framework\nconsisting of inference, retrieval, and reranking stages, which integrates\ntaxonomy-guided reasoning examples to enhance performance by aligning outputs\nwith taxonomic knowledge. Evaluations on a large-scale dataset show significant\nimprovements in classification accuracy. Furthermore, we demonstrate the\nframework's adaptability for multi-label skill classification. Our results\nindicate that the framework outperforms existing LLM-based methods, offering a\npractical and scalable solution for occupation classification and related tasks\nacross LLMs."
                },
                "authors": [
                    {
                        "name": "Palakorn Achananuparp"
                    },
                    {
                        "name": "Ee-Peng Lim"
                    }
                ],
                "author_detail": {
                    "name": "Ee-Peng Lim"
                },
                "author": "Ee-Peng Lim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12988v1",
                "updated": "2025-03-17T09:44:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    44,
                    17,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T09:44:17Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    44,
                    17,
                    0,
                    76,
                    0
                ],
                "title": "ROMA: a Read-Only-Memory-based Accelerator for QLoRA-based On-Device LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ROMA: a Read-Only-Memory-based Accelerator for QLoRA-based On-Device LLM"
                },
                "summary": "As large language models (LLMs) demonstrate powerful capabilities, deploying\nthem on edge devices has become increasingly crucial, offering advantages in\nprivacy and real-time interaction. QLoRA has emerged as the standard approach\nfor on-device LLMs, leveraging quantized models to reduce memory and\ncomputational costs while utilizing LoRA for task-specific adaptability. In\nthis work, we propose ROMA, a QLoRA accelerator with a hybrid storage\narchitecture that uses ROM for quantized base models and SRAM for LoRA weights\nand KV cache. Our insight is that the quantized base model is stable and\nconverged, making it well-suited for ROM storage. Meanwhile, LoRA modules offer\nthe flexibility to adapt to new data without requiring updates to the base\nmodel. To further reduce the area cost of ROM, we introduce a novel B-ROM\ndesign and integrate it with the compute unit to form a fused cell for\nefficient use of chip resources. ROMA can effectively store both a 4-bit 3B and\na 2-bit 8B LLaMA model entirely on-chip, achieving a notable generation speed\nexceeding 20,000 tokens/s without requiring external memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) demonstrate powerful capabilities, deploying\nthem on edge devices has become increasingly crucial, offering advantages in\nprivacy and real-time interaction. QLoRA has emerged as the standard approach\nfor on-device LLMs, leveraging quantized models to reduce memory and\ncomputational costs while utilizing LoRA for task-specific adaptability. In\nthis work, we propose ROMA, a QLoRA accelerator with a hybrid storage\narchitecture that uses ROM for quantized base models and SRAM for LoRA weights\nand KV cache. Our insight is that the quantized base model is stable and\nconverged, making it well-suited for ROM storage. Meanwhile, LoRA modules offer\nthe flexibility to adapt to new data without requiring updates to the base\nmodel. To further reduce the area cost of ROM, we introduce a novel B-ROM\ndesign and integrate it with the compute unit to form a fused cell for\nefficient use of chip resources. ROMA can effectively store both a 4-bit 3B and\na 2-bit 8B LLaMA model entirely on-chip, achieving a notable generation speed\nexceeding 20,000 tokens/s without requiring external memory."
                },
                "authors": [
                    {
                        "name": "Wenqiang Wang"
                    },
                    {
                        "name": "Yijia Zhang"
                    },
                    {
                        "name": "Zikai Zhang"
                    },
                    {
                        "name": "Guanting Huo"
                    },
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Ningyi Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ningyi Xu"
                },
                "author": "Ningyi Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12972v1",
                "updated": "2025-03-17T09:31:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    31,
                    14,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T09:31:14Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    31,
                    14,
                    0,
                    76,
                    0
                ],
                "title": "Aligning Vision to Language: Text-Free Multimodal Knowledge Graph\n  Construction for Enhanced LLMs Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Vision to Language: Text-Free Multimodal Knowledge Graph\n  Construction for Enhanced LLMs Reasoning"
                },
                "summary": "Multimodal reasoning in Large Language Models (LLMs) struggles with\nincomplete knowledge and hallucination artifacts, challenges that textual\nKnowledge Graphs (KGs) only partially mitigate due to their modality isolation.\nWhile Multimodal Knowledge Graphs (MMKGs) promise enhanced cross-modal\nunderstanding, their practical construction is impeded by semantic narrowness\nof manual text annotations and inherent noise in visual-semantic entity\nlinkages. In this paper, we propose Vision-align-to-Language integrated\nKnowledge Graph (VaLiK), a novel approach for constructing MMKGs that enhances\nLLMs reasoning through cross-modal information supplementation. Specifically,\nwe cascade pre-trained Vision-Language Models (VLMs) to align image features\nwith text, transforming them into descriptions that encapsulate image-specific\ninformation. Furthermore, we developed a cross-modal similarity verification\nmechanism to quantify semantic consistency, effectively filtering out noise\nintroduced during feature alignment. Even without manually annotated image\ncaptions, the refined descriptions alone suffice to construct the MMKG.\nCompared to conventional MMKGs construction paradigms, our approach achieves\nsubstantial storage efficiency gains while maintaining direct entity-to-image\nlinkage capability. Experimental results on multimodal reasoning tasks\ndemonstrate that LLMs augmented with VaLiK outperform previous state-of-the-art\nmodels. Our code is published at https://github.com/Wings-Of-Disaster/VaLiK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal reasoning in Large Language Models (LLMs) struggles with\nincomplete knowledge and hallucination artifacts, challenges that textual\nKnowledge Graphs (KGs) only partially mitigate due to their modality isolation.\nWhile Multimodal Knowledge Graphs (MMKGs) promise enhanced cross-modal\nunderstanding, their practical construction is impeded by semantic narrowness\nof manual text annotations and inherent noise in visual-semantic entity\nlinkages. In this paper, we propose Vision-align-to-Language integrated\nKnowledge Graph (VaLiK), a novel approach for constructing MMKGs that enhances\nLLMs reasoning through cross-modal information supplementation. Specifically,\nwe cascade pre-trained Vision-Language Models (VLMs) to align image features\nwith text, transforming them into descriptions that encapsulate image-specific\ninformation. Furthermore, we developed a cross-modal similarity verification\nmechanism to quantify semantic consistency, effectively filtering out noise\nintroduced during feature alignment. Even without manually annotated image\ncaptions, the refined descriptions alone suffice to construct the MMKG.\nCompared to conventional MMKGs construction paradigms, our approach achieves\nsubstantial storage efficiency gains while maintaining direct entity-to-image\nlinkage capability. Experimental results on multimodal reasoning tasks\ndemonstrate that LLMs augmented with VaLiK outperform previous state-of-the-art\nmodels. Our code is published at https://github.com/Wings-Of-Disaster/VaLiK."
                },
                "authors": [
                    {
                        "name": "Junming Liu"
                    },
                    {
                        "name": "Siyuan Meng"
                    },
                    {
                        "name": "Yanting Gao"
                    },
                    {
                        "name": "Song Mao"
                    },
                    {
                        "name": "Pinlong Cai"
                    },
                    {
                        "name": "Guohang Yan"
                    },
                    {
                        "name": "Yirong Chen"
                    },
                    {
                        "name": "Zilin Bian"
                    },
                    {
                        "name": "Botian Shi"
                    },
                    {
                        "name": "Ding Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ding Wang"
                },
                "author": "Ding Wang",
                "arxiv_comment": "14 pages, 7 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10624v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10624v2",
                "updated": "2025-03-17T09:28:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    28,
                    43,
                    0,
                    76,
                    0
                ],
                "published": "2024-10-14T15:30:41Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    30,
                    41,
                    0,
                    288,
                    0
                ],
                "title": "SensorLLM: Aligning Large Language Models with Motion Sensors for Human\n  Activity Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SensorLLM: Aligning Large Language Models with Motion Sensors for Human\n  Activity Recognition"
                },
                "summary": "We introduce SensorLLM, a two-stage framework that enables Large Language\nModels (LLMs) to perform human activity recognition (HAR) from sensor data.\nDespite their strong reasoning and generalization capabilities, LLMs remain\nunderutilized for motion sensor data due to the lack of semantic context in\ntime-series, computational constraints, and challenges in processing numerical\ninputs. SensorLLM addresses these limitations through a Sensor-Language\nAlignment stage, where we introduce special tokens for each sensor channel and\nautomatically generate textual trend descriptions. This alignment enables LLMs\nto capture numerical variations, channel-specific features, and data of varying\nduration--without requiring human annotations. In the subsequent Task-Aware\nTuning stage, we refine the model for HAR classification, achieving performance\nthat matches or surpasses state-of-the-art methods. Our results demonstrate\nthat SensorLLM evolves into an effective sensor learner, reasoner, and\nclassifier through Sensor-Language Alignment, generalizing across diverse HAR\ndatasets. We believe this work establishes a foundation for future research on\ntime-series and text alignment, paving the way for foundation models in sensor\ndata analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SensorLLM, a two-stage framework that enables Large Language\nModels (LLMs) to perform human activity recognition (HAR) from sensor data.\nDespite their strong reasoning and generalization capabilities, LLMs remain\nunderutilized for motion sensor data due to the lack of semantic context in\ntime-series, computational constraints, and challenges in processing numerical\ninputs. SensorLLM addresses these limitations through a Sensor-Language\nAlignment stage, where we introduce special tokens for each sensor channel and\nautomatically generate textual trend descriptions. This alignment enables LLMs\nto capture numerical variations, channel-specific features, and data of varying\nduration--without requiring human annotations. In the subsequent Task-Aware\nTuning stage, we refine the model for HAR classification, achieving performance\nthat matches or surpasses state-of-the-art methods. Our results demonstrate\nthat SensorLLM evolves into an effective sensor learner, reasoner, and\nclassifier through Sensor-Language Alignment, generalizing across diverse HAR\ndatasets. We believe this work establishes a foundation for future research on\ntime-series and text alignment, paving the way for foundation models in sensor\ndata analysis."
                },
                "authors": [
                    {
                        "name": "Zechen Li"
                    },
                    {
                        "name": "Shohreh Deldari"
                    },
                    {
                        "name": "Linyao Chen"
                    },
                    {
                        "name": "Hao Xue"
                    },
                    {
                        "name": "Flora D. Salim"
                    }
                ],
                "author_detail": {
                    "name": "Flora D. Salim"
                },
                "author": "Flora D. Salim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10624v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10624v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12960v1",
                "updated": "2025-03-17T09:15:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    15,
                    20,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T09:15:20Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    15,
                    20,
                    0,
                    76,
                    0
                ],
                "title": "Initial acquisition requirements for optical cavities in the space\n  gravitational wave antennae DECIGO and B-DECIGO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Initial acquisition requirements for optical cavities in the space\n  gravitational wave antennae DECIGO and B-DECIGO"
                },
                "summary": "DECIGO (DECi-hertz Interferometer Gravitational Wave Observatory) is a\nspace-based gravitational wave antenna concept targeting the 0.1-10 Hz band. It\nconsists of three spacecraft arranged in an equilateral triangle with 1,000 km\nsides, forming Fabry-P\\'erot cavities between them. A precursor mission,\nB-DECIGO, is also planned, featuring a smaller 100 km triangle. Operating these\ncavities requires ultra-precise formation flying, where inter-mirror distance\nand alignment must be precisely controlled. Achieving this necessitates a\nsequential improvement in precision using various sensors and actuators, from\nthe deployment of the spacecraft to laser link acquisition and ultimately to\nthe control of the Fabry-P\\'erot cavities to maintain resonance. In this paper,\nwe derive the precision requirements at each stage and discuss the feasibility\nof achieving them. We show that the relative speed between cavity mirrors must\nbe controlled at the sub-micrometer-per-second level and that relative\nalignment must be maintained at the sub-microradian level to obtain control\nsignals from the Fabry-P\\'erot cavities of DECIGO and B-DECIGO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DECIGO (DECi-hertz Interferometer Gravitational Wave Observatory) is a\nspace-based gravitational wave antenna concept targeting the 0.1-10 Hz band. It\nconsists of three spacecraft arranged in an equilateral triangle with 1,000 km\nsides, forming Fabry-P\\'erot cavities between them. A precursor mission,\nB-DECIGO, is also planned, featuring a smaller 100 km triangle. Operating these\ncavities requires ultra-precise formation flying, where inter-mirror distance\nand alignment must be precisely controlled. Achieving this necessitates a\nsequential improvement in precision using various sensors and actuators, from\nthe deployment of the spacecraft to laser link acquisition and ultimately to\nthe control of the Fabry-P\\'erot cavities to maintain resonance. In this paper,\nwe derive the precision requirements at each stage and discuss the feasibility\nof achieving them. We show that the relative speed between cavity mirrors must\nbe controlled at the sub-micrometer-per-second level and that relative\nalignment must be maintained at the sub-microradian level to obtain control\nsignals from the Fabry-P\\'erot cavities of DECIGO and B-DECIGO."
                },
                "authors": [
                    {
                        "name": "Yuta Michimura"
                    },
                    {
                        "name": "Koji Nagano"
                    },
                    {
                        "name": "Kentaro Komori"
                    },
                    {
                        "name": "Kiwamu Izumi"
                    },
                    {
                        "name": "Takahiro Ito"
                    },
                    {
                        "name": "Satoshi Ikari"
                    },
                    {
                        "name": "Tomotada Akutsu"
                    },
                    {
                        "name": "Masaki Ando"
                    },
                    {
                        "name": "Isao Kawano"
                    },
                    {
                        "name": "Mitsuru Musha"
                    },
                    {
                        "name": "Shuichi Sato"
                    }
                ],
                "author_detail": {
                    "name": "Shuichi Sato"
                },
                "author": "Shuichi Sato",
                "arxiv_comment": "8 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12952v1",
                "updated": "2025-03-17T09:06:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    6,
                    3,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T09:06:03Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    6,
                    3,
                    0,
                    76,
                    0
                ],
                "title": "Performance Analysis and Industry Deployment of Post-Quantum\n  Cryptography Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Analysis and Industry Deployment of Post-Quantum\n  Cryptography Algorithms"
                },
                "summary": "As quantum computing advances, modern cryptographic standards face an\nexistential threat, necessitating a transition to post-quantum cryptography\n(PQC). The National Institute of Standards and Technology (NIST) has selected\nCRYSTALS-Kyber and CRYSTALS-Dilithium as standardized PQC algorithms for secure\nkey exchange and digital signatures, respectively. This study conducts a\ncomprehensive performance analysis of these algorithms by benchmarking\nexecution times across cryptographic operations such as key generation,\nencapsulation, decapsulation, signing, and verification. Additionally, the\nimpact of AVX2 optimizations is evaluated to assess hardware acceleration\nbenefits. Our findings demonstrate that Kyber and Dilithium achieve efficient\nexecution times, outperforming classical cryptographic schemes such as RSA and\nECDSA at equivalent security levels. Beyond technical performance, the\nreal-world deployment of PQC introduces challenges in telecommunications\nnetworks, where large-scale infrastructure upgrades, interoperability with\nlegacy systems, and regulatory constraints must be addressed. This paper\nexamines the feasibility of PQC adoption in telecom environments, highlighting\nkey transition challenges, security risks, and implementation strategies.\nThrough industry case studies, we illustrate how telecom operators are\nintegrating PQC into 5G authentication, subscriber identity protection, and\nsecure communications. Our analysis provides insights into the computational\ntrade-offs, deployment considerations, and standardization efforts shaping the\nfuture of quantum-safe cryptographic infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As quantum computing advances, modern cryptographic standards face an\nexistential threat, necessitating a transition to post-quantum cryptography\n(PQC). The National Institute of Standards and Technology (NIST) has selected\nCRYSTALS-Kyber and CRYSTALS-Dilithium as standardized PQC algorithms for secure\nkey exchange and digital signatures, respectively. This study conducts a\ncomprehensive performance analysis of these algorithms by benchmarking\nexecution times across cryptographic operations such as key generation,\nencapsulation, decapsulation, signing, and verification. Additionally, the\nimpact of AVX2 optimizations is evaluated to assess hardware acceleration\nbenefits. Our findings demonstrate that Kyber and Dilithium achieve efficient\nexecution times, outperforming classical cryptographic schemes such as RSA and\nECDSA at equivalent security levels. Beyond technical performance, the\nreal-world deployment of PQC introduces challenges in telecommunications\nnetworks, where large-scale infrastructure upgrades, interoperability with\nlegacy systems, and regulatory constraints must be addressed. This paper\nexamines the feasibility of PQC adoption in telecom environments, highlighting\nkey transition challenges, security risks, and implementation strategies.\nThrough industry case studies, we illustrate how telecom operators are\nintegrating PQC into 5G authentication, subscriber identity protection, and\nsecure communications. Our analysis provides insights into the computational\ntrade-offs, deployment considerations, and standardization efforts shaping the\nfuture of quantum-safe cryptographic infrastructure."
                },
                "authors": [
                    {
                        "name": "Elif Dicle Demir"
                    },
                    {
                        "name": "Buse Bilgin"
                    },
                    {
                        "name": "Mehmet Cengiz Onbasli"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet Cengiz Onbasli"
                },
                "author": "Mehmet Cengiz Onbasli",
                "arxiv_comment": "6 pages, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "E.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19488v2",
                "updated": "2025-03-17T09:01:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    1,
                    38,
                    0,
                    76,
                    0
                ],
                "published": "2024-11-29T06:06:35Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    6,
                    6,
                    35,
                    4,
                    334,
                    0
                ],
                "title": "Interleaved-Modal Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interleaved-Modal Chain-of-Thought"
                },
                "summary": "Chain-of-Thought (CoT) prompting elicits large language models (LLMs) to\nproduce a series of intermediate reasoning steps before arriving at the final\nanswer. However, when transitioning to vision-language models (VLMs), their\ntext-only rationales struggle to express the fine-grained associations with the\noriginal image. In this paper, we propose an image-incorporated multimodal\nChain-of-Thought, named \\textbf{Interleaved-modal Chain-of-Thought (ICoT)},\nwhich generates sequential reasoning steps consisting of paired visual and\ntextual rationales to infer the final answer. Intuitively, the novel ICoT\nrequires VLMs to enable the generation of fine-grained interleaved-modal\ncontent, which is hard for current VLMs to fulfill. Considering that the\nrequired visual information is usually part of the input image, we propose\n\\textbf{Attention-driven Selection (ADS)} to realize ICoT over existing VLMs.\nADS intelligently inserts regions of the input image to generate the\ninterleaved-modal reasoning steps with ignorable additional latency. ADS relies\nsolely on the attention map of VLMs without the need for parameterization, and\ntherefore it is a plug-and-play strategy that can be generalized to a spectrum\nof VLMs. We apply ADS to realize ICoT on two popular VLMs of different\narchitectures. Extensive evaluations of three benchmarks have shown that ICoT\nprompting achieves substantial performance (up to 14\\%) and interpretability\nimprovements compared to existing multimodal CoT prompting methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) prompting elicits large language models (LLMs) to\nproduce a series of intermediate reasoning steps before arriving at the final\nanswer. However, when transitioning to vision-language models (VLMs), their\ntext-only rationales struggle to express the fine-grained associations with the\noriginal image. In this paper, we propose an image-incorporated multimodal\nChain-of-Thought, named \\textbf{Interleaved-modal Chain-of-Thought (ICoT)},\nwhich generates sequential reasoning steps consisting of paired visual and\ntextual rationales to infer the final answer. Intuitively, the novel ICoT\nrequires VLMs to enable the generation of fine-grained interleaved-modal\ncontent, which is hard for current VLMs to fulfill. Considering that the\nrequired visual information is usually part of the input image, we propose\n\\textbf{Attention-driven Selection (ADS)} to realize ICoT over existing VLMs.\nADS intelligently inserts regions of the input image to generate the\ninterleaved-modal reasoning steps with ignorable additional latency. ADS relies\nsolely on the attention map of VLMs without the need for parameterization, and\ntherefore it is a plug-and-play strategy that can be generalized to a spectrum\nof VLMs. We apply ADS to realize ICoT on two popular VLMs of different\narchitectures. Extensive evaluations of three benchmarks have shown that ICoT\nprompting achieves substantial performance (up to 14\\%) and interpretability\nimprovements compared to existing multimodal CoT prompting methods."
                },
                "authors": [
                    {
                        "name": "Jun Gao"
                    },
                    {
                        "name": "Yongqi Li"
                    },
                    {
                        "name": "Ziqiang Cao"
                    },
                    {
                        "name": "Wenjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Li"
                },
                "author": "Wenjie Li",
                "arxiv_comment": "CVPR 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12931v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12931v1",
                "updated": "2025-03-17T08:41:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    8,
                    41,
                    29,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T08:41:29Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    8,
                    41,
                    29,
                    0,
                    76,
                    0
                ],
                "title": "MirrorGuard: Adaptive Defense Against Jailbreaks via Entropy-Guided\n  Mirror Crafting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MirrorGuard: Adaptive Defense Against Jailbreaks via Entropy-Guided\n  Mirror Crafting"
                },
                "summary": "Defending large language models (LLMs) against jailbreak attacks is crucial\nfor ensuring their safe deployment. Existing defense strategies generally rely\non predefined static criteria to differentiate between harmful and benign\nprompts. However, such rigid rules are incapable of accommodating the inherent\ncomplexity and dynamic nature of real jailbreak attacks. In this paper, we\npropose a novel concept of ``mirror'' to enable dynamic and adaptive defense. A\nmirror refers to a dynamically generated prompt that mirrors the syntactic\nstructure of the input while ensuring semantic safety. The personalized\ndiscrepancies between the input prompts and their corresponding mirrors serve\nas the guiding principles for defense. A new defense paradigm, MirrorGuard, is\nfurther proposed to detect and calibrate risky inputs based on such mirrors. An\nentropy-based detection metric, Relative Input Uncertainty (RIU), is integrated\ninto MirrorGuard to quantify the discrepancies between input prompts and\nmirrors. MirrorGuard is evaluated on several popular datasets, demonstrating\nstate-of-the-art defense performance while maintaining general effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defending large language models (LLMs) against jailbreak attacks is crucial\nfor ensuring their safe deployment. Existing defense strategies generally rely\non predefined static criteria to differentiate between harmful and benign\nprompts. However, such rigid rules are incapable of accommodating the inherent\ncomplexity and dynamic nature of real jailbreak attacks. In this paper, we\npropose a novel concept of ``mirror'' to enable dynamic and adaptive defense. A\nmirror refers to a dynamically generated prompt that mirrors the syntactic\nstructure of the input while ensuring semantic safety. The personalized\ndiscrepancies between the input prompts and their corresponding mirrors serve\nas the guiding principles for defense. A new defense paradigm, MirrorGuard, is\nfurther proposed to detect and calibrate risky inputs based on such mirrors. An\nentropy-based detection metric, Relative Input Uncertainty (RIU), is integrated\ninto MirrorGuard to quantify the discrepancies between input prompts and\nmirrors. MirrorGuard is evaluated on several popular datasets, demonstrating\nstate-of-the-art defense performance while maintaining general effectiveness."
                },
                "authors": [
                    {
                        "name": "Rui Pu"
                    },
                    {
                        "name": "Chaozhuo Li"
                    },
                    {
                        "name": "Rui Ha"
                    },
                    {
                        "name": "Litian Zhang"
                    },
                    {
                        "name": "Lirong Qiu"
                    },
                    {
                        "name": "Xi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xi Zhang"
                },
                "author": "Xi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12931v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14598v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14598v2",
                "updated": "2025-03-17T08:34:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    8,
                    34,
                    7,
                    0,
                    76,
                    0
                ],
                "published": "2024-02-04T09:58:17Z",
                "published_parsed": [
                    2024,
                    2,
                    4,
                    9,
                    58,
                    17,
                    6,
                    35,
                    0
                ],
                "title": "EMN: Brain-inspired Elastic Memory Network for Quick Domain Adaptive\n  Feature Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMN: Brain-inspired Elastic Memory Network for Quick Domain Adaptive\n  Feature Mapping"
                },
                "summary": "Utilizing unlabeled data in the target domain to perform continuous\noptimization is critical to enhance the generalization ability of neural\nnetworks. Most domain adaptation methods focus on time-consuming optimization\nof deep feature extractors, which limits the deployment on lightweight edge\ndevices. Inspired by the memory mechanism and powerful generalization ability\nof biological neural networks in human brains, we propose a novel gradient-free\nElastic Memory Network, namely EMN, to support quick fine-tuning of the mapping\nbetween features and prediction without heavy optimization of deep features. In\nparticular, EMN adopts randomly connected neurons to memorize the association\nof features and labels, where the signals in the network are propagated as\nimpulses, and the prediction is made by associating the memories stored on\nneurons based on their confidence. More importantly, EMN supports reinforced\nmemorization of feature mapping based on unlabeled data to quickly adapt to a\nnew domain. Experiments based on four cross-domain real-world datasets show\nthat EMN can achieve up to 10% enhancement of performance while only needing\nless than 1% timing cost of traditional domain adaptation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilizing unlabeled data in the target domain to perform continuous\noptimization is critical to enhance the generalization ability of neural\nnetworks. Most domain adaptation methods focus on time-consuming optimization\nof deep feature extractors, which limits the deployment on lightweight edge\ndevices. Inspired by the memory mechanism and powerful generalization ability\nof biological neural networks in human brains, we propose a novel gradient-free\nElastic Memory Network, namely EMN, to support quick fine-tuning of the mapping\nbetween features and prediction without heavy optimization of deep features. In\nparticular, EMN adopts randomly connected neurons to memorize the association\nof features and labels, where the signals in the network are propagated as\nimpulses, and the prediction is made by associating the memories stored on\nneurons based on their confidence. More importantly, EMN supports reinforced\nmemorization of feature mapping based on unlabeled data to quickly adapt to a\nnew domain. Experiments based on four cross-domain real-world datasets show\nthat EMN can achieve up to 10% enhancement of performance while only needing\nless than 1% timing cost of traditional domain adaptation methods."
                },
                "authors": [
                    {
                        "name": "Jianming Lv"
                    },
                    {
                        "name": "Chengjun Wang"
                    },
                    {
                        "name": "Depin Liang"
                    },
                    {
                        "name": "Qianli Ma"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "15 pages,15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14598v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14598v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19951v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19951v4",
                "updated": "2025-03-17T08:33:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    8,
                    33,
                    0,
                    0,
                    76,
                    0
                ],
                "published": "2024-11-29T18:59:54Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    18,
                    59,
                    54,
                    4,
                    334,
                    0
                ],
                "title": "Sparrow: Data-Efficient Video-LLM with Text-to-Image Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparrow: Data-Efficient Video-LLM with Text-to-Image Augmentation"
                },
                "summary": "Recent years have witnessed the success of Multimodal Large Language Models\n(MLLMs) in the vision understanding domain. The success of these models can\nlargely be attributed to the dominant scaling law, which states that larger\nparameter sizes and data volumes contribute to better performance. Notably,\ndata scaling has mainly been powered by automatic data pipelines, which center\naround the self-instruction of LLMs. The paradigm has been taken for granted\nfor quite some time, but the study of the effectiveness of scaling with these\ndata has been neglected for a long time. In this context, this work revisits\nscaling with synthetic data and focuses on developing video-LLMs from a\ndata-centric perspective. Our main study approach is fine-tuning pre-trained\nimage-LLMs with video data and investigating learning efficiency through data\nscaling. Results from our preliminary experiments reveal a low learning\nefficiency phenomenon when simply scaling up video data samples, which, through\nour probing, can be ascribed to a lack of instruction diversity. Aiming at this\nissue, we propose a data augmentation method called Sparrow, which synthesizes\nvideo-like samples from pure text instruction data. Mixing these synthetic\nsamples with the video data enables a more efficient training scheme. Through\ncomprehensive experiments, we demonstrate that our proposed method achieves\nperformance comparable to or even superior to baselines trained with many more\nsamples. Meanwhile, we find that incorporating these synthetic samples can\nboost the performance of long video understanding without training with long\nvideo data. The code and data examples are available at\nhttps://github.com/VITA-MLLM/Sparrow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed the success of Multimodal Large Language Models\n(MLLMs) in the vision understanding domain. The success of these models can\nlargely be attributed to the dominant scaling law, which states that larger\nparameter sizes and data volumes contribute to better performance. Notably,\ndata scaling has mainly been powered by automatic data pipelines, which center\naround the self-instruction of LLMs. The paradigm has been taken for granted\nfor quite some time, but the study of the effectiveness of scaling with these\ndata has been neglected for a long time. In this context, this work revisits\nscaling with synthetic data and focuses on developing video-LLMs from a\ndata-centric perspective. Our main study approach is fine-tuning pre-trained\nimage-LLMs with video data and investigating learning efficiency through data\nscaling. Results from our preliminary experiments reveal a low learning\nefficiency phenomenon when simply scaling up video data samples, which, through\nour probing, can be ascribed to a lack of instruction diversity. Aiming at this\nissue, we propose a data augmentation method called Sparrow, which synthesizes\nvideo-like samples from pure text instruction data. Mixing these synthetic\nsamples with the video data enables a more efficient training scheme. Through\ncomprehensive experiments, we demonstrate that our proposed method achieves\nperformance comparable to or even superior to baselines trained with many more\nsamples. Meanwhile, we find that incorporating these synthetic samples can\nboost the performance of long video understanding without training with long\nvideo data. The code and data examples are available at\nhttps://github.com/VITA-MLLM/Sparrow."
                },
                "authors": [
                    {
                        "name": "Shukang Yin"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Sirui Zhao"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Chunjiang Ge"
                    },
                    {
                        "name": "Yan Yang"
                    },
                    {
                        "name": "Zuwei Long"
                    },
                    {
                        "name": "Yuhan Dai"
                    },
                    {
                        "name": "Yongdong Luo"
                    },
                    {
                        "name": "Haoyu Cao"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Caifeng Shan"
                    },
                    {
                        "name": "Ran He"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "arxiv_comment": "Project page: https://github.com/VITA-MLLM/Sparrow",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19951v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19951v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12918v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12918v1",
                "updated": "2025-03-17T08:29:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    8,
                    29,
                    4,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T08:29:04Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    8,
                    29,
                    4,
                    0,
                    76,
                    0
                ],
                "title": "ThinkPatterns-21k: A Systematic Study on the Impact of Thinking Patterns\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinkPatterns-21k: A Systematic Study on the Impact of Thinking Patterns\n  in LLMs"
                },
                "summary": "Large language models (LLMs) have demonstrated enhanced performance through\nthe \\textit{Thinking then Responding} paradigm, where models generate internal\nthoughts before final responses (aka, System 2 thinking). However, existing\nresearch lacks a systematic understanding of the mechanisms underlying how\nthinking patterns affect performance across model sizes. In this work, we\nconduct a comprehensive analysis of the impact of various thinking types on\nmodel performance and introduce ThinkPatterns-21k, a curated dataset comprising\n21k instruction-response pairs (QA) collected from existing\ninstruction-following datasets with five thinking types. For each pair, we\naugment it with five distinct internal thinking patterns: one unstructured\nthinking (monologue) and four structured variants (decomposition, self-ask,\nself-debate and self-critic), while maintaining the same instruction and\nresponse. Through extensive evaluation across different model sizes (3B-32B\nparameters), we have two key findings: (1) smaller models (<30B parameters) can\nbenefit from most of structured thinking patterns, while larger models (32B)\nwith structured thinking like decomposition would degrade performance and (2)\nunstructured monologue demonstrates broad effectiveness across different model\nsizes. Finally, we released all of our datasets, checkpoints, training logs of\ndiverse thinking patterns to reproducibility, aiming to facilitate further\nresearch in this direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated enhanced performance through\nthe \\textit{Thinking then Responding} paradigm, where models generate internal\nthoughts before final responses (aka, System 2 thinking). However, existing\nresearch lacks a systematic understanding of the mechanisms underlying how\nthinking patterns affect performance across model sizes. In this work, we\nconduct a comprehensive analysis of the impact of various thinking types on\nmodel performance and introduce ThinkPatterns-21k, a curated dataset comprising\n21k instruction-response pairs (QA) collected from existing\ninstruction-following datasets with five thinking types. For each pair, we\naugment it with five distinct internal thinking patterns: one unstructured\nthinking (monologue) and four structured variants (decomposition, self-ask,\nself-debate and self-critic), while maintaining the same instruction and\nresponse. Through extensive evaluation across different model sizes (3B-32B\nparameters), we have two key findings: (1) smaller models (<30B parameters) can\nbenefit from most of structured thinking patterns, while larger models (32B)\nwith structured thinking like decomposition would degrade performance and (2)\nunstructured monologue demonstrates broad effectiveness across different model\nsizes. Finally, we released all of our datasets, checkpoints, training logs of\ndiverse thinking patterns to reproducibility, aiming to facilitate further\nresearch in this direction."
                },
                "authors": [
                    {
                        "name": "Pengcheng Wen"
                    },
                    {
                        "name": "Jiaming Ji"
                    },
                    {
                        "name": "Chi-Min Chan"
                    },
                    {
                        "name": "Juntao Dai"
                    },
                    {
                        "name": "Donghai Hong"
                    },
                    {
                        "name": "Yaodong Yang"
                    },
                    {
                        "name": "Sirui Han"
                    },
                    {
                        "name": "Yike Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yike Guo"
                },
                "author": "Yike Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12918v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12908v1",
                "updated": "2025-03-17T08:17:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    8,
                    17,
                    28,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T08:17:28Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    8,
                    17,
                    28,
                    0,
                    76,
                    0
                ],
                "title": "HICD: Hallucination-Inducing via Attention Dispersion for Contrastive\n  Decoding to Mitigate Hallucinations in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HICD: Hallucination-Inducing via Attention Dispersion for Contrastive\n  Decoding to Mitigate Hallucinations in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) often generate hallucinations, producing outputs\nthat are contextually inaccurate or factually incorrect. We introduce HICD, a\nnovel method designed to induce hallucinations for contrastive decoding to\nmitigate hallucinations. Unlike existing contrastive decoding methods, HICD\nselects attention heads crucial to the model's prediction as inducing heads,\nthen induces hallucinations by dispersing attention of these inducing heads and\ncompares the hallucinated outputs with the original outputs to obtain the final\nresult. Our approach significantly improves performance on tasks requiring\ncontextual faithfulness, such as context completion, reading comprehension, and\nquestion answering. It also improves factuality in tasks requiring accurate\nknowledge recall. We demonstrate that our inducing heads selection and\nattention dispersion method leads to more \"contrast-effective\" hallucinations\nfor contrastive decoding, outperforming other hallucination-inducing methods.\nOur findings provide a promising strategy for reducing hallucinations by\ninducing hallucinations in a controlled manner, enhancing the performance of\nLLMs in a wide range of tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often generate hallucinations, producing outputs\nthat are contextually inaccurate or factually incorrect. We introduce HICD, a\nnovel method designed to induce hallucinations for contrastive decoding to\nmitigate hallucinations. Unlike existing contrastive decoding methods, HICD\nselects attention heads crucial to the model's prediction as inducing heads,\nthen induces hallucinations by dispersing attention of these inducing heads and\ncompares the hallucinated outputs with the original outputs to obtain the final\nresult. Our approach significantly improves performance on tasks requiring\ncontextual faithfulness, such as context completion, reading comprehension, and\nquestion answering. It also improves factuality in tasks requiring accurate\nknowledge recall. We demonstrate that our inducing heads selection and\nattention dispersion method leads to more \"contrast-effective\" hallucinations\nfor contrastive decoding, outperforming other hallucination-inducing methods.\nOur findings provide a promising strategy for reducing hallucinations by\ninducing hallucinations in a controlled manner, enhancing the performance of\nLLMs in a wide range of tasks."
                },
                "authors": [
                    {
                        "name": "Xinyan Jiang"
                    },
                    {
                        "name": "Hang Ye"
                    },
                    {
                        "name": "Yongxin Zhu"
                    },
                    {
                        "name": "Xiaoying Zheng"
                    },
                    {
                        "name": "Zikang Chen"
                    },
                    {
                        "name": "Jun Gong"
                    }
                ],
                "author_detail": {
                    "name": "Jun Gong"
                },
                "author": "Jun Gong",
                "arxiv_comment": "Under review at ARR - February 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18325v2",
                "updated": "2025-03-17T08:14:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    8,
                    14,
                    35,
                    0,
                    76,
                    0
                ],
                "published": "2024-10-23T23:36:06Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    23,
                    36,
                    6,
                    2,
                    297,
                    0
                ],
                "title": "AVHBench: A Cross-Modal Hallucination Benchmark for Audio-Visual Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AVHBench: A Cross-Modal Hallucination Benchmark for Audio-Visual Large\n  Language Models"
                },
                "summary": "Following the success of Large Language Models (LLMs), expanding their\nboundaries to new modalities represents a significant paradigm shift in\nmultimodal understanding. Human perception is inherently multimodal, relying\nnot only on text but also on auditory and visual cues for a complete\nunderstanding of the world. In recognition of this fact, audio-visual LLMs have\nrecently emerged. Despite promising developments, the lack of dedicated\nbenchmarks poses challenges for understanding and evaluating models. In this\nwork, we show that audio-visual LLMs struggle to discern subtle relationships\nbetween audio and visual signals, leading to hallucinations and highlighting\nthe need for reliable benchmarks. To address this, we introduce AVHBench, the\nfirst comprehensive benchmark specifically designed to evaluate the perception\nand comprehension capabilities of audio-visual LLMs. Our benchmark includes\ntests for assessing hallucinations, as well as the cross-modal matching and\nreasoning abilities of these models. Our results reveal that most existing\naudio-visual LLMs struggle with hallucinations caused by cross-interactions\nbetween modalities, due to their limited capacity to perceive complex\nmultimodal signals and their relationships. Additionally, we demonstrate that\nsimple training with our AVHBench improves robustness of audio-visual LLMs\nagainst hallucinations. Dataset: https://github.com/kaist-ami/AVHBench",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Following the success of Large Language Models (LLMs), expanding their\nboundaries to new modalities represents a significant paradigm shift in\nmultimodal understanding. Human perception is inherently multimodal, relying\nnot only on text but also on auditory and visual cues for a complete\nunderstanding of the world. In recognition of this fact, audio-visual LLMs have\nrecently emerged. Despite promising developments, the lack of dedicated\nbenchmarks poses challenges for understanding and evaluating models. In this\nwork, we show that audio-visual LLMs struggle to discern subtle relationships\nbetween audio and visual signals, leading to hallucinations and highlighting\nthe need for reliable benchmarks. To address this, we introduce AVHBench, the\nfirst comprehensive benchmark specifically designed to evaluate the perception\nand comprehension capabilities of audio-visual LLMs. Our benchmark includes\ntests for assessing hallucinations, as well as the cross-modal matching and\nreasoning abilities of these models. Our results reveal that most existing\naudio-visual LLMs struggle with hallucinations caused by cross-interactions\nbetween modalities, due to their limited capacity to perceive complex\nmultimodal signals and their relationships. Additionally, we demonstrate that\nsimple training with our AVHBench improves robustness of audio-visual LLMs\nagainst hallucinations. Dataset: https://github.com/kaist-ami/AVHBench"
                },
                "authors": [
                    {
                        "name": "Kim Sung-Bin"
                    },
                    {
                        "name": "Oh Hyun-Bin"
                    },
                    {
                        "name": "JungMok Lee"
                    },
                    {
                        "name": "Arda Senocak"
                    },
                    {
                        "name": "Joon Son Chung"
                    },
                    {
                        "name": "Tae-Hyun Oh"
                    }
                ],
                "author_detail": {
                    "name": "Tae-Hyun Oh"
                },
                "author": "Tae-Hyun Oh",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12899v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12899v1",
                "updated": "2025-03-17T07:59:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    7,
                    59,
                    42,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T07:59:42Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    7,
                    59,
                    42,
                    0,
                    76,
                    0
                ],
                "title": "A Semantic-based Optimization Approach for Repairing LLMs: Case Study on\n  Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Semantic-based Optimization Approach for Repairing LLMs: Case Study on\n  Code Generation"
                },
                "summary": "Language Models (LMs) are widely used in software engineering for code\ngeneration, but they may produce code with errors. Rather than repairing the\ngenerated code, an alternative way is to address the underlying failures of\nmodels. LM repair offers a lightweight solution to this challenge: it requires\nminimal data, reduces computational costs, and reduces the side effects. Unlike\nretraining, LM repair focuses on applying tailored updates to targeted neurons,\nmaking it ideal for scenarios with limited resources, high-performance demands,\nor strict safety requirements. In this paper, we propose \\ul{S}emantic\n\\ul{T}argeting for \\ul{A}nalytical \\ul{R}epair (\\textsc{STAR}), a pioneering\nand novel semantic-based optimization approach for repairing LLMs.\n\\textsc{STAR} realizes main operations in LM repair methods in an optimization\nprocess, including locating ``buggy neurons'', solving ``neuron patches'', and\npatching ``buggy neurons''. Correspondingly, it computes the deltas of weight\nmatrix as the prior information to guide optimization; and attributes the\ntargeted layers and neurons leveraging statistical insights. The neuron patches\nare computed with a solid semantic-based analytical formula, which directly\nbridges the changes to logits with the deltas of neurons, by steering latent\nrepresentations. Compared to the prior work of LM repair (\\textsc{MINT}) and\noptimization methods (\\textsc{SGD}), \\textsc{STAR} integrates their strengths\nwhile mitigating their limitations. \\textsc{STAR} supports solving multiple\nfailures together, significantly improving the usefulness. Evaluated on three\ncode generation tasks using popular code LMs, \\textsc{STAR} demonstrates\nsuperior effectiveness. Additionally, \\textsc{STAR} exhibits better efficiency.\nIn terms of side effects, namely the balance between generalization and\nspecificity, \\textsc{STAR} outperforms prior work by a significant margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models (LMs) are widely used in software engineering for code\ngeneration, but they may produce code with errors. Rather than repairing the\ngenerated code, an alternative way is to address the underlying failures of\nmodels. LM repair offers a lightweight solution to this challenge: it requires\nminimal data, reduces computational costs, and reduces the side effects. Unlike\nretraining, LM repair focuses on applying tailored updates to targeted neurons,\nmaking it ideal for scenarios with limited resources, high-performance demands,\nor strict safety requirements. In this paper, we propose \\ul{S}emantic\n\\ul{T}argeting for \\ul{A}nalytical \\ul{R}epair (\\textsc{STAR}), a pioneering\nand novel semantic-based optimization approach for repairing LLMs.\n\\textsc{STAR} realizes main operations in LM repair methods in an optimization\nprocess, including locating ``buggy neurons'', solving ``neuron patches'', and\npatching ``buggy neurons''. Correspondingly, it computes the deltas of weight\nmatrix as the prior information to guide optimization; and attributes the\ntargeted layers and neurons leveraging statistical insights. The neuron patches\nare computed with a solid semantic-based analytical formula, which directly\nbridges the changes to logits with the deltas of neurons, by steering latent\nrepresentations. Compared to the prior work of LM repair (\\textsc{MINT}) and\noptimization methods (\\textsc{SGD}), \\textsc{STAR} integrates their strengths\nwhile mitigating their limitations. \\textsc{STAR} supports solving multiple\nfailures together, significantly improving the usefulness. Evaluated on three\ncode generation tasks using popular code LMs, \\textsc{STAR} demonstrates\nsuperior effectiveness. Additionally, \\textsc{STAR} exhibits better efficiency.\nIn terms of side effects, namely the balance between generalization and\nspecificity, \\textsc{STAR} outperforms prior work by a significant margin."
                },
                "authors": [
                    {
                        "name": "Jian Gu"
                    },
                    {
                        "name": "Aldeida Aleti"
                    },
                    {
                        "name": "Chunyang Chen"
                    },
                    {
                        "name": "Hongyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hongyu Zhang"
                },
                "author": "Hongyu Zhang",
                "arxiv_comment": "12 pages, 6 figure, 6 tables, under peer-review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12899v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12899v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12896v1",
                "updated": "2025-03-17T07:58:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    7,
                    58,
                    5,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T07:58:05Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    7,
                    58,
                    5,
                    0,
                    76,
                    0
                ],
                "title": "Safeguarding LLM Embeddings in End-Cloud Collaboration via\n  Entropy-Driven Perturbation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safeguarding LLM Embeddings in End-Cloud Collaboration via\n  Entropy-Driven Perturbation"
                },
                "summary": "Recent studies improve on-device language model (LM) inference through\nend-cloud collaboration, where the end device retrieves useful information from\ncloud databases to enhance local processing, known as Retrieval-Augmented\nGeneration (RAG). Typically, to retrieve information from the cloud while\nsafeguarding privacy, the end device transforms original data into embeddings\nwith a local embedding model. However, the recently emerging Embedding\nInversion Attacks (EIAs) can still recover the original data from text\nembeddings (e.g., training a recovery model to map embeddings back to original\ntexts), posing a significant threat to user privacy. To address this risk, we\npropose EntroGuard, an entropy-driven perturbation-based embedding privacy\nprotection method, which can protect the privacy of text embeddings while\nmaintaining retrieval accuracy during the end-cloud collaboration.\nSpecifically, to defeat various EIAs, we perturb the embeddings to increase the\nentropy of the recovered text in the common structure of recovery models, thus\nsteering the embeddings toward meaningless texts rather than original sensitive\ntexts during the recovery process. To maintain retrieval performance in the\ncloud, we constrain the perturbations within a bound, applying the strategy of\nreducing them where redundant and increasing them where sparse. Moreover,\nEntroGuard can be directly integrated into end devices without requiring any\nmodifications to the embedding model. Extensive experimental results\ndemonstrate that EntroGuard can reduce the risk of privacy leakage by up to 8\ntimes at most with negligible loss of retrieval performance compared to\nexisting privacy-preserving methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies improve on-device language model (LM) inference through\nend-cloud collaboration, where the end device retrieves useful information from\ncloud databases to enhance local processing, known as Retrieval-Augmented\nGeneration (RAG). Typically, to retrieve information from the cloud while\nsafeguarding privacy, the end device transforms original data into embeddings\nwith a local embedding model. However, the recently emerging Embedding\nInversion Attacks (EIAs) can still recover the original data from text\nembeddings (e.g., training a recovery model to map embeddings back to original\ntexts), posing a significant threat to user privacy. To address this risk, we\npropose EntroGuard, an entropy-driven perturbation-based embedding privacy\nprotection method, which can protect the privacy of text embeddings while\nmaintaining retrieval accuracy during the end-cloud collaboration.\nSpecifically, to defeat various EIAs, we perturb the embeddings to increase the\nentropy of the recovered text in the common structure of recovery models, thus\nsteering the embeddings toward meaningless texts rather than original sensitive\ntexts during the recovery process. To maintain retrieval performance in the\ncloud, we constrain the perturbations within a bound, applying the strategy of\nreducing them where redundant and increasing them where sparse. Moreover,\nEntroGuard can be directly integrated into end devices without requiring any\nmodifications to the embedding model. Extensive experimental results\ndemonstrate that EntroGuard can reduce the risk of privacy leakage by up to 8\ntimes at most with negligible loss of retrieval performance compared to\nexisting privacy-preserving methods."
                },
                "authors": [
                    {
                        "name": "Shuaifan Jin"
                    },
                    {
                        "name": "Xiaoyi Pang"
                    },
                    {
                        "name": "Zhibo Wang"
                    },
                    {
                        "name": "He Wang"
                    },
                    {
                        "name": "Jiacheng Du"
                    },
                    {
                        "name": "Jiahui Hu"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13356v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13356v4",
                "updated": "2025-03-17T07:46:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    7,
                    46,
                    49,
                    0,
                    76,
                    0
                ],
                "published": "2024-06-19T09:03:21Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    9,
                    3,
                    21,
                    2,
                    171,
                    0
                ],
                "title": "Unlearning or Obfuscating? Jogging the Memory of Unlearned LLMs via\n  Benign Relearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlearning or Obfuscating? Jogging the Memory of Unlearned LLMs via\n  Benign Relearning"
                },
                "summary": "Machine unlearning is a promising approach to mitigate undesirable\nmemorization of training data in ML models. However, in this work we show that\nexisting approaches for unlearning in LLMs are surprisingly susceptible to a\nsimple set of $\\textit{benign relearning attacks}$. With access to only a small\nand potentially loosely related set of data, we find that we can ''jog'' the\nmemory of unlearned models to reverse the effects of unlearning. For example,\nwe show that relearning on public medical articles can lead an unlearned LLM to\noutput harmful knowledge about bioweapons, and relearning general wiki\ninformation about the book series Harry Potter can force the model to output\nverbatim memorized text. We formalize this unlearning-relearning pipeline,\nexplore the attack across three popular unlearning benchmarks, and discuss\nfuture directions and guidelines that result from our study. Our work indicates\nthat current approximate unlearning methods simply suppress the model outputs\nand fail to robustly forget target knowledge in the LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine unlearning is a promising approach to mitigate undesirable\nmemorization of training data in ML models. However, in this work we show that\nexisting approaches for unlearning in LLMs are surprisingly susceptible to a\nsimple set of $\\textit{benign relearning attacks}$. With access to only a small\nand potentially loosely related set of data, we find that we can ''jog'' the\nmemory of unlearned models to reverse the effects of unlearning. For example,\nwe show that relearning on public medical articles can lead an unlearned LLM to\noutput harmful knowledge about bioweapons, and relearning general wiki\ninformation about the book series Harry Potter can force the model to output\nverbatim memorized text. We formalize this unlearning-relearning pipeline,\nexplore the attack across three popular unlearning benchmarks, and discuss\nfuture directions and guidelines that result from our study. Our work indicates\nthat current approximate unlearning methods simply suppress the model outputs\nand fail to robustly forget target knowledge in the LLMs."
                },
                "authors": [
                    {
                        "name": "Shengyuan Hu"
                    },
                    {
                        "name": "Yiwei Fu"
                    },
                    {
                        "name": "Zhiwei Steven Wu"
                    },
                    {
                        "name": "Virginia Smith"
                    }
                ],
                "author_detail": {
                    "name": "Virginia Smith"
                },
                "author": "Virginia Smith",
                "arxiv_comment": "ICLR 2025, 32 pages, 8 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13356v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13356v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16217v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16217v2",
                "updated": "2025-03-17T07:37:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    7,
                    37,
                    50,
                    0,
                    76,
                    0
                ],
                "published": "2024-12-18T03:19:19Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    3,
                    19,
                    19,
                    2,
                    353,
                    0
                ],
                "title": "Neonpool: Reimagining cryptocurrency transaction pools for lightweight\n  clients and IoT devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neonpool: Reimagining cryptocurrency transaction pools for lightweight\n  clients and IoT devices"
                },
                "summary": "The transaction pool plays a critical role in processing and disseminating\ntransactions in cryptocurrency networks. However, increasing transaction loads\nstrain the resources of full node deployments. We present Neonpool, an\ninnovative transaction pool optimization using bloom filter variants, which\nreduces the memory footprint of the transaction pool to a fraction. Implemented\nin C++ and benchmarked using a unique Bitcoin and Ethereum dataset, our\nsolution verifies and forwards transactions with over 99.99\\% accuracy and does\nnot necessitate a hard fork. Neonpool is ideally suited for lightweight\ncryptocurrency clients and for resource-constrained devices such as browsers,\nsystems-on-a-chip, mobile or IoT devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transaction pool plays a critical role in processing and disseminating\ntransactions in cryptocurrency networks. However, increasing transaction loads\nstrain the resources of full node deployments. We present Neonpool, an\ninnovative transaction pool optimization using bloom filter variants, which\nreduces the memory footprint of the transaction pool to a fraction. Implemented\nin C++ and benchmarked using a unique Bitcoin and Ethereum dataset, our\nsolution verifies and forwards transactions with over 99.99\\% accuracy and does\nnot necessitate a hard fork. Neonpool is ideally suited for lightweight\ncryptocurrency clients and for resource-constrained devices such as browsers,\nsystems-on-a-chip, mobile or IoT devices."
                },
                "authors": [
                    {
                        "name": "Hina Binte Haq"
                    },
                    {
                        "name": "Syed Taha Ali"
                    },
                    {
                        "name": "Asad Salman"
                    },
                    {
                        "name": "Patrick McCorry"
                    },
                    {
                        "name": "Siamak F. Shahandashti"
                    }
                ],
                "author_detail": {
                    "name": "Siamak F. Shahandashti"
                },
                "author": "Siamak F. Shahandashti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16217v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16217v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17848v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17848v2",
                "updated": "2025-03-17T07:36:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    7,
                    36,
                    1,
                    0,
                    76,
                    0
                ],
                "published": "2025-02-25T04:51:17Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    4,
                    51,
                    17,
                    1,
                    56,
                    0
                ],
                "title": "LR$^2$Bench: Evaluating Long-chain Reflective Reasoning Capabilities of\n  Large Language Models via Constraint Satisfaction Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LR$^2$Bench: Evaluating Long-chain Reflective Reasoning Capabilities of\n  Large Language Models via Constraint Satisfaction Problems"
                },
                "summary": "Recent progress in o1-like models has significantly enhanced the reasoning\nabilities of Large Language Models (LLMs), empowering them to tackle\nincreasingly complex tasks through reflection capabilities, such as making\nassumptions, backtracking, and self-refinement. However, effectively evaluating\nsuch reflection capabilities remains challenging due to the lack of appropriate\nbenchmarks. To bridge this gap, we introduce LR$^2$Bench, a novel benchmark\ndesigned to evaluate the Long-chain Reflective Reasoning capabilities of LLMs.\nLR$^2$Bench comprises 850 samples across six Constraint Satisfaction Problems\n(CSPs) where reflective reasoning is crucial for deriving solutions that meet\nall given constraints. Each type of task focuses on distinct constraint\npatterns, such as knowledge-based, logical, and spatial constraints, providing\na comprehensive evaluation of diverse problem-solving scenarios. We conduct\nextensive evaluation on both conventional models and o1-like models. Our\nexperimental results reveal that even the most advanced reasoning-specific\nmodels, such as DeepSeek-R1 and OpenAI o1-preview, struggle with tasks in\nLR$^2$Bench, achieving an average Exact Match score of only 20.0% and 23.6%,\nrespectively. These findings underscore the significant room for improvement in\nthe reflective reasoning capabilities of current LLMs. The leaderboard of our\nbenchmark is available at https://huggingface.co/spaces/UltraRonin/LR2Bench",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in o1-like models has significantly enhanced the reasoning\nabilities of Large Language Models (LLMs), empowering them to tackle\nincreasingly complex tasks through reflection capabilities, such as making\nassumptions, backtracking, and self-refinement. However, effectively evaluating\nsuch reflection capabilities remains challenging due to the lack of appropriate\nbenchmarks. To bridge this gap, we introduce LR$^2$Bench, a novel benchmark\ndesigned to evaluate the Long-chain Reflective Reasoning capabilities of LLMs.\nLR$^2$Bench comprises 850 samples across six Constraint Satisfaction Problems\n(CSPs) where reflective reasoning is crucial for deriving solutions that meet\nall given constraints. Each type of task focuses on distinct constraint\npatterns, such as knowledge-based, logical, and spatial constraints, providing\na comprehensive evaluation of diverse problem-solving scenarios. We conduct\nextensive evaluation on both conventional models and o1-like models. Our\nexperimental results reveal that even the most advanced reasoning-specific\nmodels, such as DeepSeek-R1 and OpenAI o1-preview, struggle with tasks in\nLR$^2$Bench, achieving an average Exact Match score of only 20.0% and 23.6%,\nrespectively. These findings underscore the significant room for improvement in\nthe reflective reasoning capabilities of current LLMs. The leaderboard of our\nbenchmark is available at https://huggingface.co/spaces/UltraRonin/LR2Bench"
                },
                "authors": [
                    {
                        "name": "Jianghao Chen"
                    },
                    {
                        "name": "Zhenlin Wei"
                    },
                    {
                        "name": "Zhenjiang Ren"
                    },
                    {
                        "name": "Ziyong Li"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "arxiv_comment": "Submitted to ACL ARR 2025 February",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17848v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17848v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01090v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01090v2",
                "updated": "2025-03-17T07:34:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    7,
                    34,
                    41,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-03T01:30:28Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    1,
                    30,
                    28,
                    0,
                    62,
                    0
                ],
                "title": "Precise Localization of Memories: A Fine-grained Neuron-level Knowledge\n  Editing Technique for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precise Localization of Memories: A Fine-grained Neuron-level Knowledge\n  Editing Technique for LLMs"
                },
                "summary": "Knowledge editing aims to update outdated information in Large Language\nModels (LLMs). A representative line of study is locate-then-edit methods,\nwhich typically employ causal tracing to identify the modules responsible for\nrecalling factual knowledge about entities. However, we find these methods are\noften sensitive only to changes in the subject entity, leaving them less\neffective at adapting to changes in relations. This limitation results in poor\nediting locality, which can lead to the persistence of irrelevant or inaccurate\nfacts, ultimately compromising the reliability of LLMs. We believe this issue\narises from the insufficient precision of knowledge localization. To address\nthis, we propose a Fine-grained Neuron-level Knowledge Editing (FiNE) method\nthat enhances editing locality without affecting overall success rates. By\nprecisely identifying and modifying specific neurons within feed-forward\nnetworks, FiNE significantly improves knowledge localization and editing.\nQuantitative experiments demonstrate that FiNE efficiently achieves better\noverall performance compared to existing techniques, providing new insights\ninto the localization and modification of knowledge within LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge editing aims to update outdated information in Large Language\nModels (LLMs). A representative line of study is locate-then-edit methods,\nwhich typically employ causal tracing to identify the modules responsible for\nrecalling factual knowledge about entities. However, we find these methods are\noften sensitive only to changes in the subject entity, leaving them less\neffective at adapting to changes in relations. This limitation results in poor\nediting locality, which can lead to the persistence of irrelevant or inaccurate\nfacts, ultimately compromising the reliability of LLMs. We believe this issue\narises from the insufficient precision of knowledge localization. To address\nthis, we propose a Fine-grained Neuron-level Knowledge Editing (FiNE) method\nthat enhances editing locality without affecting overall success rates. By\nprecisely identifying and modifying specific neurons within feed-forward\nnetworks, FiNE significantly improves knowledge localization and editing.\nQuantitative experiments demonstrate that FiNE efficiently achieves better\noverall performance compared to existing techniques, providing new insights\ninto the localization and modification of knowledge within LLMs."
                },
                "authors": [
                    {
                        "name": "Haowen Pan"
                    },
                    {
                        "name": "Xiaozhi Wang"
                    },
                    {
                        "name": "Yixin Cao"
                    },
                    {
                        "name": "Zenglin Shi"
                    },
                    {
                        "name": "Xun Yang"
                    },
                    {
                        "name": "Juanzi Li"
                    },
                    {
                        "name": "Meng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Wang"
                },
                "author": "Meng Wang",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01090v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01090v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12884v1",
                "updated": "2025-03-17T07:29:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    7,
                    29,
                    5,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T07:29:05Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    7,
                    29,
                    5,
                    0,
                    76,
                    0
                ],
                "title": "Optimizing Ansatz Design in Quantum Generative Adversarial Networks\n  Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Ansatz Design in Quantum Generative Adversarial Networks\n  Using Large Language Models"
                },
                "summary": "We present a novel approach for improving the design of ansatzes in Quantum\nGenerative Adversarial Networks (qGANs) by leveraging Large Language Models\n(LLMs). By combining the strengths of LLMs with qGANs, our approach iteratively\nrefines ansatz structures to improve accuracy while reducing circuit depth and\nthe number of parameters. This study paves the way for further exploration in\nAI-driven quantum algorithm design. The flexibility of our proposed workflow\nextends to other quantum variational algorithms, providing a general framework\nfor optimizing quantum circuits in a variety of quantum computing tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel approach for improving the design of ansatzes in Quantum\nGenerative Adversarial Networks (qGANs) by leveraging Large Language Models\n(LLMs). By combining the strengths of LLMs with qGANs, our approach iteratively\nrefines ansatz structures to improve accuracy while reducing circuit depth and\nthe number of parameters. This study paves the way for further exploration in\nAI-driven quantum algorithm design. The flexibility of our proposed workflow\nextends to other quantum variational algorithms, providing a general framework\nfor optimizing quantum circuits in a variety of quantum computing tasks."
                },
                "authors": [
                    {
                        "name": "Kento Ueda"
                    },
                    {
                        "name": "Atsushi Matsuo"
                    }
                ],
                "author_detail": {
                    "name": "Atsushi Matsuo"
                },
                "author": "Atsushi Matsuo",
                "arxiv_comment": "8 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12880v1",
                "updated": "2025-03-17T07:20:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    7,
                    20,
                    11,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T07:20:11Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    7,
                    20,
                    11,
                    0,
                    76,
                    0
                ],
                "title": "nvBench 2.0: A Benchmark for Natural Language to Visualization under\n  Ambiguity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "nvBench 2.0: A Benchmark for Natural Language to Visualization under\n  Ambiguity"
                },
                "summary": "Natural Language to Visualization (NL2VIS) enables users to create\nvisualizations from natural language queries, making data insights more\naccessible. However, NL2VIS faces challenges in interpreting ambiguous queries,\nas users often express their visualization needs in imprecise language. To\naddress this challenge, we introduce nvBench 2.0, a new benchmark designed to\nevaluate NL2VIS systems in scenarios involving ambiguous queries. nvBench 2.0\nincludes 7,878 natural language queries and 24,076 corresponding\nvisualizations, derived from 780 tables across 153 domains. It is built using a\ncontrolled ambiguity-injection pipeline that generates ambiguous queries\nthrough a reverse-generation workflow. By starting with unambiguous seed\nvisualizations and selectively injecting ambiguities, the pipeline yields\nmultiple valid interpretations for each query, with each ambiguous query\ntraceable to its corresponding visualization through step-wise reasoning paths.\nWe evaluate various Large Language Models (LLMs) on their ability to perform\nambiguous NL2VIS tasks using nvBench 2.0. We also propose Step-NL2VIS, an\nLLM-based model trained on nvBench 2.0, which enhances performance in ambiguous\nscenarios through step-wise preference optimization. Our results show that\nStep-NL2VIS outperforms all baselines, setting a new state-of-the-art for\nambiguous NL2VIS tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language to Visualization (NL2VIS) enables users to create\nvisualizations from natural language queries, making data insights more\naccessible. However, NL2VIS faces challenges in interpreting ambiguous queries,\nas users often express their visualization needs in imprecise language. To\naddress this challenge, we introduce nvBench 2.0, a new benchmark designed to\nevaluate NL2VIS systems in scenarios involving ambiguous queries. nvBench 2.0\nincludes 7,878 natural language queries and 24,076 corresponding\nvisualizations, derived from 780 tables across 153 domains. It is built using a\ncontrolled ambiguity-injection pipeline that generates ambiguous queries\nthrough a reverse-generation workflow. By starting with unambiguous seed\nvisualizations and selectively injecting ambiguities, the pipeline yields\nmultiple valid interpretations for each query, with each ambiguous query\ntraceable to its corresponding visualization through step-wise reasoning paths.\nWe evaluate various Large Language Models (LLMs) on their ability to perform\nambiguous NL2VIS tasks using nvBench 2.0. We also propose Step-NL2VIS, an\nLLM-based model trained on nvBench 2.0, which enhances performance in ambiguous\nscenarios through step-wise preference optimization. Our results show that\nStep-NL2VIS outperforms all baselines, setting a new state-of-the-art for\nambiguous NL2VIS tasks."
                },
                "authors": [
                    {
                        "name": "Tianqi Luo"
                    },
                    {
                        "name": "Chuhan Huang"
                    },
                    {
                        "name": "Leixian Shen"
                    },
                    {
                        "name": "Boyan Li"
                    },
                    {
                        "name": "Shuyu Shen"
                    },
                    {
                        "name": "Wei Zeng"
                    },
                    {
                        "name": "Nan Tang"
                    },
                    {
                        "name": "Yuyu Luo"
                    }
                ],
                "author_detail": {
                    "name": "Yuyu Luo"
                },
                "author": "Yuyu Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12878v1",
                "updated": "2025-03-17T07:16:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    7,
                    16,
                    31,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T07:16:31Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    7,
                    16,
                    31,
                    0,
                    76,
                    0
                ],
                "title": "Enable Time-Sensitive Applications in Kubernetes with Container Network\n  Interface Plugin Agnostic Metadata Proxy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enable Time-Sensitive Applications in Kubernetes with Container Network\n  Interface Plugin Agnostic Metadata Proxy"
                },
                "summary": "Application deployment in cloud environment is dominated by\nKubernetes-orchestrated microservices. Provides a secure environment,\nnetworking, storage, isolation, scheduling, and many other abstractions that\ncan be easily extended to meet our needs. Time-Sensitive Applications (TSAs)\nhave special requirements for compute and network. Deploying TSAs in Kubernetes\nis challenging because the networking implemented by Container Network\nInterface (CNI) plugins is not aware of the traffic characteristic required by\nTime-Sensitive Network. Even if a network interface supports TSN features\n(e.g.: Scheduled Traffic) and a modified CNI plugin is aware of this interface,\nthe pod network isolation built on top of Linux deletes the metadata required\nfor TSN protocols to work with.\n  We propose TSN metadata proxy, a simple architecture that allows any TSA\nmicroservice to use the TSN capabilities of the physical NIC, without any\nmodification. This architecture is tightly integrated with the Kubernetes\nnetworking model, works with popular CNI plugins, and supports services such as\nClusterIP, NodePort, or LoadBalancer without additional configuration. Unlike\nformer proposals, this architecture does not require either bypassing the Linux\nkernel network stack, direct access to the physical NIC, escalated privileges\nfor the TSA microservice, or even modification of the TSA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application deployment in cloud environment is dominated by\nKubernetes-orchestrated microservices. Provides a secure environment,\nnetworking, storage, isolation, scheduling, and many other abstractions that\ncan be easily extended to meet our needs. Time-Sensitive Applications (TSAs)\nhave special requirements for compute and network. Deploying TSAs in Kubernetes\nis challenging because the networking implemented by Container Network\nInterface (CNI) plugins is not aware of the traffic characteristic required by\nTime-Sensitive Network. Even if a network interface supports TSN features\n(e.g.: Scheduled Traffic) and a modified CNI plugin is aware of this interface,\nthe pod network isolation built on top of Linux deletes the metadata required\nfor TSN protocols to work with.\n  We propose TSN metadata proxy, a simple architecture that allows any TSA\nmicroservice to use the TSN capabilities of the physical NIC, without any\nmodification. This architecture is tightly integrated with the Kubernetes\nnetworking model, works with popular CNI plugins, and supports services such as\nClusterIP, NodePort, or LoadBalancer without additional configuration. Unlike\nformer proposals, this architecture does not require either bypassing the Linux\nkernel network stack, direct access to the physical NIC, escalated privileges\nfor the TSA microservice, or even modification of the TSA."
                },
                "authors": [
                    {
                        "name": "Ferenc Orosi"
                    },
                    {
                        "name": "Ferenc Fejes"
                    }
                ],
                "author_detail": {
                    "name": "Ferenc Fejes"
                },
                "author": "Ferenc Fejes",
                "arxiv_comment": "Presented at netdev 0x19 conference. Conference page:\n  https://netdevconf.info/0x19/15",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11223v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11223v4",
                "updated": "2025-03-18T02:51:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    2,
                    51,
                    43,
                    1,
                    77,
                    0
                ],
                "published": "2024-11-18T01:25:58Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    1,
                    25,
                    58,
                    0,
                    323,
                    0
                ],
                "title": "Efficient Transfer Learning for Video-language Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Transfer Learning for Video-language Foundation Models"
                },
                "summary": "Pre-trained vision-language models provide a robust foundation for efficient\ntransfer learning across various downstream tasks. In the field of video action\nrecognition, mainstream approaches often introduce additional modules to\ncapture temporal information. Although the additional modules increase the\ncapacity of model, enabling it to better capture video-specific inductive\nbiases, existing methods typically introduce a substantial number of new\nparameters and are prone to catastrophic forgetting of previously acquired\ngeneralizable knowledge. In this paper, we propose a parameter-efficient\nMulti-modal Spatio-Temporal Adapter (MSTA) to enhance the alignment between\ntextual and visual representations, achieving a balance between generalizable\nknowledge and task-specific adaptation. Furthermore, to mitigate over-fitting\nand enhance generalizability, we introduce a spatio-temporal description-guided\nconsistency constraint.This constraint involves providing template inputs\n(e.g., \"a video of \\{\\textbf{cls}\\}\") to the trainable language branch and\nLLM-generated spatio-temporal descriptions to the pre-trained language branch,\nenforcing output consistency between the branches. This approach reduces\noverfitting to downstream tasks and enhances the distinguishability of the\ntrainable branch within the spatio-temporal semantic space. We evaluate the\neffectiveness of our approach across four tasks: zero-shot transfer, few-shot\nlearning, base-to-novel generalization, and fully-supervised learning. Compared\nto many state-of-the-art methods, our MSTA achieves outstanding performance\nacross all evaluations, while using only 2-7\\% of the trainable parameters in\nthe original model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained vision-language models provide a robust foundation for efficient\ntransfer learning across various downstream tasks. In the field of video action\nrecognition, mainstream approaches often introduce additional modules to\ncapture temporal information. Although the additional modules increase the\ncapacity of model, enabling it to better capture video-specific inductive\nbiases, existing methods typically introduce a substantial number of new\nparameters and are prone to catastrophic forgetting of previously acquired\ngeneralizable knowledge. In this paper, we propose a parameter-efficient\nMulti-modal Spatio-Temporal Adapter (MSTA) to enhance the alignment between\ntextual and visual representations, achieving a balance between generalizable\nknowledge and task-specific adaptation. Furthermore, to mitigate over-fitting\nand enhance generalizability, we introduce a spatio-temporal description-guided\nconsistency constraint.This constraint involves providing template inputs\n(e.g., \"a video of \\{\\textbf{cls}\\}\") to the trainable language branch and\nLLM-generated spatio-temporal descriptions to the pre-trained language branch,\nenforcing output consistency between the branches. This approach reduces\noverfitting to downstream tasks and enhances the distinguishability of the\ntrainable branch within the spatio-temporal semantic space. We evaluate the\neffectiveness of our approach across four tasks: zero-shot transfer, few-shot\nlearning, base-to-novel generalization, and fully-supervised learning. Compared\nto many state-of-the-art methods, our MSTA achieves outstanding performance\nacross all evaluations, while using only 2-7\\% of the trainable parameters in\nthe original model."
                },
                "authors": [
                    {
                        "name": "Haoxing Chen"
                    },
                    {
                        "name": "Zizheng Huang"
                    },
                    {
                        "name": "Yan Hong"
                    },
                    {
                        "name": "Yanshuo Wang"
                    },
                    {
                        "name": "Zhongcai Lyu"
                    },
                    {
                        "name": "Zhuoer Xu"
                    },
                    {
                        "name": "Jun Lan"
                    },
                    {
                        "name": "Zhangxuan Gu"
                    }
                ],
                "author_detail": {
                    "name": "Zhangxuan Gu"
                },
                "author": "Zhangxuan Gu",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11223v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11223v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12854v1",
                "updated": "2025-03-17T06:28:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    6,
                    28,
                    25,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T06:28:25Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    6,
                    28,
                    25,
                    0,
                    76,
                    0
                ],
                "title": "Enhancing LLM Reasoning with Iterative DPO: A Comprehensive Empirical\n  Investigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM Reasoning with Iterative DPO: A Comprehensive Empirical\n  Investigation"
                },
                "summary": "Recent advancements in post-training methodologies for large language models\n(LLMs) have highlighted reinforcement learning (RL) as a critical component for\nenhancing reasoning. However, the substantial computational costs associated\nwith RL-based approaches have led to growing interest in alternative paradigms,\nsuch as Direct Preference Optimization (DPO). In this study, we investigate the\neffectiveness of DPO in facilitating self-improvement for LLMs through\niterative preference-based learning. We demonstrate that a single round of DPO\nwith coarse filtering significantly enhances mathematical reasoning\nperformance, particularly for strong base model. Furthermore, we design an\niterative enhancement framework for both the generator and the reward model\n(RM), enabling their mutual improvement through online interaction across\nmultiple rounds of DPO. Finally, with simple verifiable rewards, our model\nDPO-VP achieves RL-level performance with significantly lower computational\noverhead. These findings highlight DPO as a scalable and cost-effective\nalternative to RL, offering a practical solution for enhancing LLM reasoning in\nresource-constrained situations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in post-training methodologies for large language models\n(LLMs) have highlighted reinforcement learning (RL) as a critical component for\nenhancing reasoning. However, the substantial computational costs associated\nwith RL-based approaches have led to growing interest in alternative paradigms,\nsuch as Direct Preference Optimization (DPO). In this study, we investigate the\neffectiveness of DPO in facilitating self-improvement for LLMs through\niterative preference-based learning. We demonstrate that a single round of DPO\nwith coarse filtering significantly enhances mathematical reasoning\nperformance, particularly for strong base model. Furthermore, we design an\niterative enhancement framework for both the generator and the reward model\n(RM), enabling their mutual improvement through online interaction across\nmultiple rounds of DPO. Finally, with simple verifiable rewards, our model\nDPO-VP achieves RL-level performance with significantly lower computational\noverhead. These findings highlight DPO as a scalable and cost-effective\nalternative to RL, offering a practical solution for enhancing LLM reasoning in\nresource-constrained situations."
                },
                "authors": [
                    {
                        "name": "Songjun Tu"
                    },
                    {
                        "name": "Jiahao Lin"
                    },
                    {
                        "name": "Xiangyu Tian"
                    },
                    {
                        "name": "Qichao Zhang"
                    },
                    {
                        "name": "Linjing Li"
                    },
                    {
                        "name": "Yuqian Fu"
                    },
                    {
                        "name": "Nan Xu"
                    },
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Xiangyuan Lan"
                    },
                    {
                        "name": "Dongmei Jiang"
                    },
                    {
                        "name": "Dongbin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongbin Zhao"
                },
                "author": "Dongbin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12852v1",
                "updated": "2025-03-17T06:12:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    6,
                    12,
                    36,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T06:12:36Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    6,
                    12,
                    36,
                    0,
                    76,
                    0
                ],
                "title": "ACT360: An Efficient 360-Degree Action Detection and Summarization\n  Framework for Mission-Critical Training and Debriefing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACT360: An Efficient 360-Degree Action Detection and Summarization\n  Framework for Mission-Critical Training and Debriefing"
                },
                "summary": "Effective training and debriefing are critical in high-stakes,\nmission-critical environments such as disaster response, military simulations,\nand industrial safety, where precision and minimizing errors are paramount. The\ntraditional post-training analysis relies on manually reviewing 2D videos, a\ntime-consuming process that lacks comprehensive situational awareness. To\naddress these limitations, we introduce ACT360, a system that leverages\n360-degree videos and machine learning for automated action detection and\nstructured debriefing. ACT360 integrates 360YOWO, an enhanced You Only Watch\nOnce (YOWO) model with spatial attention and equirectangular-aware convolution\n(EAC) to mitigate panoramic video distortions. To enable deployment in\nresource-constrained environments, we apply quantization and model pruning,\nreducing the model size by 74% while maintaining robust accuracy (mAP drop of\nonly 1.5%, from 0.865 to 0.850) and improving inference speed. We validate our\napproach on a publicly available dataset of 55 labeled 360-degree videos\ncovering seven key operational actions, recorded across various real-world\ntraining sessions and environmental conditions. Additionally, ACT360 integrates\n360AIE (Action Insight Explorer), a web-based interface for automatic action\ndetection, retrieval, and textual summarization using large language models\n(LLMs), significantly enhancing post-incident analysis efficiency. ACT360\nserves as a generalized framework for mission-critical debriefing,\nincorporating EAC, spatial attention, summarization, and model optimization.\nThese innovations apply to any training environment requiring lightweight\naction detection and structured post-exercise analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective training and debriefing are critical in high-stakes,\nmission-critical environments such as disaster response, military simulations,\nand industrial safety, where precision and minimizing errors are paramount. The\ntraditional post-training analysis relies on manually reviewing 2D videos, a\ntime-consuming process that lacks comprehensive situational awareness. To\naddress these limitations, we introduce ACT360, a system that leverages\n360-degree videos and machine learning for automated action detection and\nstructured debriefing. ACT360 integrates 360YOWO, an enhanced You Only Watch\nOnce (YOWO) model with spatial attention and equirectangular-aware convolution\n(EAC) to mitigate panoramic video distortions. To enable deployment in\nresource-constrained environments, we apply quantization and model pruning,\nreducing the model size by 74% while maintaining robust accuracy (mAP drop of\nonly 1.5%, from 0.865 to 0.850) and improving inference speed. We validate our\napproach on a publicly available dataset of 55 labeled 360-degree videos\ncovering seven key operational actions, recorded across various real-world\ntraining sessions and environmental conditions. Additionally, ACT360 integrates\n360AIE (Action Insight Explorer), a web-based interface for automatic action\ndetection, retrieval, and textual summarization using large language models\n(LLMs), significantly enhancing post-incident analysis efficiency. ACT360\nserves as a generalized framework for mission-critical debriefing,\nincorporating EAC, spatial attention, summarization, and model optimization.\nThese innovations apply to any training environment requiring lightweight\naction detection and structured post-exercise analysis."
                },
                "authors": [
                    {
                        "name": "Aditi Tiwari"
                    },
                    {
                        "name": "Klara Nahrstedt"
                    }
                ],
                "author_detail": {
                    "name": "Klara Nahrstedt"
                },
                "author": "Klara Nahrstedt",
                "arxiv_comment": "9 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16144v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16144v2",
                "updated": "2025-03-17T06:04:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    6,
                    4,
                    48,
                    0,
                    76,
                    0
                ],
                "published": "2024-06-23T15:50:22Z",
                "published_parsed": [
                    2024,
                    6,
                    23,
                    15,
                    50,
                    22,
                    6,
                    175,
                    0
                ],
                "title": "Chain-of-Probe: Examining the Necessity and Accuracy of CoT Step-by-Step",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Probe: Examining the Necessity and Accuracy of CoT Step-by-Step"
                },
                "summary": "Current research found the issue of Early Answering in large language models\n(LLMs), where the models already have an answer before generating the\nChain-of-Thought (CoT). This phenomenon suggests a potential lack of necessary\ndependency between the predicted answer and the reasoning process.\nConsequently, two important questions arise: (1) Is CoT still necessary if the\nmodel already has an answer? (2) Can the correctness of the answer serve as\nvalid evidence for the correctness of CoT? To address these questions, we\npropose a method, namely Chain-of-Probe (CoP), to probe changes in the mind\nduring the model's reasoning. The probing results show that in a significant\nnumber of question-answer cases, CoT appears to be unnecessary, and this\nnecessity correlates with the simplicity of the task, defined by reasoning\nsteps required. Furthermore, by analyzing patterns in mind change, we examine\nthe correctness of the model's reasoning. Our validation reveals that many\nresponses, although correct in their final answer, contain errors in their\nreasoning process. To this end, we propose a strategic approach based on CoP to\nprioritize answers with correct reasoning among multiple candidates, thereby\nbolstering the reliability of the model's reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current research found the issue of Early Answering in large language models\n(LLMs), where the models already have an answer before generating the\nChain-of-Thought (CoT). This phenomenon suggests a potential lack of necessary\ndependency between the predicted answer and the reasoning process.\nConsequently, two important questions arise: (1) Is CoT still necessary if the\nmodel already has an answer? (2) Can the correctness of the answer serve as\nvalid evidence for the correctness of CoT? To address these questions, we\npropose a method, namely Chain-of-Probe (CoP), to probe changes in the mind\nduring the model's reasoning. The probing results show that in a significant\nnumber of question-answer cases, CoT appears to be unnecessary, and this\nnecessity correlates with the simplicity of the task, defined by reasoning\nsteps required. Furthermore, by analyzing patterns in mind change, we examine\nthe correctness of the model's reasoning. Our validation reveals that many\nresponses, although correct in their final answer, contain errors in their\nreasoning process. To this end, we propose a strategic approach based on CoP to\nprioritize answers with correct reasoning among multiple candidates, thereby\nbolstering the reliability of the model's reasoning."
                },
                "authors": [
                    {
                        "name": "Zezhong Wang"
                    },
                    {
                        "name": "Xingshan Zeng"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Liangyou Li"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Kam-Fai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kam-Fai Wong"
                },
                "author": "Kam-Fai Wong",
                "arxiv_comment": "Accepted by Findings of NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16144v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16144v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18447v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18447v2",
                "updated": "2025-03-17T06:00:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    6,
                    0,
                    52,
                    0,
                    76,
                    0
                ],
                "published": "2024-10-24T05:45:04Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    45,
                    4,
                    3,
                    298,
                    0
                ],
                "title": "ToolFlow: Boosting LLM Tool-Calling Through Natural and Coherent\n  Dialogue Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolFlow: Boosting LLM Tool-Calling Through Natural and Coherent\n  Dialogue Synthesis"
                },
                "summary": "Supervised fine-tuning (SFT) is a common method to enhance the tool calling\ncapabilities of Large Language Models (LLMs), with the training data often\nbeing synthesized. The current data synthesis process generally involves\nsampling a set of tools, formulating a requirement based on these tools, and\ngenerating the call statements. However, tools sampled randomly lack relevance,\nmaking them difficult to combine and thus reducing the diversity of the data.\nAdditionally, current work overlooks the coherence between turns of dialogues,\nleading to a gap between the synthesized data and real-world scenarios. To\naddress these issues, we propose a Graph-based Sampling strategy to sample more\nrelevant tool combinations, and a Planned-generation strategy to create plans\nthat guide the synthesis of coherent dialogues. We integrate these two\nstrategies and enable multiple agents to synthesize the dialogue data\ninteractively, resulting in our tool-calling data synthesis pipeline ToolFlow.\nData quality assessments demonstrate improvements in the naturalness and\ncoherence of our synthesized dialogues. Finally, we apply SFT on LLaMA-3.1-8B\nusing 8,000 synthetic dialogues generated with ToolFlow. Results show that the\nmodel achieves tool-calling performance comparable to or even surpassing GPT-4,\nwhile maintaining strong general capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning (SFT) is a common method to enhance the tool calling\ncapabilities of Large Language Models (LLMs), with the training data often\nbeing synthesized. The current data synthesis process generally involves\nsampling a set of tools, formulating a requirement based on these tools, and\ngenerating the call statements. However, tools sampled randomly lack relevance,\nmaking them difficult to combine and thus reducing the diversity of the data.\nAdditionally, current work overlooks the coherence between turns of dialogues,\nleading to a gap between the synthesized data and real-world scenarios. To\naddress these issues, we propose a Graph-based Sampling strategy to sample more\nrelevant tool combinations, and a Planned-generation strategy to create plans\nthat guide the synthesis of coherent dialogues. We integrate these two\nstrategies and enable multiple agents to synthesize the dialogue data\ninteractively, resulting in our tool-calling data synthesis pipeline ToolFlow.\nData quality assessments demonstrate improvements in the naturalness and\ncoherence of our synthesized dialogues. Finally, we apply SFT on LLaMA-3.1-8B\nusing 8,000 synthetic dialogues generated with ToolFlow. Results show that the\nmodel achieves tool-calling performance comparable to or even surpassing GPT-4,\nwhile maintaining strong general capabilities."
                },
                "authors": [
                    {
                        "name": "Zezhong Wang"
                    },
                    {
                        "name": "Xingshan Zeng"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Liangyou Li"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Kam-Fai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kam-Fai Wong"
                },
                "author": "Kam-Fai Wong",
                "arxiv_comment": "Accepted by NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18447v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18447v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08622v2",
                "updated": "2025-03-17T05:45:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    5,
                    45,
                    58,
                    0,
                    76,
                    0
                ],
                "published": "2024-09-13T08:19:52Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    8,
                    19,
                    52,
                    4,
                    257,
                    0
                ],
                "title": "Policy Prototyping for LLMs: Pluralistic Alignment via Interactive and\n  Collaborative Policymaking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Policy Prototyping for LLMs: Pluralistic Alignment via Interactive and\n  Collaborative Policymaking"
                },
                "summary": "Emerging efforts in AI alignment seek to broaden participation in shaping\nmodel behavior by eliciting and integrating collective input into a policy for\nmodel finetuning. While pluralistic, these processes are often linear and do\nnot allow participating stakeholders to confirm whether potential outcomes of\ntheir contributions are indeed consistent with their intentions. Design\nprototyping has long advocated for rapid iteration using tight feedback loops\nof ideation, experimentation, and evaluation to mitigate these issues. We thus\npropose policy prototyping for LLMs, a new process that draws inspiration from\nprototyping practices to enable stakeholders to collaboratively and\ninteractively draft LLM policies. Through learnings from a real-world LLM\npolicymaking initiative at an industrial AI lab, we motivate our approach and\ncharacterize policy prototyping with four guiding principles. Because policy\nprototyping emphasizes a contrasting set of priorities compared to previous\napproaches, we envision our approach to be a valuable addition to the\nmethodological repertoire for collaborative, pluralistic alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging efforts in AI alignment seek to broaden participation in shaping\nmodel behavior by eliciting and integrating collective input into a policy for\nmodel finetuning. While pluralistic, these processes are often linear and do\nnot allow participating stakeholders to confirm whether potential outcomes of\ntheir contributions are indeed consistent with their intentions. Design\nprototyping has long advocated for rapid iteration using tight feedback loops\nof ideation, experimentation, and evaluation to mitigate these issues. We thus\npropose policy prototyping for LLMs, a new process that draws inspiration from\nprototyping practices to enable stakeholders to collaboratively and\ninteractively draft LLM policies. Through learnings from a real-world LLM\npolicymaking initiative at an industrial AI lab, we motivate our approach and\ncharacterize policy prototyping with four guiding principles. Because policy\nprototyping emphasizes a contrasting set of priorities compared to previous\napproaches, we envision our approach to be a valuable addition to the\nmethodological repertoire for collaborative, pluralistic alignment."
                },
                "authors": [
                    {
                        "name": "K. J. Kevin Feng"
                    },
                    {
                        "name": "Inyoung Cheong"
                    },
                    {
                        "name": "Quan Ze Chen"
                    },
                    {
                        "name": "Amy X. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Amy X. Zhang"
                },
                "author": "Amy X. Zhang",
                "arxiv_comment": "Bidirectional Human-AI Alignment (Bi-Align) Workshop @ ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09620v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09620v2",
                "updated": "2025-03-17T05:40:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    5,
                    40,
                    49,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-08T18:01:11Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    18,
                    1,
                    11,
                    5,
                    67,
                    0
                ],
                "title": "Exploiting Edited Large Language Models as General Scientific Optimizers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Edited Large Language Models as General Scientific Optimizers"
                },
                "summary": "Large language models (LLMs) have been widely adopted in mathematical\noptimization in scientific scenarios for their extensive knowledge and advanced\nreasoning capabilities. Existing methods mainly focus on utilizing LLMs to\nsolve optimization problems in a prompt-based manner, which takes observational\nfeedback as additional textual descriptions. However, due to LLM's \\textbf{high\nsensitivity to the prompts} and \\textbf{tendency to get lost in lengthy\nprompts}, these methods struggle to effectively utilize the {observational}\nfeedback from each optimization step, which severely hinders the applications\nfor real-world scenarios. To address these challenges, we propose a\nconceptually simple and general {bi-level} optimization method, namely\n\\textbf{G}eneral \\textbf{S}cientific \\textbf{O}ptimizers (GSO). Specifically,\nGSO first utilizes inner-level simulators as experimental platforms to evaluate\nthe current solution and provide observational feedback. Then, LLMs serve as\nknowledgeable and versatile scientists, generating new solutions by refining\npotential errors from the feedback as the outer-level optimization. Finally,\nsimulations together with the expert knowledge in LLMs are jointly updated with\nbi-level interactions via model editing. Extensive experiments show that GSO\nconsistently outperforms existing state-of-the-art methods using \\textit{six}\ndifferent LLM backbones on \\textit{seven} different tasks, demonstrating the\neffectiveness and a wide range of applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely adopted in mathematical\noptimization in scientific scenarios for their extensive knowledge and advanced\nreasoning capabilities. Existing methods mainly focus on utilizing LLMs to\nsolve optimization problems in a prompt-based manner, which takes observational\nfeedback as additional textual descriptions. However, due to LLM's \\textbf{high\nsensitivity to the prompts} and \\textbf{tendency to get lost in lengthy\nprompts}, these methods struggle to effectively utilize the {observational}\nfeedback from each optimization step, which severely hinders the applications\nfor real-world scenarios. To address these challenges, we propose a\nconceptually simple and general {bi-level} optimization method, namely\n\\textbf{G}eneral \\textbf{S}cientific \\textbf{O}ptimizers (GSO). Specifically,\nGSO first utilizes inner-level simulators as experimental platforms to evaluate\nthe current solution and provide observational feedback. Then, LLMs serve as\nknowledgeable and versatile scientists, generating new solutions by refining\npotential errors from the feedback as the outer-level optimization. Finally,\nsimulations together with the expert knowledge in LLMs are jointly updated with\nbi-level interactions via model editing. Extensive experiments show that GSO\nconsistently outperforms existing state-of-the-art methods using \\textit{six}\ndifferent LLM backbones on \\textit{seven} different tasks, demonstrating the\neffectiveness and a wide range of applications."
                },
                "authors": [
                    {
                        "name": "Qitan Lv"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Hong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hong Wang"
                },
                "author": "Hong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09620v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09620v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12829v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12829v1",
                "updated": "2025-03-17T05:21:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    5,
                    21,
                    54,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T05:21:54Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    5,
                    21,
                    54,
                    0,
                    76,
                    0
                ],
                "title": "SparseLUT: Sparse Connectivity Optimization for Lookup Table-based Deep\n  Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseLUT: Sparse Connectivity Optimization for Lookup Table-based Deep\n  Neural Networks"
                },
                "summary": "The deployment of deep neural networks (DNNs) on resource-constrained edge\ndevices such as field-programmable gate arrays (FPGAs) requires a careful\nbalance of latency, power, and resource usage while maintaining high accuracy.\nExisting Lookup Table (LUT)-based DNNs, including LogicNets, PolyLUT,\nPolyLUT-Add, and NeuraLUT, exploit native FPGA resources with random sparse\nconnectivity. This paper introduces SparseLUT, a connectivity-centric training\ntechnique tailored for LUT-based DNNs. SparseLUT leverages a non-greedy\ntraining strategy that prioritizes the pruning of less significant connections\nand strategically regrows alternative ones, resulting in efficient convergence\nto the target sparsity. Experimental results show consistent accuracy\nimprovements across benchmarks, including up to a 2.13\\% increase on MNIST and\na 0.94\\% improvement for Jet Substructure Classification compared to random\nsparsity. This is done without any hardware overhead and achieves\nstate-of-the-art results for LUT-based DNNs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of deep neural networks (DNNs) on resource-constrained edge\ndevices such as field-programmable gate arrays (FPGAs) requires a careful\nbalance of latency, power, and resource usage while maintaining high accuracy.\nExisting Lookup Table (LUT)-based DNNs, including LogicNets, PolyLUT,\nPolyLUT-Add, and NeuraLUT, exploit native FPGA resources with random sparse\nconnectivity. This paper introduces SparseLUT, a connectivity-centric training\ntechnique tailored for LUT-based DNNs. SparseLUT leverages a non-greedy\ntraining strategy that prioritizes the pruning of less significant connections\nand strategically regrows alternative ones, resulting in efficient convergence\nto the target sparsity. Experimental results show consistent accuracy\nimprovements across benchmarks, including up to a 2.13\\% increase on MNIST and\na 0.94\\% improvement for Jet Substructure Classification compared to random\nsparsity. This is done without any hardware overhead and achieves\nstate-of-the-art results for LUT-based DNNs."
                },
                "authors": [
                    {
                        "name": "Binglei Lou"
                    },
                    {
                        "name": "Ruilin Wu"
                    },
                    {
                        "name": "Philip Leong"
                    }
                ],
                "author_detail": {
                    "name": "Philip Leong"
                },
                "author": "Philip Leong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12829v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14675v2",
                "updated": "2025-03-17T04:47:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    4,
                    47,
                    58,
                    0,
                    76,
                    0
                ],
                "published": "2024-10-18T17:59:47Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    59,
                    47,
                    4,
                    292,
                    0
                ],
                "title": "To Trust or Not to Trust? Enhancing Large Language Models' Situated\n  Faithfulness to External Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Trust or Not to Trust? Enhancing Large Language Models' Situated\n  Faithfulness to External Contexts"
                },
                "summary": "Large Language Models (LLMs) are often augmented with external contexts, such\nas those used in retrieval-augmented generation (RAG). However, these contexts\ncan be inaccurate or intentionally misleading, leading to conflicts with the\nmodel's internal knowledge. We argue that robust LLMs should demonstrate\nsituated faithfulness, dynamically calibrating their trust in external\ninformation based on their confidence in the internal knowledge and the\nexternal context to resolve knowledge conflicts. To benchmark this capability,\nwe evaluate LLMs across several QA datasets, including a newly created dataset\nfeaturing in-the-wild incorrect contexts sourced from Reddit posts. We show\nthat when provided with both correct and incorrect contexts, both open-source\nand proprietary models tend to overly rely on external information, regardless\nof its factual accuracy. To enhance situated faithfulness, we propose two\napproaches: Self-Guided Confidence Reasoning (SCR) and Rule-Based Confidence\nReasoning (RCR). SCR enables models to self-assess the confidence of external\ninformation relative to their own internal knowledge to produce the most\naccurate answer. RCR, in contrast, extracts explicit confidence signals from\nthe LLM and determines the final answer using predefined rules. Our results\nshow that for LLMs with strong reasoning capabilities, such as GPT-4o and\nGPT-4o mini, SCR outperforms RCR, achieving improvements of up to 24.2% over a\ndirect input augmentation baseline. Conversely, for a smaller model like\nLlama-3-8B, RCR outperforms SCR. Fine-tuning SCR with our proposed Confidence\nReasoning Direct Preference Optimization (CR-DPO) method improves performance\non both seen and unseen datasets, yielding an average improvement of 8.9% on\nLlama-3-8B. In addition to quantitative results, we offer insights into the\nrelative strengths of SCR and RCR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are often augmented with external contexts, such\nas those used in retrieval-augmented generation (RAG). However, these contexts\ncan be inaccurate or intentionally misleading, leading to conflicts with the\nmodel's internal knowledge. We argue that robust LLMs should demonstrate\nsituated faithfulness, dynamically calibrating their trust in external\ninformation based on their confidence in the internal knowledge and the\nexternal context to resolve knowledge conflicts. To benchmark this capability,\nwe evaluate LLMs across several QA datasets, including a newly created dataset\nfeaturing in-the-wild incorrect contexts sourced from Reddit posts. We show\nthat when provided with both correct and incorrect contexts, both open-source\nand proprietary models tend to overly rely on external information, regardless\nof its factual accuracy. To enhance situated faithfulness, we propose two\napproaches: Self-Guided Confidence Reasoning (SCR) and Rule-Based Confidence\nReasoning (RCR). SCR enables models to self-assess the confidence of external\ninformation relative to their own internal knowledge to produce the most\naccurate answer. RCR, in contrast, extracts explicit confidence signals from\nthe LLM and determines the final answer using predefined rules. Our results\nshow that for LLMs with strong reasoning capabilities, such as GPT-4o and\nGPT-4o mini, SCR outperforms RCR, achieving improvements of up to 24.2% over a\ndirect input augmentation baseline. Conversely, for a smaller model like\nLlama-3-8B, RCR outperforms SCR. Fine-tuning SCR with our proposed Confidence\nReasoning Direct Preference Optimization (CR-DPO) method improves performance\non both seen and unseen datasets, yielding an average improvement of 8.9% on\nLlama-3-8B. In addition to quantitative results, we offer insights into the\nrelative strengths of SCR and RCR."
                },
                "authors": [
                    {
                        "name": "Yukun Huang"
                    },
                    {
                        "name": "Sanxing Chen"
                    },
                    {
                        "name": "Hongyi Cai"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    }
                ],
                "author_detail": {
                    "name": "Bhuwan Dhingra"
                },
                "author": "Bhuwan Dhingra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11197v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11197v2",
                "updated": "2025-03-17T04:20:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    4,
                    20,
                    29,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-14T08:43:53Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    8,
                    43,
                    53,
                    4,
                    73,
                    0
                ],
                "title": "Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study\n  on Audio Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study\n  on Audio Question Answering"
                },
                "summary": "Recently, reinforcement learning (RL) has been shown to greatly enhance the\nreasoning capabilities of large language models (LLMs), and RL-based approaches\nhave been progressively applied to visual multimodal tasks. However, the audio\nmodality has largely been overlooked in these developments. Thus, we conduct a\nseries of RL explorations in audio understanding and reasoning, specifically\nfocusing on the audio question answering (AQA) task. We leverage the group\nrelative policy optimization (GRPO) algorithm to Qwen2-Audio-7B-Instruct, and\nour experiments demonstrated state-of-the-art performance on the MMAU Test-mini\nbenchmark, achieving an accuracy rate of 64.5%. The main findings in this\ntechnical report are as follows: 1) The GRPO algorithm can be effectively\napplied to large audio language models (LALMs), even when the model has only\n8.2B parameters; 2) With only 38k post-training samples, RL significantly\noutperforms supervised fine-tuning (SFT), indicating that RL-based approaches\ncan be effective without large datasets; 3) The explicit reasoning process has\nnot shown significant benefits for AQA tasks, and how to efficiently utilize\ndeep thinking remains an open question for further research; 4) LALMs still lag\nfar behind humans auditory-language reasoning, suggesting that the RL-based\napproaches warrant further exploration. Our project is available at\nhttps://github.com/xiaomi-research/r1-aqa and\nhttps://huggingface.co/mispeech/r1-aqa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, reinforcement learning (RL) has been shown to greatly enhance the\nreasoning capabilities of large language models (LLMs), and RL-based approaches\nhave been progressively applied to visual multimodal tasks. However, the audio\nmodality has largely been overlooked in these developments. Thus, we conduct a\nseries of RL explorations in audio understanding and reasoning, specifically\nfocusing on the audio question answering (AQA) task. We leverage the group\nrelative policy optimization (GRPO) algorithm to Qwen2-Audio-7B-Instruct, and\nour experiments demonstrated state-of-the-art performance on the MMAU Test-mini\nbenchmark, achieving an accuracy rate of 64.5%. The main findings in this\ntechnical report are as follows: 1) The GRPO algorithm can be effectively\napplied to large audio language models (LALMs), even when the model has only\n8.2B parameters; 2) With only 38k post-training samples, RL significantly\noutperforms supervised fine-tuning (SFT), indicating that RL-based approaches\ncan be effective without large datasets; 3) The explicit reasoning process has\nnot shown significant benefits for AQA tasks, and how to efficiently utilize\ndeep thinking remains an open question for further research; 4) LALMs still lag\nfar behind humans auditory-language reasoning, suggesting that the RL-based\napproaches warrant further exploration. Our project is available at\nhttps://github.com/xiaomi-research/r1-aqa and\nhttps://huggingface.co/mispeech/r1-aqa."
                },
                "authors": [
                    {
                        "name": "Gang Li"
                    },
                    {
                        "name": "Jizhong Liu"
                    },
                    {
                        "name": "Heinrich Dinkel"
                    },
                    {
                        "name": "Yadong Niu"
                    },
                    {
                        "name": "Junbo Zhang"
                    },
                    {
                        "name": "Jian Luan"
                    }
                ],
                "author_detail": {
                    "name": "Jian Luan"
                },
                "author": "Jian Luan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11197v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11197v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19243v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19243v5",
                "updated": "2025-03-17T04:11:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    4,
                    11,
                    1,
                    0,
                    76,
                    0
                ],
                "published": "2024-03-28T08:58:20Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    8,
                    58,
                    20,
                    3,
                    88,
                    0
                ],
                "title": "Efficient Learning With Sine-Activated Low-rank Matrices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Learning With Sine-Activated Low-rank Matrices"
                },
                "summary": "Low-rank decomposition has emerged as a vital tool for enhancing parameter\nefficiency in neural network architectures, gaining traction across diverse\napplications in machine learning. These techniques significantly lower the\nnumber of parameters, striking a balance between compactness and performance.\nHowever, a common challenge has been the compromise between parameter\nefficiency and the accuracy of the model, where reduced parameters often lead\nto diminished accuracy compared to their full-rank counterparts. In this work,\nwe propose a novel theoretical framework that integrates a sinusoidal function\nwithin the low-rank decomposition process. This approach not only preserves the\nbenefits of the parameter efficiency characteristic of low-rank methods but\nalso increases the decomposition's rank, thereby enhancing model performance.\nOur method proves to be a plug in enhancement for existing low-rank models, as\nevidenced by its successful application in Vision Transformers (ViT), Large\nLanguage Models (LLMs), Neural Radiance Fields (NeRF) and 3D shape modelling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-rank decomposition has emerged as a vital tool for enhancing parameter\nefficiency in neural network architectures, gaining traction across diverse\napplications in machine learning. These techniques significantly lower the\nnumber of parameters, striking a balance between compactness and performance.\nHowever, a common challenge has been the compromise between parameter\nefficiency and the accuracy of the model, where reduced parameters often lead\nto diminished accuracy compared to their full-rank counterparts. In this work,\nwe propose a novel theoretical framework that integrates a sinusoidal function\nwithin the low-rank decomposition process. This approach not only preserves the\nbenefits of the parameter efficiency characteristic of low-rank methods but\nalso increases the decomposition's rank, thereby enhancing model performance.\nOur method proves to be a plug in enhancement for existing low-rank models, as\nevidenced by its successful application in Vision Transformers (ViT), Large\nLanguage Models (LLMs), Neural Radiance Fields (NeRF) and 3D shape modelling."
                },
                "authors": [
                    {
                        "name": "Yiping Ji"
                    },
                    {
                        "name": "Hemanth Saratchandran"
                    },
                    {
                        "name": "Cameron Gordon"
                    },
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Simon Lucey"
                    }
                ],
                "author_detail": {
                    "name": "Simon Lucey"
                },
                "author": "Simon Lucey",
                "arxiv_comment": "The first two authors contributed equally. Paper accepted at ICLR\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19243v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19243v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]