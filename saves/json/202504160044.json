[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.10326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10326v1",
                "updated": "2025-04-14T15:34:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    34,
                    26,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:34:26Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    34,
                    26,
                    0,
                    104,
                    0
                ],
                "title": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference"
                },
                "summary": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks."
                },
                "authors": [
                    {
                        "name": "Yangshen Deng"
                    },
                    {
                        "name": "Zhengxin You"
                    },
                    {
                        "name": "Long Xiang"
                    },
                    {
                        "name": "Qilong Li"
                    },
                    {
                        "name": "Peiqi Yuan"
                    },
                    {
                        "name": "Zhaoyang Hong"
                    },
                    {
                        "name": "Yitao Zheng"
                    },
                    {
                        "name": "Wanting Li"
                    },
                    {
                        "name": "Runzhong Li"
                    },
                    {
                        "name": "Haotian Liu"
                    },
                    {
                        "name": "Kyriakos Mouratidis"
                    },
                    {
                        "name": "Man Lung Yiu"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Qiaomu Shen"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Bo Tang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Tang"
                },
                "author": "Bo Tang",
                "arxiv_comment": "14 pages, 12 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.1; H.3.2; H.3.3; H.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10318v1",
                "updated": "2025-04-14T15:27:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    27,
                    32,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:27:32Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    27,
                    32,
                    0,
                    104,
                    0
                ],
                "title": "Shield Bash: Abusing Defensive Coherence State Retrieval to Break Timing\n  Obfuscation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shield Bash: Abusing Defensive Coherence State Retrieval to Break Timing\n  Obfuscation"
                },
                "summary": "Microarchitectural attacks are a significant concern, leading to many\nhardware-based defense proposals. However, different defenses target different\nclasses of attacks, and their impact on each other has not been fully\nconsidered. To raise awareness of this problem, we study an interaction between\ntwo state-of-the art defenses in this paper, timing obfuscations of remote\ncache lines (TORC) and delaying speculative changes to remote cache lines\n(DSRC). TORC mitigates cache-hit based attacks and DSRC mitigates speculative\ncoherence state change attacks.\n  We observe that DSRC enables coherence information to be retrieved into the\nprocessor core, where it is out of the reach of timing obfuscations to protect.\nThis creates an unforeseen consequence that redo operations can be triggered\nwithin the core to detect the presence or absence of remote cache lines, which\nconstitutes a security vulnerability. We demonstrate that a new covert channel\nattack is possible using this vulnerability. We propose two ways to mitigate\nthe attack, whose performance varies depending on an application's cache usage.\nOne way is to never send remote exclusive coherence state (E) information to\nthe core even if it is created. The other way is to never create a remote E\nstate, which is responsible for triggering redos.\n  We demonstrate the timing difference caused by this microarchitectural\ndefense assumption violation using GEM5 simulations. Performance evaluation on\nSPECrate 2017 and PARSEC benchmarks of the two fixes show less than 32\\%\naverage overhead across both sets of benchmarks. The repair which prevented the\ncreation of remote E state had less than 2.8% average overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microarchitectural attacks are a significant concern, leading to many\nhardware-based defense proposals. However, different defenses target different\nclasses of attacks, and their impact on each other has not been fully\nconsidered. To raise awareness of this problem, we study an interaction between\ntwo state-of-the art defenses in this paper, timing obfuscations of remote\ncache lines (TORC) and delaying speculative changes to remote cache lines\n(DSRC). TORC mitigates cache-hit based attacks and DSRC mitigates speculative\ncoherence state change attacks.\n  We observe that DSRC enables coherence information to be retrieved into the\nprocessor core, where it is out of the reach of timing obfuscations to protect.\nThis creates an unforeseen consequence that redo operations can be triggered\nwithin the core to detect the presence or absence of remote cache lines, which\nconstitutes a security vulnerability. We demonstrate that a new covert channel\nattack is possible using this vulnerability. We propose two ways to mitigate\nthe attack, whose performance varies depending on an application's cache usage.\nOne way is to never send remote exclusive coherence state (E) information to\nthe core even if it is created. The other way is to never create a remote E\nstate, which is responsible for triggering redos.\n  We demonstrate the timing difference caused by this microarchitectural\ndefense assumption violation using GEM5 simulations. Performance evaluation on\nSPECrate 2017 and PARSEC benchmarks of the two fixes show less than 32\\%\naverage overhead across both sets of benchmarks. The repair which prevented the\ncreation of remote E state had less than 2.8% average overhead."
                },
                "authors": [
                    {
                        "name": "Kartik Ramkrishnan"
                    },
                    {
                        "name": "Antonia Zhai"
                    },
                    {
                        "name": "Stephen McCamant"
                    },
                    {
                        "name": "Pen Chung Yew"
                    }
                ],
                "author_detail": {
                    "name": "Pen Chung Yew"
                },
                "author": "Pen Chung Yew",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10181v1",
                "updated": "2025-04-14T12:34:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    34,
                    20,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T12:34:20Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    34,
                    20,
                    0,
                    104,
                    0
                ],
                "title": "A New Paradigm in IBR Modeling for Power Flow and Short Circuit Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Paradigm in IBR Modeling for Power Flow and Short Circuit Analysis"
                },
                "summary": "The fault characteristics of inverter-based resources (IBRs) are different\nfrom conventional synchronous generators. The fault response of IBRs is\nnon-linear due to saturation states and mainly determined by fault ride through\n(FRT) strategies of the associated voltage source converter (VSC). This results\nin prohibitively large solution times for power flows considering these short\ncircuit characteristics, especially when the power system states change fast\ndue to uncertainty in IBR generations. To overcome this, a phasor-domain steady\nstate (SS) short circuit (SC) solver for IBR dominated power systems is\nproposed in this paper, and subsequently the developed IBR models are\nincorporated with a novel Jacobian-based Power Flow (PF) solver. In this\nmultiphase PF solver, any power system components can be modeled by considering\ntheir original non-linear or linear mathematical representations. Moreover, two\nnovel FRT strategies are proposed to fully utilize the converter capacity and\nto comply with IEEE-2800 2022 std and German grid code. The results are\ncompared with the Electromagnetic Transient (EMT) simulation on the IEEE 34\ntest network and the 120 kV EPRI benchmark system. The developed IBR sequence\ndomain PF model demonstrates more accurate behavior compared to the classical\nIBR generator model. The error in calculating the short circuit current with\nthe proposed SC solver is less than 3%, while achieving significant speed\nimprovements of three order of magnitudes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fault characteristics of inverter-based resources (IBRs) are different\nfrom conventional synchronous generators. The fault response of IBRs is\nnon-linear due to saturation states and mainly determined by fault ride through\n(FRT) strategies of the associated voltage source converter (VSC). This results\nin prohibitively large solution times for power flows considering these short\ncircuit characteristics, especially when the power system states change fast\ndue to uncertainty in IBR generations. To overcome this, a phasor-domain steady\nstate (SS) short circuit (SC) solver for IBR dominated power systems is\nproposed in this paper, and subsequently the developed IBR models are\nincorporated with a novel Jacobian-based Power Flow (PF) solver. In this\nmultiphase PF solver, any power system components can be modeled by considering\ntheir original non-linear or linear mathematical representations. Moreover, two\nnovel FRT strategies are proposed to fully utilize the converter capacity and\nto comply with IEEE-2800 2022 std and German grid code. The results are\ncompared with the Electromagnetic Transient (EMT) simulation on the IEEE 34\ntest network and the 120 kV EPRI benchmark system. The developed IBR sequence\ndomain PF model demonstrates more accurate behavior compared to the classical\nIBR generator model. The error in calculating the short circuit current with\nthe proposed SC solver is less than 3%, while achieving significant speed\nimprovements of three order of magnitudes."
                },
                "authors": [
                    {
                        "name": "Zahid Javid"
                    },
                    {
                        "name": "Firdous Ul Nazir"
                    },
                    {
                        "name": "Wentao Zhu"
                    },
                    {
                        "name": "Diptargha Chakravorty"
                    },
                    {
                        "name": "Ahmed Aboushady"
                    },
                    {
                        "name": "Mohamed Galeela"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Galeela"
                },
                "author": "Mohamed Galeela",
                "arxiv_comment": "12 Pages, First Revision Submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00601v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00601v3",
                "updated": "2025-04-14T11:20:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    20,
                    56,
                    0,
                    104,
                    0
                ],
                "published": "2024-11-01T14:03:21Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    3,
                    21,
                    4,
                    306,
                    0
                ],
                "title": "Diversity in Network-Friendly Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity in Network-Friendly Recommendations"
                },
                "summary": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms."
                },
                "authors": [
                    {
                        "name": "Evangelia Tzimpimpaki"
                    },
                    {
                        "name": "Thrasyvoulos Spyropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Thrasyvoulos Spyropoulos"
                },
                "author": "Thrasyvoulos Spyropoulos",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00601v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00601v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09984v1",
                "updated": "2025-04-14T08:51:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    8,
                    51,
                    35,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T08:51:35Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    8,
                    51,
                    35,
                    0,
                    104,
                    0
                ],
                "title": "On Precomputation and Caching in Information Retrieval Experiments with\n  Pipeline Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Precomputation and Caching in Information Retrieval Experiments with\n  Pipeline Architectures"
                },
                "summary": "Modern information retrieval systems often rely on multiple components\nexecuted in a pipeline. In a research setting, this can lead to substantial\nredundant computations (e.g., retrieving the same query multiple times for\nevaluating different downstream rerankers). To overcome this, researchers take\ncached \"result\" files as inputs, which represent the output of another\npipeline. However, these result files can be brittle and can cause a disconnect\nbetween the conceptual design of the pipeline and its logical implementation.\nTo overcome both the redundancy problem (when executing complete pipelines) and\nthe disconnect problem (when relying on intermediate result files), we describe\nour recent efforts to improve the caching capabilities in the open-source\nPyTerrier IR platform. We focus on two main directions: (1) automatic implicit\ncaching of common pipeline prefixes when comparing systems and (2) explicit\ncaching of operations through a new extension package, pyterrier-caching. These\napproaches allow for the best of both worlds: pipelines can be fully expressed\nend-to-end, while also avoiding redundant computations between pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern information retrieval systems often rely on multiple components\nexecuted in a pipeline. In a research setting, this can lead to substantial\nredundant computations (e.g., retrieving the same query multiple times for\nevaluating different downstream rerankers). To overcome this, researchers take\ncached \"result\" files as inputs, which represent the output of another\npipeline. However, these result files can be brittle and can cause a disconnect\nbetween the conceptual design of the pipeline and its logical implementation.\nTo overcome both the redundancy problem (when executing complete pipelines) and\nthe disconnect problem (when relying on intermediate result files), we describe\nour recent efforts to improve the caching capabilities in the open-source\nPyTerrier IR platform. We focus on two main directions: (1) automatic implicit\ncaching of common pipeline prefixes when comparing systems and (2) explicit\ncaching of operations through a new extension package, pyterrier-caching. These\napproaches allow for the best of both worlds: pipelines can be fully expressed\nend-to-end, while also avoiding redundant computations between pipelines."
                },
                "authors": [
                    {
                        "name": "Sean MacAvaney"
                    },
                    {
                        "name": "Craig Macdonald"
                    }
                ],
                "author_detail": {
                    "name": "Craig Macdonald"
                },
                "author": "Craig Macdonald",
                "arxiv_comment": "WOWS @ ECIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09952v1",
                "updated": "2025-04-14T07:30:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    7,
                    30,
                    3,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T07:30:03Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    7,
                    30,
                    3,
                    0,
                    104,
                    0
                ],
                "title": "Secrecy and Privacy in Multi-Access Combinatorial Topology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secrecy and Privacy in Multi-Access Combinatorial Topology"
                },
                "summary": "In this work, we consider the multi-access combinatorial topology with $C$\ncaches where each user accesses a unique set of $r$ caches. For this setup, we\nconsider secrecy, where each user should not know anything about the files it\ndid not request, and demand privacy, where each user's demand must be kept\nprivate from other non-colluding users. We propose a scheme satisfying both\nconditions and derive a lower bound based on cut-set arguments. Also, we prove\nthat our scheme is optimal when $r\\geq C-1$, and it is order-optimal when the\ncache memory size $M$ is greater than or equal to a certain threshold for\n$r<C-1$. When $r=1$, in most of the memory region, our scheme achieves the same\nrate as the one given by the secretive scheme for the dedicated cache setup by\nRavindrakumar et al. ( 'Private Coded Caching,' in \\textit{IEEE Transactions on\nInformation Forensics and Security}, 2018), while satisfying both secrecy and\ndemand privacy conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we consider the multi-access combinatorial topology with $C$\ncaches where each user accesses a unique set of $r$ caches. For this setup, we\nconsider secrecy, where each user should not know anything about the files it\ndid not request, and demand privacy, where each user's demand must be kept\nprivate from other non-colluding users. We propose a scheme satisfying both\nconditions and derive a lower bound based on cut-set arguments. Also, we prove\nthat our scheme is optimal when $r\\geq C-1$, and it is order-optimal when the\ncache memory size $M$ is greater than or equal to a certain threshold for\n$r<C-1$. When $r=1$, in most of the memory region, our scheme achieves the same\nrate as the one given by the secretive scheme for the dedicated cache setup by\nRavindrakumar et al. ( 'Private Coded Caching,' in \\textit{IEEE Transactions on\nInformation Forensics and Security}, 2018), while satisfying both secrecy and\ndemand privacy conditions."
                },
                "authors": [
                    {
                        "name": "Mallikharjuna Chinnapadamala"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "11 pages and 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09936v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09936v1",
                "updated": "2025-04-14T06:58:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    6,
                    58,
                    0,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T06:58:00Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    6,
                    58,
                    0,
                    0,
                    104,
                    0
                ],
                "title": "KeepKV: Eliminating Output Perturbation in KV Cache Compression for\n  Efficient LLMs Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeepKV: Eliminating Output Perturbation in KV Cache Compression for\n  Efficient LLMs Inference"
                },
                "summary": "Efficient inference of large language models (LLMs) is hindered by an\never-growing key-value (KV) cache, making KV cache compression a critical\nresearch direction. Traditional methods selectively evict less important KV\ncache entries based on attention scores or position heuristics, which leads to\ninformation loss and hallucinations. Recently, merging-based strategies have\nbeen explored to retain more information by merging KV pairs that would be\ndiscarded; however, these existing approaches inevitably introduce\ninconsistencies in attention distributions before and after merging, causing\noutput perturbation and degraded generation quality. To overcome this\nchallenge, we propose KeepKV, a novel adaptive KV cache merging method designed\nto eliminate output perturbation while preserving performance under strict\nmemory constraints. KeepKV introduces the Electoral Votes mechanism that\nrecords merging history and adaptively adjusts attention scores. Moreover, it\nfurther leverages a novel Zero Inference-Perturbation Merging methods, keeping\nattention consistency and compensating for attention loss resulting from cache\nmerging. KeepKV successfully retains essential context information within a\nsignificantly compressed cache. Extensive experiments on various benchmarks and\nLLM architectures demonstrate that KeepKV substantially reduces memory usage,\nenhances inference throughput by more than 2x and keeps superior generation\nquality even with 10% KV cache budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient inference of large language models (LLMs) is hindered by an\never-growing key-value (KV) cache, making KV cache compression a critical\nresearch direction. Traditional methods selectively evict less important KV\ncache entries based on attention scores or position heuristics, which leads to\ninformation loss and hallucinations. Recently, merging-based strategies have\nbeen explored to retain more information by merging KV pairs that would be\ndiscarded; however, these existing approaches inevitably introduce\ninconsistencies in attention distributions before and after merging, causing\noutput perturbation and degraded generation quality. To overcome this\nchallenge, we propose KeepKV, a novel adaptive KV cache merging method designed\nto eliminate output perturbation while preserving performance under strict\nmemory constraints. KeepKV introduces the Electoral Votes mechanism that\nrecords merging history and adaptively adjusts attention scores. Moreover, it\nfurther leverages a novel Zero Inference-Perturbation Merging methods, keeping\nattention consistency and compensating for attention loss resulting from cache\nmerging. KeepKV successfully retains essential context information within a\nsignificantly compressed cache. Extensive experiments on various benchmarks and\nLLM architectures demonstrate that KeepKV substantially reduces memory usage,\nenhances inference throughput by more than 2x and keeps superior generation\nquality even with 10% KV cache budgets."
                },
                "authors": [
                    {
                        "name": "Yuxuan Tian"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Yebo Peng"
                    },
                    {
                        "name": "Aomufei Yuan"
                    },
                    {
                        "name": "Zhiming Wang"
                    },
                    {
                        "name": "Bairen Yi"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Yong Cui"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09936v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09775v1",
                "updated": "2025-04-14T00:29:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    0,
                    29,
                    49,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T00:29:49Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    0,
                    29,
                    49,
                    0,
                    104,
                    0
                ],
                "title": "Understanding and Optimizing Multi-Stage AI Inference Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Optimizing Multi-Stage AI Inference Pipelines"
                },
                "summary": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads."
                },
                "authors": [
                    {
                        "name": "Abhimanyu Rajeshkumar Bambhaniya"
                    },
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Suvinay Subramanian"
                    },
                    {
                        "name": "Sudarshan Srinivasan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Amir Yazdanbakhsh"
                    },
                    {
                        "name": "Midhilesh Elavazhagan"
                    },
                    {
                        "name": "Madhu Kumar"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "arxiv_comment": "Inference System Design for Multi-Stage AI Inference Pipelines. 13\n  Pages, 15 Figues, 3 Tables. Code can shared at request",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12280v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12280v2",
                "updated": "2025-04-13T14:17:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    17,
                    57,
                    6,
                    103,
                    0
                ],
                "published": "2024-02-19T16:47:04Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    16,
                    47,
                    4,
                    0,
                    50,
                    0
                ],
                "title": "Plato: Plan to Efficiently Decode for Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plato: Plan to Efficiently Decode for Large Language Model Inference"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage tasks, but their inference incurs substantial computational and memory\noverhead. To improve efficiency, parallel decoding methods like\nSkeleton-of-Thought (SoT) decompose prompts into sub-problems for concurrent\nprocessing. However, these methods significantly compromise answer quality by\ntreating semantically linked sub-problems as independent. We propose Plato, a\nnovel approach that co-designs algorithms and systems for semantic-aware\nparallel decoding. Plato leverages LLMs to organize sub-problems into a\ndependency graph based on logical and causal relationships, enabling concurrent\ndecoding of non-dependent nodes while preserving answer coherence and quality.\nTo further enhance efficiency, Plato pipelines planning and node decoding\nstages, implements a global context cache, and carefully structures node\ninference prompts to maximize key-value cache reuse and minimize overhead. Our\nevaluations show that Plato improves throughput by 68% over autoregressive\ndecoding while achieving a 40% net win rate in answer quality. Compared to SoT,\nPlato demonstrates a remarkable 90% quality net-win rate. Ablation studies\nreveal that our pipeline design improves speedup by 29%, while our KV cache\nreuse optimization reduces overhead by 75%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage tasks, but their inference incurs substantial computational and memory\noverhead. To improve efficiency, parallel decoding methods like\nSkeleton-of-Thought (SoT) decompose prompts into sub-problems for concurrent\nprocessing. However, these methods significantly compromise answer quality by\ntreating semantically linked sub-problems as independent. We propose Plato, a\nnovel approach that co-designs algorithms and systems for semantic-aware\nparallel decoding. Plato leverages LLMs to organize sub-problems into a\ndependency graph based on logical and causal relationships, enabling concurrent\ndecoding of non-dependent nodes while preserving answer coherence and quality.\nTo further enhance efficiency, Plato pipelines planning and node decoding\nstages, implements a global context cache, and carefully structures node\ninference prompts to maximize key-value cache reuse and minimize overhead. Our\nevaluations show that Plato improves throughput by 68% over autoregressive\ndecoding while achieving a 40% net win rate in answer quality. Compared to SoT,\nPlato demonstrates a remarkable 90% quality net-win rate. Ablation studies\nreveal that our pipeline design improves speedup by 29%, while our KV cache\nreuse optimization reduces overhead by 75%."
                },
                "authors": [
                    {
                        "name": "Shuowei Jin"
                    },
                    {
                        "name": "Xueshen Liu"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Haizhong Zheng"
                    },
                    {
                        "name": "Qingzhao Zhang"
                    },
                    {
                        "name": "Atul Prakash"
                    },
                    {
                        "name": "Matthew Lentz"
                    },
                    {
                        "name": "Danyang Zhuo"
                    },
                    {
                        "name": "Feng Qian"
                    },
                    {
                        "name": "Z. Morley Mao"
                    }
                ],
                "author_detail": {
                    "name": "Z. Morley Mao"
                },
                "author": "Z. Morley Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12280v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12280v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09590v1",
                "updated": "2025-04-13T14:16:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    16,
                    57,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T14:16:57Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    16,
                    57,
                    6,
                    103,
                    0
                ],
                "title": "Efficient LLM Serving on Hybrid Real-time and Best-effort Requests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Serving on Hybrid Real-time and Best-effort Requests"
                },
                "summary": "Recent breakthroughs in large Language Models (LLMs) have enabled various\ngenerative tasks on a single model. Real-world services (e.g., OpenAI's ChatGPT\n[27]) powered by an LLM often concurrently support latency-critical requests\nfor interactive applications (e.g., question-answering systems, referred to as\nreal-time or RT requests) and throughput-oriented requests for back-of-house\nprocessing (e.g., documents batch processing [28], referred to best-effort or\nBE requests), with complex hybrid inference workloads to the underlying model.\nState-of-the-art (SOTA) LLM serving systems dedicate machines to each type of\nrequest, towards either low inference latency or high serving throughput,\nrespectively. This practice simplifies request scheduling and management but\nsuffers from poor resource utilization. We propose BROS, a hybrid LLM serving\nsystem that aims to collocate RT/BE requests, meeting RT requests' latency\nrequirements while maintaining BE requests' throughput. BROS formulates the\nproblem of hybrid RT/BE request scheduling and solves it with a dynamic\npriority-based algorithm. BROS designs a bidirectional KV cache management\nmechanism, allowing RT requests to share KV memory with BE requests to remove\nthe scheduling restrictions caused by insufficient KV memory and improve\nutilization. Extensive experiments validate that BROS achieves a good trade-off\nwhen serving hybrid RT and BE requests. It significantly reduces the latency of\nRT requests (up to 74.20%), improving their fine-grained service level\nobjectives (SLOs) attainments (up to 36.38x), with negligible throughput\nreduction for BE requests, showing significant advantages over SOTA systems\nlike vLLM and TGI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent breakthroughs in large Language Models (LLMs) have enabled various\ngenerative tasks on a single model. Real-world services (e.g., OpenAI's ChatGPT\n[27]) powered by an LLM often concurrently support latency-critical requests\nfor interactive applications (e.g., question-answering systems, referred to as\nreal-time or RT requests) and throughput-oriented requests for back-of-house\nprocessing (e.g., documents batch processing [28], referred to best-effort or\nBE requests), with complex hybrid inference workloads to the underlying model.\nState-of-the-art (SOTA) LLM serving systems dedicate machines to each type of\nrequest, towards either low inference latency or high serving throughput,\nrespectively. This practice simplifies request scheduling and management but\nsuffers from poor resource utilization. We propose BROS, a hybrid LLM serving\nsystem that aims to collocate RT/BE requests, meeting RT requests' latency\nrequirements while maintaining BE requests' throughput. BROS formulates the\nproblem of hybrid RT/BE request scheduling and solves it with a dynamic\npriority-based algorithm. BROS designs a bidirectional KV cache management\nmechanism, allowing RT requests to share KV memory with BE requests to remove\nthe scheduling restrictions caused by insufficient KV memory and improve\nutilization. Extensive experiments validate that BROS achieves a good trade-off\nwhen serving hybrid RT and BE requests. It significantly reduces the latency of\nRT requests (up to 74.20%), improving their fine-grained service level\nobjectives (SLOs) attainments (up to 36.38x), with negligible throughput\nreduction for BE requests, showing significant advantages over SOTA systems\nlike vLLM and TGI."
                },
                "authors": [
                    {
                        "name": "Wan Borui"
                    },
                    {
                        "name": "Zhao Juntao"
                    },
                    {
                        "name": "Jiang Chenyu"
                    },
                    {
                        "name": "Guo Chuanxiong"
                    },
                    {
                        "name": "Wu Chuan"
                    }
                ],
                "author_detail": {
                    "name": "Wu Chuan"
                },
                "author": "Wu Chuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v5",
                "updated": "2025-04-13T14:02:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    2,
                    47,
                    6,
                    103,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient Prefilling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient Prefilling"
                },
                "summary": "We introduce Block-attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context in an\nauto-regressive manner. Instead, Block-attention divides retrieved documents\ninto discrete blocks, with each block independently calculating key-value (KV)\nstates except for the final block. In RAG scenarios, by defining each passage\nas a block, Block-attention enables us to reuse the KV states of passages that\nhave been seen before, thereby significantly reducing the latency and the\ncomputation overhead during inference. The implementation of Block-attention\ninvolves block segmentation, position re-encoding, and fine-tuning the LLM to\nadapt to the Block-attention mechanism. Experiments on 11 diverse benchmarks,\nincluding RAG, ICL, and general domains, demonstrate that after block\nfine-tuning, the Block-attention model not only achieves performance comparable\nto that of full-attention models, but can also seamlessly switch between the\nblock and full attention modes without any performance loss. Notably,\nBlock-attention significantly reduces the time to first token (TTFT) and\nfloating point operations (FLOPs) to a very low level. It only takes 45 ms to\noutput the first token for an input sequence with a total length of 32K.\nCompared to the full-attention models, the TTFT and corresponding FLOPs are\nreduced by 98.7% and 99.8%, respectively. Additionally, in Appendix A, we\nelaborate on how Block-attention is applied in Game AI scenario and the\nsubstantial potential benefits it entails. We strongly suggest researchers in\nthe gaming field not to overlook this section.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context in an\nauto-regressive manner. Instead, Block-attention divides retrieved documents\ninto discrete blocks, with each block independently calculating key-value (KV)\nstates except for the final block. In RAG scenarios, by defining each passage\nas a block, Block-attention enables us to reuse the KV states of passages that\nhave been seen before, thereby significantly reducing the latency and the\ncomputation overhead during inference. The implementation of Block-attention\ninvolves block segmentation, position re-encoding, and fine-tuning the LLM to\nadapt to the Block-attention mechanism. Experiments on 11 diverse benchmarks,\nincluding RAG, ICL, and general domains, demonstrate that after block\nfine-tuning, the Block-attention model not only achieves performance comparable\nto that of full-attention models, but can also seamlessly switch between the\nblock and full attention modes without any performance loss. Notably,\nBlock-attention significantly reduces the time to first token (TTFT) and\nfloating point operations (FLOPs) to a very low level. It only takes 45 ms to\noutput the first token for an input sequence with a total length of 32K.\nCompared to the full-attention models, the TTFT and corresponding FLOPs are\nreduced by 98.7% and 99.8%, respectively. Additionally, in Appendix A, we\nelaborate on how Block-attention is applied in Game AI scenario and the\nsubstantial potential benefits it entails. We strongly suggest researchers in\nthe gaming field not to overlook this section."
                },
                "authors": [
                    {
                        "name": "Dongyang Ma"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09431v1",
                "updated": "2025-04-13T04:46:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    4,
                    46,
                    2,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T04:46:02Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    4,
                    46,
                    2,
                    6,
                    103,
                    0
                ],
                "title": "Sub-nanosecond in-plane magnetization switching induced by field-like\n  spin-orbit torques from ferromagnets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-nanosecond in-plane magnetization switching induced by field-like\n  spin-orbit torques from ferromagnets"
                },
                "summary": "Spin-orbit torques (SOTs) generated in SOT-material/ferromagnet structures\nare classified as damping-like (DL) and field-like (FL) torques for\ncurrent-driven magnetization switching. It is well known that both DL- and\nFL-SOTs originate from the SOT-material and DL-SOT dominates the current-driven\nswitching process while FL-SOT contributes limitedly, resulting in an\nincubation time (several nanoseconds) during collinear magnetization switching\nwith the spin polarization because of the DL attributes. Here we report a\nFL-SOT originated from the ferromagnet, different from the origin of DL-SOT,\nand demonstrate that it dominates the collinear magnetization switching. We\nshow that the FL-SOT and resultant collinear switching can be modulated, one\norder of magnitude and sign reversal, by controlling the ferromagnet. Because\nof no incubation time and higher charge-to-spin efficiencies in the FL\nswitching, we further show that the switching time can be down to 200 ps with\none order lower critical switching current density compared to DL switching.\nThese results indicate that the FL switching may provide a practical solution\nfor magnetic memory in speed-priority cache applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spin-orbit torques (SOTs) generated in SOT-material/ferromagnet structures\nare classified as damping-like (DL) and field-like (FL) torques for\ncurrent-driven magnetization switching. It is well known that both DL- and\nFL-SOTs originate from the SOT-material and DL-SOT dominates the current-driven\nswitching process while FL-SOT contributes limitedly, resulting in an\nincubation time (several nanoseconds) during collinear magnetization switching\nwith the spin polarization because of the DL attributes. Here we report a\nFL-SOT originated from the ferromagnet, different from the origin of DL-SOT,\nand demonstrate that it dominates the collinear magnetization switching. We\nshow that the FL-SOT and resultant collinear switching can be modulated, one\norder of magnitude and sign reversal, by controlling the ferromagnet. Because\nof no incubation time and higher charge-to-spin efficiencies in the FL\nswitching, we further show that the switching time can be down to 200 ps with\none order lower critical switching current density compared to DL switching.\nThese results indicate that the FL switching may provide a practical solution\nfor magnetic memory in speed-priority cache applications."
                },
                "authors": [
                    {
                        "name": "Hanying Zhang"
                    },
                    {
                        "name": "Ziqian Cui"
                    },
                    {
                        "name": "Baiqing Jiang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "C. Bi"
                    }
                ],
                "author_detail": {
                    "name": "C. Bi"
                },
                "author": "C. Bi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09261v1",
                "updated": "2025-04-12T15:42:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    12,
                    15,
                    42,
                    17,
                    5,
                    102,
                    0
                ],
                "published": "2025-04-12T15:42:17Z",
                "published_parsed": [
                    2025,
                    4,
                    12,
                    15,
                    42,
                    17,
                    5,
                    102,
                    0
                ],
                "title": "Head-Aware KV Cache Compression for Efficient Visual Autoregressive\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Head-Aware KV Cache Compression for Efficient Visual Autoregressive\n  Modeling"
                },
                "summary": "Visual Autoregressive (VAR) models have emerged as a powerful approach for\nmulti-modal content creation, offering high efficiency and quality across\ndiverse multimedia applications. However, they face significant memory\nbottlenecks due to extensive KV cache accumulation during inference. Existing\nKV cache compression techniques for large language models are suboptimal for\nVAR models due to, as we identify in this paper, two distinct categories of\nattention heads in VAR models: Structural Heads, which preserve spatial\ncoherence through diagonal attention patterns, and Contextual Heads, which\nmaintain semantic consistency through vertical attention patterns. These\ndifferences render single-strategy KV compression techniques ineffective for\nVAR models. To address this, we propose HACK, a training-free Head-Aware\nCompression method for KV cache. HACK allocates asymmetric cache budgets and\nemploys pattern-specific compression strategies tailored to the essential\ncharacteristics of each head category. Experiments on Infinity-2B, Infinity-8B,\nand VAR-d30 demonstrate its effectiveness in text-to-image and\nclass-conditional generation tasks. HACK can hack down up to 50\\% and 70\\% of\ncache with minimal performance degradation for VAR-d30 and Infinity-8B,\nrespectively. Even with 70\\% and 90\\% KV cache compression in VAR-d30 and\nInfinity-8B, HACK still maintains high-quality generation while reducing memory\nusage by 44.2\\% and 58.9\\%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) models have emerged as a powerful approach for\nmulti-modal content creation, offering high efficiency and quality across\ndiverse multimedia applications. However, they face significant memory\nbottlenecks due to extensive KV cache accumulation during inference. Existing\nKV cache compression techniques for large language models are suboptimal for\nVAR models due to, as we identify in this paper, two distinct categories of\nattention heads in VAR models: Structural Heads, which preserve spatial\ncoherence through diagonal attention patterns, and Contextual Heads, which\nmaintain semantic consistency through vertical attention patterns. These\ndifferences render single-strategy KV compression techniques ineffective for\nVAR models. To address this, we propose HACK, a training-free Head-Aware\nCompression method for KV cache. HACK allocates asymmetric cache budgets and\nemploys pattern-specific compression strategies tailored to the essential\ncharacteristics of each head category. Experiments on Infinity-2B, Infinity-8B,\nand VAR-d30 demonstrate its effectiveness in text-to-image and\nclass-conditional generation tasks. HACK can hack down up to 50\\% and 70\\% of\ncache with minimal performance degradation for VAR-d30 and Infinity-8B,\nrespectively. Even with 70\\% and 90\\% KV cache compression in VAR-d30 and\nInfinity-8B, HACK still maintains high-quality generation while reducing memory\nusage by 44.2\\% and 58.9\\%, respectively."
                },
                "authors": [
                    {
                        "name": "Ziran Qin"
                    },
                    {
                        "name": "Youru Lv"
                    },
                    {
                        "name": "Mingbao Lin"
                    },
                    {
                        "name": "Zeren Zhang"
                    },
                    {
                        "name": "Danping Zou"
                    },
                    {
                        "name": "Weiyao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Weiyao Lin"
                },
                "author": "Weiyao Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17459v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17459v3",
                "updated": "2025-04-11T12:31:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    12,
                    31,
                    7,
                    4,
                    101,
                    0
                ],
                "published": "2024-11-26T14:23:53Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    23,
                    53,
                    1,
                    331,
                    0
                ],
                "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model"
                },
                "summary": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE."
                },
                "authors": [
                    {
                        "name": "Zongjian Li"
                    },
                    {
                        "name": "Bin Lin"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Liuhan Chen"
                    },
                    {
                        "name": "Xinhua Cheng"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17459v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17459v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08378v1",
                "updated": "2025-04-11T09:26:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T09:26:47Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "title": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash"
                },
                "summary": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods."
                },
                "authors": [
                    {
                        "name": "Fucheng Jia"
                    },
                    {
                        "name": "Zewen Wu"
                    },
                    {
                        "name": "Shiqi Jiang"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Ju Ren"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Ting Cao"
                    }
                ],
                "author_detail": {
                    "name": "Ting Cao"
                },
                "author": "Ting Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08334v1",
                "updated": "2025-04-11T07:59:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    59,
                    6,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T07:59:06Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    59,
                    6,
                    4,
                    101,
                    0
                ],
                "title": "Efficient Architecture for RISC-V Vector Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Architecture for RISC-V Vector Memory Access"
                },
                "summary": "Vector processors frequently suffer from inefficient memory accesses,\nparticularly for strided and segment patterns. While coalescing strided\naccesses is a natural solution, effectively gathering or scattering elements at\nfixed strides remains challenging. Naive approaches rely on high-overhead\ncrossbars that remap any byte between memory and registers, leading to physical\ndesign issues. Segment operations require row-column transpositions, typically\nhandled using either element-level in-place transposition (degrading\nperformance) or large buffer-based bulk transposition (incurring high area\noverhead). In this paper, we present EARTH, a novel vector memory access\narchitecture designed to overcome these challenges through shifting-based\noptimizations. For strided accesses, EARTH integrates specialized shift\nnetworks for gathering and scattering elements. After coalescing multiple\naccesses within the same cache line, data is routed between memory and\nregisters through the shifting network with minimal overhead. For segment\noperations, EARTH employs a shifted register bank enabling direct column-wise\naccess, eliminating dedicated segment buffers while providing high-performance,\nin-place bulk transposition. Implemented on FPGA with Chisel HDL based on an\nopen-source RISC-V vector unit, EARTH enhances performance for strided memory\naccesses, achieving 4x-8x speedups in benchmarks dominated by strided\noperations. Compared to conventional designs, EARTH reduces hardware area by 9%\nand power consumption by 41%, significantly advancing both performance and\nefficiency of vector processors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector processors frequently suffer from inefficient memory accesses,\nparticularly for strided and segment patterns. While coalescing strided\naccesses is a natural solution, effectively gathering or scattering elements at\nfixed strides remains challenging. Naive approaches rely on high-overhead\ncrossbars that remap any byte between memory and registers, leading to physical\ndesign issues. Segment operations require row-column transpositions, typically\nhandled using either element-level in-place transposition (degrading\nperformance) or large buffer-based bulk transposition (incurring high area\noverhead). In this paper, we present EARTH, a novel vector memory access\narchitecture designed to overcome these challenges through shifting-based\noptimizations. For strided accesses, EARTH integrates specialized shift\nnetworks for gathering and scattering elements. After coalescing multiple\naccesses within the same cache line, data is routed between memory and\nregisters through the shifting network with minimal overhead. For segment\noperations, EARTH employs a shifted register bank enabling direct column-wise\naccess, eliminating dedicated segment buffers while providing high-performance,\nin-place bulk transposition. Implemented on FPGA with Chisel HDL based on an\nopen-source RISC-V vector unit, EARTH enhances performance for strided memory\naccesses, achieving 4x-8x speedups in benchmarks dominated by strided\noperations. Compared to conventional designs, EARTH reduces hardware area by 9%\nand power consumption by 41%, significantly advancing both performance and\nefficiency of vector processors."
                },
                "authors": [
                    {
                        "name": "Hongyi Guan"
                    },
                    {
                        "name": "Yichuan Gao"
                    },
                    {
                        "name": "Chenlu Miao"
                    },
                    {
                        "name": "Haoyang Wu"
                    },
                    {
                        "name": "Hang Zhu"
                    },
                    {
                        "name": "Mingfeng Lin"
                    },
                    {
                        "name": "Huayue Liang"
                    }
                ],
                "author_detail": {
                    "name": "Huayue Liang"
                },
                "author": "Huayue Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08204v1",
                "updated": "2025-04-11T02:10:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    10,
                    2,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T02:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    10,
                    2,
                    4,
                    101,
                    0
                ],
                "title": "II-NVM: Enhancing Map Accuracy and Consistency with Normal\n  Vector-Assisted Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "II-NVM: Enhancing Map Accuracy and Consistency with Normal\n  Vector-Assisted Mapping"
                },
                "summary": "SLAM technology plays a crucial role in indoor mapping and localization. A\ncommon challenge in indoor environments is the \"double-sided mapping issue\",\nwhere closely positioned walls, doors, and other surfaces are mistakenly\nidentified as a single plane, significantly hindering map accuracy and\nconsistency. To address this issue this paper introduces a SLAM approach that\nensures accurate mapping using normal vector consistency. We enhance the voxel\nmap structure to store both point cloud data and normal vector information,\nenabling the system to evaluate consistency during nearest neighbor searches\nand map updates. This process distinguishes between the front and back sides of\nsurfaces, preventing incorrect point-to-plane constraints. Moreover, we\nimplement an adaptive radius KD-tree search method that dynamically adjusts the\nsearch radius based on the local density of the point cloud, thereby enhancing\nthe accuracy of normal vector calculations. To further improve realtime\nperformance and storage efficiency, we incorporate a Least Recently Used (LRU)\ncache strategy, which facilitates efficient incremental updates of the voxel\nmap. The code is released as open-source and validated in both simulated\nenvironments and real indoor scenarios. Experimental results demonstrate that\nthis approach effectively resolves the \"double-sided mapping issue\" and\nsignificantly improves mapping precision. Additionally, we have developed and\nopen-sourced the first simulation and real world dataset specifically tailored\nfor the \"double-sided mapping issue\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLAM technology plays a crucial role in indoor mapping and localization. A\ncommon challenge in indoor environments is the \"double-sided mapping issue\",\nwhere closely positioned walls, doors, and other surfaces are mistakenly\nidentified as a single plane, significantly hindering map accuracy and\nconsistency. To address this issue this paper introduces a SLAM approach that\nensures accurate mapping using normal vector consistency. We enhance the voxel\nmap structure to store both point cloud data and normal vector information,\nenabling the system to evaluate consistency during nearest neighbor searches\nand map updates. This process distinguishes between the front and back sides of\nsurfaces, preventing incorrect point-to-plane constraints. Moreover, we\nimplement an adaptive radius KD-tree search method that dynamically adjusts the\nsearch radius based on the local density of the point cloud, thereby enhancing\nthe accuracy of normal vector calculations. To further improve realtime\nperformance and storage efficiency, we incorporate a Least Recently Used (LRU)\ncache strategy, which facilitates efficient incremental updates of the voxel\nmap. The code is released as open-source and validated in both simulated\nenvironments and real indoor scenarios. Experimental results demonstrate that\nthis approach effectively resolves the \"double-sided mapping issue\" and\nsignificantly improves mapping precision. Additionally, we have developed and\nopen-sourced the first simulation and real world dataset specifically tailored\nfor the \"double-sided mapping issue\"."
                },
                "authors": [
                    {
                        "name": "Chengwei Zhao"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Yina Jian"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Linji Wang"
                    },
                    {
                        "name": "Yongxin Ma"
                    },
                    {
                        "name": "Xinglai Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xinglai Jin"
                },
                "author": "Xinglai Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07596v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07596v2",
                "updated": "2025-04-11T02:05:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    5,
                    1,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-10T09:48:56Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    48,
                    56,
                    3,
                    100,
                    0
                ],
                "title": "Boosting Universal LLM Reward Design through Heuristic Reward\n  Observation Space Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Universal LLM Reward Design through Heuristic Reward\n  Observation Space Evolution"
                },
                "summary": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward."
                },
                "authors": [
                    {
                        "name": "Zen Kit Heng"
                    },
                    {
                        "name": "Zimeng Zhao"
                    },
                    {
                        "name": "Tianhao Wu"
                    },
                    {
                        "name": "Yuanfei Wang"
                    },
                    {
                        "name": "Mingdong Wu"
                    },
                    {
                        "name": "Yangang Wang"
                    },
                    {
                        "name": "Hao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Hao Dong"
                },
                "author": "Hao Dong",
                "arxiv_comment": "7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07596v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07596v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07815v1",
                "updated": "2025-04-10T14:52:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    52,
                    3,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T14:52:03Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    52,
                    3,
                    3,
                    100,
                    0
                ],
                "title": "Siren Federate: Bridging document, relational, and graph models for\n  exploratory graph analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Siren Federate: Bridging document, relational, and graph models for\n  exploratory graph analysis"
                },
                "summary": "Investigative workflows require interactive exploratory analysis on large\nheterogeneous knowledge graphs. Current databases show limitations in enabling\nsuch task. This paper discusses the architecture of Siren Federate, a system\nthat efficiently supports exploratory graph analysis by bridging\ndocument-oriented, relational and graph models. Technical contributions include\ndistributed join algorithms, adaptive query planning, query plan folding,\nsemantic caching, and semi-join decomposition for path query. Semi-join\ndecomposition addresses the exponential growth of intermediate results in\npath-based queries. Experiments show that Siren Federate exhibits low latency\nand scales well with the amount of data, the number of users, and the number of\ncomputing nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigative workflows require interactive exploratory analysis on large\nheterogeneous knowledge graphs. Current databases show limitations in enabling\nsuch task. This paper discusses the architecture of Siren Federate, a system\nthat efficiently supports exploratory graph analysis by bridging\ndocument-oriented, relational and graph models. Technical contributions include\ndistributed join algorithms, adaptive query planning, query plan folding,\nsemantic caching, and semi-join decomposition for path query. Semi-join\ndecomposition addresses the exponential growth of intermediate results in\npath-based queries. Experiments show that Siren Federate exhibits low latency\nand scales well with the amount of data, the number of users, and the number of\ncomputing nodes."
                },
                "authors": [
                    {
                        "name": "Georgeta Bordea"
                    },
                    {
                        "name": "Stephane Campinas"
                    },
                    {
                        "name": "Matteo Catena"
                    },
                    {
                        "name": "Renaud Delbru"
                    }
                ],
                "author_detail": {
                    "name": "Renaud Delbru"
                },
                "author": "Renaud Delbru",
                "arxiv_comment": "36 pages, 16 figures, submitted to the ComSIS journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.11; E.1; H.2.4; H.3.3; H.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07642v1",
                "updated": "2025-04-10T10:43:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    43,
                    42,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T10:43:42Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    43,
                    42,
                    3,
                    100,
                    0
                ],
                "title": "Cache-a-lot: Pushing the Limits of Unsatisfiable Core Reuse in SMT-Based\n  Program Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-a-lot: Pushing the Limits of Unsatisfiable Core Reuse in SMT-Based\n  Program Analysis"
                },
                "summary": "Satisfiability Modulo Theories (SMT) solvers are integral to program analysis\ntechniques like concolic and symbolic execution, where they help assess the\nsatisfiability of logical formulae to explore execution paths of the program\nunder test. However, frequent solver invocations are still the main performance\nbottleneck of these techniques. One way to mitigate this challenge is through\noptimizations such as caching and reusing solver results. While current methods\ntypically focus on reusing results from fully equivalent or closely related\nformulas, they often miss broader opportunities for reuse. In this paper, we\npropose a novel approach, Cache-a-lot, that extends the reuse of unsatisfiable\n(unsat) results by systematically considering all possible variable\nsubstitutions. This enables more extensive reuse of results, thereby reducing\nthe number of SMT solver invocations and improving the overall efficiency of\nconcolic and symbolic execution. Our evaluation, conducted against the\nstate-of-the-art Utopia solution using two benchmark sets, shows significant\nimprovements, particularly with more complex formulas. Our method achieves up\nto 74% unsat core reuse, compared to Utopia's 41%, and significant increase in\nthe time savings. These results demonstrate that, despite the additional\ncomputational complexity, the broader reuse of unsat results significantly\nenhances performance, offering valuable advancements for formal verification\nand program analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satisfiability Modulo Theories (SMT) solvers are integral to program analysis\ntechniques like concolic and symbolic execution, where they help assess the\nsatisfiability of logical formulae to explore execution paths of the program\nunder test. However, frequent solver invocations are still the main performance\nbottleneck of these techniques. One way to mitigate this challenge is through\noptimizations such as caching and reusing solver results. While current methods\ntypically focus on reusing results from fully equivalent or closely related\nformulas, they often miss broader opportunities for reuse. In this paper, we\npropose a novel approach, Cache-a-lot, that extends the reuse of unsatisfiable\n(unsat) results by systematically considering all possible variable\nsubstitutions. This enables more extensive reuse of results, thereby reducing\nthe number of SMT solver invocations and improving the overall efficiency of\nconcolic and symbolic execution. Our evaluation, conducted against the\nstate-of-the-art Utopia solution using two benchmark sets, shows significant\nimprovements, particularly with more complex formulas. Our method achieves up\nto 74% unsat core reuse, compared to Utopia's 41%, and significant increase in\nthe time savings. These results demonstrate that, despite the additional\ncomputational complexity, the broader reuse of unsat results significantly\nenhances performance, offering valuable advancements for formal verification\nand program analysis."
                },
                "authors": [
                    {
                        "name": "Rustam Sadykov"
                    },
                    {
                        "name": "Azat Abdullin"
                    },
                    {
                        "name": "Marat Akhin"
                    }
                ],
                "author_detail": {
                    "name": "Marat Akhin"
                },
                "author": "Marat Akhin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07494v1",
                "updated": "2025-04-10T06:51:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    51,
                    23,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T06:51:23Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    51,
                    23,
                    3,
                    100,
                    0
                ],
                "title": "Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM\n  Inference Serving"
                },
                "summary": "Large language model (LLM) inference serving systems are essential to various\nLLM-based applications. As demand for LLM services continues to grow, scaling\nthese systems to handle high request rates while meeting latency Service-Level\nObjectives (SLOs), referred to as effective throughput, becomes critical.\nHowever, existing systems often struggle to improve effective throughput,\nprimarily due to a significant decline in Time To First Token (TTFT) SLO\nattainment. We identify two major causes of this bottleneck: (1)\nmemory-intensive KV cache that limits batch size expansion under GPU memory\nconstraints, and (2) rigid batch composition enforced by the default\nFirst-Come-First-Serve scheduling policy. In this paper, we introduce\nApt-Serve, a scalable framework designed to enhance effective throughput in LLM\ninference serving. Apt-Serve features a new hybrid cache scheme that combines\nKV cache with a memory-efficient hidden cache for reusable input hidden state\nvectors, allowing large batch sizes and improving request concurrency. Based on\nthe hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism\nthat dynamically optimizes batch composition. We formally define the adaptive\nscheduling optimization problem and propose an efficient algorithm with\ntheoretical guarantees. Extensive evaluations on three real-world datasets and\nLLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up\nto 8.8x improvement in effective throughput compared to the state-of-the-art\ninference serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference serving systems are essential to various\nLLM-based applications. As demand for LLM services continues to grow, scaling\nthese systems to handle high request rates while meeting latency Service-Level\nObjectives (SLOs), referred to as effective throughput, becomes critical.\nHowever, existing systems often struggle to improve effective throughput,\nprimarily due to a significant decline in Time To First Token (TTFT) SLO\nattainment. We identify two major causes of this bottleneck: (1)\nmemory-intensive KV cache that limits batch size expansion under GPU memory\nconstraints, and (2) rigid batch composition enforced by the default\nFirst-Come-First-Serve scheduling policy. In this paper, we introduce\nApt-Serve, a scalable framework designed to enhance effective throughput in LLM\ninference serving. Apt-Serve features a new hybrid cache scheme that combines\nKV cache with a memory-efficient hidden cache for reusable input hidden state\nvectors, allowing large batch sizes and improving request concurrency. Based on\nthe hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism\nthat dynamically optimizes batch composition. We formally define the adaptive\nscheduling optimization problem and propose an efficient algorithm with\ntheoretical guarantees. Extensive evaluations on three real-world datasets and\nLLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up\nto 8.8x improvement in effective throughput compared to the state-of-the-art\ninference serving systems."
                },
                "authors": [
                    {
                        "name": "Shihong Gao"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Yanyan Shen"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_doi": "10.1145/3725394",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3725394",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.07494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07479v1",
                "updated": "2025-04-10T06:13:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    13,
                    30,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T06:13:30Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    13,
                    30,
                    3,
                    100,
                    0
                ],
                "title": "UniCAIM: A Unified CAM/CIM Architecture with Static-Dynamic KV Cache\n  Pruning for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniCAIM: A Unified CAM/CIM Architecture with Static-Dynamic KV Cache\n  Pruning for Efficient Long-Context LLM Inference"
                },
                "summary": "Transformer-based large language models (LLMs) have achieved impressive\nperformance in various natural language processing (NLP) applications. However,\nthe high memory and computation cost induced by the KV cache limits the\ninference efficiency, especially for long input sequences. Compute-in-memory\n(CIM)-based accelerators have been proposed for LLM acceleration with KV cache\npruning. However, as existing accelerators only support static pruning with a\nfixed pattern or dynamic pruning with primitive implementations, they suffer\nfrom either high accuracy degradation or low efficiency. In this paper, we\npropose a ferroelectric FET (FeFET)-based unified content addressable memory\n(CAM) and CIM architecture, dubbed as UniCAIM. UniCAIM features simultaneous\nsupport for static and dynamic pruning with 3 computation modes: 1) in the CAM\nmode, UniCAIM enables approximate similarity measurement in O(1) time for\ndynamic KV cache pruning with high energy efficiency; 2) in the charge-domain\nCIM mode, static pruning can be supported based on accumulative similarity\nscore, which is much more flexible compared to fixed patterns; 3) in the\ncurrent-domain mode, exact attention computation can be conducted with a subset\nof selected KV cache. We further propose a novel CAM/CIM cell design that\nleverages the multi-level characteristics of FeFETs for signed multibit storage\nof the KV cache and in-place attention computation. With extensive experimental\nresults, we demonstrate UniCAIM can reduce the area-energy-delay product (AEDP)\nby 8.2-831x over the state-ofthe-art CIM-based LLM accelerators at the circuit\nlevel, along with high accuracy comparable with dense attention at the\napplication level, showing its great potential for efficient long-context LLM\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have achieved impressive\nperformance in various natural language processing (NLP) applications. However,\nthe high memory and computation cost induced by the KV cache limits the\ninference efficiency, especially for long input sequences. Compute-in-memory\n(CIM)-based accelerators have been proposed for LLM acceleration with KV cache\npruning. However, as existing accelerators only support static pruning with a\nfixed pattern or dynamic pruning with primitive implementations, they suffer\nfrom either high accuracy degradation or low efficiency. In this paper, we\npropose a ferroelectric FET (FeFET)-based unified content addressable memory\n(CAM) and CIM architecture, dubbed as UniCAIM. UniCAIM features simultaneous\nsupport for static and dynamic pruning with 3 computation modes: 1) in the CAM\nmode, UniCAIM enables approximate similarity measurement in O(1) time for\ndynamic KV cache pruning with high energy efficiency; 2) in the charge-domain\nCIM mode, static pruning can be supported based on accumulative similarity\nscore, which is much more flexible compared to fixed patterns; 3) in the\ncurrent-domain mode, exact attention computation can be conducted with a subset\nof selected KV cache. We further propose a novel CAM/CIM cell design that\nleverages the multi-level characteristics of FeFETs for signed multibit storage\nof the KV cache and in-place attention computation. With extensive experimental\nresults, we demonstrate UniCAIM can reduce the area-energy-delay product (AEDP)\nby 8.2-831x over the state-ofthe-art CIM-based LLM accelerators at the circuit\nlevel, along with high accuracy comparable with dense attention at the\napplication level, showing its great potential for efficient long-context LLM\ninference."
                },
                "authors": [
                    {
                        "name": "Weikai Xu"
                    },
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Qianqian Huang"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Ru Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ru Huang"
                },
                "author": "Ru Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19379v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19379v3",
                "updated": "2025-04-10T05:06:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    6,
                    29,
                    3,
                    100,
                    0
                ],
                "published": "2024-11-28T21:10:20Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marconi: Prefix Caching for the Era of Hybrid LLMs"
                },
                "summary": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuang Wang"
                    },
                    {
                        "name": "Zhen Jia"
                    },
                    {
                        "name": "Can Karakus"
                    },
                    {
                        "name": "Luca Zancato"
                    },
                    {
                        "name": "Tri Dao"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ravi Netravali"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Netravali"
                },
                "author": "Ravi Netravali",
                "arxiv_comment": "MLSys 2025 camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19379v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19379v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07056v2",
                "updated": "2025-04-09T21:47:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    21,
                    47,
                    31,
                    2,
                    99,
                    0
                ],
                "published": "2025-01-13T04:31:04Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "title": "MAGNUS: Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAGNUS: Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs"
                },
                "summary": "Sparse general matrix-matrix multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines, including Intel MKL, for a variety of different matrices on three\nIntel architectures. For matrices from the SuiteSparse collection, MAGNUS is\nfaster than all the baselines in most cases and is often an order of magnitude\nfaster than at least one baseline. For massive random matrices, MAGNUS scales\nto the largest matrix sizes, while the baselines do not. Furthermore, MAGNUS is\nclose to the optimal bound for these matrices, regardless of the matrix size,\nstructure, and density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse general matrix-matrix multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines, including Intel MKL, for a variety of different matrices on three\nIntel architectures. For matrices from the SuiteSparse collection, MAGNUS is\nfaster than all the baselines in most cases and is often an order of magnitude\nfaster than at least one baseline. For massive random matrices, MAGNUS scales\nto the largest matrix sizes, while the baselines do not. Furthermore, MAGNUS is\nclose to the optimal bound for these matrices, regardless of the matrix size,\nstructure, and density."
                },
                "authors": [
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Jan Laukemann"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Petrini"
                },
                "author": "Fabrizio Petrini",
                "arxiv_comment": "Accepted to ICS25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v3",
                "updated": "2025-04-09T20:51:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    20,
                    51,
                    8,
                    2,
                    99,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "31 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06261v2",
                "updated": "2025-04-09T17:56:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    56,
                    8,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-08T17:59:41Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    41,
                    1,
                    98,
                    0
                ],
                "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention"
                },
                "summary": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning."
                },
                "authors": [
                    {
                        "name": "Gleb Rodionov"
                    },
                    {
                        "name": "Roman Garipov"
                    },
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "George Yakushev"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Anton Sinitsin"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04514v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04514v2",
                "updated": "2025-04-09T14:36:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    36,
                    19,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-06T15:15:07Z",
                "published_parsed": [
                    2025,
                    4,
                    6,
                    15,
                    15,
                    7,
                    6,
                    96,
                    0
                ],
                "title": "Saliency-driven Dynamic Token Pruning for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saliency-driven Dynamic Token Pruning for Large Language Models"
                },
                "summary": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression."
                },
                "authors": [
                    {
                        "name": "Yao Tao"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Yun Wang"
                    },
                    {
                        "name": "Mingjian Zhu"
                    },
                    {
                        "name": "Hailin Hu"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04514v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04514v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06813v1",
                "updated": "2025-04-09T12:07:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    7,
                    26,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T12:07:26Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    7,
                    26,
                    2,
                    99,
                    0
                ],
                "title": "Introducing the Arm-membench Throughput Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introducing the Arm-membench Throughput Benchmark"
                },
                "summary": "Application performance of modern day processors is often limited by the\nmemory subsystem rather than actual compute capabilities. Therefore, data\nthroughput specifications play a key role in modeling application performance\nand determining possible bottlenecks. However, while peak instruction\nthroughputs and bandwidths for local caches are often documented, the\nachievable throughput can also depend on the relation between memory access and\ncompute instructions. In this paper, we present an Arm version of the well\nestablished x86-membench throughput benchmark, which we have adapted to support\nall current SIMD extensions of the Armv8 instruction set architecture. We\ndescribe aspects of the Armv8 ISA that need to be considered in the portable\ndesign of this benchmark. We use the benchmark to analyze the memory subsystem\nat a fine spatial granularity and to unveil microarchitectural details of three\nprocessors: Fujitsu A64FX, Ampere Altra and Cavium ThunderX2. Based on the\nresulting performance information, we show that instruction fetch and decoder\nwidths become a potential bottleneck for cache-bandwidth-sensitive workloads\ndue to the load-store concept of the Arm ISA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application performance of modern day processors is often limited by the\nmemory subsystem rather than actual compute capabilities. Therefore, data\nthroughput specifications play a key role in modeling application performance\nand determining possible bottlenecks. However, while peak instruction\nthroughputs and bandwidths for local caches are often documented, the\nachievable throughput can also depend on the relation between memory access and\ncompute instructions. In this paper, we present an Arm version of the well\nestablished x86-membench throughput benchmark, which we have adapted to support\nall current SIMD extensions of the Armv8 instruction set architecture. We\ndescribe aspects of the Armv8 ISA that need to be considered in the portable\ndesign of this benchmark. We use the benchmark to analyze the memory subsystem\nat a fine spatial granularity and to unveil microarchitectural details of three\nprocessors: Fujitsu A64FX, Ampere Altra and Cavium ThunderX2. Based on the\nresulting performance information, we show that instruction fetch and decoder\nwidths become a potential bottleneck for cache-bandwidth-sensitive workloads\ndue to the load-store concept of the Arm ISA."
                },
                "authors": [
                    {
                        "name": "Cyrill Burth"
                    },
                    {
                        "name": "Markus Velten"
                    },
                    {
                        "name": "Robert Schne"
                    }
                ],
                "author_detail": {
                    "name": "Robert Schne"
                },
                "author": "Robert Schne",
                "arxiv_doi": "10.1007/978-3-031-85697-6_7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-85697-6_7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.06813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 6 figures, published in Parallel Processing and Applied\n  Mathematics (PPAM 2024), see https://doi.org/10.1007/978-3-031-85697-6_7",
                "arxiv_journal_ref": "Parallel Processing and Applied Mathematics. PPAM 2024. Lecture\n  Notes in Computer Science, vol 15579. Springer, Cham",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05821v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05821v2",
                "updated": "2025-04-09T10:23:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    10,
                    23,
                    39,
                    2,
                    99,
                    0
                ],
                "published": "2024-03-09T07:01:44Z",
                "published_parsed": [
                    2024,
                    3,
                    9,
                    7,
                    1,
                    44,
                    5,
                    69,
                    0
                ],
                "title": "Optimizing LLM Queries in Relational Data Analytics Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing LLM Queries in Relational Data Analytics Workloads"
                },
                "summary": "Batch data analytics is a growing application for Large Language Models\n(LLMs). LLMs enable users to perform a wide range of natural language tasks,\nsuch as classification, entity extraction, and translation, over large\ndatasets. However, LLM inference is highly costly and slow: for example, an\nNVIDIA L4 GPU running Llama3-8B can only process 6 KB of text per second,\ntaking about a day to handle 15 GB of data; processing a similar amount of data\ncosts around $10K on OpenAI's GPT-4o. In this paper, we propose novel\ntechniques that can significantly reduce the cost of LLM calls for relational\ndata analytics workloads. Our key contribution is developing efficient\nalgorithms for reordering the rows and the fields within each row of an input\ntable to maximize key-value (KV) cache reuse when performing LLM serving. As\nsuch, our approach can be easily applied to existing analytics systems and\nserving platforms. Our evaluation shows that our solution can yield up to 3.4x\nimprovement in job completion time on a benchmark of diverse LLM-based queries\nusing Llama 3 models. Our solution also achieves a 32% cost savings under\nOpenAI and Anthropic pricing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch data analytics is a growing application for Large Language Models\n(LLMs). LLMs enable users to perform a wide range of natural language tasks,\nsuch as classification, entity extraction, and translation, over large\ndatasets. However, LLM inference is highly costly and slow: for example, an\nNVIDIA L4 GPU running Llama3-8B can only process 6 KB of text per second,\ntaking about a day to handle 15 GB of data; processing a similar amount of data\ncosts around $10K on OpenAI's GPT-4o. In this paper, we propose novel\ntechniques that can significantly reduce the cost of LLM calls for relational\ndata analytics workloads. Our key contribution is developing efficient\nalgorithms for reordering the rows and the fields within each row of an input\ntable to maximize key-value (KV) cache reuse when performing LLM serving. As\nsuch, our approach can be easily applied to existing analytics systems and\nserving platforms. Our evaluation shows that our solution can yield up to 3.4x\nimprovement in job completion time on a benchmark of diverse LLM-based queries\nusing Llama 3 models. Our solution also achieves a 32% cost savings under\nOpenAI and Anthropic pricing models."
                },
                "authors": [
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Asim Biswal"
                    },
                    {
                        "name": "Amog Kamsetty"
                    },
                    {
                        "name": "Audrey Cheng"
                    },
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Liana Patel"
                    },
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Matei Zaharia"
                    }
                ],
                "author_detail": {
                    "name": "Matei Zaharia"
                },
                "author": "Matei Zaharia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05821v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05821v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05591v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05591v3",
                "updated": "2025-04-09T09:09:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    9,
                    9,
                    37,
                    2,
                    99,
                    0
                ],
                "published": "2024-09-09T13:20:31Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    13,
                    20,
                    31,
                    0,
                    253,
                    0
                ],
                "title": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced\n  Retrieval Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced\n  Retrieval Augmentation"
                },
                "summary": "Processing long contexts presents a significant challenge for large language\nmodels (LLMs). While recent advancements allow LLMs to handle much longer\ncontexts than before (e.g., 32K or 128K tokens), it is computationally\nexpensive and can still be insufficient for many applications.\nRetrieval-Augmented Generation (RAG) is considered a promising strategy to\naddress this problem. However, conventional RAG methods face inherent\nlimitations because of two underlying requirements: 1) explicitly stated\nqueries, and 2) well-structured knowledge. These conditions, however, do not\nhold in general long-context processing tasks.\n  In this work, we propose MemoRAG, a novel RAG framework empowered by global\nmemory-augmented retrieval. MemoRAG features a dual-system architecture. First,\nit employs a light but long-range system to create a global memory of the long\ncontext. Once a task is presented, it generates draft answers, providing useful\nclues for the retrieval tools to locate relevant information within the long\ncontext. Second, it leverages an expensive but expressive system, which\ngenerates the final answer based on the retrieved information. Building upon\nthis fundamental framework, we realize the memory module in the form of KV\ncompression, and reinforce its memorization and cluing capacity from the\nGeneration quality's Feedback (a.k.a. RLGF). In our experiments, MemoRAG\nachieves superior performances across a variety of long-context evaluation\ntasks, not only complex scenarios where traditional RAG methods struggle, but\nalso simpler ones where RAG is typically applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts presents a significant challenge for large language\nmodels (LLMs). While recent advancements allow LLMs to handle much longer\ncontexts than before (e.g., 32K or 128K tokens), it is computationally\nexpensive and can still be insufficient for many applications.\nRetrieval-Augmented Generation (RAG) is considered a promising strategy to\naddress this problem. However, conventional RAG methods face inherent\nlimitations because of two underlying requirements: 1) explicitly stated\nqueries, and 2) well-structured knowledge. These conditions, however, do not\nhold in general long-context processing tasks.\n  In this work, we propose MemoRAG, a novel RAG framework empowered by global\nmemory-augmented retrieval. MemoRAG features a dual-system architecture. First,\nit employs a light but long-range system to create a global memory of the long\ncontext. Once a task is presented, it generates draft answers, providing useful\nclues for the retrieval tools to locate relevant information within the long\ncontext. Second, it leverages an expensive but expressive system, which\ngenerates the final answer based on the retrieved information. Building upon\nthis fundamental framework, we realize the memory module in the form of KV\ncompression, and reinforce its memorization and cluing capacity from the\nGeneration quality's Feedback (a.k.a. RLGF). In our experiments, MemoRAG\nachieves superior performances across a variety of long-context evaluation\ntasks, not only complex scenarios where traditional RAG methods struggle, but\nalso simpler ones where RAG is typically applied."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Kelong Mao"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Tiejun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Tiejun Huang"
                },
                "author": "Tiejun Huang",
                "arxiv_comment": "theWebConf 2025. Codes and models are in\n  https://github.com/qhjqhj00/MemoRAG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05591v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05591v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v5",
                "updated": "2025-04-09T07:55:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    55,
                    43,
                    2,
                    99,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14396v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14396v5",
                "updated": "2025-04-09T03:49:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    3,
                    49,
                    16,
                    2,
                    99,
                    0
                ],
                "published": "2023-12-22T02:52:59Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    2,
                    52,
                    59,
                    4,
                    356,
                    0
                ],
                "title": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing"
                },
                "summary": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation."
                },
                "authors": [
                    {
                        "name": "Hongfu Li"
                    },
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Feng Yao"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "This paper was accepted by VLDB2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14396v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14396v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06419v1",
                "updated": "2025-04-08T20:39:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    39,
                    20,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T20:39:20Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    39,
                    20,
                    1,
                    98,
                    0
                ],
                "title": "SPIRe: Boosting LLM Inference Throughput with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPIRe: Boosting LLM Inference Throughput with Speculative Decoding"
                },
                "summary": "Speculative decoding (SD) has been shown to reduce the latency of\nautoregressive decoding (AD) by 2-3x for small batch sizes. However, increasing\nthroughput and therefore reducing the cost per token requires decoding with\nlarge batch sizes. Recent work shows that SD can accelerate decoding with large\nbatch sizes too if the context is sufficiently long and the draft model's KV\ncache is sparse. We introduce SPIRe, a draft model that combines static sparse\nattention, pruned initialization, and feedback memory to increase the modeled\nthroughput of speculative decoding by over 100% compared to speculation with a\nmuch smaller draft model and by over 35% compared to the strong baseline of\nsparse self-speculation. Our approach is particularly effective when context\nlengths vary significantly across requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD) has been shown to reduce the latency of\nautoregressive decoding (AD) by 2-3x for small batch sizes. However, increasing\nthroughput and therefore reducing the cost per token requires decoding with\nlarge batch sizes. Recent work shows that SD can accelerate decoding with large\nbatch sizes too if the context is sufficiently long and the draft model's KV\ncache is sparse. We introduce SPIRe, a draft model that combines static sparse\nattention, pruned initialization, and feedback memory to increase the modeled\nthroughput of speculative decoding by over 100% compared to speculation with a\nmuch smaller draft model and by over 35% compared to the strong baseline of\nsparse self-speculation. Our approach is particularly effective when context\nlengths vary significantly across requests."
                },
                "authors": [
                    {
                        "name": "Sanjit Neelam"
                    },
                    {
                        "name": "Daniel Heinlein"
                    },
                    {
                        "name": "Vaclav Cvicek"
                    },
                    {
                        "name": "Akshay Mishra"
                    },
                    {
                        "name": "Reiner Pope"
                    }
                ],
                "author_detail": {
                    "name": "Reiner Pope"
                },
                "author": "Reiner Pope",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06416v1",
                "updated": "2025-04-08T20:32:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    32,
                    10,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T20:32:10Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    32,
                    10,
                    1,
                    98,
                    0
                ],
                "title": "Unifying Autoregressive and Diffusion-Based Sequence Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Autoregressive and Diffusion-Based Sequence Generation"
                },
                "summary": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation."
                },
                "authors": [
                    {
                        "name": "Nima Fathi"
                    },
                    {
                        "name": "Torsten Scholak"
                    },
                    {
                        "name": "Pierre-Andr Nol"
                    }
                ],
                "author_detail": {
                    "name": "Pierre-Andr Nol"
                },
                "author": "Pierre-Andr Nol",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.17692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.17692v2",
                "updated": "2025-04-08T19:26:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    19,
                    26,
                    41,
                    1,
                    98,
                    0
                ],
                "published": "2024-04-26T20:44:36Z",
                "published_parsed": [
                    2024,
                    4,
                    26,
                    20,
                    44,
                    36,
                    4,
                    117,
                    0
                ],
                "title": "Walking on Spheres and Talking to Neighbors: Variance Reduction for\n  Laplace's Equation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Walking on Spheres and Talking to Neighbors: Variance Reduction for\n  Laplace's Equation"
                },
                "summary": "Walk on Spheres algorithms leverage properties of Brownian Motion to create\nMonte Carlo estimates of solutions to a class of elliptic partial differential\nequations. We propose a new caching strategy which leverages the continuity of\npaths of Brownian Motion. In the case of Laplace's equation with Dirichlet\nboundary conditions, our algorithm has improved asymptotic runtime compared to\nprevious approaches. Until recently, estimates were constructed pointwise and\ndid not use the relationship between solutions at nearby points within a\ndomain. Instead, our results are achieved by passing information from a cache\nof fixed size. We also provide bounds on the performance of our algorithm and\ndemonstrate its performance on example problems of increasing complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Walk on Spheres algorithms leverage properties of Brownian Motion to create\nMonte Carlo estimates of solutions to a class of elliptic partial differential\nequations. We propose a new caching strategy which leverages the continuity of\npaths of Brownian Motion. In the case of Laplace's equation with Dirichlet\nboundary conditions, our algorithm has improved asymptotic runtime compared to\nprevious approaches. Until recently, estimates were constructed pointwise and\ndid not use the relationship between solutions at nearby points within a\ndomain. Instead, our results are achieved by passing information from a cache\nof fixed size. We also provide bounds on the performance of our algorithm and\ndemonstrate its performance on example problems of increasing complexity."
                },
                "authors": [
                    {
                        "name": "Michael Czekanski"
                    },
                    {
                        "name": "Benjamin Faber"
                    },
                    {
                        "name": "Margaret Fairborn"
                    },
                    {
                        "name": "Adelle Wright"
                    },
                    {
                        "name": "David Bindel"
                    }
                ],
                "author_detail": {
                    "name": "David Bindel"
                },
                "author": "David Bindel",
                "arxiv_comment": "21 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.17692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.17692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06067v1",
                "updated": "2025-04-08T14:09:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    9,
                    23,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T14:09:23Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    9,
                    23,
                    1,
                    98,
                    0
                ],
                "title": "GPU-accelerated Evolutionary Many-objective Optimization Using\n  Tensorized NSGA-III",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU-accelerated Evolutionary Many-objective Optimization Using\n  Tensorized NSGA-III"
                },
                "summary": "NSGA-III is one of the most widely adopted algorithms for tackling\nmany-objective optimization problems. However, its CPU-based design severely\nlimits scalability and computational efficiency. To address the limitations, we\npropose {TensorNSGA-III}, a fully tensorized implementation of NSGA-III that\nleverages GPU parallelism for large-scale many-objective optimization. Unlike\nconventional GPU-accelerated evolutionary algorithms that rely on heuristic\napproximations to improve efficiency, TensorNSGA-III maintains the exact\nselection and variation mechanisms of NSGA-III while achieving significant\nacceleration. By reformulating the selection process with tensorized data\nstructures and an optimized caching strategy, our approach effectively\neliminates computational bottlenecks inherent in traditional CPU-based and\nna\\\"ive GPU implementations. Experimental results on widely used numerical\nbenchmarks show that TensorNSGA-III achieves speedups of up to $3629\\times$\nover the CPU version of NSGA-III. Additionally, we validate its effectiveness\nin multiobjective robotic control tasks, where it discovers diverse and\nhigh-quality behavioral solutions. Furthermore, we investigate the critical\nrole of large population sizes in many-objective optimization and demonstrate\nthe scalability of TensorNSGA-III in such scenarios. The source code is\navailable at https://github.com/EMI-Group/evomo",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NSGA-III is one of the most widely adopted algorithms for tackling\nmany-objective optimization problems. However, its CPU-based design severely\nlimits scalability and computational efficiency. To address the limitations, we\npropose {TensorNSGA-III}, a fully tensorized implementation of NSGA-III that\nleverages GPU parallelism for large-scale many-objective optimization. Unlike\nconventional GPU-accelerated evolutionary algorithms that rely on heuristic\napproximations to improve efficiency, TensorNSGA-III maintains the exact\nselection and variation mechanisms of NSGA-III while achieving significant\nacceleration. By reformulating the selection process with tensorized data\nstructures and an optimized caching strategy, our approach effectively\neliminates computational bottlenecks inherent in traditional CPU-based and\nna\\\"ive GPU implementations. Experimental results on widely used numerical\nbenchmarks show that TensorNSGA-III achieves speedups of up to $3629\\times$\nover the CPU version of NSGA-III. Additionally, we validate its effectiveness\nin multiobjective robotic control tasks, where it discovers diverse and\nhigh-quality behavioral solutions. Furthermore, we investigate the critical\nrole of large population sizes in many-objective optimization and demonstrate\nthe scalability of TensorNSGA-III in such scenarios. The source code is\navailable at https://github.com/EMI-Group/evomo"
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Zhenyu Liang"
                    },
                    {
                        "name": "Ran Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Ran Cheng"
                },
                "author": "Ran Cheng",
                "arxiv_comment": "Accepted by IEEE CEC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v2",
                "updated": "2025-04-08T14:05:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    5,
                    12,
                    1,
                    98,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "Unifying KV Cache Compression for Large Language Models with LeanKV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying KV Cache Compression for Large Language Models with LeanKV"
                },
                "summary": "Large language models (LLMs) exhibit exceptional performance but incur\nsignificant serving costs due to their substantial memory requirements, with\nthe key-value (KV) cache being a primary bottleneck. Existing KV cache\ncompression techniques, such as quantization and pruning, apply uniform\ntreatment to both keys and values, and discard unimportant tokens entirely,\noverlooking the fine-grained differences in significance of various components\nwithin the KV cache. To address these limitations, we introduce LeanKV, a\nframework that advances KV cache compression by exploiting three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. At the core of LeanKV\nis an on-GPU memory manager that compacts fragmented free memory list into\ncontiguous regions in parallel, effectively translating sparsity in the KV\ncache into performance gains. We evaluate LeanKV on several mainstream models,\nincluding the recent \"thinking model\". LeanKV is able to compress the KV cache\nby $2.7\\times$ to $5.7\\times$ with near-lossless accuracy on complex workloads\nrequiring sophisticated reasoning and long-generation capabilities, and\nenhances throughput by $1.9\\times$ to $5.4\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit exceptional performance but incur\nsignificant serving costs due to their substantial memory requirements, with\nthe key-value (KV) cache being a primary bottleneck. Existing KV cache\ncompression techniques, such as quantization and pruning, apply uniform\ntreatment to both keys and values, and discard unimportant tokens entirely,\noverlooking the fine-grained differences in significance of various components\nwithin the KV cache. To address these limitations, we introduce LeanKV, a\nframework that advances KV cache compression by exploiting three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. At the core of LeanKV\nis an on-GPU memory manager that compacts fragmented free memory list into\ncontiguous regions in parallel, effectively translating sparsity in the KV\ncache into performance gains. We evaluate LeanKV on several mainstream models,\nincluding the recent \"thinking model\". LeanKV is able to compress the KV cache\nby $2.7\\times$ to $5.7\\times$ with near-lossless accuracy on complex workloads\nrequiring sophisticated reasoning and long-generation capabilities, and\nenhances throughput by $1.9\\times$ to $5.4\\times$."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04760v2",
                "updated": "2025-04-08T12:46:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    12,
                    46,
                    45,
                    1,
                    98,
                    0
                ],
                "published": "2025-02-07T08:48:06Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    48,
                    6,
                    4,
                    38,
                    0
                ],
                "title": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing"
                },
                "summary": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05897v1",
                "updated": "2025-04-08T10:47:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    47,
                    37,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T10:47:37Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    47,
                    37,
                    1,
                    98,
                    0
                ],
                "title": "HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient\n  MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient\n  MoE Inference"
                },
                "summary": "The Mixture of Experts (MoE) architecture has demonstrated significant\nadvantages as it enables to increase the model capacity without a proportional\nincrease in computation. However, the large MoE model size still introduces\nsubstantial memory demands, which usually requires expert offloading on\nresource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU\ninference has been proposed to leverage CPU computation to reduce expert\nloading overhead but faces major challenges: on one hand, the expert activation\npatterns of MoE models are highly unstable, rendering the fixed mapping\nstrategies in existing works inefficient; on the other hand, the hybrid CPU-GPU\nschedule for MoE is inherently complex due to the diverse expert sizes,\nstructures, uneven workload distribution, etc. To address these challenges, in\nthis paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that\nimproves resource utilization through a novel CPU-GPU scheduling and cache\nmanagement system. HybriMoE introduces (i) a dynamic intra-layer scheduling\nstrategy to balance workloads across CPU and GPU, (ii) an impact-driven\ninter-layer prefetching algorithm, and (iii) a score-based caching algorithm to\nmitigate expert activation instability. We implement HybriMoE on top of the\nkTransformers framework and evaluate it on three widely used MoE-based LLMs.\nExperimental results demonstrate that HybriMoE achieves an average speedup of\n1.33$\\times$ in the prefill stage and 1.70$\\times$ in the decode stage compared\nto state-of-the-art hybrid MoE inference framework. Our code is available at:\nhttps://github.com/PKU-SEC-Lab/HybriMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture of Experts (MoE) architecture has demonstrated significant\nadvantages as it enables to increase the model capacity without a proportional\nincrease in computation. However, the large MoE model size still introduces\nsubstantial memory demands, which usually requires expert offloading on\nresource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU\ninference has been proposed to leverage CPU computation to reduce expert\nloading overhead but faces major challenges: on one hand, the expert activation\npatterns of MoE models are highly unstable, rendering the fixed mapping\nstrategies in existing works inefficient; on the other hand, the hybrid CPU-GPU\nschedule for MoE is inherently complex due to the diverse expert sizes,\nstructures, uneven workload distribution, etc. To address these challenges, in\nthis paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that\nimproves resource utilization through a novel CPU-GPU scheduling and cache\nmanagement system. HybriMoE introduces (i) a dynamic intra-layer scheduling\nstrategy to balance workloads across CPU and GPU, (ii) an impact-driven\ninter-layer prefetching algorithm, and (iii) a score-based caching algorithm to\nmitigate expert activation instability. We implement HybriMoE on top of the\nkTransformers framework and evaluate it on three widely used MoE-based LLMs.\nExperimental results demonstrate that HybriMoE achieves an average speedup of\n1.33$\\times$ in the prefill stage and 1.70$\\times$ in the decode stage compared\nto state-of-the-art hybrid MoE inference framework. Our code is available at:\nhttps://github.com/PKU-SEC-Lab/HybriMoE."
                },
                "authors": [
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Yanfan Sun"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "Accepted by DAC 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06319v1",
                "updated": "2025-04-08T09:17:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    17,
                    35,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T09:17:35Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    17,
                    35,
                    1,
                    98,
                    0
                ],
                "title": "Accelerating LLM Inference Throughput via Asynchronous KV Cache\n  Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating LLM Inference Throughput via Asynchronous KV Cache\n  Prefetching"
                },
                "summary": "Large Language Models (LLMs) exhibit pronounced memory-bound characteristics\nduring inference due to High Bandwidth Memory (HBM) bandwidth constraints. In\nthis paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching\nmethod to break through the memory bandwidth bottleneck in LLM inference\nthrough computation-load overlap. By strategically scheduling idle memory\nbandwidth during active computation windows, our method proactively prefetches\nrequired KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for\nsubsequent accesses and effectively hiding HBM access latency within\ncomputational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that\nthe proposed method achieves 2.15x improvement in attention kernel efficiency\nand up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art\nbaseline FlashAttention-3. Notably, our solution maintains orthogonality to\nexisting optimization techniques and can be integrated with current inference\nframeworks, providing a scalable latency-hiding solution for next-generation\nLLM inference engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit pronounced memory-bound characteristics\nduring inference due to High Bandwidth Memory (HBM) bandwidth constraints. In\nthis paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching\nmethod to break through the memory bandwidth bottleneck in LLM inference\nthrough computation-load overlap. By strategically scheduling idle memory\nbandwidth during active computation windows, our method proactively prefetches\nrequired KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for\nsubsequent accesses and effectively hiding HBM access latency within\ncomputational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that\nthe proposed method achieves 2.15x improvement in attention kernel efficiency\nand up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art\nbaseline FlashAttention-3. Notably, our solution maintains orthogonality to\nexisting optimization techniques and can be integrated with current inference\nframeworks, providing a scalable latency-hiding solution for next-generation\nLLM inference engines."
                },
                "authors": [
                    {
                        "name": "Yanhao Dong"
                    },
                    {
                        "name": "Yubo Miao"
                    },
                    {
                        "name": "Weinan Li"
                    },
                    {
                        "name": "Xiao Zheng"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Feng Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Lyu"
                },
                "author": "Feng Lyu",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05807v1",
                "updated": "2025-04-08T08:40:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    40,
                    36,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T08:40:36Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    40,
                    36,
                    1,
                    98,
                    0
                ],
                "title": "Low-Complexity AoI-Optimal Status Update Control with Partial Battery\n  State Information in Energy Harvesting IoT Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Complexity AoI-Optimal Status Update Control with Partial Battery\n  State Information in Energy Harvesting IoT Networks"
                },
                "summary": "For a two-hop IoT system consisting of multiple energy harvesting sensors, a\ncache-enabled edge node, and multiple monitors, the status update control at\nthe edge node, which has partial battery state information (pBSI) of the\nsensors, is formulated as a pBSI problem. The concept of inferred pBSI is\nintroduced to reduce the noiseless single-sensor pBSI problem to a Markov\ndecision process with a moderate state-space size, enabling the optimal policy\nto be obtained through a value iteration algorithm. A lower bound on the\nexpected time-average on-demand age of information performance is established\nfor the general single-sensor status update problem. For the single-sensor pBSI\nproblem, a semi-closed-form policy called the current-next (CN) policy is\nproposed, along with an efficient post-update value iteration algorithm with a\nper-iteration time complexity proportional to the square of the battery\ncapacity. A weighted-update-gain-competition (WUGC) approach is further\nleveraged to extend the CN policy to the multi-sensor case. Numerical results\nin the single-sensor case demonstrate the near-optimal performance of the CN\npolicy across various energy arrival processes. Simulations for an IoT system\nwith $100$ sensors reveal that the WUGC-CN policy outperforms the\nmaximum-age-first policy and the random-scheduling-based CN policy under\nBernoulli energy arrival processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For a two-hop IoT system consisting of multiple energy harvesting sensors, a\ncache-enabled edge node, and multiple monitors, the status update control at\nthe edge node, which has partial battery state information (pBSI) of the\nsensors, is formulated as a pBSI problem. The concept of inferred pBSI is\nintroduced to reduce the noiseless single-sensor pBSI problem to a Markov\ndecision process with a moderate state-space size, enabling the optimal policy\nto be obtained through a value iteration algorithm. A lower bound on the\nexpected time-average on-demand age of information performance is established\nfor the general single-sensor status update problem. For the single-sensor pBSI\nproblem, a semi-closed-form policy called the current-next (CN) policy is\nproposed, along with an efficient post-update value iteration algorithm with a\nper-iteration time complexity proportional to the square of the battery\ncapacity. A weighted-update-gain-competition (WUGC) approach is further\nleveraged to extend the CN policy to the multi-sensor case. Numerical results\nin the single-sensor case demonstrate the near-optimal performance of the CN\npolicy across various energy arrival processes. Simulations for an IoT system\nwith $100$ sensors reveal that the WUGC-CN policy outperforms the\nmaximum-age-first policy and the random-scheduling-based CN policy under\nBernoulli energy arrival processes."
                },
                "authors": [
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Shengtian Yang"
                    },
                    {
                        "name": "Jun Chen"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Anding Wang"
                    }
                ],
                "author_detail": {
                    "name": "Anding Wang"
                },
                "author": "Anding Wang",
                "arxiv_comment": "18 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05718v1",
                "updated": "2025-04-08T06:38:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    6,
                    38,
                    27,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T06:38:27Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    6,
                    38,
                    27,
                    1,
                    98,
                    0
                ],
                "title": "CVA6-VMRT: A Modular Approach Towards Time-Predictable Virtual Memory in\n  a 64-bit Application Class RISC-V Processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CVA6-VMRT: A Modular Approach Towards Time-Predictable Virtual Memory in\n  a 64-bit Application Class RISC-V Processor"
                },
                "summary": "The increasing complexity of autonomous systems has driven a shift to\nintegrated heterogeneous SoCs with real-time and safety demands. Ensuring\ndeterministic WCETs and low-latency for critical tasks requires minimizing\ninterference on shared resources like virtual memory. Existing techniques, such\nas software coloring and memory replication, introduce significant area and\nperformance overhead, especially with virtualized memory where address\ntranslation adds latency uncertainty. To address these limitations, we propose\nCVA6-VMRT, an extension of the open-source RISC-V CVA6 core, adding hardware\nsupport for predictability in virtual memory access with minimal area overhead.\nCVA6-VMRT features dynamically partitioned Translation Look-aside Buffers\n(TLBs) and hybrid L1 cache/scratchpad memory (SPM) functionality. It allows\nfine-grained per-thread control of resources, enabling the operating system to\nmanage TLB replacements, including static overwrites, to ensure single-cycle\naddress translation for critical memory regions. Additionally, CVA6-VMRT\nenables runtime partitioning of data and instruction caches into cache and SPM\nsections, providing low and predictable access times for critical data without\nimpacting other accesses. In a virtualized setting, CVA6-VMRT enhances\nexecution time determinism for critical guests by 94% during interference from\nnon-critical guests, with minimal impact on their average absolute execution\ntime compared to isolated execution of the critical guests only. This\ninterference-aware behaviour is achieved with just a 4% area overhead and no\ntiming penalty compared to the baseline CVA6 core.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of autonomous systems has driven a shift to\nintegrated heterogeneous SoCs with real-time and safety demands. Ensuring\ndeterministic WCETs and low-latency for critical tasks requires minimizing\ninterference on shared resources like virtual memory. Existing techniques, such\nas software coloring and memory replication, introduce significant area and\nperformance overhead, especially with virtualized memory where address\ntranslation adds latency uncertainty. To address these limitations, we propose\nCVA6-VMRT, an extension of the open-source RISC-V CVA6 core, adding hardware\nsupport for predictability in virtual memory access with minimal area overhead.\nCVA6-VMRT features dynamically partitioned Translation Look-aside Buffers\n(TLBs) and hybrid L1 cache/scratchpad memory (SPM) functionality. It allows\nfine-grained per-thread control of resources, enabling the operating system to\nmanage TLB replacements, including static overwrites, to ensure single-cycle\naddress translation for critical memory regions. Additionally, CVA6-VMRT\nenables runtime partitioning of data and instruction caches into cache and SPM\nsections, providing low and predictable access times for critical data without\nimpacting other accesses. In a virtualized setting, CVA6-VMRT enhances\nexecution time determinism for critical guests by 94% during interference from\nnon-critical guests, with minimal impact on their average absolute execution\ntime compared to isolated execution of the critical guests only. This\ninterference-aware behaviour is achieved with just a 4% area overhead and no\ntiming penalty compared to the baseline CVA6 core."
                },
                "authors": [
                    {
                        "name": "Christopher Reinwardt"
                    },
                    {
                        "name": "Robert Balas"
                    },
                    {
                        "name": "Alessandro Ottaviano"
                    },
                    {
                        "name": "Angelo Garofalo"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "8 pages, 7 figures, accepted at the 22nd ACM International Conference\n  on Computing Frontiers 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22926v2",
                "updated": "2025-04-08T05:27:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    5,
                    27,
                    15,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-29T01:06:54Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    1,
                    6,
                    54,
                    5,
                    88,
                    0
                ],
                "title": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction"
                },
                "summary": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware."
                },
                "authors": [
                    {
                        "name": "Zikang Yuan"
                    },
                    {
                        "name": "Ruiye Ming"
                    },
                    {
                        "name": "Chengwei Zhao"
                    },
                    {
                        "name": "Yonghao Tan"
                    },
                    {
                        "name": "Pingcheng Dong"
                    },
                    {
                        "name": "Hongcheng Luo"
                    },
                    {
                        "name": "Yuzhong Jiao"
                    },
                    {
                        "name": "Xin Yang"
                    },
                    {
                        "name": "Kwang-Ting Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Kwang-Ting Cheng"
                },
                "author": "Kwang-Ting Cheng",
                "arxiv_comment": "10 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03661v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03661v2",
                "updated": "2025-04-08T04:34:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    4,
                    34,
                    44,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-12T13:32:50Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    32,
                    50,
                    2,
                    71,
                    0
                ],
                "title": "MILLION: Mastering Long-Context LLM Inference Via Outlier-Immunized KV\n  Product Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MILLION: Mastering Long-Context LLM Inference Via Outlier-Immunized KV\n  Product Quantization"
                },
                "summary": "Large language models (LLMs) are increasingly utilized for complex tasks\nrequiring longer context lengths, with some models supporting up to 128K or 1M\ntokens. This trend, however, presents significant challenges in inference speed\nand memory management. Quantization emerges as a promising approach to address\nthe widening gap between LLM size and memory capacity. However, traditional\nquantization schemes often yield suboptimal compression results for KV caches\ndue to two key factors: i) On-the-fly quantization and de-quantization, causing\nsignificant performance overhead; ii) Prevalence of outliers in KV values,\nchallenging low-bitwidth uniform quantization. To this end, we propose MILLION,\na novel quantization framework achieving low-bitwidth KV cache through product\nquantization. First, we conduct a thorough analysis of KV cache distribution,\nrevealing the limitations of existing quantization schemes. Second, we\nintroduce a non-uniform quantization algorithm based on product quantization,\nwhich efficiently compresses data while preserving accuracy. Third, we develop\na high-performance GPU inference framework with efficient attention kernel and\npipeline design for MILLION that leverages sparse computation and asynchronous\nquantization, significantly enhancing inference speed. Comprehensive evaluation\nresults demonstrate that MILLION can achieve 4 bits quantization with trivial\nperplexity and accuracy loss, and achieve 2.09x end-to-end performance gains at\n32K context length. Code is released at https://github.com/ZongwuWang/MILLION.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly utilized for complex tasks\nrequiring longer context lengths, with some models supporting up to 128K or 1M\ntokens. This trend, however, presents significant challenges in inference speed\nand memory management. Quantization emerges as a promising approach to address\nthe widening gap between LLM size and memory capacity. However, traditional\nquantization schemes often yield suboptimal compression results for KV caches\ndue to two key factors: i) On-the-fly quantization and de-quantization, causing\nsignificant performance overhead; ii) Prevalence of outliers in KV values,\nchallenging low-bitwidth uniform quantization. To this end, we propose MILLION,\na novel quantization framework achieving low-bitwidth KV cache through product\nquantization. First, we conduct a thorough analysis of KV cache distribution,\nrevealing the limitations of existing quantization schemes. Second, we\nintroduce a non-uniform quantization algorithm based on product quantization,\nwhich efficiently compresses data while preserving accuracy. Third, we develop\na high-performance GPU inference framework with efficient attention kernel and\npipeline design for MILLION that leverages sparse computation and asynchronous\nquantization, significantly enhancing inference speed. Comprehensive evaluation\nresults demonstrate that MILLION can achieve 4 bits quantization with trivial\nperplexity and accuracy loss, and achieve 2.09x end-to-end performance gains at\n32K context length. Code is released at https://github.com/ZongwuWang/MILLION."
                },
                "authors": [
                    {
                        "name": "Zongwu Wang"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Fangxin Liu"
                    },
                    {
                        "name": "Yiwei Hu"
                    },
                    {
                        "name": "Qingxiao Sun"
                    },
                    {
                        "name": "Gezi Li"
                    },
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Xuan Wang"
                    },
                    {
                        "name": "Li Jiang"
                    },
                    {
                        "name": "Haibing Guan"
                    }
                ],
                "author_detail": {
                    "name": "Haibing Guan"
                },
                "author": "Haibing Guan",
                "arxiv_comment": "7 pages, 7 figures and 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03661v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03661v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05646v1",
                "updated": "2025-04-08T03:48:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    3,
                    48,
                    43,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T03:48:43Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    3,
                    48,
                    43,
                    1,
                    98,
                    0
                ],
                "title": "Lattice: Learning to Efficiently Compress the Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lattice: Learning to Efficiently Compress the Memory"
                },
                "summary": "Attention mechanisms have revolutionized sequence learning but suffer from\nquadratic computational complexity. This paper introduces Lattice, a novel\nrecurrent neural network (RNN) mechanism that leverages the inherent low-rank\nstructure of K-V matrices to efficiently compress the cache into a fixed number\nof memory slots, achieving sub-quadratic complexity. We formulate this\ncompression as an online optimization problem and derive a dynamic memory\nupdate rule based on a single gradient descent step. The resulting recurrence\nfeatures a state- and input-dependent gating mechanism, offering an\ninterpretable memory update process. The core innovation is the orthogonal\nupdate: each memory slot is updated exclusively with information orthogonal to\nits current state hence incorporation of only novel, non-redundant data, which\nminimizes the interference with previously stored information. The experimental\nresults show that Lattice achieves the best perplexity compared to all\nbaselines across diverse context lengths, with performance improvement becoming\nmore pronounced as the context length increases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms have revolutionized sequence learning but suffer from\nquadratic computational complexity. This paper introduces Lattice, a novel\nrecurrent neural network (RNN) mechanism that leverages the inherent low-rank\nstructure of K-V matrices to efficiently compress the cache into a fixed number\nof memory slots, achieving sub-quadratic complexity. We formulate this\ncompression as an online optimization problem and derive a dynamic memory\nupdate rule based on a single gradient descent step. The resulting recurrence\nfeatures a state- and input-dependent gating mechanism, offering an\ninterpretable memory update process. The core innovation is the orthogonal\nupdate: each memory slot is updated exclusively with information orthogonal to\nits current state hence incorporation of only novel, non-redundant data, which\nminimizes the interference with previously stored information. The experimental\nresults show that Lattice achieves the best perplexity compared to all\nbaselines across diverse context lengths, with performance improvement becoming\nmore pronounced as the context length increases."
                },
                "authors": [
                    {
                        "name": "Mahdi Karami"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    }
                ],
                "author_detail": {
                    "name": "Vahab Mirrokni"
                },
                "author": "Vahab Mirrokni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02533v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02533v3",
                "updated": "2025-04-07T22:48:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    22,
                    48,
                    33,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-03T12:36:01Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    12,
                    36,
                    1,
                    3,
                    93,
                    0
                ],
                "title": "ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions"
                },
                "summary": "Modern data-driven applications expose limitations of von Neumann\narchitectures - extensive data movement, low throughput, and poor energy\nefficiency. Accelerators improve performance but lack flexibility and require\ndata transfers. Existing compute in- and near-memory solutions mitigate these\nissues but face usability challenges due to data placement constraints. We\npropose a novel cache architecture that doubles as a tightly-coupled\ncompute-near-memory coprocessor. Our RISC-V cache controller executes custom\ninstructions from the host CPU using vector operations dispatched to\nnear-memory vector processing units within the cache memory subsystem. This\narchitecture abstracts memory synchronization and data mapping from application\nsoftware while offering software-based Instruction Set Architecture\nextensibility. Our implementation shows $30\\times$ to $84\\times$ performance\nimprovement when operating on 8-bit data over the same system with a\ntraditional cache when executing a worst-case 32-bit CNN workload, with only\n$41.3\\%$ area overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern data-driven applications expose limitations of von Neumann\narchitectures - extensive data movement, low throughput, and poor energy\nefficiency. Accelerators improve performance but lack flexibility and require\ndata transfers. Existing compute in- and near-memory solutions mitigate these\nissues but face usability challenges due to data placement constraints. We\npropose a novel cache architecture that doubles as a tightly-coupled\ncompute-near-memory coprocessor. Our RISC-V cache controller executes custom\ninstructions from the host CPU using vector operations dispatched to\nnear-memory vector processing units within the cache memory subsystem. This\narchitecture abstracts memory synchronization and data mapping from application\nsoftware while offering software-based Instruction Set Architecture\nextensibility. Our implementation shows $30\\times$ to $84\\times$ performance\nimprovement when operating on 8-bit data over the same system with a\ntraditional cache when executing a worst-case 32-bit CNN workload, with only\n$41.3\\%$ area overhead."
                },
                "authors": [
                    {
                        "name": "Vincenzo Petrolo"
                    },
                    {
                        "name": "Flavia Guella"
                    },
                    {
                        "name": "Michele Caon"
                    },
                    {
                        "name": "Pasquale Davide Schiavone"
                    },
                    {
                        "name": "Guido Masera"
                    },
                    {
                        "name": "Maurizio Martina"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Martina"
                },
                "author": "Maurizio Martina",
                "arxiv_comment": "6 pages, 4 figures, accepted at the Design Automation Conference\n  (DAC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02533v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02533v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07467v2",
                "updated": "2025-04-07T20:52:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    20,
                    52,
                    4,
                    0,
                    97,
                    0
                ],
                "published": "2024-06-11T17:13:18Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    17,
                    13,
                    18,
                    1,
                    163,
                    0
                ],
                "title": "LLM meets ML: Data-efficient Anomaly Detection on Unseen Unstable Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM meets ML: Data-efficient Anomaly Detection on Unseen Unstable Logs"
                },
                "summary": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for ULAD, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\nin F1 score while using 62.87 percentage points less labeled data. When trained\non the same amount of data as the baselines, FlexLog achieves up to a 13\npercentage points increase in F1 score on ADFA-U across varying training\ndataset sizes. Additionally, FlexLog maintains inference time under one second\nper log sequence, making it suitable for most applications except\nlatency-sensitive systems. Further analysis reveals the positive impact of\nFlexLog's key components: cache, RAG and ensemble learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for ULAD, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\nin F1 score while using 62.87 percentage points less labeled data. When trained\non the same amount of data as the baselines, FlexLog achieves up to a 13\npercentage points increase in F1 score on ADFA-U across varying training\ndataset sizes. Additionally, FlexLog maintains inference time under one second\nper log sequence, making it suitable for most applications except\nlatency-sensitive systems. Further analysis reveals the positive impact of\nFlexLog's key components: cache, RAG and ensemble learning."
                },
                "authors": [
                    {
                        "name": "Fatemeh Hadadi"
                    },
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Domenico Bianculli"
                    },
                    {
                        "name": "Lionel Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Briand"
                },
                "author": "Lionel Briand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05097v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05097v1",
                "updated": "2025-04-07T14:04:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    4,
                    30,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T14:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    4,
                    30,
                    0,
                    97,
                    0
                ],
                "title": "State Tuning: State-based Test-Time Scaling on RWKV-7",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State Tuning: State-based Test-Time Scaling on RWKV-7"
                },
                "summary": "Test-time scaling has emerged as a prominent research direction in machine\nlearning, enabling models to enhance their expressive capabilities during\ninference.Transformers, renowned for striking a delicate balance between\nefficiency and expressiveness, have benefited from test-time scaling techniques\nthat leverage an expanding key-value (KV) cache to significantly improve\nperformance.In this paper, we introduce a novel state-based approach to\ntest-time scaling, which we term state tuning, tailored to the RNN-based RWKV-7\nmodel.By exploiting the unique strengths of RWKV-7, our method achieves\nstate-of-the-art performance on the target task without altering the model's\npre-trained weights. Our approach centers on three key innovations. First, we\ndevelop an observer framework that allows a smaller model to replicate and\nlearn the state dynamics of the RWKV-7 model. Second, we employ a kernel method\nto dynamically upscale the state size, enhancing the model's capacity to\ncapture intricate patterns. Third, we integrate Decorrelated Backpropagation\n(DBP) to optimize the upscaled state matrix, thereby improving convergence and\nexpressivity. By tuning only the state matrix, we demonstrate that a smaller\nmodel can outperform larger models on the given task. This method preserves the\nefficiency of the original RWKV-7 architecture while harnessing the power of\ntest-time scaling to deliver superior results. Our findings underscore the\npotential of state tuning as an effective strategy for advancing model\nperformance in resource-constrained settings. Our code is\nhttps://github.com/TorchRWKV/flash-linear-attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling has emerged as a prominent research direction in machine\nlearning, enabling models to enhance their expressive capabilities during\ninference.Transformers, renowned for striking a delicate balance between\nefficiency and expressiveness, have benefited from test-time scaling techniques\nthat leverage an expanding key-value (KV) cache to significantly improve\nperformance.In this paper, we introduce a novel state-based approach to\ntest-time scaling, which we term state tuning, tailored to the RNN-based RWKV-7\nmodel.By exploiting the unique strengths of RWKV-7, our method achieves\nstate-of-the-art performance on the target task without altering the model's\npre-trained weights. Our approach centers on three key innovations. First, we\ndevelop an observer framework that allows a smaller model to replicate and\nlearn the state dynamics of the RWKV-7 model. Second, we employ a kernel method\nto dynamically upscale the state size, enhancing the model's capacity to\ncapture intricate patterns. Third, we integrate Decorrelated Backpropagation\n(DBP) to optimize the upscaled state matrix, thereby improving convergence and\nexpressivity. By tuning only the state matrix, we demonstrate that a smaller\nmodel can outperform larger models on the given task. This method preserves the\nefficiency of the original RWKV-7 architecture while harnessing the power of\ntest-time scaling to deliver superior results. Our findings underscore the\npotential of state tuning as an effective strategy for advancing model\nperformance in resource-constrained settings. Our code is\nhttps://github.com/TorchRWKV/flash-linear-attention."
                },
                "authors": [
                    {
                        "name": "Liu Xiao"
                    },
                    {
                        "name": "Li Zhiyuan"
                    },
                    {
                        "name": "Lin Yueyu"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yueyu"
                },
                "author": "Lin Yueyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05097v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05097v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04823v1",
                "updated": "2025-04-07T08:22:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    22,
                    45,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T08:22:45Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    22,
                    45,
                    0,
                    97,
                    0
                ],
                "title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models"
                },
                "summary": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this study, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\nactivation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes will be open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this study, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\nactivation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes will be open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models."
                },
                "authors": [
                    {
                        "name": "Ruikang Liu"
                    },
                    {
                        "name": "Yuxuan Sun"
                    },
                    {
                        "name": "Manyi Zhang"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Tiezheng Yu"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Lu Hou"
                    }
                ],
                "author_detail": {
                    "name": "Lu Hou"
                },
                "author": "Lu Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00414v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00414v3",
                "updated": "2025-04-07T06:27:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    6,
                    27,
                    48,
                    0,
                    97,
                    0
                ],
                "published": "2024-10-01T05:46:22Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    5,
                    46,
                    22,
                    1,
                    275,
                    0
                ],
                "title": "Semantic Parsing with Candidate Expressions for Knowledge Base Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Parsing with Candidate Expressions for Knowledge Base Question\n  Answering"
                },
                "summary": "Semantic parsers convert natural language to logical forms, which can be\nevaluated on knowledge bases (KBs) to produce denotations. Recent semantic\nparsers have been developed with sequence-to-sequence (seq2seq) pre-trained\nlanguage models (PLMs) or large language models, where the models treat logical\nforms as sequences of tokens. For syntactic and semantic validity, the semantic\nparsers use grammars that enable constrained decoding. However, the grammars\nlack the ability to utilize large information of KBs, although logical forms\ncontain representations of KB elements, such as entities or relations. In this\nwork, we propose a grammar augmented with candidate expressions for semantic\nparsing on a large KB with a seq2seq PLM. The grammar defines actions as\nproduction rules, and our semantic parser predicts actions during inference\nunder the constraints by types and candidate expressions. We apply the grammar\nto knowledge base question answering, where the constraints by candidate\nexpressions assist a semantic parser to generate valid KB elements. We also\nintroduce two special rules, sub-type inference and union types, and a mask\ncaching algorithm. In particular, sub-type inference and the mask caching\nalgorithm greatly increase the decoding speed of our semantic parser. We\nexperimented on two benchmarks, KQA Pro and Overnight, where the constraints by\ncandidate expressions increased the accuracy of our semantic parser, whether it\nwas trained with strong supervision or weak supervision. In addition, our\nsemantic parser had a fast decoding speed in the experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic parsers convert natural language to logical forms, which can be\nevaluated on knowledge bases (KBs) to produce denotations. Recent semantic\nparsers have been developed with sequence-to-sequence (seq2seq) pre-trained\nlanguage models (PLMs) or large language models, where the models treat logical\nforms as sequences of tokens. For syntactic and semantic validity, the semantic\nparsers use grammars that enable constrained decoding. However, the grammars\nlack the ability to utilize large information of KBs, although logical forms\ncontain representations of KB elements, such as entities or relations. In this\nwork, we propose a grammar augmented with candidate expressions for semantic\nparsing on a large KB with a seq2seq PLM. The grammar defines actions as\nproduction rules, and our semantic parser predicts actions during inference\nunder the constraints by types and candidate expressions. We apply the grammar\nto knowledge base question answering, where the constraints by candidate\nexpressions assist a semantic parser to generate valid KB elements. We also\nintroduce two special rules, sub-type inference and union types, and a mask\ncaching algorithm. In particular, sub-type inference and the mask caching\nalgorithm greatly increase the decoding speed of our semantic parser. We\nexperimented on two benchmarks, KQA Pro and Overnight, where the constraints by\ncandidate expressions increased the accuracy of our semantic parser, whether it\nwas trained with strong supervision or weak supervision. In addition, our\nsemantic parser had a fast decoding speed in the experiments."
                },
                "authors": [
                    {
                        "name": "Daehwan Nam"
                    },
                    {
                        "name": "Gary Geunbae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Gary Geunbae Lee"
                },
                "author": "Gary Geunbae Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00414v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00414v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04704v1",
                "updated": "2025-04-07T03:22:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    22,
                    15,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T03:22:15Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    22,
                    15,
                    0,
                    97,
                    0
                ],
                "title": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important"
                },
                "summary": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modifiation of the inference infrastructure and\nsignificant computation overhead. Base on the fact that the Large Lanuage\nmodels are autoregresssive models, we propose {\\it LagKV}, a KV allocation\nstrategy only relying on straight forward comparison among KV themself. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on LongBench and PasskeyRetrieval show that, our\napproach achieves nearly zero loss when the ratio is $2\\times$ and $\\approx\n90\\%$ of the original model performance for $8\\times$. Especially in the\n64-digit passkey retrieval task, our mehod outperforms the attention weight\nbased method $H_2O$ over $60\\%$ with same compression ratios. Our code is\navailable at \\url{https://github.com/AI-Lab-China-Merchants-Bank/LagKV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modifiation of the inference infrastructure and\nsignificant computation overhead. Base on the fact that the Large Lanuage\nmodels are autoregresssive models, we propose {\\it LagKV}, a KV allocation\nstrategy only relying on straight forward comparison among KV themself. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on LongBench and PasskeyRetrieval show that, our\napproach achieves nearly zero loss when the ratio is $2\\times$ and $\\approx\n90\\%$ of the original model performance for $8\\times$. Especially in the\n64-digit passkey retrieval task, our mehod outperforms the attention weight\nbased method $H_2O$ over $60\\%$ with same compression ratios. Our code is\navailable at \\url{https://github.com/AI-Lab-China-Merchants-Bank/LagKV}."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "JiaMing Zhang"
                    },
                    {
                        "name": "Xiong Li"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23367v2",
                "updated": "2025-04-07T01:35:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    1,
                    35,
                    39,
                    0,
                    97,
                    0
                ],
                "published": "2025-03-30T08:51:19Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    51,
                    19,
                    6,
                    89,
                    0
                ],
                "title": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning"
                },
                "summary": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR."
                },
                "authors": [
                    {
                        "name": "Hang Guo"
                    },
                    {
                        "name": "Yawei Li"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Jiangshan Wang"
                    },
                    {
                        "name": "Tao Dai"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10714v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10714v2",
                "updated": "2025-04-06T12:20:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    6,
                    12,
                    20,
                    25,
                    6,
                    96,
                    0
                ],
                "published": "2025-03-13T03:36:03Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    36,
                    3,
                    3,
                    72,
                    0
                ],
                "title": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient\n  Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient\n  Long-Context LLMs"
                },
                "summary": "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Pei Liu"
                    },
                    {
                        "name": "Guoming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Guoming Tang"
                },
                "author": "Guoming Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10714v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10714v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04005v1",
                "updated": "2025-04-05T00:59:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    5,
                    0,
                    59,
                    52,
                    5,
                    95,
                    0
                ],
                "published": "2025-04-05T00:59:52Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    0,
                    59,
                    52,
                    5,
                    95,
                    0
                ],
                "title": "Learning Cache Coherence Traffic for NoC Routing Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Cache Coherence Traffic for NoC Routing Design"
                },
                "summary": "The rapid growth of multi-core systems highlights the need for efficient\nNetwork-on-Chip (NoC) design to ensure seamless communication. Cache coherence,\nessential for data consistency, substantially reduces task computation time by\nenabling data sharing among caches. As a result, routing serves two roles:\nfacilitating data sharing (influenced by topology) and managing NoC-level\ncommunication. However, cache coherence is often overlooked in routing, causing\nmismatches between design expectations and evaluation outcomes. Two main\nchallenges are the lack of specialized tools to assess cache coherence's impact\nand the neglect of topology selection in routing. In this work, we propose a\ncache coherence-aware routing approach with integrated topology selection,\nguided by our Cache Coherence Traffic Analyzer (CCTA). Our method achieves up\nto 10.52% lower packet latency, 55.51% faster execution time, and 49.02% total\nenergy savings, underscoring the critical role of cache coherence in NoC design\nand enabling effective co-design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of multi-core systems highlights the need for efficient\nNetwork-on-Chip (NoC) design to ensure seamless communication. Cache coherence,\nessential for data consistency, substantially reduces task computation time by\nenabling data sharing among caches. As a result, routing serves two roles:\nfacilitating data sharing (influenced by topology) and managing NoC-level\ncommunication. However, cache coherence is often overlooked in routing, causing\nmismatches between design expectations and evaluation outcomes. Two main\nchallenges are the lack of specialized tools to assess cache coherence's impact\nand the neglect of topology selection in routing. In this work, we propose a\ncache coherence-aware routing approach with integrated topology selection,\nguided by our Cache Coherence Traffic Analyzer (CCTA). Our method achieves up\nto 10.52% lower packet latency, 55.51% faster execution time, and 49.02% total\nenergy savings, underscoring the critical role of cache coherence in NoC design\nand enabling effective co-design."
                },
                "authors": [
                    {
                        "name": "Guochu Xiong"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Weichen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weichen Liu"
                },
                "author": "Weichen Liu",
                "arxiv_comment": "7 pages, 14 figures. Preprint version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03632v1",
                "updated": "2025-04-04T17:56:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    56,
                    44,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T17:56:44Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    56,
                    44,
                    4,
                    94,
                    0
                ],
                "title": "Performance Analysis of HPC applications on the Aurora Supercomputer:\n  Exploring the Impact of HBM-Enabled Intel Xeon Max CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Analysis of HPC applications on the Aurora Supercomputer:\n  Exploring the Impact of HBM-Enabled Intel Xeon Max CPUs"
                },
                "summary": "The Aurora supercomputer is an exascale-class system designed to tackle some\nof the most demanding computational workloads. Equipped with both High\nBandwidth Memory (HBM) and DDR memory, it provides unique trade-offs in\nperformance, latency, and capacity. This paper presents a comprehensive\nanalysis of the memory systems on the Aurora supercomputer, with a focus on\nevaluating the trade-offs between HBM and DDR memory systems. We explore how\ndifferent memory configurations, including memory modes (Flat and Cache) and\nclustering modes (Quad and SNC4), influence key system performance metrics such\nas memory bandwidth, latency, CPU-GPU PCIe bandwidth, and MPI communication\nbandwidth. Additionally, we examine the performance of three representative HPC\napplications -- HACC, QMCPACK, and BFS -- each illustrating the impact of\nmemory configurations on performance. By using microbenchmarks and\napplication-level analysis, we provide insights into how to select the optimal\nmemory system and configuration to maximize performance based on the\napplication characteristics. The findings presented in this paper offer\nguidance for users of the Aurora system and similar exascale systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Aurora supercomputer is an exascale-class system designed to tackle some\nof the most demanding computational workloads. Equipped with both High\nBandwidth Memory (HBM) and DDR memory, it provides unique trade-offs in\nperformance, latency, and capacity. This paper presents a comprehensive\nanalysis of the memory systems on the Aurora supercomputer, with a focus on\nevaluating the trade-offs between HBM and DDR memory systems. We explore how\ndifferent memory configurations, including memory modes (Flat and Cache) and\nclustering modes (Quad and SNC4), influence key system performance metrics such\nas memory bandwidth, latency, CPU-GPU PCIe bandwidth, and MPI communication\nbandwidth. Additionally, we examine the performance of three representative HPC\napplications -- HACC, QMCPACK, and BFS -- each illustrating the impact of\nmemory configurations on performance. By using microbenchmarks and\napplication-level analysis, we provide insights into how to select the optimal\nmemory system and configuration to maximize performance based on the\napplication characteristics. The findings presented in this paper offer\nguidance for users of the Aurora system and similar exascale systems."
                },
                "authors": [
                    {
                        "name": "Huda Ibeid"
                    },
                    {
                        "name": "Vikram Narayana"
                    },
                    {
                        "name": "Jeongnim Kim"
                    },
                    {
                        "name": "Anthony Nguyen"
                    },
                    {
                        "name": "Vitali Morozov"
                    },
                    {
                        "name": "Ye Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ye Luo"
                },
                "author": "Ye Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v2",
                "updated": "2025-04-04T16:51:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    51,
                    15,
                    4,
                    94,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "Adaptive Semantic Prompt Caching with VectorQ",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Semantic Prompt Caching with VectorQ"
                },
                "summary": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different embeddings.\nWe propose VectorQ, an online framework with a threshold convergence guarantee\nto learn embedding-specific threshold regions that adapt to the uncertainty of\nan embedding. Through evaluations on a combination of three diverse datasets,\nwe show that VectorQ consistently outperforms state-of-the-art systems across\nall static thresholds, achieving up to 26x increases in cache hit rate and\nerror rate reductions up to 74%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different embeddings.\nWe propose VectorQ, an online framework with a threshold convergence guarantee\nto learn embedding-specific threshold regions that adapt to the uncertainty of\nan embedding. Through evaluations on a combination of three diverse datasets,\nwe show that VectorQ consistently outperforms state-of-the-art systems across\nall static thresholds, achieving up to 26x increases in cache hit rate and\nerror rate reductions up to 74%."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.02073v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.02073v2",
                "updated": "2025-04-04T15:30:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    15,
                    30,
                    20,
                    4,
                    94,
                    0
                ],
                "published": "2023-07-05T07:30:53Z",
                "published_parsed": [
                    2023,
                    7,
                    5,
                    7,
                    30,
                    53,
                    2,
                    186,
                    0
                ],
                "title": "Performance Modeling of Data Storage Systems using Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Modeling of Data Storage Systems using Generative Models"
                },
                "summary": "High-precision modeling of systems is one of the main areas of industrial\ndata analysis. Models of systems, their digital twins, are used to predict\ntheir behavior under various conditions. We have developed several models of a\nstorage system using machine learning-based generative models. The system\nconsists of several components: hard disk drive (HDD) and solid-state drive\n(SSD) storage pools with different RAID schemes and cache. Each storage\ncomponent is represented by a probabilistic model that describes the\nprobability distribution of the component performance in terms of IOPS and\nlatency, depending on their configuration and external data load parameters.\nThe results of the experiments demonstrate the errors of 4-10 % for IOPS and\n3-16 % for latency predictions depending on the components and models of the\nsystem. The predictions show up to 0.99 Pearson correlation with Little's law,\nwhich can be used for unsupervised reliability checks of the models. In\naddition, we present novel data sets that can be used for benchmarking\nregression algorithms, conditional generative models, and uncertainty\nestimation methods in machine learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-precision modeling of systems is one of the main areas of industrial\ndata analysis. Models of systems, their digital twins, are used to predict\ntheir behavior under various conditions. We have developed several models of a\nstorage system using machine learning-based generative models. The system\nconsists of several components: hard disk drive (HDD) and solid-state drive\n(SSD) storage pools with different RAID schemes and cache. Each storage\ncomponent is represented by a probabilistic model that describes the\nprobability distribution of the component performance in terms of IOPS and\nlatency, depending on their configuration and external data load parameters.\nThe results of the experiments demonstrate the errors of 4-10 % for IOPS and\n3-16 % for latency predictions depending on the components and models of the\nsystem. The predictions show up to 0.99 Pearson correlation with Little's law,\nwhich can be used for unsupervised reliability checks of the models. In\naddition, we present novel data sets that can be used for benchmarking\nregression algorithms, conditional generative models, and uncertainty\nestimation methods in machine learning."
                },
                "authors": [
                    {
                        "name": "Abdalaziz Rashid Al-Maeeni"
                    },
                    {
                        "name": "Aziz Temirkhanov"
                    },
                    {
                        "name": "Artem Ryzhikov"
                    },
                    {
                        "name": "Mikhail Hushchyn"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Hushchyn"
                },
                "author": "Mikhail Hushchyn",
                "arxiv_doi": "10.1109/ACCESS.2025.3552409",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2025.3552409",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2307.02073v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.02073v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Access 2025 ( Volume: 13) 49643 - 49658",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03499v1",
                "updated": "2025-04-04T14:55:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    55,
                    27,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T14:55:27Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    55,
                    27,
                    4,
                    94,
                    0
                ],
                "title": "Optimistic Learning for Communication Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimistic Learning for Communication Networks"
                },
                "summary": "AI/ML-based tools are at the forefront of resource management solutions for\ncommunication networks. Deep learning, in particular, is highly effective in\nfacilitating fast and high-performing decision-making whenever representative\ntraining data is available to build offline accurate models. Conversely, online\nlearning solutions do not require training and enable adaptive decisions based\non runtime observations, alas are often overly conservative. This extensive\ntutorial proposes the use of optimistic learning (OpL) as a decision engine for\nresource management frameworks in modern communication systems. When properly\ndesigned, such solutions can achieve fast and high-performing decisions --\ncomparable to offline-trained models -- while preserving the robustness and\nperformance guarantees of the respective online learning approaches. We\nintroduce the fundamental concepts, algorithms and results of OpL, discuss the\nroots of this theory and present different approaches to defining and achieving\noptimism. We proceed to showcase how OpL can enhance resource management in\ncommunication networks for several key problems such as caching, edge\ncomputing, network slicing, and workload assignment in decentralized O-RAN\nplatforms. Finally, we discuss the open challenges that must be addressed to\nunlock the full potential of this new resource management approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI/ML-based tools are at the forefront of resource management solutions for\ncommunication networks. Deep learning, in particular, is highly effective in\nfacilitating fast and high-performing decision-making whenever representative\ntraining data is available to build offline accurate models. Conversely, online\nlearning solutions do not require training and enable adaptive decisions based\non runtime observations, alas are often overly conservative. This extensive\ntutorial proposes the use of optimistic learning (OpL) as a decision engine for\nresource management frameworks in modern communication systems. When properly\ndesigned, such solutions can achieve fast and high-performing decisions --\ncomparable to offline-trained models -- while preserving the robustness and\nperformance guarantees of the respective online learning approaches. We\nintroduce the fundamental concepts, algorithms and results of OpL, discuss the\nroots of this theory and present different approaches to defining and achieving\noptimism. We proceed to showcase how OpL can enhance resource management in\ncommunication networks for several key problems such as caching, edge\ncomputing, network slicing, and workload assignment in decentralized O-RAN\nplatforms. Finally, we discuss the open challenges that must be addressed to\nunlock the full potential of this new resource management approach."
                },
                "authors": [
                    {
                        "name": "George Iosifidis"
                    },
                    {
                        "name": "Naram Mhaisen"
                    },
                    {
                        "name": "Douglas J. Leith"
                    }
                ],
                "author_detail": {
                    "name": "Douglas J. Leith"
                },
                "author": "Douglas J. Leith",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10153v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10153v3",
                "updated": "2025-04-04T13:27:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    13,
                    27,
                    49,
                    4,
                    94,
                    0
                ],
                "published": "2024-12-13T14:11:42Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "title": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector"
                },
                "summary": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhang"
                    },
                    {
                        "name": "Shuzhao Xie"
                    },
                    {
                        "name": "Chengwei Ren"
                    },
                    {
                        "name": "Siyi Xie"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Shijia Ge"
                    },
                    {
                        "name": "Mingzi Wang"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10153v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10153v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03140v1",
                "updated": "2025-04-04T03:30:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    30,
                    15,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T03:30:15Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    30,
                    15,
                    4,
                    94,
                    0
                ],
                "title": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models"
                },
                "summary": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation."
                },
                "authors": [
                    {
                        "name": "Xuran Ma"
                    },
                    {
                        "name": "Yexin Liu"
                    },
                    {
                        "name": "Yaofu Liu"
                    },
                    {
                        "name": "Xianfeng Wu"
                    },
                    {
                        "name": "Mingzhe Zheng"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Ser-Nam Lim"
                    },
                    {
                        "name": "Harry Yang"
                    }
                ],
                "author_detail": {
                    "name": "Harry Yang"
                },
                "author": "Harry Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16444v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16444v3",
                "updated": "2025-04-03T22:49:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    22,
                    49,
                    22,
                    3,
                    93,
                    0
                ],
                "published": "2024-05-26T06:00:17Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    6,
                    0,
                    17,
                    6,
                    147,
                    0
                ],
                "title": "CacheBlend: Fast Large Language Model Serving for RAG with Cached\n  Knowledge Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheBlend: Fast Large Language Model Serving for RAG with Cached\n  Knowledge Fusion"
                },
                "summary": "Large language models (LLMs) often incorporate multiple text chunks in their\ninputs to provide the necessary contexts. To speed up the prefill of the long\nLLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache\nwhen the context is reused as the prefix of another LLM input. However, the\nreused text chunks are not always the input prefix, which makes precomputed KV\ncaches not directly usable since they ignore the text's cross-attention with\nthe preceding texts. Thus, the benefits of reusing KV caches remain largely\nunrealized.\n  This paper tackles just one challenge: when an LLM input contains multiple\ntext chunks, how to quickly combine their precomputed KV caches in order to\nachieve the same generation quality as the expensive full prefill (i.e.,\nwithout reusing KV cache)? This challenge naturally arises in\nretrieval-augmented generation (RAG) where the input is supplemented with\nmultiple retrieved texts as the context. We present CacheBlend, a scheme that\nreuses the precomputed KV caches, regardless prefix or not, and selectively\nrecomputes the KV values of a small subset of tokens to partially update each\nreused KV cache. In the meantime, the small extra delay for recomputing some\ntokens can be pipelined with the retrieval of KV caches within the same job,\nallowing CacheBlend to store KV caches in slower devices with more storage\ncapacity while retrieving them without increasing the inference delay. By\ncomparing CacheBlend with the state-of-the-art KV cache reusing schemes on\nthree open-source LLMs of various sizes and four popular benchmark datasets of\ndifferent tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by\n2.2-3.3x and increases the inference throughput by 2.8-5x from full KV\nrecompute without compromising generation quality. The code is available at\nhttps://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often incorporate multiple text chunks in their\ninputs to provide the necessary contexts. To speed up the prefill of the long\nLLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache\nwhen the context is reused as the prefix of another LLM input. However, the\nreused text chunks are not always the input prefix, which makes precomputed KV\ncaches not directly usable since they ignore the text's cross-attention with\nthe preceding texts. Thus, the benefits of reusing KV caches remain largely\nunrealized.\n  This paper tackles just one challenge: when an LLM input contains multiple\ntext chunks, how to quickly combine their precomputed KV caches in order to\nachieve the same generation quality as the expensive full prefill (i.e.,\nwithout reusing KV cache)? This challenge naturally arises in\nretrieval-augmented generation (RAG) where the input is supplemented with\nmultiple retrieved texts as the context. We present CacheBlend, a scheme that\nreuses the precomputed KV caches, regardless prefix or not, and selectively\nrecomputes the KV values of a small subset of tokens to partially update each\nreused KV cache. In the meantime, the small extra delay for recomputing some\ntokens can be pipelined with the retrieval of KV caches within the same job,\nallowing CacheBlend to store KV caches in slower devices with more storage\ncapacity while retrieving them without increasing the inference delay. By\ncomparing CacheBlend with the state-of-the-art KV cache reusing schemes on\nthree open-source LLMs of various sizes and four popular benchmark datasets of\ndifferent tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by\n2.2-3.3x and increases the inference throughput by 2.8-5x from full KV\nrecompute without compromising generation quality. The code is available at\nhttps://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Siddhant Ray"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16444v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16444v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03048v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03048v1",
                "updated": "2025-04-03T21:53:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    21,
                    53,
                    51,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T21:53:51Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    21,
                    53,
                    51,
                    3,
                    93,
                    0
                ],
                "title": "LLM Library Learning Fails: A LEGO-Prover Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Library Learning Fails: A LEGO-Prover Case Study"
                },
                "summary": "Recent advancements in the coding, reasoning, and tool-using abilities of\nLLMs have spurred interest in library learning (i.e., online learning through\nthe creation, storage, and retrieval of reusable and composable functions,\nknowledge, checklists, or lemmas). Such systems often promise improved task\nperformance through the automatic creation of broadly applicable tools, as well\nas superior computational performance through the caching of reasoning (i.e.,\nthe storage of generated tools). However, we find strong reason to be\nskeptical. We perform a deep dive into one such system, LEGO-Prover, which\npurports to learn reusable lemmas for mathematical reasoning. We find no\nevidence of the direct reuse of learned lemmas, and find evidence against the\nsoft reuse of learned lemmas (i.e., reuse by modifying relevant examples).\nCrucially, we find that LEGO-Prover does not in fact improve over the simple\nbaseline of prompting the model - the improvements in task accuracy vanish once\ncomputational cost is accounted for. Our findings suggest that serious\nmisconceptions exist as to the effectiveness of these techniques, that a\nserious re-examination of the state of LLM-based library learning is required,\nand that we require much stronger standards for evaluation including\nbehavioural analysis and ensuring that an equal computational budget is used\nfor baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in the coding, reasoning, and tool-using abilities of\nLLMs have spurred interest in library learning (i.e., online learning through\nthe creation, storage, and retrieval of reusable and composable functions,\nknowledge, checklists, or lemmas). Such systems often promise improved task\nperformance through the automatic creation of broadly applicable tools, as well\nas superior computational performance through the caching of reasoning (i.e.,\nthe storage of generated tools). However, we find strong reason to be\nskeptical. We perform a deep dive into one such system, LEGO-Prover, which\npurports to learn reusable lemmas for mathematical reasoning. We find no\nevidence of the direct reuse of learned lemmas, and find evidence against the\nsoft reuse of learned lemmas (i.e., reuse by modifying relevant examples).\nCrucially, we find that LEGO-Prover does not in fact improve over the simple\nbaseline of prompting the model - the improvements in task accuracy vanish once\ncomputational cost is accounted for. Our findings suggest that serious\nmisconceptions exist as to the effectiveness of these techniques, that a\nserious re-examination of the state of LLM-based library learning is required,\nand that we require much stronger standards for evaluation including\nbehavioural analysis and ensuring that an equal computational budget is used\nfor baselines."
                },
                "authors": [
                    {
                        "name": "Ian Berlot-Attwell"
                    },
                    {
                        "name": "Frank Rudzicz"
                    },
                    {
                        "name": "Xujie Si"
                    }
                ],
                "author_detail": {
                    "name": "Xujie Si"
                },
                "author": "Xujie Si",
                "arxiv_comment": "24 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03048v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02976v1",
                "updated": "2025-04-03T18:54:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    54,
                    50,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T18:54:50Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    54,
                    50,
                    3,
                    93,
                    0
                ],
                "title": "Localized Definitions and Distributed Reasoning: A Proof-of-Concept\n  Mechanistic Interpretability Study via Activation Patching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localized Definitions and Distributed Reasoning: A Proof-of-Concept\n  Mechanistic Interpretability Study via Activation Patching"
                },
                "summary": "This study investigates the localization of knowledge representation in\nfine-tuned GPT-2 models using Causal Layer Attribution via Activation Patching\n(CLAP), a method that identifies critical neural layers responsible for correct\nanswer generation. The model was fine-tuned on 9,958 PubMed abstracts\n(epilepsy: 20,595 mentions, EEG: 11,674 mentions, seizure: 13,921 mentions)\nusing two configurations with validation loss monitoring for early stopping.\nCLAP involved (1) caching clean (correct answer) and corrupted (incorrect\nanswer) activations, (2) computing logit difference to quantify model\npreference, and (3) patching corrupted activations with clean ones to assess\nrecovery. Results revealed three findings: First, patching the first\nfeedforward layer recovered 56% of correct preference, demonstrating that\nassociative knowledge is distributed across multiple layers. Second, patching\nthe final output layer completely restored accuracy (100% recovery), indicating\nthat definitional knowledge is localised. The stronger clean logit difference\nfor definitional questions further supports this localized representation.\nThird, minimal recovery from convolutional layer patching (13.6%) suggests\nlow-level features contribute marginally to high-level reasoning. Statistical\nanalysis confirmed significant layer-specific effects (p<0.01). These findings\ndemonstrate that factual knowledge is more localized and associative knowledge\ndepends on distributed representations. We also showed that editing efficacy\ndepends on task type. Our findings not only reconcile conflicting observations\nabout localization in model editing but also emphasize on using task-adaptive\ntechniques for reliable, interpretable updates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the localization of knowledge representation in\nfine-tuned GPT-2 models using Causal Layer Attribution via Activation Patching\n(CLAP), a method that identifies critical neural layers responsible for correct\nanswer generation. The model was fine-tuned on 9,958 PubMed abstracts\n(epilepsy: 20,595 mentions, EEG: 11,674 mentions, seizure: 13,921 mentions)\nusing two configurations with validation loss monitoring for early stopping.\nCLAP involved (1) caching clean (correct answer) and corrupted (incorrect\nanswer) activations, (2) computing logit difference to quantify model\npreference, and (3) patching corrupted activations with clean ones to assess\nrecovery. Results revealed three findings: First, patching the first\nfeedforward layer recovered 56% of correct preference, demonstrating that\nassociative knowledge is distributed across multiple layers. Second, patching\nthe final output layer completely restored accuracy (100% recovery), indicating\nthat definitional knowledge is localised. The stronger clean logit difference\nfor definitional questions further supports this localized representation.\nThird, minimal recovery from convolutional layer patching (13.6%) suggests\nlow-level features contribute marginally to high-level reasoning. Statistical\nanalysis confirmed significant layer-specific effects (p<0.01). These findings\ndemonstrate that factual knowledge is more localized and associative knowledge\ndepends on distributed representations. We also showed that editing efficacy\ndepends on task type. Our findings not only reconcile conflicting observations\nabout localization in model editing but also emphasize on using task-adaptive\ntechniques for reliable, interpretable updates."
                },
                "authors": [
                    {
                        "name": "Nooshin Bahador"
                    }
                ],
                "author_detail": {
                    "name": "Nooshin Bahador"
                },
                "author": "Nooshin Bahador",
                "arxiv_comment": "15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02972v1",
                "updated": "2025-04-03T18:47:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    47,
                    26,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T18:47:26Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    47,
                    26,
                    3,
                    93,
                    0
                ],
                "title": "Improved Compact Genetic Algorithms with Efficient Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Compact Genetic Algorithms with Efficient Caching"
                },
                "summary": "Compact Genetic Algorithms (cGAs) are condensed variants of classical Genetic\nAlgorithms (GAs) that use a probability vector representation of the population\ninstead of the complete population. cGAs have been shown to significantly\nreduce the number of function evaluations required while producing outcomes\nsimilar to those of classical GAs. However, cGAs have a tendency to repeatedly\ngenerate the same chromosomes as they approach convergence, resulting in\nunnecessary evaluations of identical chromosomes. This article introduces the\nconcept of caching in cGAs as a means of avoiding redundant evaluations of the\nsame chromosomes. Our proposed approach operates equivalently to cGAs, but\nenhances the algorithm's time efficiency by reducing the number of function\nevaluations. We also present a data structure for efficient cache maintenance\nto ensure low overhead. The proposed caching approach has an asymptotically\nconstant time complexity on average. The proposed method further generalizes\nthe caching mechanism with higher selection pressure for elitism-based cGAs. We\nconduct a rigorous analysis based on experiments on benchmark optimization\nproblems using two well-known cache replacement strategies. The results\ndemonstrate that caching significantly reduces the number of function\nevaluations required while maintaining the same level of performance accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compact Genetic Algorithms (cGAs) are condensed variants of classical Genetic\nAlgorithms (GAs) that use a probability vector representation of the population\ninstead of the complete population. cGAs have been shown to significantly\nreduce the number of function evaluations required while producing outcomes\nsimilar to those of classical GAs. However, cGAs have a tendency to repeatedly\ngenerate the same chromosomes as they approach convergence, resulting in\nunnecessary evaluations of identical chromosomes. This article introduces the\nconcept of caching in cGAs as a means of avoiding redundant evaluations of the\nsame chromosomes. Our proposed approach operates equivalently to cGAs, but\nenhances the algorithm's time efficiency by reducing the number of function\nevaluations. We also present a data structure for efficient cache maintenance\nto ensure low overhead. The proposed caching approach has an asymptotically\nconstant time complexity on average. The proposed method further generalizes\nthe caching mechanism with higher selection pressure for elitism-based cGAs. We\nconduct a rigorous analysis based on experiments on benchmark optimization\nproblems using two well-known cache replacement strategies. The results\ndemonstrate that caching significantly reduces the number of function\nevaluations required while maintaining the same level of performance accuracy."
                },
                "authors": [
                    {
                        "name": "Prasanta Dutta"
                    },
                    {
                        "name": "Anirban Mukhopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Anirban Mukhopadhyay"
                },
                "author": "Anirban Mukhopadhyay",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02921v1",
                "updated": "2025-04-03T17:08:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    8,
                    42,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T17:08:42Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    8,
                    42,
                    3,
                    93,
                    0
                ],
                "title": "HyperRAG: Enhancing Quality-Efficiency Tradeoffs in Retrieval-Augmented\n  Generation with Reranker KV-Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperRAG: Enhancing Quality-Efficiency Tradeoffs in Retrieval-Augmented\n  Generation with Reranker KV-Cache Reuse"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing the performance of large language models (LLMs) by integrating\nexternal knowledge into the generation process. A key component of RAG\npipelines is the reranker, which selects the most relevant documents from a\npool of retrieved candidates and significantly improves the quality of the\ngenerated responses. While rerankers refine the selection of retrieved\ndocuments in RAG pipelines, they introduce computational challenges that hinder\nhigh throughput and low latency. To address this problem, we propose HyperRAG,\na system that optimizes the trade-off between quality and efficiency in RAG\npipelines by leveraging KV-cache reuse for efficient reranker inference. By\nreusing document-side KV-cache, HyperRAG achieves both high-quality generation\nand system-level efficiency. To fully realize the benefits of KV-cache reuse,\nHyperRAG incorporates a range of system-level optimizations designed to enhance\nefficiency and scalability. Experiments show that HyperRAG achieves a 2 - 3\nthroughput improvement with decoder-only rerankers while also delivering higher\ndownstream performance compared with traditional RAG service.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing the performance of large language models (LLMs) by integrating\nexternal knowledge into the generation process. A key component of RAG\npipelines is the reranker, which selects the most relevant documents from a\npool of retrieved candidates and significantly improves the quality of the\ngenerated responses. While rerankers refine the selection of retrieved\ndocuments in RAG pipelines, they introduce computational challenges that hinder\nhigh throughput and low latency. To address this problem, we propose HyperRAG,\na system that optimizes the trade-off between quality and efficiency in RAG\npipelines by leveraging KV-cache reuse for efficient reranker inference. By\nreusing document-side KV-cache, HyperRAG achieves both high-quality generation\nand system-level efficiency. To fully realize the benefits of KV-cache reuse,\nHyperRAG incorporates a range of system-level optimizations designed to enhance\nefficiency and scalability. Experiments show that HyperRAG achieves a 2 - 3\nthroughput improvement with decoder-only rerankers while also delivering higher\ndownstream performance compared with traditional RAG service."
                },
                "authors": [
                    {
                        "name": "Yuwei An"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Seo Jin Park"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01380v2",
                "updated": "2025-04-03T13:28:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    13,
                    28,
                    51,
                    3,
                    93,
                    0
                ],
                "published": "2024-12-02T11:07:51Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "title": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking"
                },
                "summary": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which results in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46\\% reduction in memory and 40\\% increase in throughput with $<$\n0.1 loss in perplexity when compared to streaming the dense model from Flash.\nThe open source code for HW simulator, methods, and experiments in this paper\nis available at https://github.com/Qualcomm-AI-research/dynamic-sparsity .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which results in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46\\% reduction in memory and 40\\% increase in throughput with $<$\n0.1 loss in perplexity when compared to streaming the dense model from Flash.\nThe open source code for HW simulator, methods, and experiments in this paper\nis available at https://github.com/Qualcomm-AI-research/dynamic-sparsity ."
                },
                "authors": [
                    {
                        "name": "Marco Federici"
                    },
                    {
                        "name": "Davide Belli"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Amir Jalalirad"
                    },
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Bence Major"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    }
                ],
                "author_detail": {
                    "name": "Paul Whatmough"
                },
                "author": "Paul Whatmough",
                "arxiv_comment": "Main Text: 10 pages, 11 figures. Appendix: 6 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02441v1",
                "updated": "2025-04-03T09:58:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T09:58:19Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "title": "Cognitive Memory in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Memory in Large Language Models"
                },
                "summary": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions."
                },
                "authors": [
                    {
                        "name": "Lianlei Shan"
                    },
                    {
                        "name": "Shixian Luo"
                    },
                    {
                        "name": "Zezhou Zhu"
                    },
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Yong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Wu"
                },
                "author": "Yong Wu",
                "arxiv_comment": "37 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03775v1",
                "updated": "2025-04-03T08:58:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    58,
                    5,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T08:58:05Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    58,
                    5,
                    3,
                    93,
                    0
                ],
                "title": "FlowKV: A Disaggregated Inference Framework with Low-Latency KV Cache\n  Transfer and Load-Aware Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowKV: A Disaggregated Inference Framework with Low-Latency KV Cache\n  Transfer and Load-Aware Scheduling"
                },
                "summary": "Disaggregated inference has become an essential framework that separates the\nprefill (P) and decode (D) stages in large language model inference to improve\nthroughput. However, the KV cache transfer faces significant delays between\nprefill and decode nodes. The block-wise calling method and discontinuous KV\ncache memory allocation increase the number of calls to the transmission\nkernel. Additionally, existing frameworks often fix the roles of P and D nodes,\nleading to computational imbalances. In this paper, we propose FlowKV, a novel\ndisaggregated inference framework, which reduces the average transmission\nlatency of KV cache by 96%, from 0.944s to 0.053s, almost eliminating the\ntransfer time relative to the total request latency by optimizing the KV cache\ntransfer. FlowKV introduces the Load-Aware Scheduler for balanced request\nscheduling and flexible PD node allocation. This design maximizes hardware\nresource utilization, achieving peak system throughput across various\nscenarios, including normal, computational imbalance, and extreme overload\nconditions. Experimental results demonstrate that FlowKV significantly\naccelerates inference by 15.2%-48.9% on LongBench dataset compared to the\nbaseline and supports applications with heterogeneous GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated inference has become an essential framework that separates the\nprefill (P) and decode (D) stages in large language model inference to improve\nthroughput. However, the KV cache transfer faces significant delays between\nprefill and decode nodes. The block-wise calling method and discontinuous KV\ncache memory allocation increase the number of calls to the transmission\nkernel. Additionally, existing frameworks often fix the roles of P and D nodes,\nleading to computational imbalances. In this paper, we propose FlowKV, a novel\ndisaggregated inference framework, which reduces the average transmission\nlatency of KV cache by 96%, from 0.944s to 0.053s, almost eliminating the\ntransfer time relative to the total request latency by optimizing the KV cache\ntransfer. FlowKV introduces the Load-Aware Scheduler for balanced request\nscheduling and flexible PD node allocation. This design maximizes hardware\nresource utilization, achieving peak system throughput across various\nscenarios, including normal, computational imbalance, and extreme overload\nconditions. Experimental results demonstrate that FlowKV significantly\naccelerates inference by 15.2%-48.9% on LongBench dataset compared to the\nbaseline and supports applications with heterogeneous GPUs."
                },
                "authors": [
                    {
                        "name": "Weiqing Li"
                    },
                    {
                        "name": "Guochao Jiang"
                    },
                    {
                        "name": "Xiangyong Ding"
                    },
                    {
                        "name": "Zhangcheng Tao"
                    },
                    {
                        "name": "Chuzhan Hao"
                    },
                    {
                        "name": "Chenfeng Xu"
                    },
                    {
                        "name": "Yuewei Zhang"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02268v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02268v1",
                "updated": "2025-04-03T04:27:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    27,
                    2,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T04:27:02Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    27,
                    2,
                    3,
                    93,
                    0
                ],
                "title": "Advancing Semantic Caching for LLMs with Domain-Specific Embeddings and\n  Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Semantic Caching for LLMs with Domain-Specific Embeddings and\n  Synthetic Data"
                },
                "summary": "This report investigates enhancing semantic caching effectiveness by\nemploying specialized, fine-tuned embedding models. Semantic caching relies on\nembedding similarity rather than exact key matching, presenting unique\nchallenges in balancing precision, query latency, and computational efficiency.\nWe propose leveraging smaller, domain-specific embedding models, fine-tuned\nwith targeted real-world and synthetically generated datasets. Our empirical\nevaluations demonstrate that compact embedding models fine-tuned for just one\nepoch on specialized datasets significantly surpass both state-of-the-art\nopen-source and proprietary alternatives in precision and recall. Moreover, we\nintroduce a novel synthetic data generation pipeline for the semantic cache\nthat mitigates the challenge of limited domain-specific annotated data, further\nboosting embedding performance. Our approach effectively balances computational\noverhead and accuracy, establishing a viable and efficient strategy for\npractical semantic caching implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report investigates enhancing semantic caching effectiveness by\nemploying specialized, fine-tuned embedding models. Semantic caching relies on\nembedding similarity rather than exact key matching, presenting unique\nchallenges in balancing precision, query latency, and computational efficiency.\nWe propose leveraging smaller, domain-specific embedding models, fine-tuned\nwith targeted real-world and synthetically generated datasets. Our empirical\nevaluations demonstrate that compact embedding models fine-tuned for just one\nepoch on specialized datasets significantly surpass both state-of-the-art\nopen-source and proprietary alternatives in precision and recall. Moreover, we\nintroduce a novel synthetic data generation pipeline for the semantic cache\nthat mitigates the challenge of limited domain-specific annotated data, further\nboosting embedding performance. Our approach effectively balances computational\noverhead and accuracy, establishing a viable and efficient strategy for\npractical semantic caching implementations."
                },
                "authors": [
                    {
                        "name": "Waris Gill"
                    },
                    {
                        "name": "Justin Cechmanek"
                    },
                    {
                        "name": "Tyler Hutcherson"
                    },
                    {
                        "name": "Srijith Rajamohan"
                    },
                    {
                        "name": "Jen Agarwal"
                    },
                    {
                        "name": "Muhammad Ali Gulzar"
                    },
                    {
                        "name": "Manvinder Singh"
                    },
                    {
                        "name": "Benoit Dion"
                    }
                ],
                "author_detail": {
                    "name": "Benoit Dion"
                },
                "arxiv_affiliation": "Redis",
                "author": "Benoit Dion",
                "arxiv_comment": "Initial study on embedding fine tuning for semantic cache. It also\n  explores synthetic data. Total pages are 12, including refrences",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02268v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02268v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02220v1",
                "updated": "2025-04-03T02:24:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    2,
                    24,
                    21,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T02:24:21Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    2,
                    24,
                    21,
                    3,
                    93,
                    0
                ],
                "title": "Comparative Analysis of Distributed Caching Algorithms: Performance\n  Metrics and Implementation Considerations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of Distributed Caching Algorithms: Performance\n  Metrics and Implementation Considerations"
                },
                "summary": "This paper presents a comprehensive comparison of distributed caching\nalgorithms employed in modern distributed systems. We evaluate various caching\nstrategies including Least Recently Used (LRU), Least Frequently Used (LFU),\nAdaptive Replacement Cache (ARC), and Time-Aware Least Recently Used (TLRU)\nagainst metrics such as hit ratio, latency reduction, memory overhead, and\nscalability. Our analysis reveals that while traditional algorithms like LRU\nremain prevalent, hybrid approaches incorporating machine learning techniques\ndemonstrate superior performance in dynamic environments. Additionally, we\nanalyze implementation patterns across different distributed architectures and\nprovide recommendations for algorithm selection based on specific workload\ncharacteristics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive comparison of distributed caching\nalgorithms employed in modern distributed systems. We evaluate various caching\nstrategies including Least Recently Used (LRU), Least Frequently Used (LFU),\nAdaptive Replacement Cache (ARC), and Time-Aware Least Recently Used (TLRU)\nagainst metrics such as hit ratio, latency reduction, memory overhead, and\nscalability. Our analysis reveals that while traditional algorithms like LRU\nremain prevalent, hybrid approaches incorporating machine learning techniques\ndemonstrate superior performance in dynamic environments. Additionally, we\nanalyze implementation patterns across different distributed architectures and\nprovide recommendations for algorithm selection based on specific workload\ncharacteristics."
                },
                "authors": [
                    {
                        "name": "Helen Mayer"
                    },
                    {
                        "name": "James Richards"
                    }
                ],
                "author_detail": {
                    "name": "James Richards"
                },
                "author": "James Richards",
                "arxiv_comment": "International Conference on Computing Technologies and Artificial\n  Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01281v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01281v2",
                "updated": "2025-04-03T01:23:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    1,
                    23,
                    22,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-02T01:16:10Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    16,
                    10,
                    2,
                    92,
                    0
                ],
                "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding"
                },
                "summary": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01281v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01281v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22875v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22875v2",
                "updated": "2025-04-02T18:51:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    18,
                    51,
                    53,
                    2,
                    92,
                    0
                ],
                "published": "2025-03-28T21:02:32Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    21,
                    2,
                    32,
                    4,
                    87,
                    0
                ],
                "title": "A Pilot Study on Tunable Precision Emulation via Automatic BLAS\n  Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Pilot Study on Tunable Precision Emulation via Automatic BLAS\n  Offloading"
                },
                "summary": "This study explores the use of automatic BLAS offloading and INT8-based\nemulation for accelerating traditional HPC workloads on modern GPU\narchitectures. Through the use of low-bitwidth integer units and cache-coherent\nUnified Memory Architecture, we emulate double-precision matrix multiplications\nin the MuST application without code changes. We find that accuracy depends on\nboth arithmetic precision and the properties of the operator, which can be\ndealt with through tunable precision emulation. Unlike traditional\nmixed-precision approaches, this method preserves original algorithms while\noptimizing hardware utilization. We showcases the potential of improving\naccuracy and performance at the same time. This work highlights the potential\nof AI-driven hardware to transform HPC, advocating for adaptive precision\nstrategies in future scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the use of automatic BLAS offloading and INT8-based\nemulation for accelerating traditional HPC workloads on modern GPU\narchitectures. Through the use of low-bitwidth integer units and cache-coherent\nUnified Memory Architecture, we emulate double-precision matrix multiplications\nin the MuST application without code changes. We find that accuracy depends on\nboth arithmetic precision and the properties of the operator, which can be\ndealt with through tunable precision emulation. Unlike traditional\nmixed-precision approaches, this method preserves original algorithms while\noptimizing hardware utilization. We showcases the potential of improving\naccuracy and performance at the same time. This work highlights the potential\nof AI-driven hardware to transform HPC, advocating for adaptive precision\nstrategies in future scientific computing."
                },
                "authors": [
                    {
                        "name": "Hang Liu"
                    },
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Yinzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yinzhi Wang"
                },
                "author": "Yinzhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22875v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22875v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01582v1",
                "updated": "2025-04-02T10:38:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    38,
                    25,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T10:38:25Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    38,
                    25,
                    2,
                    92,
                    0
                ],
                "title": "MERE: Hardware-Software Co-Design for Masking Cache Miss Latency in\n  Embedded Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MERE: Hardware-Software Co-Design for Masking Cache Miss Latency in\n  Embedded Processors"
                },
                "summary": "Runahead execution is a technique to mask memory latency caused by irregular\nmemory accesses. By pre-executing the application code during occurrences of\nlong-latency operations and prefetching anticipated cache-missed data into the\ncache hierarchy, runahead effectively masks memory latency for subsequent cache\nmisses and achieves high prefetching accuracy; however, this technique has been\nlimited to superscalar out-of-order and superscalar in-order cores. For\nimplementation in scalar in-order cores, the challenges of\narea-/energy-constraint and severe cache contention remain.\n  Here, we build the first full-stack system featuring runahead, MERE, from SoC\nand a dedicated ISA to the OS and programming model. Through this deployment,\nwe show that enabling runahead in scalar in-order cores is possible, with\nminimal area and power overheads, while still achieving high performance. By\nre-constructing the sequential runahead employing a hardware/software co-design\napproach, the system can be implemented on a mature processor and SoC. Building\non this, an adaptive runahead mechanism is proposed to mitigate the severe\ncache contention in scalar in-order cores. Combining this, we provide a\ncomprehensive solution for embedded processors managing irregular workloads.\nOur evaluation demonstrates that the proposed MERE attains 93.5% of a 2-wide\nout-of-order core's performance while constraining area and power overheads\nbelow 5%, with the adaptive runahead mechanism delivering an additional 20.1%\nperformance gain through mitigating the severe cache contention issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Runahead execution is a technique to mask memory latency caused by irregular\nmemory accesses. By pre-executing the application code during occurrences of\nlong-latency operations and prefetching anticipated cache-missed data into the\ncache hierarchy, runahead effectively masks memory latency for subsequent cache\nmisses and achieves high prefetching accuracy; however, this technique has been\nlimited to superscalar out-of-order and superscalar in-order cores. For\nimplementation in scalar in-order cores, the challenges of\narea-/energy-constraint and severe cache contention remain.\n  Here, we build the first full-stack system featuring runahead, MERE, from SoC\nand a dedicated ISA to the OS and programming model. Through this deployment,\nwe show that enabling runahead in scalar in-order cores is possible, with\nminimal area and power overheads, while still achieving high performance. By\nre-constructing the sequential runahead employing a hardware/software co-design\napproach, the system can be implemented on a mature processor and SoC. Building\non this, an adaptive runahead mechanism is proposed to mitigate the severe\ncache contention in scalar in-order cores. Combining this, we provide a\ncomprehensive solution for embedded processors managing irregular workloads.\nOur evaluation demonstrates that the proposed MERE attains 93.5% of a 2-wide\nout-of-order core's performance while constraining area and power overheads\nbelow 5%, with the adaptive runahead mechanism delivering an additional 20.1%\nperformance gain through mitigating the severe cache contention issues."
                },
                "authors": [
                    {
                        "name": "Dean You"
                    },
                    {
                        "name": "Jieyu Jiang"
                    },
                    {
                        "name": "Xiaoxuan Wang"
                    },
                    {
                        "name": "Yushu Du"
                    },
                    {
                        "name": "Zhihang Tan"
                    },
                    {
                        "name": "Wenbo Xu"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Jiapeng Guan"
                    },
                    {
                        "name": "Zhenyuan Wang"
                    },
                    {
                        "name": "Ran Wei"
                    },
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v4",
                "updated": "2025-04-02T04:57:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    4,
                    57,
                    15,
                    2,
                    92,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "Hexa-MoE: Efficient and Heterogeneous-aware Training for\n  Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hexa-MoE: Efficient and Heterogeneous-aware Training for\n  Mixture-of-Experts"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Hanrui Wang"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v5",
                "updated": "2025-04-02T01:58:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    58,
                    38,
                    2,
                    92,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks."
                },
                "authors": [
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01291v1",
                "updated": "2025-04-02T01:49:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    49,
                    58,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T01:49:58Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    49,
                    58,
                    2,
                    92,
                    0
                ],
                "title": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures"
                },
                "summary": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs."
                },
                "authors": [
                    {
                        "name": "Seungheon Shin"
                    },
                    {
                        "name": "Kyle Liddy"
                    },
                    {
                        "name": "Yinxuan Zhu"
                    },
                    {
                        "name": "Chandan Joishi"
                    },
                    {
                        "name": "Brianna A. Klein"
                    },
                    {
                        "name": "Andrew Armstrong"
                    },
                    {
                        "name": "Andrew A. Allerman"
                    },
                    {
                        "name": "Siddharth Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Siddharth Rajan"
                },
                "author": "Siddharth Rajan",
                "arxiv_comment": "11 pages, 6 figures, and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01157v1",
                "updated": "2025-04-01T19:48:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    19,
                    48,
                    17,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T19:48:17Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    19,
                    48,
                    17,
                    1,
                    91,
                    0
                ],
                "title": "Beyond Quacking: Deep Integration of Language Models and RAG into DuckDB",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Quacking: Deep Integration of Language Models and RAG into DuckDB"
                },
                "summary": "Knowledge-intensive analytical applications retrieve context from both\nstructured tabular data and unstructured, text-free documents for effective\ndecision-making. Large language models (LLMs) have made it significantly easier\nto prototype such retrieval and reasoning data pipelines. However, implementing\nthese pipelines efficiently still demands significant effort and has several\nchallenges. This often involves orchestrating heterogeneous data systems,\nmanaging data movement, and handling low-level implementation details, e.g.,\nLLM context management.\n  To address these challenges, we introduce FlockMTL: an extension for DBMSs\nthat deeply integrates LLM capabilities and retrieval-augmented generation\n(RAG). FlockMTL includes model-driven scalar and aggregate functions, enabling\nchained predictions through tuple-level mappings and reductions. Drawing\ninspiration from the relational model, FlockMTL incorporates: (i) cost-based\noptimizations, which seamlessly apply techniques such as batching and caching;\nand (ii) resource independence, enabled through novel SQL DDL abstractions:\nPROMPT and MODEL, introduced as first-class schema objects alongside TABLE.\nFlockMTL streamlines the development of knowledge-intensive analytical\napplications, and its optimizations ease the implementation burden.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-intensive analytical applications retrieve context from both\nstructured tabular data and unstructured, text-free documents for effective\ndecision-making. Large language models (LLMs) have made it significantly easier\nto prototype such retrieval and reasoning data pipelines. However, implementing\nthese pipelines efficiently still demands significant effort and has several\nchallenges. This often involves orchestrating heterogeneous data systems,\nmanaging data movement, and handling low-level implementation details, e.g.,\nLLM context management.\n  To address these challenges, we introduce FlockMTL: an extension for DBMSs\nthat deeply integrates LLM capabilities and retrieval-augmented generation\n(RAG). FlockMTL includes model-driven scalar and aggregate functions, enabling\nchained predictions through tuple-level mappings and reductions. Drawing\ninspiration from the relational model, FlockMTL incorporates: (i) cost-based\noptimizations, which seamlessly apply techniques such as batching and caching;\nand (ii) resource independence, enabled through novel SQL DDL abstractions:\nPROMPT and MODEL, introduced as first-class schema objects alongside TABLE.\nFlockMTL streamlines the development of knowledge-intensive analytical\napplications, and its optimizations ease the implementation burden."
                },
                "authors": [
                    {
                        "name": "Anas Dorbani"
                    },
                    {
                        "name": "Sunny Yasser"
                    },
                    {
                        "name": "Jimmy Lin"
                    },
                    {
                        "name": "Amine Mhedhbi"
                    }
                ],
                "author_detail": {
                    "name": "Amine Mhedhbi"
                },
                "author": "Amine Mhedhbi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01104v1",
                "updated": "2025-04-01T18:21:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    21,
                    43,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T18:21:43Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    21,
                    43,
                    1,
                    91,
                    0
                ],
                "title": "Fundamentals of Caching Layered Data objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamentals of Caching Layered Data objects"
                },
                "summary": "The effective management of large amounts of data processed or required by\ntoday's cloud or edge computing systems remains a fundamental challenge. This\npaper focuses on cache management for applications where data objects can be\nstored in layered representations. In such representations, each additional\ndata layer enhances the \"quality\" of the object's version but comes with an\nincremental cost of memory space. This layered approach proves beneficial in\nvarious scenarios, including the delivery of zoomable maps, video coding,\nfuture Virtual Reality gaming, and layered neural network models where\nadditional data layers improve inference accuracy. In systems where users or\ndevices demand different versions of a data object, layered representations\noffer flexibility for caching policies to achieve improved hit rates.\n  In this paper, we explore the performance of various traditionally studied\ncaching policies, such as Belady, LRU, and LFU, both with and without layering.\nTo this end, we develop an asymptotically accurate analytical model for Layered\nLRU (LLRU). We study how the performance of LLRU is impacted by factors such as\nthe number of layers, the popularity of different objects and layers, and\noverheads associated with storing layered representations. For instance, we\nshow that, for LLRU, more layers are not always beneficial and indeed\nperformance depends in subtle ways on the popularity and size profiles of\nlayers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effective management of large amounts of data processed or required by\ntoday's cloud or edge computing systems remains a fundamental challenge. This\npaper focuses on cache management for applications where data objects can be\nstored in layered representations. In such representations, each additional\ndata layer enhances the \"quality\" of the object's version but comes with an\nincremental cost of memory space. This layered approach proves beneficial in\nvarious scenarios, including the delivery of zoomable maps, video coding,\nfuture Virtual Reality gaming, and layered neural network models where\nadditional data layers improve inference accuracy. In systems where users or\ndevices demand different versions of a data object, layered representations\noffer flexibility for caching policies to achieve improved hit rates.\n  In this paper, we explore the performance of various traditionally studied\ncaching policies, such as Belady, LRU, and LFU, both with and without layering.\nTo this end, we develop an asymptotically accurate analytical model for Layered\nLRU (LLRU). We study how the performance of LLRU is impacted by factors such as\nthe number of layers, the popularity of different objects and layers, and\noverheads associated with storing layered representations. For instance, we\nshow that, for LLRU, more layers are not always beneficial and indeed\nperformance depends in subtle ways on the popularity and size profiles of\nlayers."
                },
                "authors": [
                    {
                        "name": "Agrim Bari"
                    },
                    {
                        "name": "Gustavo de Veciana"
                    },
                    {
                        "name": "George Kesidis"
                    }
                ],
                "author_detail": {
                    "name": "George Kesidis"
                },
                "author": "George Kesidis",
                "arxiv_comment": "An abridged version of this paper has been accepted at the 45th IEEE\n  International Conference on Distributed Computing Systems (ICDCS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01084v1",
                "updated": "2025-04-01T18:00:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    0,
                    48,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T18:00:48Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    0,
                    48,
                    1,
                    91,
                    0
                ],
                "title": "Surfactants Screen Slide Electrification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surfactants Screen Slide Electrification"
                },
                "summary": "Water drops spontaneously accumulate charges when they move on hydrophobic\ndielectric surfaces by slide electrification. On the one hand, slide\nelectrification generates electricity with possible applications on tiny\ndevices. On the other hand, the potential of up to 1 KV generated by slide\nelectrification alters wetting and drop motion. Therefore, it is important to\nknow the factors that affect slide electrification. To find out how surfactants\naffect slide electrification, we measured drop charges of aqueous drops\ncontaining cationic CTAB, anionic SDS and neutral C8E3 sliding on different\nhydrophobic surfaces. The result is: addition of surfactant significantly\nreduces the spontaneous charging of moving water drops. Based on zeta potential\nmeasurements, confocal microscopy of deposited surface-active dyes and drop\nimpact studies, we propose that several factors contribute to this suppression\nof charge separation: (1) Surfactants tend to lower the contact angles, which\nreduces charge separation. (2) Surfactant adsorption at the solid-liquid\ninterface can reduce the density of primary ions, particularly for anionic\nsurfactants. (3) Anionic and neutral surfactants are mostly transferred to the\nliquid-air interface at the rear of the sliding drop, retaining primary ions\nwithin the drop. (4) Deposited cationic surfactant directly reduces the charge\nof the drop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Water drops spontaneously accumulate charges when they move on hydrophobic\ndielectric surfaces by slide electrification. On the one hand, slide\nelectrification generates electricity with possible applications on tiny\ndevices. On the other hand, the potential of up to 1 KV generated by slide\nelectrification alters wetting and drop motion. Therefore, it is important to\nknow the factors that affect slide electrification. To find out how surfactants\naffect slide electrification, we measured drop charges of aqueous drops\ncontaining cationic CTAB, anionic SDS and neutral C8E3 sliding on different\nhydrophobic surfaces. The result is: addition of surfactant significantly\nreduces the spontaneous charging of moving water drops. Based on zeta potential\nmeasurements, confocal microscopy of deposited surface-active dyes and drop\nimpact studies, we propose that several factors contribute to this suppression\nof charge separation: (1) Surfactants tend to lower the contact angles, which\nreduces charge separation. (2) Surfactant adsorption at the solid-liquid\ninterface can reduce the density of primary ions, particularly for anionic\nsurfactants. (3) Anionic and neutral surfactants are mostly transferred to the\nliquid-air interface at the rear of the sliding drop, retaining primary ions\nwithin the drop. (4) Deposited cationic surfactant directly reduces the charge\nof the drop."
                },
                "authors": [
                    {
                        "name": "Xiaomei Li"
                    },
                    {
                        "name": "Zhongyuan Ni"
                    },
                    {
                        "name": "Xiaoteng Zhou"
                    },
                    {
                        "name": "Lisa S. Bauer"
                    },
                    {
                        "name": "Diego Diaz"
                    },
                    {
                        "name": "Gabriele Schfer"
                    },
                    {
                        "name": "Hans-Jrgen Butt"
                    }
                ],
                "author_detail": {
                    "name": "Hans-Jrgen Butt"
                },
                "author": "Hans-Jrgen Butt",
                "arxiv_comment": "13 pages, 4 figures, 50 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00999v1",
                "updated": "2025-04-01T17:39:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    39,
                    19,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T17:39:19Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    39,
                    19,
                    1,
                    91,
                    0
                ],
                "title": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization"
                },
                "summary": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great\nsuccess in both self-supervised pre-training and image generation. However,\nmost existing methods struggle to address the trade-off in shared latent space\nfor generation quality vs. representation learning and efficiency. To push the\nlimits of this paradigm, we propose MergeVQ, which incorporates token merging\ntechniques into VQ-based generative models to bridge the gap between image\ngeneration and visual representation learning in a unified architecture. During\npre-training, MergeVQ decouples top-k semantics from latent space with the\ntoken merge module after self-attention blocks in the encoder for subsequent\nLook-up Free Quantization (LFQ) and global alignment and recovers their\nfine-grained details through cross-attention in the decoder for reconstruction.\nAs for the second-stage generation, we introduce MergeAR, which performs KV\nCache compression for efficient raster-order prediction. Extensive experiments\non ImageNet verify that MergeVQ as an AR generative model achieves competitive\nperformance in both visual representation learning and image generation tasks\nwhile maintaining favorable token efficiency and inference speed. The code and\nmodel will be available at https://apexgen-x.github.io/MergeVQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great\nsuccess in both self-supervised pre-training and image generation. However,\nmost existing methods struggle to address the trade-off in shared latent space\nfor generation quality vs. representation learning and efficiency. To push the\nlimits of this paradigm, we propose MergeVQ, which incorporates token merging\ntechniques into VQ-based generative models to bridge the gap between image\ngeneration and visual representation learning in a unified architecture. During\npre-training, MergeVQ decouples top-k semantics from latent space with the\ntoken merge module after self-attention blocks in the encoder for subsequent\nLook-up Free Quantization (LFQ) and global alignment and recovers their\nfine-grained details through cross-attention in the decoder for reconstruction.\nAs for the second-stage generation, we introduce MergeAR, which performs KV\nCache compression for efficient raster-order prediction. Extensive experiments\non ImageNet verify that MergeVQ as an AR generative model achieves competitive\nperformance in both visual representation learning and image generation tasks\nwhile maintaining favorable token efficiency and inference speed. The code and\nmodel will be available at https://apexgen-x.github.io/MergeVQ."
                },
                "authors": [
                    {
                        "name": "Siyuan Li"
                    },
                    {
                        "name": "Luyuan Zhang"
                    },
                    {
                        "name": "Zedong Wang"
                    },
                    {
                        "name": "Juanxi Tian"
                    },
                    {
                        "name": "Cheng Tan"
                    },
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Chang Yu"
                    },
                    {
                        "name": "Qingsong Xie"
                    },
                    {
                        "name": "Haonan Lu"
                    },
                    {
                        "name": "Haoqian Wang"
                    },
                    {
                        "name": "Zhen Lei"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Lei"
                },
                "author": "Zhen Lei",
                "arxiv_comment": "CVPR2025 (in process for more analysis and extension)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00970v1",
                "updated": "2025-04-01T17:08:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    8,
                    57,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T17:08:57Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    8,
                    57,
                    1,
                    91,
                    0
                ],
                "title": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching"
                },
                "summary": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Ali Falahati"
                    },
                    {
                        "name": "David H. Yang"
                    },
                    {
                        "name": "Mohammad Mohammadi Amiri"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Mohammadi Amiri"
                },
                "author": "Mohammad Mohammadi Amiri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13275v2",
                "updated": "2025-04-01T14:21:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    14,
                    21,
                    15,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-17T15:27:02Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "title": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems"
                },
                "summary": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability."
                },
                "authors": [
                    {
                        "name": "Seyoung Song"
                    }
                ],
                "author_detail": {
                    "name": "Seyoung Song"
                },
                "author": "Seyoung Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7; I.2.11; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00726v1",
                "updated": "2025-04-01T12:34:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    34,
                    58,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T12:34:58Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    34,
                    58,
                    1,
                    91,
                    0
                ],
                "title": "EMO: Edge Model Overlays to Scale Model Size in Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMO: Edge Model Overlays to Scale Model Size in Federated Learning"
                },
                "summary": "Federated Learning (FL) trains machine learning models on edge devices with\ndistributed data. However, the computational and memory limitations of these\ndevices restrict the training of large models using FL. Split Federated\nLearning (SFL) addresses this challenge by distributing the model across the\ndevice and server, but it introduces a tightly coupled data flow, leading to\ncomputational bottlenecks and high communication costs. We propose EMO as a\nsolution to enable the training of large models in FL while mitigating the\nchallenges of SFL. EMO introduces Edge Model Overlay(s) between the device and\nserver, enabling the creation of a larger ensemble model without modifying the\nFL workflow. The key innovation in EMO is Augmented Federated Learning (AFL),\nwhich builds an ensemble model by connecting the original (smaller) FL model\nwith model(s) trained in the overlay(s) to facilitate horizontal or vertical\nscaling. This is accomplished through three key modules: a hierarchical\nactivation replay cache to decouple AFL from FL, a convergence-aware\ncommunication controller to optimize communication overhead, and an ensemble\ninference module. Evaluations on a real-world prototype show that EMO improves\naccuracy by up to 17.77% compared to FL, and reduces communication costs by up\nto 7.17x and decreases training time by up to 6.9x compared to SFL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) trains machine learning models on edge devices with\ndistributed data. However, the computational and memory limitations of these\ndevices restrict the training of large models using FL. Split Federated\nLearning (SFL) addresses this challenge by distributing the model across the\ndevice and server, but it introduces a tightly coupled data flow, leading to\ncomputational bottlenecks and high communication costs. We propose EMO as a\nsolution to enable the training of large models in FL while mitigating the\nchallenges of SFL. EMO introduces Edge Model Overlay(s) between the device and\nserver, enabling the creation of a larger ensemble model without modifying the\nFL workflow. The key innovation in EMO is Augmented Federated Learning (AFL),\nwhich builds an ensemble model by connecting the original (smaller) FL model\nwith model(s) trained in the overlay(s) to facilitate horizontal or vertical\nscaling. This is accomplished through three key modules: a hierarchical\nactivation replay cache to decouple AFL from FL, a convergence-aware\ncommunication controller to optimize communication overhead, and an ensemble\ninference module. Evaluations on a real-world prototype show that EMO improves\naccuracy by up to 17.77% compared to FL, and reduces communication costs by up\nto 7.17x and decreases training time by up to 6.9x compared to SFL."
                },
                "authors": [
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Weibo He"
                    },
                    {
                        "name": "Wanglei Feng"
                    },
                    {
                        "name": "Zhenyu Wen"
                    },
                    {
                        "name": "Bin Qian"
                    },
                    {
                        "name": "Blesson Varghese"
                    }
                ],
                "author_detail": {
                    "name": "Blesson Varghese"
                },
                "author": "Blesson Varghese",
                "arxiv_comment": "Poster accepted at IEEE ICDCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00557v1",
                "updated": "2025-04-01T09:10:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    10,
                    32,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T09:10:32Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    10,
                    32,
                    1,
                    91,
                    0
                ],
                "title": "Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features"
                },
                "summary": "Visual token reduction lowers inference costs caused by extensive image\nfeatures in large vision-language models (LVLMs). Unlike relevant studies that\nprune tokens in self-attention-only LVLMs, our work uniquely addresses\ncross-attention-based models, which achieve superior performance. We identify\nthat the key-value (KV) cache size for image tokens in cross-attention layers\nsignificantly exceeds that of text tokens in self-attention layers, posing a\nmajor compute bottleneck. To mitigate this issue, we exploit the sparse nature\nin cross-attention maps to selectively prune redundant visual features. Our\nTrimmed Llama effectively reduces KV cache demands without requiring additional\ntraining. By benefiting from 50%-reduced visual features, our model can reduce\ninference latency and memory usage while achieving benchmark parity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual token reduction lowers inference costs caused by extensive image\nfeatures in large vision-language models (LVLMs). Unlike relevant studies that\nprune tokens in self-attention-only LVLMs, our work uniquely addresses\ncross-attention-based models, which achieve superior performance. We identify\nthat the key-value (KV) cache size for image tokens in cross-attention layers\nsignificantly exceeds that of text tokens in self-attention layers, posing a\nmajor compute bottleneck. To mitigate this issue, we exploit the sparse nature\nin cross-attention maps to selectively prune redundant visual features. Our\nTrimmed Llama effectively reduces KV cache demands without requiring additional\ntraining. By benefiting from 50%-reduced visual features, our model can reduce\ninference latency and memory usage while achieving benchmark parity."
                },
                "authors": [
                    {
                        "name": "Jewon Lee"
                    },
                    {
                        "name": "Ki-Ung Song"
                    },
                    {
                        "name": "Seungmin Yang"
                    },
                    {
                        "name": "Donguk Lim"
                    },
                    {
                        "name": "Jaeyeon Kim"
                    },
                    {
                        "name": "Wooksu Shin"
                    },
                    {
                        "name": "Bo-Kyeong Kim"
                    },
                    {
                        "name": "Yong Jae Lee"
                    },
                    {
                        "name": "Tae-Ho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Tae-Ho Kim"
                },
                "author": "Tae-Ho Kim",
                "arxiv_comment": "accepted at CVPR 2025 Workshop on ELVM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00474v1",
                "updated": "2025-04-01T07:04:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    7,
                    4,
                    30,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T07:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    7,
                    4,
                    30,
                    1,
                    91,
                    0
                ],
                "title": "High specific impulse electrospray propulsion with small capillary\n  emitters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High specific impulse electrospray propulsion with small capillary\n  emitters"
                },
                "summary": "This study demonstrates the feasibility of using smaller capillary emitters\nto achieve higher specific impulse ($I_\\text{sp}$) in electrospray propulsion.\nFour ionic liquids were characterized using capillary emitters with tip\ndiameters from 15 to 50 $\\mu$m. Smaller diameter capillaries produced smaller\nand more stable Taylor cones. This stabilization enabled steady cone-jet\noperation at significantly lower flow rates compared to larger emitters. This\nwas unexpected because when the jet diameter is much smaller than far-field\ngeometric features, the minimum flow rate is thought to be solely determined by\nthe physical properties of the propellant. Using the smaller emitters and\nacceleration voltages of 10 kV, specific impulses up to 3000 s could be\nachieved with efficiencies above 50%, approximately doubling the $I_\\text{sp}$\nobserved with larger emitters. For one of the liquids and the smallest\nemitters, the beam consisted solely of ions at the lowest flow rates, similarly\nto studies using externally wetted and porous emitters. Another important\nfinding was that at sufficiently low flow rates, a significant fraction of the\npropellant fed to the emitter is not accelerated by the electrostatic field.\nThese propellant losses make the time-of-flight technique unreliable for\ndetermining the $I_\\text{sp}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study demonstrates the feasibility of using smaller capillary emitters\nto achieve higher specific impulse ($I_\\text{sp}$) in electrospray propulsion.\nFour ionic liquids were characterized using capillary emitters with tip\ndiameters from 15 to 50 $\\mu$m. Smaller diameter capillaries produced smaller\nand more stable Taylor cones. This stabilization enabled steady cone-jet\noperation at significantly lower flow rates compared to larger emitters. This\nwas unexpected because when the jet diameter is much smaller than far-field\ngeometric features, the minimum flow rate is thought to be solely determined by\nthe physical properties of the propellant. Using the smaller emitters and\nacceleration voltages of 10 kV, specific impulses up to 3000 s could be\nachieved with efficiencies above 50%, approximately doubling the $I_\\text{sp}$\nobserved with larger emitters. For one of the liquids and the smallest\nemitters, the beam consisted solely of ions at the lowest flow rates, similarly\nto studies using externally wetted and porous emitters. Another important\nfinding was that at sufficiently low flow rates, a significant fraction of the\npropellant fed to the emitter is not accelerated by the electrostatic field.\nThese propellant losses make the time-of-flight technique unreliable for\ndetermining the $I_\\text{sp}$."
                },
                "authors": [
                    {
                        "name": "Manel Caballero-Prez"
                    },
                    {
                        "name": "Marc Galobardes-Esteban"
                    },
                    {
                        "name": "Manuel Gamero-Castao"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Gamero-Castao"
                },
                "author": "Manuel Gamero-Castao",
                "arxiv_comment": "29 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24358v1",
                "updated": "2025-03-31T17:37:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:37:32Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "title": "SQuat: Subspace-orthogonal KV Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQuat: Subspace-orthogonal KV Cache Quantization"
                },
                "summary": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Ligong Han"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Akash Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Akash Srivastava"
                },
                "author": "Akash Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24007v1",
                "updated": "2025-03-31T12:32:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T12:32:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting"
                },
                "summary": "Covariates play an indispensable role in practical time series forecasting,\noffering rich context from the past and sometimes extending into the future.\nHowever, their availability varies depending on the scenario, and situations\noften involve multiple target variables simultaneously. Moreover, the\ncross-variate dependencies between them are multi-granular, with some\ncovariates having a short-term impact on target variables and others showing\nlong-term correlations. This heterogeneity and the intricate dependencies\narising in covariate-informed forecasting present significant challenges to\nexisting deep models. To address these issues, we propose CITRAS, a patch-based\nTransformer that flexibly leverages multiple targets and covariates covering\nboth the past and the future forecasting horizon. While preserving the strong\nautoregressive capabilities of the canonical Transformer, CITRAS introduces two\nnovel mechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift\nand Attention Score Smoothing. KV Shift seamlessly incorporates future known\ncovariates into the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing transforms locally\naccurate patch-wise cross-variate dependencies into global variate-level\ndependencies by smoothing the past series of attention scores. Experimentally,\nCITRAS achieves state-of-the-art performance in both covariate-informed and\nmultivariate forecasting, demonstrating its versatile ability to leverage\ncross-variate dependency for improved forecasting accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Covariates play an indispensable role in practical time series forecasting,\noffering rich context from the past and sometimes extending into the future.\nHowever, their availability varies depending on the scenario, and situations\noften involve multiple target variables simultaneously. Moreover, the\ncross-variate dependencies between them are multi-granular, with some\ncovariates having a short-term impact on target variables and others showing\nlong-term correlations. This heterogeneity and the intricate dependencies\narising in covariate-informed forecasting present significant challenges to\nexisting deep models. To address these issues, we propose CITRAS, a patch-based\nTransformer that flexibly leverages multiple targets and covariates covering\nboth the past and the future forecasting horizon. While preserving the strong\nautoregressive capabilities of the canonical Transformer, CITRAS introduces two\nnovel mechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift\nand Attention Score Smoothing. KV Shift seamlessly incorporates future known\ncovariates into the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing transforms locally\naccurate patch-wise cross-variate dependencies into global variate-level\ndependencies by smoothing the past series of attention scores. Experimentally,\nCITRAS achieves state-of-the-art performance in both covariate-informed and\nmultivariate forecasting, demonstrating its versatile ability to leverage\ncross-variate dependency for improved forecasting accuracy."
                },
                "authors": [
                    {
                        "name": "Yosuke Yamaguchi"
                    },
                    {
                        "name": "Issei Suemitsu"
                    },
                    {
                        "name": "Wenpeng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Wenpeng Wei"
                },
                "author": "Wenpeng Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24000v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24000v1",
                "updated": "2025-03-31T12:23:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    23,
                    31,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T12:23:31Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    23,
                    31,
                    0,
                    90,
                    0
                ],
                "title": "Rethinking Key-Value Cache Compression Techniques for Large Language\n  Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Key-Value Cache Compression Techniques for Large Language\n  Model Serving"
                },
                "summary": "Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a\npromising technique to optimize Large Language Model (LLM) serving. It\nprimarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to\nreduce the computation cost. Despite the development of many compression\nalgorithms, their applications in production environments are still not\nprevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache}\ncompression solutions from a practical perspective. Our contributions are\nthree-fold. First, we comprehensively review existing algorithmic designs and\nbenchmark studies for \\texttt{KV} \\texttt{cache} compression and identify\nmissing pieces in their performance measurement, which could hinder their\nadoption in practice. Second, we empirically evaluate representative\n\\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that\naffect the computational efficiency: (1) while compressing \\texttt{KV}\n\\texttt{cache} can reduce memory consumption, current implementations (e.g.,\nFlashAttention, PagedAttention) do not optimize for production-level LLM\nserving, resulting in suboptimal throughput performance; (2) compressing\n\\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased\nend-to-end latency. We further investigate the accuracy performance of\nindividual samples rather than the overall performance, revealing the intrinsic\nlimitations in \\texttt{KV} \\texttt{cache} compression when handling specific\nLLM tasks. Third, we provide tools to shed light on future \\texttt{KV}\n\\texttt{cache} compression studies and facilitate their practical deployment in\nproduction. They are open-sourced in\n\\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a\npromising technique to optimize Large Language Model (LLM) serving. It\nprimarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to\nreduce the computation cost. Despite the development of many compression\nalgorithms, their applications in production environments are still not\nprevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache}\ncompression solutions from a practical perspective. Our contributions are\nthree-fold. First, we comprehensively review existing algorithmic designs and\nbenchmark studies for \\texttt{KV} \\texttt{cache} compression and identify\nmissing pieces in their performance measurement, which could hinder their\nadoption in practice. Second, we empirically evaluate representative\n\\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that\naffect the computational efficiency: (1) while compressing \\texttt{KV}\n\\texttt{cache} can reduce memory consumption, current implementations (e.g.,\nFlashAttention, PagedAttention) do not optimize for production-level LLM\nserving, resulting in suboptimal throughput performance; (2) compressing\n\\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased\nend-to-end latency. We further investigate the accuracy performance of\nindividual samples rather than the overall performance, revealing the intrinsic\nlimitations in \\texttt{KV} \\texttt{cache} compression when handling specific\nLLM tasks. Third, we provide tools to shed light on future \\texttt{KV}\n\\texttt{cache} compression studies and facilitate their practical deployment in\nproduction. They are open-sourced in\n\\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}."
                },
                "authors": [
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Yonggang Wen"
                    }
                ],
                "author_detail": {
                    "name": "Yonggang Wen"
                },
                "author": "Yonggang Wen",
                "arxiv_comment": "21 pages, 18 figures, published to MLSys2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24000v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24000v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23988v1",
                "updated": "2025-03-31T11:58:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    58,
                    37,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T11:58:37Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    58,
                    37,
                    0,
                    90,
                    0
                ],
                "title": "Deep Learning Model Deployment in Multiple Cloud Providers: an\n  Exploratory Study Using Low Computing Power Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning Model Deployment in Multiple Cloud Providers: an\n  Exploratory Study Using Low Computing Power Environments"
                },
                "summary": "The deployment of Machine Learning models at cloud have grown by tech\ncompanies. Hardware requirements are higher when these models involve Deep\nLearning (DL) techniques and the cloud providers' costs may be a barrier. We\nexplore deploying DL models using for experiments the GECToR model, a DL\nsolution for Grammatical Error Correction, across three of the major cloud\nplatforms (AWS, Google Cloud, Azure). We evaluate real-time latency, hardware\nusage and cost at each cloud provider by 7 execution environments with 10\nexperiments reproduced. We found that while GPUs excel in performance, they had\nan average cost 300% higher than solutions without GPU. Our analysis also\nidentifies that processor cache size is crucial for cost-effective CPU\ndeployments, enabling over 50% of cost reduction compared to GPUs. This study\ndemonstrates the feasibility and affordability of cloud-based DL inference\nsolutions without GPUs, benefiting resource-constrained users like startups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Machine Learning models at cloud have grown by tech\ncompanies. Hardware requirements are higher when these models involve Deep\nLearning (DL) techniques and the cloud providers' costs may be a barrier. We\nexplore deploying DL models using for experiments the GECToR model, a DL\nsolution for Grammatical Error Correction, across three of the major cloud\nplatforms (AWS, Google Cloud, Azure). We evaluate real-time latency, hardware\nusage and cost at each cloud provider by 7 execution environments with 10\nexperiments reproduced. We found that while GPUs excel in performance, they had\nan average cost 300% higher than solutions without GPU. Our analysis also\nidentifies that processor cache size is crucial for cost-effective CPU\ndeployments, enabling over 50% of cost reduction compared to GPUs. This study\ndemonstrates the feasibility and affordability of cloud-based DL inference\nsolutions without GPUs, benefiting resource-constrained users like startups."
                },
                "authors": [
                    {
                        "name": "Elayne Lemos"
                    },
                    {
                        "name": "Rodrigo Oliveira"
                    },
                    {
                        "name": "Jairson Rodrigues"
                    },
                    {
                        "name": "Rosalvo F. Oliveira Neto"
                    }
                ],
                "author_detail": {
                    "name": "Rosalvo F. Oliveira Neto"
                },
                "author": "Rosalvo F. Oliveira Neto",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68U01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; I.2.0; B.8.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23956v1",
                "updated": "2025-03-31T11:13:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T11:13:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference"
                },
                "summary": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches."
                },
                "authors": [
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Hao Zou"
                    },
                    {
                        "name": "Bochen Wang"
                    },
                    {
                        "name": "Ye Xi"
                    },
                    {
                        "name": "Zhen Xie"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18334v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18334v2",
                "updated": "2025-03-31T10:28:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    10,
                    28,
                    4,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-24T04:32:35Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    4,
                    32,
                    35,
                    0,
                    83,
                    0
                ],
                "title": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language\n  Models"
                },
                "summary": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n\"Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n\"Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability."
                },
                "authors": [
                    {
                        "name": "Haotian Zhai"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Can Zhang"
                    },
                    {
                        "name": "Tianming Sha"
                    },
                    {
                        "name": "Ruirui Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruirui Li"
                },
                "author": "Ruirui Li",
                "arxiv_comment": "Accepted by ICME 2025 and ICLR 2025 Workshop on Foundation Models in\n  the Wild",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18334v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23897v1",
                "updated": "2025-03-31T09:46:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    46,
                    56,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T09:46:56Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    46,
                    56,
                    0,
                    90,
                    0
                ],
                "title": "Training-Free Text-Guided Image Editing with Visual Autoregressive Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Text-Guided Image Editing with Visual Autoregressive Model"
                },
                "summary": "Text-guided image editing is an essential task that enables users to modify\nimages through natural language descriptions. Recent advances in diffusion\nmodels and rectified flows have significantly improved editing quality,\nprimarily relying on inversion techniques to extract structured noise from\ninput images. However, inaccuracies in inversion can propagate errors, leading\nto unintended modifications and compromising fidelity. Moreover, even with\nperfect inversion, the entanglement between textual prompts and image features\noften results in global changes when only local edits are intended. To address\nthese challenges, we propose a novel text-guided image editing framework based\non VAR (Visual AutoRegressive modeling), which eliminates the need for explicit\ninversion while ensuring precise and controlled modifications. Our method\nintroduces a caching mechanism that stores token indices and probability\ndistributions from the original image, capturing the relationship between the\nsource prompt and the image. Using this cache, we design an adaptive\nfine-grained masking strategy that dynamically identifies and constrains\nmodifications to relevant regions, preventing unintended changes. A token\nreassembling approach further refines the editing process, enhancing diversity,\nfidelity, and control. Our framework operates in a training-free manner and\nachieves high-fidelity editing with faster inference speeds, processing a 1K\nresolution image in as fast as 1.2 seconds. Extensive experiments demonstrate\nthat our method achieves performance comparable to, or even surpassing,\nexisting diffusion- and rectified flow-based approaches in both quantitative\nmetrics and visual quality. The code will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-guided image editing is an essential task that enables users to modify\nimages through natural language descriptions. Recent advances in diffusion\nmodels and rectified flows have significantly improved editing quality,\nprimarily relying on inversion techniques to extract structured noise from\ninput images. However, inaccuracies in inversion can propagate errors, leading\nto unintended modifications and compromising fidelity. Moreover, even with\nperfect inversion, the entanglement between textual prompts and image features\noften results in global changes when only local edits are intended. To address\nthese challenges, we propose a novel text-guided image editing framework based\non VAR (Visual AutoRegressive modeling), which eliminates the need for explicit\ninversion while ensuring precise and controlled modifications. Our method\nintroduces a caching mechanism that stores token indices and probability\ndistributions from the original image, capturing the relationship between the\nsource prompt and the image. Using this cache, we design an adaptive\nfine-grained masking strategy that dynamically identifies and constrains\nmodifications to relevant regions, preventing unintended changes. A token\nreassembling approach further refines the editing process, enhancing diversity,\nfidelity, and control. Our framework operates in a training-free manner and\nachieves high-fidelity editing with faster inference speeds, processing a 1K\nresolution image in as fast as 1.2 seconds. Extensive experiments demonstrate\nthat our method achieves performance comparable to, or even surpassing,\nexisting diffusion- and rectified flow-based approaches in both quantitative\nmetrics and visual quality. The code will be released."
                },
                "authors": [
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Lanqing Guo"
                    },
                    {
                        "name": "Zhihao Li"
                    },
                    {
                        "name": "Jiaxing Huang"
                    },
                    {
                        "name": "Pichao Wang"
                    },
                    {
                        "name": "Bihan Wen"
                    },
                    {
                        "name": "Jian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Wang"
                },
                "author": "Jian Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v4",
                "updated": "2025-03-31T03:28:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    3,
                    28,
                    44,
                    0,
                    90,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21817v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21817v2",
                "updated": "2025-03-31T02:19:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    2,
                    19,
                    29,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-26T04:16:48Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    4,
                    16,
                    48,
                    2,
                    85,
                    0
                ],
                "title": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping"
                },
                "summary": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency."
                },
                "authors": [
                    {
                        "name": "Weili Zeng"
                    },
                    {
                        "name": "Ziyuan Huang"
                    },
                    {
                        "name": "Kaixiang Ji"
                    },
                    {
                        "name": "Yichao Yan"
                    }
                ],
                "author_detail": {
                    "name": "Yichao Yan"
                },
                "author": "Yichao Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21817v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21817v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10270v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10270v2",
                "updated": "2025-03-30T11:14:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    11,
                    14,
                    17,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-13T11:26:45Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "title": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing"
                },
                "summary": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit"
                },
                "authors": [
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Wenteng Chen"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "17 pages,fix figure mistake(inv/fwd skipping) in fig2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10270v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10270v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23397v1",
                "updated": "2025-03-30T11:09:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    11,
                    9,
                    6,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-30T11:09:06Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    11,
                    9,
                    6,
                    6,
                    89,
                    0
                ],
                "title": "FB$^+$-tree: A Memory-Optimized B$^+$-tree with Latch-Free Update",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FB$^+$-tree: A Memory-Optimized B$^+$-tree with Latch-Free Update"
                },
                "summary": "B$^+$-trees are prevalent in traditional database systems due to their\nversatility and balanced structure. While binary search is typically utilized\nfor branch operations, it may lead to inefficient cache utilization in\nmain-memory scenarios. In contrast, trie-based index structures drive branch\noperations through prefix matching. While these structures generally produce\nfewer cache misses and are thus increasingly popular, they may underperform in\nrange scans because of frequent pointer chasing. This paper proposes a new\nhigh-performance B$^+$-tree variant called \\textbf{Feature B$^+$-tree\n(FB$^+$-tree)}. Similar to employing bit or byte for branch operation in tries,\nFB$^+$-tree progressively considers several bytes following the common prefix\non each level of its inner nodes\\textemdash referred to as features, which\nallows FB$^+$-tree to benefit from prefix skewness. FB$^+$-tree blurs the lines\nbetween B$^+$-trees and tries, while still retaining balance. In the best case,\nFB$^+$-tree almost becomes a trie, whereas in the worst case, it continues to\nfunction as a B$^+$-tree. Meanwhile, a crafted synchronization protocol that\ncombines the link technique and optimistic lock is designed to support\nefficient concurrent index access. Distinctively, FB$^+$-tree leverages subtle\natomic operations seamlessly coordinated with optimistic lock to facilitate\nlatch-free updates, which can be easily extended to other structures. Intensive\nexperiments on multiple workload-dataset combinations demonstrate that\nFB$^+$-tree shows comparable lookup performance to state-of-the-art trie-based\nindexes and outperforms popular B$^+$-trees by 2.3x$\\ \\sim\\ $3.7x under 96\nthreads. FB$^+$-tree also exhibits significant potential on other workloads,\nespecially update workloads under contention and scan workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "B$^+$-trees are prevalent in traditional database systems due to their\nversatility and balanced structure. While binary search is typically utilized\nfor branch operations, it may lead to inefficient cache utilization in\nmain-memory scenarios. In contrast, trie-based index structures drive branch\noperations through prefix matching. While these structures generally produce\nfewer cache misses and are thus increasingly popular, they may underperform in\nrange scans because of frequent pointer chasing. This paper proposes a new\nhigh-performance B$^+$-tree variant called \\textbf{Feature B$^+$-tree\n(FB$^+$-tree)}. Similar to employing bit or byte for branch operation in tries,\nFB$^+$-tree progressively considers several bytes following the common prefix\non each level of its inner nodes\\textemdash referred to as features, which\nallows FB$^+$-tree to benefit from prefix skewness. FB$^+$-tree blurs the lines\nbetween B$^+$-trees and tries, while still retaining balance. In the best case,\nFB$^+$-tree almost becomes a trie, whereas in the worst case, it continues to\nfunction as a B$^+$-tree. Meanwhile, a crafted synchronization protocol that\ncombines the link technique and optimistic lock is designed to support\nefficient concurrent index access. Distinctively, FB$^+$-tree leverages subtle\natomic operations seamlessly coordinated with optimistic lock to facilitate\nlatch-free updates, which can be easily extended to other structures. Intensive\nexperiments on multiple workload-dataset combinations demonstrate that\nFB$^+$-tree shows comparable lookup performance to state-of-the-art trie-based\nindexes and outperforms popular B$^+$-trees by 2.3x$\\ \\sim\\ $3.7x under 96\nthreads. FB$^+$-tree also exhibits significant potential on other workloads,\nespecially update workloads under contention and scan workloads."
                },
                "authors": [
                    {
                        "name": "Yuan Chen"
                    },
                    {
                        "name": "Ao Li"
                    },
                    {
                        "name": "Wenhai Li"
                    },
                    {
                        "name": "Lingfeng Deng"
                    }
                ],
                "author_detail": {
                    "name": "Lingfeng Deng"
                },
                "author": "Lingfeng Deng",
                "arxiv_comment": "14 pages,17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23388v1",
                "updated": "2025-03-30T10:34:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    10,
                    34,
                    45,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-30T10:34:45Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    10,
                    34,
                    45,
                    6,
                    89,
                    0
                ],
                "title": "COSMIC: Clique-Oriented Semantic Multi-space Integration for Robust CLIP\n  Test-Time Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COSMIC: Clique-Oriented Semantic Multi-space Integration for Robust CLIP\n  Test-Time Adaptation"
                },
                "summary": "Recent vision-language models (VLMs) face significant challenges in test-time\nadaptation to novel domains. While cache-based methods show promise by\nleveraging historical information, they struggle with both caching unreliable\nfeature-label pairs and indiscriminately using single-class information during\nquerying, significantly compromising adaptation accuracy. To address these\nlimitations, we propose COSMIC (Clique-Oriented Semantic Multi-space\nIntegration for CLIP), a robust test-time adaptation framework that enhances\nadaptability through multi-granular, cross-modal semantic caching and\ngraph-based querying mechanisms. Our framework introduces two key innovations:\nDual Semantics Graph (DSG) and Clique Guided Hyper-class (CGH). The Dual\nSemantics Graph constructs complementary semantic spaces by incorporating\ntextual features, coarse-grained CLIP features, and fine-grained DINOv2\nfeatures to capture rich semantic relationships. Building upon these dual\ngraphs, the Clique Guided Hyper-class component leverages structured class\nrelationships to enhance prediction robustness through correlated class\nselection. Extensive experiments demonstrate COSMIC's superior performance\nacross multiple benchmarks, achieving significant improvements over\nstate-of-the-art methods: 15.81% gain on out-of-distribution tasks and 5.33% on\ncross-domain generation with CLIP RN-50. Code is available at\ngithub.com/hf618/COSMIC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent vision-language models (VLMs) face significant challenges in test-time\nadaptation to novel domains. While cache-based methods show promise by\nleveraging historical information, they struggle with both caching unreliable\nfeature-label pairs and indiscriminately using single-class information during\nquerying, significantly compromising adaptation accuracy. To address these\nlimitations, we propose COSMIC (Clique-Oriented Semantic Multi-space\nIntegration for CLIP), a robust test-time adaptation framework that enhances\nadaptability through multi-granular, cross-modal semantic caching and\ngraph-based querying mechanisms. Our framework introduces two key innovations:\nDual Semantics Graph (DSG) and Clique Guided Hyper-class (CGH). The Dual\nSemantics Graph constructs complementary semantic spaces by incorporating\ntextual features, coarse-grained CLIP features, and fine-grained DINOv2\nfeatures to capture rich semantic relationships. Building upon these dual\ngraphs, the Clique Guided Hyper-class component leverages structured class\nrelationships to enhance prediction robustness through correlated class\nselection. Extensive experiments demonstrate COSMIC's superior performance\nacross multiple benchmarks, achieving significant improvements over\nstate-of-the-art methods: 15.81% gain on out-of-distribution tasks and 5.33% on\ncross-domain generation with CLIP RN-50. Code is available at\ngithub.com/hf618/COSMIC."
                },
                "authors": [
                    {
                        "name": "Fanding Huang"
                    },
                    {
                        "name": "Jingyan Jiang"
                    },
                    {
                        "name": "Qinting Jiang"
                    },
                    {
                        "name": "Hebei Li"
                    },
                    {
                        "name": "Faisal Nadeem Khan"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "arxiv_comment": "Accepted to CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16588v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16588v2",
                "updated": "2025-03-30T09:46:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    9,
                    46,
                    34,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-20T17:37:15Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    37,
                    15,
                    3,
                    79,
                    0
                ],
                "title": "A Unified Framework for Quantitative Cache Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Quantitative Cache Analysis"
                },
                "summary": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU."
                },
                "authors": [
                    {
                        "name": "Sophie Kahlen"
                    },
                    {
                        "name": "Jan Reineke"
                    }
                ],
                "author_detail": {
                    "name": "Jan Reineke"
                },
                "author": "Jan Reineke",
                "arxiv_comment": "Extended version of RTAS 2025 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16588v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16588v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16897v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16897v2",
                "updated": "2025-03-30T09:19:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    9,
                    19,
                    53,
                    6,
                    89,
                    0
                ],
                "published": "2024-12-22T07:14:45Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "title": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context"
                },
                "summary": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC"
                },
                "authors": [
                    {
                        "name": "Shuai Lyu"
                    },
                    {
                        "name": "Rongchen Zhang"
                    },
                    {
                        "name": "Zeqi Ma"
                    },
                    {
                        "name": "Fangjian Liao"
                    },
                    {
                        "name": "Dongmei Mo"
                    },
                    {
                        "name": "Waikeung Wong"
                    }
                ],
                "author_detail": {
                    "name": "Waikeung Wong"
                },
                "author": "Waikeung Wong",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16897v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16897v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12820v2",
                "updated": "2025-03-30T08:13:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    13,
                    50,
                    6,
                    89,
                    0
                ],
                "published": "2024-07-01T13:05:42Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    13,
                    5,
                    42,
                    0,
                    183,
                    0
                ],
                "title": "PQCache: Product Quantization-based KVCache for Long Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PQCache: Product Quantization-based KVCache for Long Context LLM\n  Inference"
                },
                "summary": "As the field of Large Language Models (LLMs) continues to evolve, the context\nlength in inference is steadily growing. Key-Value Cache (KVCache), the\nintermediate representations of tokens within LLM inference, has now become the\nprimary memory bottleneck due to limited GPU memory. Current methods\nselectively determine suitable keys and values for self-attention computation\nin LLMs to address the issue. However, they either fall short in maintaining\nmodel quality or result in high serving latency. Drawing inspiration from\nadvanced embedding retrieval techniques prevalent in the data management\ncommunity, we consider the storage and retrieval of KVCache as a typical\nembedding retrieval problem. We propose PQCache, which employs Product\nQuantization (PQ) to manage KVCache, maintaining model quality while ensuring\nlow serving latency. During the prefilling phase, we apply PQ to tokens' keys\nfor each LLM layer and head. During the autoregressive decoding phase, we use\nPQ codes and centroids to approximately identify important preceding tokens,\nthen fetch the corresponding key-value pairs for self-attention computation.\nThrough meticulous design of overlapping and caching, we minimize any\nadditional computation and communication overhead during both phases. Extensive\nexperiments demonstrate that PQCache achieves both effectiveness and\nefficiency, with 4.60% score improvement over existing methods on InfiniteBench\nand low system latency in both prefilling and decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the field of Large Language Models (LLMs) continues to evolve, the context\nlength in inference is steadily growing. Key-Value Cache (KVCache), the\nintermediate representations of tokens within LLM inference, has now become the\nprimary memory bottleneck due to limited GPU memory. Current methods\nselectively determine suitable keys and values for self-attention computation\nin LLMs to address the issue. However, they either fall short in maintaining\nmodel quality or result in high serving latency. Drawing inspiration from\nadvanced embedding retrieval techniques prevalent in the data management\ncommunity, we consider the storage and retrieval of KVCache as a typical\nembedding retrieval problem. We propose PQCache, which employs Product\nQuantization (PQ) to manage KVCache, maintaining model quality while ensuring\nlow serving latency. During the prefilling phase, we apply PQ to tokens' keys\nfor each LLM layer and head. During the autoregressive decoding phase, we use\nPQ codes and centroids to approximately identify important preceding tokens,\nthen fetch the corresponding key-value pairs for self-attention computation.\nThrough meticulous design of overlapping and caching, we minimize any\nadditional computation and communication overhead during both phases. Extensive\nexperiments demonstrate that PQCache achieves both effectiveness and\nefficiency, with 4.60% score improvement over existing methods on InfiniteBench\nand low system latency in both prefilling and decoding."
                },
                "authors": [
                    {
                        "name": "Hailin Zhang"
                    },
                    {
                        "name": "Xiaodong Ji"
                    },
                    {
                        "name": "Yilin Chen"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Xupeng Miao"
                    },
                    {
                        "name": "Xiaonan Nie"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.10486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10486v1",
                "updated": "2025-04-14T17:59:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    59,
                    58,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T17:59:58Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    59,
                    58,
                    0,
                    104,
                    0
                ],
                "title": "DNF-Avatar: Distilling Neural Fields for Real-time Animatable Avatar\n  Relighting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DNF-Avatar: Distilling Neural Fields for Real-time Animatable Avatar\n  Relighting"
                },
                "summary": "Creating relightable and animatable human avatars from monocular videos is a\nrising research topic with a range of applications, e.g. virtual reality,\nsports, and video games. Previous works utilize neural fields together with\nphysically based rendering (PBR), to estimate geometry and disentangle\nappearance properties of human avatars. However, one drawback of these methods\nis the slow rendering speed due to the expensive Monte Carlo ray tracing. To\ntackle this problem, we proposed to distill the knowledge from implicit neural\nfields (teacher) to explicit 2D Gaussian splatting (student) representation to\ntake advantage of the fast rasterization property of Gaussian splatting. To\navoid ray-tracing, we employ the split-sum approximation for PBR appearance. We\nalso propose novel part-wise ambient occlusion probes for shadow computation.\nShadow prediction is achieved by querying these probes only once per pixel,\nwhich paves the way for real-time relighting of avatars. These techniques\ncombined give high-quality relighting results with realistic shadow effects.\nOur experiments demonstrate that the proposed student model achieves comparable\nor even better relighting results with our teacher model while being 370 times\nfaster at inference time, achieving a 67 FPS rendering speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating relightable and animatable human avatars from monocular videos is a\nrising research topic with a range of applications, e.g. virtual reality,\nsports, and video games. Previous works utilize neural fields together with\nphysically based rendering (PBR), to estimate geometry and disentangle\nappearance properties of human avatars. However, one drawback of these methods\nis the slow rendering speed due to the expensive Monte Carlo ray tracing. To\ntackle this problem, we proposed to distill the knowledge from implicit neural\nfields (teacher) to explicit 2D Gaussian splatting (student) representation to\ntake advantage of the fast rasterization property of Gaussian splatting. To\navoid ray-tracing, we employ the split-sum approximation for PBR appearance. We\nalso propose novel part-wise ambient occlusion probes for shadow computation.\nShadow prediction is achieved by querying these probes only once per pixel,\nwhich paves the way for real-time relighting of avatars. These techniques\ncombined give high-quality relighting results with realistic shadow effects.\nOur experiments demonstrate that the proposed student model achieves comparable\nor even better relighting results with our teacher model while being 370 times\nfaster at inference time, achieving a 67 FPS rendering speed."
                },
                "authors": [
                    {
                        "name": "Zeren Jiang"
                    },
                    {
                        "name": "Shaofei Wang"
                    },
                    {
                        "name": "Siyu Tang"
                    }
                ],
                "author_detail": {
                    "name": "Siyu Tang"
                },
                "author": "Siyu Tang",
                "arxiv_comment": "16 pages, 8 figures, Project pages:\n  https://jzr99.github.io/DNF-Avatar/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10481v1",
                "updated": "2025-04-14T17:59:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    59,
                    36,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T17:59:36Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    59,
                    36,
                    0,
                    104,
                    0
                ],
                "title": "xVerify: Efficient Answer Verifier for Reasoning Model Evaluations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xVerify: Efficient Answer Verifier for Reasoning Model Evaluations"
                },
                "summary": "With the release of the o1 model by OpenAI, reasoning models adopting slow\nthinking strategies have gradually emerged. As the responses generated by such\nmodels often include complex reasoning, intermediate steps, and\nself-reflection, existing evaluation methods are often inadequate. They\nstruggle to determine whether the LLM output is truly equivalent to the\nreference answer, and also have difficulty identifying and extracting the final\nanswer from long, complex responses. To address this issue, we propose xVerify,\nan efficient answer verifier for reasoning model evaluations. xVerify\ndemonstrates strong capability in equivalence judgment, enabling it to\neffectively determine whether the answers produced by reasoning models are\nequivalent to reference answers across various types of objective questions. To\ntrain and evaluate xVerify, we construct the VAR dataset by collecting\nquestion-answer pairs generated by multiple LLMs across various datasets,\nleveraging multiple reasoning models and challenging evaluation sets designed\nspecifically for reasoning model assessment. A multi-round annotation process\nis employed to ensure label accuracy. Based on the VAR dataset, we train\nmultiple xVerify models of different scales. In evaluation experiments\nconducted on both the test set and generalization set, all xVerify models\nachieve overall F1 scores and accuracy exceeding 95\\%. Notably, the smallest\nvariant, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o,\nwhile xVerify-3B-Ib surpasses GPT-4o in overall performance. These results\nvalidate the effectiveness and generalizability of xVerify.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the release of the o1 model by OpenAI, reasoning models adopting slow\nthinking strategies have gradually emerged. As the responses generated by such\nmodels often include complex reasoning, intermediate steps, and\nself-reflection, existing evaluation methods are often inadequate. They\nstruggle to determine whether the LLM output is truly equivalent to the\nreference answer, and also have difficulty identifying and extracting the final\nanswer from long, complex responses. To address this issue, we propose xVerify,\nan efficient answer verifier for reasoning model evaluations. xVerify\ndemonstrates strong capability in equivalence judgment, enabling it to\neffectively determine whether the answers produced by reasoning models are\nequivalent to reference answers across various types of objective questions. To\ntrain and evaluate xVerify, we construct the VAR dataset by collecting\nquestion-answer pairs generated by multiple LLMs across various datasets,\nleveraging multiple reasoning models and challenging evaluation sets designed\nspecifically for reasoning model assessment. A multi-round annotation process\nis employed to ensure label accuracy. Based on the VAR dataset, we train\nmultiple xVerify models of different scales. In evaluation experiments\nconducted on both the test set and generalization set, all xVerify models\nachieve overall F1 scores and accuracy exceeding 95\\%. Notably, the smallest\nvariant, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o,\nwhile xVerify-3B-Ib surpasses GPT-4o in overall performance. These results\nvalidate the effectiveness and generalizability of xVerify."
                },
                "authors": [
                    {
                        "name": "Ding Chen"
                    },
                    {
                        "name": "Qingchen Yu"
                    },
                    {
                        "name": "Pengyuan Wang"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Xinchi Li"
                    },
                    {
                        "name": "Minchuan Yang"
                    },
                    {
                        "name": "Zhiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Li"
                },
                "author": "Zhiyu Li",
                "arxiv_comment": "32 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13423v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13423v2",
                "updated": "2025-04-14T17:59:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    59,
                    35,
                    0,
                    104,
                    0
                ],
                "published": "2025-03-17T17:53:23Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    17,
                    53,
                    23,
                    0,
                    76,
                    0
                ],
                "title": "SuperBPE: Space Travel for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperBPE: Space Travel for Language Models"
                },
                "summary": "The assumption across nearly all language model (LM) tokenization schemes is\nthat tokens should be subwords, i.e., contained within word boundaries. While\nproviding a seemingly reasonable inductive bias, is this common practice\nlimiting the potential of modern LMs? Whitespace is not a reliable delimiter of\nmeaning, as evidenced by multi-word expressions (e.g., \"by the way\"),\ncrosslingual variation in the number of words needed to express a concept\n(e.g., \"spacesuit helmet\" in German is \"raumanzughelm\"), and languages that do\nnot use whitespace at all (e.g., Chinese). To explore the potential of\ntokenization beyond subwords, we introduce a \"superword\" tokenizer, SuperBPE,\nwhich incorporates a simple pretokenization curriculum into the byte-pair\nencoding (BPE) algorithm to first learn subwords, then superwords that bridge\nwhitespace. This brings dramatic improvements in encoding efficiency: when\nfixing the vocabulary size to 200k, SuperBPE encodes a fixed piece of text with\nup to 33% fewer tokens than BPE on average. In experiments, we pretrain 8B\ntransformer LMs from scratch while fixing the model size, vocabulary size, and\ntrain compute, varying *only* the algorithm for learning the vocabulary. Our\nmodel trained with SuperBPE achieves an average +4.0% absolute improvement over\nthe BPE baseline across 30 downstream tasks (including +8.2% on MMLU), while\nsimultaneously requiring 27% less compute at inference time. In analysis, we\nfind that SuperBPE results in segmentations of text that are more uniform in\nper-token difficulty. Qualitatively, this may be because SuperBPE tokens often\ncapture common multi-word expressions that function semantically as a single\nunit. SuperBPE is a straightforward, local modification to tokenization that\nimproves both encoding efficiency and downstream performance, yielding better\nlanguage models overall.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The assumption across nearly all language model (LM) tokenization schemes is\nthat tokens should be subwords, i.e., contained within word boundaries. While\nproviding a seemingly reasonable inductive bias, is this common practice\nlimiting the potential of modern LMs? Whitespace is not a reliable delimiter of\nmeaning, as evidenced by multi-word expressions (e.g., \"by the way\"),\ncrosslingual variation in the number of words needed to express a concept\n(e.g., \"spacesuit helmet\" in German is \"raumanzughelm\"), and languages that do\nnot use whitespace at all (e.g., Chinese). To explore the potential of\ntokenization beyond subwords, we introduce a \"superword\" tokenizer, SuperBPE,\nwhich incorporates a simple pretokenization curriculum into the byte-pair\nencoding (BPE) algorithm to first learn subwords, then superwords that bridge\nwhitespace. This brings dramatic improvements in encoding efficiency: when\nfixing the vocabulary size to 200k, SuperBPE encodes a fixed piece of text with\nup to 33% fewer tokens than BPE on average. In experiments, we pretrain 8B\ntransformer LMs from scratch while fixing the model size, vocabulary size, and\ntrain compute, varying *only* the algorithm for learning the vocabulary. Our\nmodel trained with SuperBPE achieves an average +4.0% absolute improvement over\nthe BPE baseline across 30 downstream tasks (including +8.2% on MMLU), while\nsimultaneously requiring 27% less compute at inference time. In analysis, we\nfind that SuperBPE results in segmentations of text that are more uniform in\nper-token difficulty. Qualitatively, this may be because SuperBPE tokens often\ncapture common multi-word expressions that function semantically as a single\nunit. SuperBPE is a straightforward, local modification to tokenization that\nimproves both encoding efficiency and downstream performance, yielding better\nlanguage models overall."
                },
                "authors": [
                    {
                        "name": "Alisa Liu"
                    },
                    {
                        "name": "Jonathan Hayase"
                    },
                    {
                        "name": "Valentin Hofmann"
                    },
                    {
                        "name": "Sewoong Oh"
                    },
                    {
                        "name": "Noah A. Smith"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "arxiv_comment": "updated related work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13423v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13423v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10479v1",
                "updated": "2025-04-14T17:59:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    59,
                    25,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T17:59:25Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    59,
                    25,
                    0,
                    104,
                    0
                ],
                "title": "InternVL3: Exploring Advanced Training and Test-Time Recipes for\n  Open-Source Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InternVL3: Exploring Advanced Training and Test-Time Recipes for\n  Open-Source Multimodal Models"
                },
                "summary": "We introduce InternVL3, a significant advancement in the InternVL series\nfeaturing a native multimodal pre-training paradigm. Rather than adapting a\ntext-only large language model (LLM) into a multimodal large language model\n(MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and\nlinguistic capabilities from both diverse multimodal data and pure-text corpora\nduring a single pre-training stage. This unified training paradigm effectively\naddresses the complexities and alignment challenges commonly encountered in\nconventional post-hoc training pipelines for MLLMs. To further improve\nperformance and scalability, InternVL3 incorporates variable visual position\nencoding (V2PE) to support extended multimodal contexts, employs advanced\npost-training techniques such as supervised fine-tuning (SFT) and mixed\npreference optimization (MPO), and adopts test-time scaling strategies\nalongside an optimized training infrastructure. Extensive empirical evaluations\ndemonstrate that InternVL3 delivers superior performance across a wide range of\nmulti-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the\nMMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its\ncapabilities remain highly competitive with leading proprietary models,\nincluding ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also\nmaintaining strong pure-language proficiency. In pursuit of open-science\nprinciples, we will publicly release both the training data and model weights\nto foster further research and development in next-generation MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce InternVL3, a significant advancement in the InternVL series\nfeaturing a native multimodal pre-training paradigm. Rather than adapting a\ntext-only large language model (LLM) into a multimodal large language model\n(MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and\nlinguistic capabilities from both diverse multimodal data and pure-text corpora\nduring a single pre-training stage. This unified training paradigm effectively\naddresses the complexities and alignment challenges commonly encountered in\nconventional post-hoc training pipelines for MLLMs. To further improve\nperformance and scalability, InternVL3 incorporates variable visual position\nencoding (V2PE) to support extended multimodal contexts, employs advanced\npost-training techniques such as supervised fine-tuning (SFT) and mixed\npreference optimization (MPO), and adopts test-time scaling strategies\nalongside an optimized training infrastructure. Extensive empirical evaluations\ndemonstrate that InternVL3 delivers superior performance across a wide range of\nmulti-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the\nMMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its\ncapabilities remain highly competitive with leading proprietary models,\nincluding ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also\nmaintaining strong pure-language proficiency. In pursuit of open-science\nprinciples, we will publicly release both the training data and model weights\nto foster further research and development in next-generation MLLMs."
                },
                "authors": [
                    {
                        "name": "Jinguo Zhu"
                    },
                    {
                        "name": "Weiyun Wang"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Zhaoyang Liu"
                    },
                    {
                        "name": "Shenglong Ye"
                    },
                    {
                        "name": "Lixin Gu"
                    },
                    {
                        "name": "Yuchen Duan"
                    },
                    {
                        "name": "Hao Tian"
                    },
                    {
                        "name": "Weijie Su"
                    },
                    {
                        "name": "Jie Shao"
                    },
                    {
                        "name": "Zhangwei Gao"
                    },
                    {
                        "name": "Erfei Cui"
                    },
                    {
                        "name": "Yue Cao"
                    },
                    {
                        "name": "Yangzhou Liu"
                    },
                    {
                        "name": "Weiye Xu"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Han Lv"
                    },
                    {
                        "name": "Dengnian Chen"
                    },
                    {
                        "name": "Songze Li"
                    },
                    {
                        "name": "Yinan He"
                    },
                    {
                        "name": "Tan Jiang"
                    },
                    {
                        "name": "Jiapeng Luo"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Botian Shi"
                    },
                    {
                        "name": "Xingcheng Zhang"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Yingtong Xiong"
                    },
                    {
                        "name": "Wenwen Qu"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Penglong Jiao"
                    },
                    {
                        "name": "Lijun Wu"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Huipeng Deng"
                    },
                    {
                        "name": "Jiaye Ge"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Limin Wang"
                    },
                    {
                        "name": "Min Dou"
                    },
                    {
                        "name": "Lewei Lu"
                    },
                    {
                        "name": "Xizhou Zhu"
                    },
                    {
                        "name": "Tong Lu"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Jifeng Dai"
                    },
                    {
                        "name": "Wenhai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhai Wang"
                },
                "author": "Wenhai Wang",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00566v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00566v2",
                "updated": "2025-04-14T17:58:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    58,
                    10,
                    0,
                    104,
                    0
                ],
                "published": "2024-11-30T19:41:18Z",
                "published_parsed": [
                    2024,
                    11,
                    30,
                    19,
                    41,
                    18,
                    5,
                    335,
                    0
                ],
                "title": "Parameter estimation of microlensed gravitational waves with Conditional\n  Variational Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter estimation of microlensed gravitational waves with Conditional\n  Variational Autoencoders"
                },
                "summary": "Gravitational lensing of gravitational waves (GWs) provides a unique\nopportunity to study cosmology and astrophysics at multiple scales. Detecting\nmicrolensing signatures, in particular, requires efficient parameter estimation\nmethods due to the high computational cost of traditional Bayesian inference.\nIn this paper we explore the use of deep learning, namely Conditional\nVariational Autoencoders (CVAE), to estimate parameters of microlensed binary\nblack hole (simulated) waveforms. We find that our CVAE model yields accurate\nparameter estimation and significant computational savings compared to Bayesian\nmethods such as Bilby (up to five orders of magnitude faster inferences).\nMoreover, the incorporation of CVAE-generated priors into Bilby, based on the\n95% confidence intervals of the CVAE posterior for the lensing parameters,\nreduces Bilby's average runtime by around 48% without any penalty on accuracy.\nOur results suggest that a CVAE model is a promising tool for future\nlow-latency searches of lensed signals. Further applications to actual signals\nand integration with advanced pipelines could help extend the capabilities of\nGW observatories in detecting microlensing events.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational lensing of gravitational waves (GWs) provides a unique\nopportunity to study cosmology and astrophysics at multiple scales. Detecting\nmicrolensing signatures, in particular, requires efficient parameter estimation\nmethods due to the high computational cost of traditional Bayesian inference.\nIn this paper we explore the use of deep learning, namely Conditional\nVariational Autoencoders (CVAE), to estimate parameters of microlensed binary\nblack hole (simulated) waveforms. We find that our CVAE model yields accurate\nparameter estimation and significant computational savings compared to Bayesian\nmethods such as Bilby (up to five orders of magnitude faster inferences).\nMoreover, the incorporation of CVAE-generated priors into Bilby, based on the\n95% confidence intervals of the CVAE posterior for the lensing parameters,\nreduces Bilby's average runtime by around 48% without any penalty on accuracy.\nOur results suggest that a CVAE model is a promising tool for future\nlow-latency searches of lensed signals. Further applications to actual signals\nand integration with advanced pipelines could help extend the capabilities of\nGW observatories in detecting microlensing events."
                },
                "authors": [
                    {
                        "name": "Roberto Bada-Nerin"
                    },
                    {
                        "name": "Oleg Bulashenko"
                    },
                    {
                        "name": "Osvaldo Gramaxo Freitas"
                    },
                    {
                        "name": "Jos A. Font"
                    }
                ],
                "author_detail": {
                    "name": "Jos A. Font"
                },
                "author": "Jos A. Font",
                "arxiv_comment": "15 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00566v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00566v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07288v2",
                "updated": "2025-04-14T17:48:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    48,
                    8,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-09T21:28:17Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    21,
                    28,
                    17,
                    2,
                    99,
                    0
                ],
                "title": "MDIT: A Model-free Data Interpolation Method for Diverse Instruction\n  Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MDIT: A Model-free Data Interpolation Method for Diverse Instruction\n  Tuning"
                },
                "summary": "As Large Language Models (LLMs) are increasingly applied across various\ntasks, instruction tuning has emerged as a critical method for enhancing model\nperformance. However, current data management strategies face substantial\nchallenges in generating diverse and comprehensive data, restricting further\nimprovements in model performance. To address this gap, we propose MDIT, a\nnovel model-free data interpolation method for diverse instruction tuning,\nwhich generates varied and high-quality instruction data by performing task\ninterpolation. Moreover, it contains diversity-based clustering strategies to\nensure the diversity of the training data. Extensive experiments show that our\nmethod achieves superior performance in multiple benchmark tasks. The LLMs\nfinetuned with MDIT show significant improvements in numerous tasks such as\ngeneral question answering, math reasoning, and code generation. MDIT offers an\nefficient and automatic data synthetic method, generating diverse instruction\ndata without depending on external resources while expanding the application\npotential of LLMs in complex environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) are increasingly applied across various\ntasks, instruction tuning has emerged as a critical method for enhancing model\nperformance. However, current data management strategies face substantial\nchallenges in generating diverse and comprehensive data, restricting further\nimprovements in model performance. To address this gap, we propose MDIT, a\nnovel model-free data interpolation method for diverse instruction tuning,\nwhich generates varied and high-quality instruction data by performing task\ninterpolation. Moreover, it contains diversity-based clustering strategies to\nensure the diversity of the training data. Extensive experiments show that our\nmethod achieves superior performance in multiple benchmark tasks. The LLMs\nfinetuned with MDIT show significant improvements in numerous tasks such as\ngeneral question answering, math reasoning, and code generation. MDIT offers an\nefficient and automatic data synthetic method, generating diverse instruction\ndata without depending on external resources while expanding the application\npotential of LLMs in complex environments."
                },
                "authors": [
                    {
                        "name": "Yangning Li"
                    },
                    {
                        "name": "Zihua Lan"
                    },
                    {
                        "name": "Lv Qingsong"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Hai-Tao Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Hai-Tao Zheng"
                },
                "author": "Hai-Tao Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04486v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04486v2",
                "updated": "2025-04-14T17:45:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    45,
                    58,
                    0,
                    104,
                    0
                ],
                "published": "2025-01-08T13:13:52Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    13,
                    13,
                    52,
                    2,
                    8,
                    0
                ],
                "title": "MB-TaylorFormer V2: Improved Multi-branch Linear Transformer Expanded by\n  Taylor Formula for Image Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MB-TaylorFormer V2: Improved Multi-branch Linear Transformer Expanded by\n  Taylor Formula for Image Restoration"
                },
                "summary": "Recently, Transformer networks have demonstrated outstanding performance in\nthe field of image restoration due to the global receptive field and\nadaptability to input. However, the quadratic computational complexity of\nSoftmax-attention poses a significant limitation on its extensive application\nin image restoration tasks, particularly for high-resolution images. To tackle\nthis challenge, we propose a novel variant of the Transformer. This variant\nleverages the Taylor expansion to approximate the Softmax-attention and\nutilizes the concept of norm-preserving mapping to approximate the remainder of\nthe first-order Taylor expansion, resulting in a linear computational\ncomplexity. Moreover, we introduce a multi-branch architecture featuring\nmulti-scale patch embedding into the proposed Transformer, which has four\ndistinct advantages: 1) various sizes of the receptive field; 2) multi-level\nsemantic information; 3) flexible shapes of the receptive field; 4) accelerated\ntraining and inference speed. Hence, the proposed model, named the second\nversion of Taylor formula expansion-based Transformer (for short\nMB-TaylorFormer V2) has the capability to concurrently process coarse-to-fine\nfeatures, capture long-distance pixel interactions with limited computational\ncost, and improve the approximation of the Taylor expansion remainder.\nExperimental results across diverse image restoration benchmarks demonstrate\nthat MB-TaylorFormer V2 achieves state-of-the-art performance in multiple image\nrestoration tasks, such as image dehazing, deraining, desnowing, motion\ndeblurring, and denoising, with very little computational overhead. The source\ncode is available at https://github.com/FVL2020/MB-TaylorFormerV2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Transformer networks have demonstrated outstanding performance in\nthe field of image restoration due to the global receptive field and\nadaptability to input. However, the quadratic computational complexity of\nSoftmax-attention poses a significant limitation on its extensive application\nin image restoration tasks, particularly for high-resolution images. To tackle\nthis challenge, we propose a novel variant of the Transformer. This variant\nleverages the Taylor expansion to approximate the Softmax-attention and\nutilizes the concept of norm-preserving mapping to approximate the remainder of\nthe first-order Taylor expansion, resulting in a linear computational\ncomplexity. Moreover, we introduce a multi-branch architecture featuring\nmulti-scale patch embedding into the proposed Transformer, which has four\ndistinct advantages: 1) various sizes of the receptive field; 2) multi-level\nsemantic information; 3) flexible shapes of the receptive field; 4) accelerated\ntraining and inference speed. Hence, the proposed model, named the second\nversion of Taylor formula expansion-based Transformer (for short\nMB-TaylorFormer V2) has the capability to concurrently process coarse-to-fine\nfeatures, capture long-distance pixel interactions with limited computational\ncost, and improve the approximation of the Taylor expansion remainder.\nExperimental results across diverse image restoration benchmarks demonstrate\nthat MB-TaylorFormer V2 achieves state-of-the-art performance in multiple image\nrestoration tasks, such as image dehazing, deraining, desnowing, motion\ndeblurring, and denoising, with very little computational overhead. The source\ncode is available at https://github.com/FVL2020/MB-TaylorFormerV2."
                },
                "authors": [
                    {
                        "name": "Zhi Jin"
                    },
                    {
                        "name": "Yuwei Qiu"
                    },
                    {
                        "name": "Kaihao Zhang"
                    },
                    {
                        "name": "Hongdong Li"
                    },
                    {
                        "name": "Wenhan Luo"
                    }
                ],
                "author_detail": {
                    "name": "Wenhan Luo"
                },
                "author": "Wenhan Luo",
                "arxiv_comment": "accepted by IEEE TPAMI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04486v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04486v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15583v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15583v2",
                "updated": "2025-04-14T17:42:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    42,
                    35,
                    0,
                    104,
                    0
                ],
                "published": "2024-06-21T18:31:49Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    18,
                    31,
                    49,
                    4,
                    173,
                    0
                ],
                "title": "Detecting AI-Generated Text: Factors Influencing Detectability with\n  Current Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting AI-Generated Text: Factors Influencing Detectability with\n  Current Methods"
                },
                "summary": "Large language models (LLMs) have advanced to a point that even humans have\ndifficulty discerning whether a text was generated by another human, or by a\ncomputer. However, knowing whether a text was produced by human or artificial\nintelligence (AI) is important to determining its trustworthiness, and has\napplications in many domains including detecting fraud and academic dishonesty,\nas well as combating the spread of misinformation and political propaganda. The\ntask of AI-generated text (AIGT) detection is therefore both very challenging,\nand highly critical. In this survey, we summarize state-of-the art approaches\nto AIGT detection, including watermarking, statistical and stylistic analysis,\nand machine learning classification. We also provide information about existing\ndatasets for this task. Synthesizing the research findings, we aim to provide\ninsight into the salient factors that combine to determine how \"detectable\"\nAIGT text is under different scenarios, and to make practical recommendations\nfor future work towards this significant technical and societal challenge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have advanced to a point that even humans have\ndifficulty discerning whether a text was generated by another human, or by a\ncomputer. However, knowing whether a text was produced by human or artificial\nintelligence (AI) is important to determining its trustworthiness, and has\napplications in many domains including detecting fraud and academic dishonesty,\nas well as combating the spread of misinformation and political propaganda. The\ntask of AI-generated text (AIGT) detection is therefore both very challenging,\nand highly critical. In this survey, we summarize state-of-the art approaches\nto AIGT detection, including watermarking, statistical and stylistic analysis,\nand machine learning classification. We also provide information about existing\ndatasets for this task. Synthesizing the research findings, we aim to provide\ninsight into the salient factors that combine to determine how \"detectable\"\nAIGT text is under different scenarios, and to make practical recommendations\nfor future work towards this significant technical and societal challenge."
                },
                "authors": [
                    {
                        "name": "Kathleen C. Fraser"
                    },
                    {
                        "name": "Hillary Dawkins"
                    },
                    {
                        "name": "Svetlana Kiritchenko"
                    }
                ],
                "author_detail": {
                    "name": "Svetlana Kiritchenko"
                },
                "author": "Svetlana Kiritchenko",
                "arxiv_doi": "10.1613/jair.1.16665",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1613/jair.1.16665",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.15583v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15583v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Journal of Artificial Intelligence Research Vol. 82 (2025)\n  2233-2278",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10453v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10453v1",
                "updated": "2025-04-14T17:40:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    40,
                    18,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T17:40:18Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    40,
                    18,
                    0,
                    104,
                    0
                ],
                "title": "Anchors no more: Using peculiar velocities to constrain $H_0$ and the\n  primordial Universe without calibrators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anchors no more: Using peculiar velocities to constrain $H_0$ and the\n  primordial Universe without calibrators"
                },
                "summary": "We develop a novel approach to constrain the Hubble parameter $H_0$ and the\nprimordial power spectrum amplitude $A_\\mathrm{s}$ using supernovae type Ia\n(SNIa) data. By considering SNIa as tracers of the peculiar velocity field, we\ncan model their distance and their covariance as a function of cosmological\nparameters without the need of calibrators like Cepheids; this yields a new\nindependent probe of the large-scale structure based on SNIa data without\ndistance anchors. Crucially, we implement a differentiable pipeline in JAX,\nincluding efficient emulators and affine sampling, reducing inference time from\nyears to hours on a single GPU. We first validate our method on mock datasets,\ndemonstrating that we can constrain $H_0$ and $\\log 10^{10}A_\\mathrm{s}$ within\n$\\sim10\\%$ using $\\sim10^3$ SNIa. We then test our pipeline with SNIa from an\n$N$-body simulation, obtaining $7\\%$-level unbiased constraints on $H_0$ with a\nmoderate noise level. We finally apply our method to Pantheon+ data,\nconstraining $H_0$ at the $10\\%$ level without Cepheids when fixing\n$A_\\mathrm{s}$ to its $\\it{Planck}$ value. On the other hand, we obtain\n$15\\%$-level constraints on $\\log 10^{10}A_\\mathrm{s}$ in agreement with\n$\\it{Planck}$ when including Cepheids in the analysis. In light of upcoming\nobservations of low redshift SNIa from the Zwicky Transient Facility and the\nVera Rubin Legacy Survey of Space and Time, surveys for which our method will\ndevelop its full potential, we make our code publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop a novel approach to constrain the Hubble parameter $H_0$ and the\nprimordial power spectrum amplitude $A_\\mathrm{s}$ using supernovae type Ia\n(SNIa) data. By considering SNIa as tracers of the peculiar velocity field, we\ncan model their distance and their covariance as a function of cosmological\nparameters without the need of calibrators like Cepheids; this yields a new\nindependent probe of the large-scale structure based on SNIa data without\ndistance anchors. Crucially, we implement a differentiable pipeline in JAX,\nincluding efficient emulators and affine sampling, reducing inference time from\nyears to hours on a single GPU. We first validate our method on mock datasets,\ndemonstrating that we can constrain $H_0$ and $\\log 10^{10}A_\\mathrm{s}$ within\n$\\sim10\\%$ using $\\sim10^3$ SNIa. We then test our pipeline with SNIa from an\n$N$-body simulation, obtaining $7\\%$-level unbiased constraints on $H_0$ with a\nmoderate noise level. We finally apply our method to Pantheon+ data,\nconstraining $H_0$ at the $10\\%$ level without Cepheids when fixing\n$A_\\mathrm{s}$ to its $\\it{Planck}$ value. On the other hand, we obtain\n$15\\%$-level constraints on $\\log 10^{10}A_\\mathrm{s}$ in agreement with\n$\\it{Planck}$ when including Cepheids in the analysis. In light of upcoming\nobservations of low redshift SNIa from the Zwicky Transient Facility and the\nVera Rubin Legacy Survey of Space and Time, surveys for which our method will\ndevelop its full potential, we make our code publicly available."
                },
                "authors": [
                    {
                        "name": "Davide Piras"
                    },
                    {
                        "name": "Francesco Sorrenti"
                    },
                    {
                        "name": "Ruth Durrer"
                    },
                    {
                        "name": "Martin Kunz"
                    }
                ],
                "author_detail": {
                    "name": "Martin Kunz"
                },
                "author": "Martin Kunz",
                "arxiv_comment": "22 pages, 5 figures, comments welcome. Code available at\n  https://github.com/dpiras/veloce",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10453v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10453v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10449v1",
                "updated": "2025-04-14T17:38:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    38,
                    25,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T17:38:25Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    38,
                    25,
                    0,
                    104,
                    0
                ],
                "title": "M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models"
                },
                "summary": "Effective reasoning is crucial to solving complex mathematical problems.\nRecent large language models (LLMs) have boosted performance by scaling\ntest-time computation through long chain-of-thought reasoning. However,\ntransformer-based models are inherently limited in extending context length due\nto their quadratic computational complexity and linear memory requirements. In\nthis paper, we introduce a novel hybrid linear RNN reasoning model, M1, built\non the Mamba architecture, which allows memory-efficient inference. Our\napproach leverages a distillation process from existing reasoning models and is\nfurther enhanced through RL training. Experimental results on the AIME and MATH\nbenchmarks show that M1 not only outperforms previous linear RNN models but\nalso matches the performance of state-of-the-art Deepseek R1 distilled\nreasoning models at a similar scale. We also compare our generation speed with\na highly performant general purpose inference engine, vLLM, and observe more\nthan a 3x speedup compared to a same size transformer. With throughput speedup,\nwe are able to achieve higher accuracy compared to DeepSeek R1 distilled\ntransformer reasoning models under a fixed generation time budget using\nself-consistency voting. Overall, we introduce a hybrid Mamba reasoning model\nand provide a more effective approach to scaling test-time generation using\nself-consistency or long chain of thought reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective reasoning is crucial to solving complex mathematical problems.\nRecent large language models (LLMs) have boosted performance by scaling\ntest-time computation through long chain-of-thought reasoning. However,\ntransformer-based models are inherently limited in extending context length due\nto their quadratic computational complexity and linear memory requirements. In\nthis paper, we introduce a novel hybrid linear RNN reasoning model, M1, built\non the Mamba architecture, which allows memory-efficient inference. Our\napproach leverages a distillation process from existing reasoning models and is\nfurther enhanced through RL training. Experimental results on the AIME and MATH\nbenchmarks show that M1 not only outperforms previous linear RNN models but\nalso matches the performance of state-of-the-art Deepseek R1 distilled\nreasoning models at a similar scale. We also compare our generation speed with\na highly performant general purpose inference engine, vLLM, and observe more\nthan a 3x speedup compared to a same size transformer. With throughput speedup,\nwe are able to achieve higher accuracy compared to DeepSeek R1 distilled\ntransformer reasoning models under a fixed generation time budget using\nself-consistency voting. Overall, we introduce a hybrid Mamba reasoning model\nand provide a more effective approach to scaling test-time generation using\nself-consistency or long chain of thought reasoning."
                },
                "authors": [
                    {
                        "name": "Junxiong Wang"
                    },
                    {
                        "name": "Wen-Ding Li"
                    },
                    {
                        "name": "Daniele Paliotta"
                    },
                    {
                        "name": "Daniel Ritter"
                    },
                    {
                        "name": "Alexander M. Rush"
                    },
                    {
                        "name": "Tri Dao"
                    }
                ],
                "author_detail": {
                    "name": "Tri Dao"
                },
                "author": "Tri Dao",
                "arxiv_comment": "Code is available https://github.com/jxiw/M1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17391v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17391v2",
                "updated": "2025-04-14T17:34:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    34,
                    19,
                    0,
                    104,
                    0
                ],
                "published": "2025-01-29T02:52:32Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    2,
                    52,
                    32,
                    2,
                    29,
                    0
                ],
                "title": "Learning Free Token Reduction for Multi-Modal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Free Token Reduction for Multi-Modal Large Language Models"
                },
                "summary": "Vision-Language Models (VLMs) have achieved remarkable success across a range\nof multimodal tasks; however, their practical deployment is often constrained\nby high computational costs and prolonged inference times. Since the vision\nmodality typically carries more information than the text modality, compressing\nvisual prompts offers a promising solution to alleviate these challenges.\nExisting approaches predominantly focus on refining model architectures or\ndirectly reducing the number of visual tokens. However, these methods often\ncompromise inference performance due to a lack of consideration for the unique\nspatial and temporal characteristics of visual data. In this work, we propose a\ntoken compression paradigm that operates on both spatial and temporal\ndimensions. Our approach includes a learning-free, plug-and-play compression\npipeline that can be seamlessly integrated into most Multimodal Large Language\nModel (MLLM) frameworks. By leveraging this method, we enhance the model\ninference capability while simultaneously reducing its computational cost.\nExperimental results on the Video-QA task demonstrate the effectiveness of the\nproposed approach, showcasing significant improvements in efficiency without\nsacrificing performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have achieved remarkable success across a range\nof multimodal tasks; however, their practical deployment is often constrained\nby high computational costs and prolonged inference times. Since the vision\nmodality typically carries more information than the text modality, compressing\nvisual prompts offers a promising solution to alleviate these challenges.\nExisting approaches predominantly focus on refining model architectures or\ndirectly reducing the number of visual tokens. However, these methods often\ncompromise inference performance due to a lack of consideration for the unique\nspatial and temporal characteristics of visual data. In this work, we propose a\ntoken compression paradigm that operates on both spatial and temporal\ndimensions. Our approach includes a learning-free, plug-and-play compression\npipeline that can be seamlessly integrated into most Multimodal Large Language\nModel (MLLM) frameworks. By leveraging this method, we enhance the model\ninference capability while simultaneously reducing its computational cost.\nExperimental results on the Video-QA task demonstrate the effectiveness of the\nproposed approach, showcasing significant improvements in efficiency without\nsacrificing performance."
                },
                "authors": [
                    {
                        "name": "Zihui Zhao"
                    },
                    {
                        "name": "Yingxin Li"
                    },
                    {
                        "name": "Yang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yang Li"
                },
                "author": "Yang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17391v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17391v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10443v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10443v1",
                "updated": "2025-04-14T17:34:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    34,
                    6,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T17:34:06Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    34,
                    6,
                    0,
                    104,
                    0
                ],
                "title": "Multimodal Long Video Modeling Based on Temporal Dynamic Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Long Video Modeling Based on Temporal Dynamic Context"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have led to significant\nbreakthroughs in video understanding. However, existing models still struggle\nwith long video processing due to the context length constraint of LLMs and the\nvast amount of information within the video. Although some recent methods are\ndesigned for long video understanding, they often lose crucial information\nduring token compression and struggle with additional modality like audio. In\nthis work, we propose a dynamic long video encoding method utilizing the\ntemporal relationship between frames, named Temporal Dynamic Context (TDC).\nFirstly, we segment the video into semantically consistent scenes based on\ninter-frame similarities, then encode each frame into tokens using visual-audio\nencoders. Secondly, we propose a novel temporal context compressor to reduce\nthe number of tokens within each segment. Specifically, we employ a query-based\nTransformer to aggregate video, audio, and instruction text tokens into a\nlimited set of temporal context tokens. Finally, we feed the static frame\ntokens and the temporal context tokens into the LLM for video understanding.\nFurthermore, to handle extremely long videos, we propose a training-free\nchain-of-thought strategy that progressively extracts answers from multiple\nvideo segments. These intermediate answers serve as part of the reasoning\nprocess and contribute to the final answer. We conduct extensive experiments on\ngeneral video understanding and audio-video understanding benchmarks, where our\nmethod demonstrates strong performance. The code and models are available at\nhttps://github.com/Hoar012/TDC-Video.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have led to significant\nbreakthroughs in video understanding. However, existing models still struggle\nwith long video processing due to the context length constraint of LLMs and the\nvast amount of information within the video. Although some recent methods are\ndesigned for long video understanding, they often lose crucial information\nduring token compression and struggle with additional modality like audio. In\nthis work, we propose a dynamic long video encoding method utilizing the\ntemporal relationship between frames, named Temporal Dynamic Context (TDC).\nFirstly, we segment the video into semantically consistent scenes based on\ninter-frame similarities, then encode each frame into tokens using visual-audio\nencoders. Secondly, we propose a novel temporal context compressor to reduce\nthe number of tokens within each segment. Specifically, we employ a query-based\nTransformer to aggregate video, audio, and instruction text tokens into a\nlimited set of temporal context tokens. Finally, we feed the static frame\ntokens and the temporal context tokens into the LLM for video understanding.\nFurthermore, to handle extremely long videos, we propose a training-free\nchain-of-thought strategy that progressively extracts answers from multiple\nvideo segments. These intermediate answers serve as part of the reasoning\nprocess and contribute to the final answer. We conduct extensive experiments on\ngeneral video understanding and audio-video understanding benchmarks, where our\nmethod demonstrates strong performance. The code and models are available at\nhttps://github.com/Hoar012/TDC-Video."
                },
                "authors": [
                    {
                        "name": "Haoran Hao"
                    },
                    {
                        "name": "Jiaming Han"
                    },
                    {
                        "name": "Yiyuan Zhang"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10443v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08727v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08727v2",
                "updated": "2025-04-14T17:30:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    30,
                    56,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-11T17:55:45Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    55,
                    45,
                    4,
                    101,
                    0
                ],
                "title": "Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections\n  of Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections\n  of Images"
                },
                "summary": "We present a system using Multimodal LLMs (MLLMs) to analyze a large database\nwith tens of millions of images captured at different times, with the aim of\ndiscovering patterns in temporal changes. Specifically, we aim to capture\nfrequent co-occurring changes (\"trends\") across a city over a certain period.\nUnlike previous visual analyses, our analysis answers open-ended queries (e.g.,\n\"what are the frequent types of changes in the city?\") without any\npredetermined target subjects or training labels. These properties cast prior\nlearning-based or unsupervised visual analysis tools unsuitable. We identify\nMLLMs as a novel tool for their open-ended semantic understanding capabilities.\nYet, our datasets are four orders of magnitude too large for an MLLM to ingest\nas context. So we introduce a bottom-up procedure that decomposes the massive\nvisual analysis problem into more tractable sub-problems. We carefully design\nMLLM-based solutions to each sub-problem. During experiments and ablation\nstudies with our system, we find it significantly outperforms baselines and is\nable to discover interesting trends from images captured in large cities (e.g.,\n\"addition of outdoor dining,\", \"overpass was painted blue,\" etc.). See more\nresults and interactive demos at https://boyangdeng.com/visual-chronicles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a system using Multimodal LLMs (MLLMs) to analyze a large database\nwith tens of millions of images captured at different times, with the aim of\ndiscovering patterns in temporal changes. Specifically, we aim to capture\nfrequent co-occurring changes (\"trends\") across a city over a certain period.\nUnlike previous visual analyses, our analysis answers open-ended queries (e.g.,\n\"what are the frequent types of changes in the city?\") without any\npredetermined target subjects or training labels. These properties cast prior\nlearning-based or unsupervised visual analysis tools unsuitable. We identify\nMLLMs as a novel tool for their open-ended semantic understanding capabilities.\nYet, our datasets are four orders of magnitude too large for an MLLM to ingest\nas context. So we introduce a bottom-up procedure that decomposes the massive\nvisual analysis problem into more tractable sub-problems. We carefully design\nMLLM-based solutions to each sub-problem. During experiments and ablation\nstudies with our system, we find it significantly outperforms baselines and is\nable to discover interesting trends from images captured in large cities (e.g.,\n\"addition of outdoor dining,\", \"overpass was painted blue,\" etc.). See more\nresults and interactive demos at https://boyangdeng.com/visual-chronicles."
                },
                "authors": [
                    {
                        "name": "Boyang Deng"
                    },
                    {
                        "name": "Songyou Peng"
                    },
                    {
                        "name": "Kyle Genova"
                    },
                    {
                        "name": "Gordon Wetzstein"
                    },
                    {
                        "name": "Noah Snavely"
                    },
                    {
                        "name": "Leonidas Guibas"
                    },
                    {
                        "name": "Thomas Funkhouser"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Funkhouser"
                },
                "author": "Thomas Funkhouser",
                "arxiv_comment": "Project page: https://boyangdeng.com/visual-chronicles , second and\n  third listed authors have equal contributions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08727v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08727v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10430v1",
                "updated": "2025-04-14T17:20:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    20,
                    34,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T17:20:34Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    20,
                    34,
                    0,
                    104,
                    0
                ],
                "title": "LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety\n  in Large Language Models"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have enabled them to\napproach human-level persuasion capabilities. However, such potential also\nraises concerns about the safety risks of LLM-driven persuasion, particularly\ntheir potential for unethical influence through manipulation, deception,\nexploitation of vulnerabilities, and many other harmful tactics. In this work,\nwe present a systematic investigation of LLM persuasion safety through two\ncritical aspects: (1) whether LLMs appropriately reject unethical persuasion\ntasks and avoid unethical strategies during execution, including cases where\nthe initial persuasion goal appears ethically neutral, and (2) how influencing\nfactors like personality traits and external pressures affect their behavior.\nTo this end, we introduce PersuSafety, the first comprehensive framework for\nthe assessment of persuasion safety which consists of three stages, i.e.,\npersuasion scene creation, persuasive conversation simulation, and persuasion\nsafety assessment. PersuSafety covers 6 diverse unethical persuasion topics and\n15 common unethical strategies. Through extensive experiments across 8 widely\nused LLMs, we observe significant safety concerns in most LLMs, including\nfailing to identify harmful persuasion tasks and leveraging various unethical\npersuasion strategies. Our study calls for more attention to improve safety\nalignment in progressive and goal-driven conversations such as persuasion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have enabled them to\napproach human-level persuasion capabilities. However, such potential also\nraises concerns about the safety risks of LLM-driven persuasion, particularly\ntheir potential for unethical influence through manipulation, deception,\nexploitation of vulnerabilities, and many other harmful tactics. In this work,\nwe present a systematic investigation of LLM persuasion safety through two\ncritical aspects: (1) whether LLMs appropriately reject unethical persuasion\ntasks and avoid unethical strategies during execution, including cases where\nthe initial persuasion goal appears ethically neutral, and (2) how influencing\nfactors like personality traits and external pressures affect their behavior.\nTo this end, we introduce PersuSafety, the first comprehensive framework for\nthe assessment of persuasion safety which consists of three stages, i.e.,\npersuasion scene creation, persuasive conversation simulation, and persuasion\nsafety assessment. PersuSafety covers 6 diverse unethical persuasion topics and\n15 common unethical strategies. Through extensive experiments across 8 widely\nused LLMs, we observe significant safety concerns in most LLMs, including\nfailing to identify harmful persuasion tasks and leveraging various unethical\npersuasion strategies. Our study calls for more attention to improve safety\nalignment in progressive and goal-driven conversations such as persuasion."
                },
                "authors": [
                    {
                        "name": "Minqian Liu"
                    },
                    {
                        "name": "Zhiyang Xu"
                    },
                    {
                        "name": "Xinyi Zhang"
                    },
                    {
                        "name": "Heajun An"
                    },
                    {
                        "name": "Sarvech Qadir"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Pamela J. Wisniewski"
                    },
                    {
                        "name": "Jin-Hee Cho"
                    },
                    {
                        "name": "Sang Won Lee"
                    },
                    {
                        "name": "Ruoxi Jia"
                    },
                    {
                        "name": "Lifu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Lifu Huang"
                },
                "author": "Lifu Huang",
                "arxiv_comment": "20 pages, 7 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10421v1",
                "updated": "2025-04-14T17:08:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    8,
                    20,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T17:08:20Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    8,
                    20,
                    0,
                    104,
                    0
                ],
                "title": "Can We Edit LLMs for Long-Tail Biomedical Knowledge?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can We Edit LLMs for Long-Tail Biomedical Knowledge?"
                },
                "summary": "Knowledge editing has emerged as an effective approach for updating large\nlanguage models (LLMs) by modifying their internal knowledge. However, their\napplication to the biomedical domain faces unique challenges due to the\nlong-tailed distribution of biomedical knowledge, where rare and infrequent\ninformation is prevalent. In this paper, we conduct the first comprehensive\nstudy to investigate the effectiveness of knowledge editing methods for editing\nlong-tail biomedical knowledge. Our results indicate that, while existing\nediting methods can enhance LLMs' performance on long-tail biomedical\nknowledge, their performance on long-tail knowledge remains inferior to that on\nhigh-frequency popular knowledge, even after editing. Our further analysis\nreveals that long-tail biomedical knowledge contains a significant amount of\none-to-many knowledge, where one subject and relation link to multiple objects.\nThis high prevalence of one-to-many knowledge limits the effectiveness of\nknowledge editing in improving LLMs' understanding of long-tail biomedical\nknowledge, highlighting the need for tailored strategies to bridge this\nperformance gap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge editing has emerged as an effective approach for updating large\nlanguage models (LLMs) by modifying their internal knowledge. However, their\napplication to the biomedical domain faces unique challenges due to the\nlong-tailed distribution of biomedical knowledge, where rare and infrequent\ninformation is prevalent. In this paper, we conduct the first comprehensive\nstudy to investigate the effectiveness of knowledge editing methods for editing\nlong-tail biomedical knowledge. Our results indicate that, while existing\nediting methods can enhance LLMs' performance on long-tail biomedical\nknowledge, their performance on long-tail knowledge remains inferior to that on\nhigh-frequency popular knowledge, even after editing. Our further analysis\nreveals that long-tail biomedical knowledge contains a significant amount of\none-to-many knowledge, where one subject and relation link to multiple objects.\nThis high prevalence of one-to-many knowledge limits the effectiveness of\nknowledge editing in improving LLMs' understanding of long-tail biomedical\nknowledge, highlighting the need for tailored strategies to bridge this\nperformance gap."
                },
                "authors": [
                    {
                        "name": "Xinhao Yi"
                    },
                    {
                        "name": "Jake Lever"
                    },
                    {
                        "name": "Kevin Bryson"
                    },
                    {
                        "name": "Zaiqiao Meng"
                    }
                ],
                "author_detail": {
                    "name": "Zaiqiao Meng"
                },
                "author": "Zaiqiao Meng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10418v1",
                "updated": "2025-04-14T17:06:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    6,
                    47,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T17:06:47Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    6,
                    47,
                    0,
                    104,
                    0
                ],
                "title": "CliniChat: A Multi-Source Knowledge-Driven Framework for Clinical\n  Interview Dialogue Reconstruction and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CliniChat: A Multi-Source Knowledge-Driven Framework for Clinical\n  Interview Dialogue Reconstruction and Evaluation"
                },
                "summary": "Large language models (LLMs) hold great promise for assisting clinical\ninterviews due to their fluent interactive capabilities and extensive medical\nknowledge. However, the lack of high-quality interview dialogue data and widely\naccepted evaluation methods has significantly impeded this process. So we\npropose CliniChat, a framework that integrates multi-source knowledge to enable\nLLMs to simulate real-world clinical interviews. It consists of two modules:\nClini-Recon and Clini-Eval, each responsible for reconstructing and evaluating\ninterview dialogues, respectively. By incorporating three sources of knowledge,\nClini-Recon transforms clinical notes into systematic, professional, and\nempathetic interview dialogues. Clini-Eval combines a comprehensive evaluation\nmetric system with a two-phase automatic evaluation approach, enabling LLMs to\nassess interview performance like experts. We contribute MedQA-Dialog, a\nhigh-quality synthetic interview dialogue dataset, and CliniChatGLM, a model\nspecialized for clinical interviews. Experimental results demonstrate that\nCliniChatGLM's interview capabilities undergo a comprehensive upgrade,\nparticularly in history-taking, achieving state-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) hold great promise for assisting clinical\ninterviews due to their fluent interactive capabilities and extensive medical\nknowledge. However, the lack of high-quality interview dialogue data and widely\naccepted evaluation methods has significantly impeded this process. So we\npropose CliniChat, a framework that integrates multi-source knowledge to enable\nLLMs to simulate real-world clinical interviews. It consists of two modules:\nClini-Recon and Clini-Eval, each responsible for reconstructing and evaluating\ninterview dialogues, respectively. By incorporating three sources of knowledge,\nClini-Recon transforms clinical notes into systematic, professional, and\nempathetic interview dialogues. Clini-Eval combines a comprehensive evaluation\nmetric system with a two-phase automatic evaluation approach, enabling LLMs to\nassess interview performance like experts. We contribute MedQA-Dialog, a\nhigh-quality synthetic interview dialogue dataset, and CliniChatGLM, a model\nspecialized for clinical interviews. Experimental results demonstrate that\nCliniChatGLM's interview capabilities undergo a comprehensive upgrade,\nparticularly in history-taking, achieving state-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Jing Chen"
                    },
                    {
                        "name": "Zhihua Wei"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Yingying Hu"
                    },
                    {
                        "name": "Qiong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qiong Zhang"
                },
                "author": "Qiong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10415v1",
                "updated": "2025-04-14T17:00:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    0,
                    13,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T17:00:13Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    0,
                    13,
                    0,
                    104,
                    0
                ],
                "title": "LLM-SRBench: A New Benchmark for Scientific Equation Discovery with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-SRBench: A New Benchmark for Scientific Equation Discovery with\n  Large Language Models"
                },
                "summary": "Scientific equation discovery is a fundamental task in the history of\nscientific progress, enabling the derivation of laws governing natural\nphenomena. Recently, Large Language Models (LLMs) have gained interest for this\ntask due to their potential to leverage embedded scientific knowledge for\nhypothesis generation. However, evaluating the true discovery capabilities of\nthese methods remains challenging, as existing benchmarks often rely on common\nequations that are susceptible to memorization by LLMs, leading to inflated\nperformance metrics that do not reflect discovery. In this paper, we introduce\nLLM-SRBench, a comprehensive benchmark with 239 challenging problems across\nfour scientific domains specifically designed to evaluate LLM-based scientific\nequation discovery methods while preventing trivial memorization. Our benchmark\ncomprises two main categories: LSR-Transform, which transforms common physical\nmodels into less common mathematical representations to test reasoning beyond\nmemorized forms, and LSR-Synth, which introduces synthetic, discovery-driven\nproblems requiring data-driven reasoning. Through extensive evaluation of\nseveral state-of-the-art methods, using both open and closed LLMs, we find that\nthe best-performing system so far achieves only 31.5% symbolic accuracy. These\nfindings highlight the challenges of scientific equation discovery, positioning\nLLM-SRBench as a valuable resource for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific equation discovery is a fundamental task in the history of\nscientific progress, enabling the derivation of laws governing natural\nphenomena. Recently, Large Language Models (LLMs) have gained interest for this\ntask due to their potential to leverage embedded scientific knowledge for\nhypothesis generation. However, evaluating the true discovery capabilities of\nthese methods remains challenging, as existing benchmarks often rely on common\nequations that are susceptible to memorization by LLMs, leading to inflated\nperformance metrics that do not reflect discovery. In this paper, we introduce\nLLM-SRBench, a comprehensive benchmark with 239 challenging problems across\nfour scientific domains specifically designed to evaluate LLM-based scientific\nequation discovery methods while preventing trivial memorization. Our benchmark\ncomprises two main categories: LSR-Transform, which transforms common physical\nmodels into less common mathematical representations to test reasoning beyond\nmemorized forms, and LSR-Synth, which introduces synthetic, discovery-driven\nproblems requiring data-driven reasoning. Through extensive evaluation of\nseveral state-of-the-art methods, using both open and closed LLMs, we find that\nthe best-performing system so far achieves only 31.5% symbolic accuracy. These\nfindings highlight the challenges of scientific equation discovery, positioning\nLLM-SRBench as a valuable resource for future research."
                },
                "authors": [
                    {
                        "name": "Parshin Shojaee"
                    },
                    {
                        "name": "Ngoc-Hieu Nguyen"
                    },
                    {
                        "name": "Kazem Meidani"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    },
                    {
                        "name": "Khoa D Doan"
                    },
                    {
                        "name": "Chandan K Reddy"
                    }
                ],
                "author_detail": {
                    "name": "Chandan K Reddy"
                },
                "author": "Chandan K Reddy",
                "arxiv_comment": "Project page:\n  https://github.com/deep-symbolic-mathematics/llm-srbench , Benchmark page:\n  https://huggingface.co/datasets/nnheui/llm-srbench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10414v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10414v1",
                "updated": "2025-04-14T16:59:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    59,
                    29,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T16:59:29Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    59,
                    29,
                    0,
                    104,
                    0
                ],
                "title": "HUMOTO: A 4D Dataset of Mocap Human Object Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HUMOTO: A 4D Dataset of Mocap Human Object Interactions"
                },
                "summary": "We present Human Motions with Objects (HUMOTO), a high-fidelity dataset of\nhuman-object interactions for motion generation, computer vision, and robotics\napplications. Featuring 736 sequences (7,875 seconds at 30 fps), HUMOTO\ncaptures interactions with 63 precisely modeled objects and 72 articulated\nparts. Our innovations include a scene-driven LLM scripting pipeline creating\ncomplete, purposeful tasks with natural progression, and a mocap-and-camera\nrecording setup to effectively handle occlusions. Spanning diverse activities\nfrom cooking to outdoor picnics, HUMOTO preserves both physical accuracy and\nlogical task flow. Professional artists rigorously clean and verify each\nsequence, minimizing foot sliding and object penetrations. We also provide\nbenchmarks compared to other datasets. HUMOTO's comprehensive full-body motion\nand simultaneous multi-object interactions address key data-capturing\nchallenges and provide opportunities to advance realistic human-object\ninteraction modeling across research domains with practical applications in\nanimation, robotics, and embodied AI systems. Project:\nhttps://jiaxin-lu.github.io/humoto/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Human Motions with Objects (HUMOTO), a high-fidelity dataset of\nhuman-object interactions for motion generation, computer vision, and robotics\napplications. Featuring 736 sequences (7,875 seconds at 30 fps), HUMOTO\ncaptures interactions with 63 precisely modeled objects and 72 articulated\nparts. Our innovations include a scene-driven LLM scripting pipeline creating\ncomplete, purposeful tasks with natural progression, and a mocap-and-camera\nrecording setup to effectively handle occlusions. Spanning diverse activities\nfrom cooking to outdoor picnics, HUMOTO preserves both physical accuracy and\nlogical task flow. Professional artists rigorously clean and verify each\nsequence, minimizing foot sliding and object penetrations. We also provide\nbenchmarks compared to other datasets. HUMOTO's comprehensive full-body motion\nand simultaneous multi-object interactions address key data-capturing\nchallenges and provide opportunities to advance realistic human-object\ninteraction modeling across research domains with practical applications in\nanimation, robotics, and embodied AI systems. Project:\nhttps://jiaxin-lu.github.io/humoto/ ."
                },
                "authors": [
                    {
                        "name": "Jiaxin Lu"
                    },
                    {
                        "name": "Chun-Hao Paul Huang"
                    },
                    {
                        "name": "Uttaran Bhattacharya"
                    },
                    {
                        "name": "Qixing Huang"
                    },
                    {
                        "name": "Yi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zhou"
                },
                "author": "Yi Zhou",
                "arxiv_comment": "19 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10414v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10414v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01436v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01436v2",
                "updated": "2025-04-14T16:58:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    58,
                    48,
                    0,
                    104,
                    0
                ],
                "published": "2025-02-03T15:19:28Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    19,
                    28,
                    0,
                    34,
                    0
                ],
                "title": "Towards Safer Chatbots: A Framework for Policy Compliance Evaluation of\n  Custom GPTs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Safer Chatbots: A Framework for Policy Compliance Evaluation of\n  Custom GPTs"
                },
                "summary": "Large Language Models (LLMs) have gained unprecedented prominence, achieving\nwidespread adoption across diverse domains and integrating deeply into society.\nThe capability to fine-tune general-purpose LLMs, such as Generative\nPre-trained Transformers (GPT), for specific tasks has facilitated the\nemergence of numerous Custom GPTs. These tailored models are increasingly made\navailable through dedicated marketplaces, such as OpenAI's GPT Store. However,\ntheir black-box nature introduces significant safety and compliance risks. In\nthis work, we present a scalable framework for the automated evaluation of\nCustom GPTs against OpenAI's usage policies, which define the permissible\nbehaviors of these systems. Our framework integrates three core components: (1)\nautomated discovery and data collection of models from the GPT store, (2) a\nred-teaming prompt generator tailored to specific policy categories and the\ncharacteristics of each target GPT, and (3) an LLM-as-a-judge technique to\nanalyze each prompt-response pair for potential policy violations. We validate\nour framework with a manually annotated ground truth, and evaluate it through a\nlarge-scale study with 782 Custom GPTs across three categories: Romantic,\nCybersecurity, and Academic GPTs. Our manual annotation process achieved an F1\nscore of 0.975 in identifying policy violations, confirming the reliability of\nthe framework's assessments. The results reveal that 58.7% of the analyzed\nmodels exhibit indications of non-compliance, exposing weaknesses in the GPT\nstore's review and approval processes. Furthermore, our findings indicate that\na model's popularity does not correlate with compliance, and non-compliance\nissues largely stem from behaviors inherited from base models rather than\nuser-driven customizations. We believe this approach is extendable to other\nchatbot platforms and policy domains, improving LLM-based systems safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained unprecedented prominence, achieving\nwidespread adoption across diverse domains and integrating deeply into society.\nThe capability to fine-tune general-purpose LLMs, such as Generative\nPre-trained Transformers (GPT), for specific tasks has facilitated the\nemergence of numerous Custom GPTs. These tailored models are increasingly made\navailable through dedicated marketplaces, such as OpenAI's GPT Store. However,\ntheir black-box nature introduces significant safety and compliance risks. In\nthis work, we present a scalable framework for the automated evaluation of\nCustom GPTs against OpenAI's usage policies, which define the permissible\nbehaviors of these systems. Our framework integrates three core components: (1)\nautomated discovery and data collection of models from the GPT store, (2) a\nred-teaming prompt generator tailored to specific policy categories and the\ncharacteristics of each target GPT, and (3) an LLM-as-a-judge technique to\nanalyze each prompt-response pair for potential policy violations. We validate\nour framework with a manually annotated ground truth, and evaluate it through a\nlarge-scale study with 782 Custom GPTs across three categories: Romantic,\nCybersecurity, and Academic GPTs. Our manual annotation process achieved an F1\nscore of 0.975 in identifying policy violations, confirming the reliability of\nthe framework's assessments. The results reveal that 58.7% of the analyzed\nmodels exhibit indications of non-compliance, exposing weaknesses in the GPT\nstore's review and approval processes. Furthermore, our findings indicate that\na model's popularity does not correlate with compliance, and non-compliance\nissues largely stem from behaviors inherited from base models rather than\nuser-driven customizations. We believe this approach is extendable to other\nchatbot platforms and policy domains, improving LLM-based systems safety."
                },
                "authors": [
                    {
                        "name": "David Rodriguez"
                    },
                    {
                        "name": "William Seymour"
                    },
                    {
                        "name": "Jose M. Del Alamo"
                    },
                    {
                        "name": "Jose Such"
                    }
                ],
                "author_detail": {
                    "name": "Jose Such"
                },
                "author": "Jose Such",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01436v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01436v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14701v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14701v3",
                "updated": "2025-04-14T16:58:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    58,
                    34,
                    0,
                    104,
                    0
                ],
                "published": "2024-02-22T16:56:44Z",
                "published_parsed": [
                    2024,
                    2,
                    22,
                    16,
                    56,
                    44,
                    3,
                    53,
                    0
                ],
                "title": "COMPASS: Computational Mapping of Patient-Therapist Alliance Strategies\n  with Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COMPASS: Computational Mapping of Patient-Therapist Alliance Strategies\n  with Language Modeling"
                },
                "summary": "The therapeutic working alliance is a critical predictor of psychotherapy\nsuccess. Traditionally, working alliance assessment relies on questionnaires\ncompleted by both therapists and patients. In this paper, we present COMPASS, a\nnovel framework to directly infer the therapeutic working alliance from the\nnatural language used in psychotherapy sessions. Our approach leverages\nadvanced large language models (LLMs) to analyze session transcripts and map\nthem to distributed representations. These representations capture the semantic\nsimilarities between the dialogues and psychometric instruments, such as the\nWorking Alliance Inventory. Analyzing a dataset of over 950 sessions spanning\ndiverse psychiatric conditions -- including anxiety (N=498), depression\n(N=377), schizophrenia (N=71), and suicidal tendencies (N=12) -- collected\nbetween 1970 and 2012, we demonstrate the effectiveness of our method in\nproviding fine-grained mapping of patient-therapist alignment trajectories,\noffering interpretable insights for clinical practice, and identifying emerging\npatterns related to the condition being treated. By employing various deep\nlearning-based topic modeling techniques in combination with prompting\ngenerative language models, we analyze the topical characteristics of different\npsychiatric conditions and how these topics evolve during each turn of the\nconversation. This integrated framework enhances the understanding of\ntherapeutic interactions, enables timely feedback for therapists on the quality\nof therapeutic relationships, and provides clear, actionable insights to\nimprove the effectiveness of psychotherapy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The therapeutic working alliance is a critical predictor of psychotherapy\nsuccess. Traditionally, working alliance assessment relies on questionnaires\ncompleted by both therapists and patients. In this paper, we present COMPASS, a\nnovel framework to directly infer the therapeutic working alliance from the\nnatural language used in psychotherapy sessions. Our approach leverages\nadvanced large language models (LLMs) to analyze session transcripts and map\nthem to distributed representations. These representations capture the semantic\nsimilarities between the dialogues and psychometric instruments, such as the\nWorking Alliance Inventory. Analyzing a dataset of over 950 sessions spanning\ndiverse psychiatric conditions -- including anxiety (N=498), depression\n(N=377), schizophrenia (N=71), and suicidal tendencies (N=12) -- collected\nbetween 1970 and 2012, we demonstrate the effectiveness of our method in\nproviding fine-grained mapping of patient-therapist alignment trajectories,\noffering interpretable insights for clinical practice, and identifying emerging\npatterns related to the condition being treated. By employing various deep\nlearning-based topic modeling techniques in combination with prompting\ngenerative language models, we analyze the topical characteristics of different\npsychiatric conditions and how these topics evolve during each turn of the\nconversation. This integrated framework enhances the understanding of\ntherapeutic interactions, enables timely feedback for therapists on the quality\nof therapeutic relationships, and provides clear, actionable insights to\nimprove the effectiveness of psychotherapy."
                },
                "authors": [
                    {
                        "name": "Baihan Lin"
                    },
                    {
                        "name": "Djallel Bouneffouf"
                    },
                    {
                        "name": "Yulia Landa"
                    },
                    {
                        "name": "Rachel Jespersen"
                    },
                    {
                        "name": "Cheryl Corcoran"
                    },
                    {
                        "name": "Guillermo Cecchi"
                    }
                ],
                "author_detail": {
                    "name": "Guillermo Cecchi"
                },
                "author": "Guillermo Cecchi",
                "arxiv_comment": "Translational Psychiatry, in press. This work extends our research\n  series in computational psychiatry (e.g auto annotation in arXiv:2204.05522,\n  topic extraction in arXiv:2204.10189, and diagnosis in arXiv:2210.15603) with\n  the introduction of LLMs to complete the full cycle of interpreting and\n  understanding psychotherapy strategies as a comprehensive analytical\n  framework",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14701v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14701v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10405v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10405v1",
                "updated": "2025-04-14T16:53:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    53,
                    59,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T16:53:59Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    53,
                    59,
                    0,
                    104,
                    0
                ],
                "title": "Performance of Large Language Models in Supporting Medical Diagnosis and\n  Treatment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance of Large Language Models in Supporting Medical Diagnosis and\n  Treatment"
                },
                "summary": "The integration of Large Language Models (LLMs) into healthcare holds\nsignificant potential to enhance diagnostic accuracy and support medical\ntreatment planning. These AI-driven systems can analyze vast datasets,\nassisting clinicians in identifying diseases, recommending treatments, and\npredicting patient outcomes. This study evaluates the performance of a range of\ncontemporary LLMs, including both open-source and closed-source models, on the\n2024 Portuguese National Exam for medical specialty access (PNA), a\nstandardized medical knowledge assessment. Our results highlight considerable\nvariation in accuracy and cost-effectiveness, with several models demonstrating\nperformance exceeding human benchmarks for medical students on this specific\ntask. We identify leading models based on a combined score of accuracy and\ncost, discuss the implications of reasoning methodologies like\nChain-of-Thought, and underscore the potential for LLMs to function as valuable\ncomplementary tools aiding medical professionals in complex clinical\ndecision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into healthcare holds\nsignificant potential to enhance diagnostic accuracy and support medical\ntreatment planning. These AI-driven systems can analyze vast datasets,\nassisting clinicians in identifying diseases, recommending treatments, and\npredicting patient outcomes. This study evaluates the performance of a range of\ncontemporary LLMs, including both open-source and closed-source models, on the\n2024 Portuguese National Exam for medical specialty access (PNA), a\nstandardized medical knowledge assessment. Our results highlight considerable\nvariation in accuracy and cost-effectiveness, with several models demonstrating\nperformance exceeding human benchmarks for medical students on this specific\ntask. We identify leading models based on a combined score of accuracy and\ncost, discuss the implications of reasoning methodologies like\nChain-of-Thought, and underscore the potential for LLMs to function as valuable\ncomplementary tools aiding medical professionals in complex clinical\ndecision-making."
                },
                "authors": [
                    {
                        "name": "Diogo Sousa"
                    },
                    {
                        "name": "Guilherme Barbosa"
                    },
                    {
                        "name": "Catarina Rocha"
                    },
                    {
                        "name": "Dulce Oliveira"
                    }
                ],
                "author_detail": {
                    "name": "Dulce Oliveira"
                },
                "author": "Dulce Oliveira",
                "arxiv_comment": "21 pages, 6 figures, 4 tables. Acknowledgements: The authors\n  acknowledge the support of the AITriage4SU Project (2024.07400.IACDC/2024),\n  funded by the FCT (Foundation for Science and Technology), Portugal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10405v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10400v1",
                "updated": "2025-04-14T16:51:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    51,
                    10,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T16:51:10Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    51,
                    10,
                    0,
                    104,
                    0
                ],
                "title": "Towards Low-Latency Event-based Obstacle Avoidance on a FPGA-Drone",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Low-Latency Event-based Obstacle Avoidance on a FPGA-Drone"
                },
                "summary": "This work quantitatively evaluates the performance of event-based vision\nsystems (EVS) against conventional RGB-based models for action prediction in\ncollision avoidance on an FPGA accelerator. Our experiments demonstrate that\nthe EVS model achieves a significantly higher effective frame rate (1 kHz) and\nlower temporal (-20 ms) and spatial prediction errors (-20 mm) compared to the\nRGB-based model, particularly when tested on out-of-distribution data. The EVS\nmodel also exhibits superior robustness in selecting optimal evasion maneuvers.\nIn particular, in distinguishing between movement and stationary states, it\nachieves a 59 percentage point advantage in precision (78% vs. 19%) and a\nsubstantially higher F1 score (0.73 vs. 0.06), highlighting the susceptibility\nof the RGB model to overfitting. Further analysis in different combinations of\nspatial classes confirms the consistent performance of the EVS model in both\ntest data sets. Finally, we evaluated the system end-to-end and achieved a\nlatency of approximately 2.14 ms, with event aggregation (1 ms) and inference\non the processing unit (0.94 ms) accounting for the largest components. These\nresults underscore the advantages of event-based vision for real-time collision\navoidance and demonstrate its potential for deployment in resource-constrained\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work quantitatively evaluates the performance of event-based vision\nsystems (EVS) against conventional RGB-based models for action prediction in\ncollision avoidance on an FPGA accelerator. Our experiments demonstrate that\nthe EVS model achieves a significantly higher effective frame rate (1 kHz) and\nlower temporal (-20 ms) and spatial prediction errors (-20 mm) compared to the\nRGB-based model, particularly when tested on out-of-distribution data. The EVS\nmodel also exhibits superior robustness in selecting optimal evasion maneuvers.\nIn particular, in distinguishing between movement and stationary states, it\nachieves a 59 percentage point advantage in precision (78% vs. 19%) and a\nsubstantially higher F1 score (0.73 vs. 0.06), highlighting the susceptibility\nof the RGB model to overfitting. Further analysis in different combinations of\nspatial classes confirms the consistent performance of the EVS model in both\ntest data sets. Finally, we evaluated the system end-to-end and achieved a\nlatency of approximately 2.14 ms, with event aggregation (1 ms) and inference\non the processing unit (0.94 ms) accounting for the largest components. These\nresults underscore the advantages of event-based vision for real-time collision\navoidance and demonstrate its potential for deployment in resource-constrained\nenvironments."
                },
                "authors": [
                    {
                        "name": "Pietro Bonazzi"
                    },
                    {
                        "name": "Christian Vogt"
                    },
                    {
                        "name": "Michael Jost"
                    },
                    {
                        "name": "Lyes Khacef"
                    },
                    {
                        "name": "Federico Paredes-Valls"
                    },
                    {
                        "name": "Michele Magno"
                    }
                ],
                "author_detail": {
                    "name": "Michele Magno"
                },
                "author": "Michele Magno",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10397v1",
                "updated": "2025-04-14T16:45:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    45,
                    52,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T16:45:52Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    45,
                    52,
                    0,
                    104,
                    0
                ],
                "title": "Can LLMs Assist Expert Elicitation for Probabilistic Causal Modeling?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Assist Expert Elicitation for Probabilistic Causal Modeling?"
                },
                "summary": "Objective: This study investigates the potential of Large Language Models\n(LLMs) as an alternative to human expert elicitation for extracting structured\ncausal knowledge and facilitating causal modeling in biometric and healthcare\napplications.\n  Material and Methods: LLM-generated causal structures, specifically Bayesian\nnetworks (BNs), were benchmarked against traditional statistical methods (e.g.,\nBayesian Information Criterion) using healthcare datasets. Validation\ntechniques included structural equation modeling (SEM) to verifying\nrelationships, and measures such as entropy, predictive accuracy, and\nrobustness to compare network structures.\n  Results and Discussion: LLM-generated BNs demonstrated lower entropy than\nexpert-elicited and statistically generated BNs, suggesting higher confidence\nand precision in predictions. However, limitations such as contextual\nconstraints, hallucinated dependencies, and potential biases inherited from\ntraining data require further investigation.\n  Conclusion: LLMs represent a novel frontier in expert elicitation for\nprobabilistic causal modeling, promising to improve transparency and reduce\nuncertainty in the decision-making using such models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objective: This study investigates the potential of Large Language Models\n(LLMs) as an alternative to human expert elicitation for extracting structured\ncausal knowledge and facilitating causal modeling in biometric and healthcare\napplications.\n  Material and Methods: LLM-generated causal structures, specifically Bayesian\nnetworks (BNs), were benchmarked against traditional statistical methods (e.g.,\nBayesian Information Criterion) using healthcare datasets. Validation\ntechniques included structural equation modeling (SEM) to verifying\nrelationships, and measures such as entropy, predictive accuracy, and\nrobustness to compare network structures.\n  Results and Discussion: LLM-generated BNs demonstrated lower entropy than\nexpert-elicited and statistically generated BNs, suggesting higher confidence\nand precision in predictions. However, limitations such as contextual\nconstraints, hallucinated dependencies, and potential biases inherited from\ntraining data require further investigation.\n  Conclusion: LLMs represent a novel frontier in expert elicitation for\nprobabilistic causal modeling, promising to improve transparency and reduce\nuncertainty in the decision-making using such models."
                },
                "authors": [
                    {
                        "name": "Olha Shaposhnyk"
                    },
                    {
                        "name": "Daria Zahorska"
                    },
                    {
                        "name": "Svetlana Yanushkevich"
                    }
                ],
                "author_detail": {
                    "name": "Svetlana Yanushkevich"
                },
                "author": "Svetlana Yanushkevich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10391v1",
                "updated": "2025-04-14T16:38:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    38,
                    28,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T16:38:28Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    38,
                    28,
                    0,
                    104,
                    0
                ],
                "title": "LLM-driven Constrained Copy Generation through Iterative Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-driven Constrained Copy Generation through Iterative Refinement"
                },
                "summary": "Crafting a marketing message (copy), or copywriting is a challenging\ngeneration task, as the copy must adhere to various constraints. Copy creation\nis inherently iterative for humans, starting with an initial draft followed by\nsuccessive refinements. However, manual copy creation is time-consuming and\nexpensive, resulting in only a few copies for each use case. This limitation\nrestricts our ability to personalize content to customers. Contrary to the\nmanual approach, LLMs can generate copies quickly, but the generated content\ndoes not consistently meet all the constraints on the first attempt (similar to\nhumans). While recent studies have shown promise in improving constrained\ngeneration through iterative refinement, they have primarily addressed tasks\nwith only a few simple constraints. Consequently, the effectiveness of\niterative refinement for tasks such as copy generation, which involves many\nintricate constraints, remains unclear. To address this gap, we propose an\nLLM-based end-to-end framework for scalable copy generation using iterative\nrefinement. To the best of our knowledge, this is the first study to address\nmultiple challenging constraints simultaneously in copy generation. Examples of\nthese constraints include length, topics, keywords, preferred lexical ordering,\nand tone of voice. We demonstrate the performance of our framework by creating\ncopies for e-commerce banners for three different use cases of varying\ncomplexity. Our results show that iterative refinement increases the copy\nsuccess rate by $16.25-35.91$% across use cases. Furthermore, the copies\ngenerated using our approach outperformed manually created content in multiple\npilot studies using a multi-armed bandit framework. The winning copy improved\nthe click-through rate by $38.5-45.21$%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crafting a marketing message (copy), or copywriting is a challenging\ngeneration task, as the copy must adhere to various constraints. Copy creation\nis inherently iterative for humans, starting with an initial draft followed by\nsuccessive refinements. However, manual copy creation is time-consuming and\nexpensive, resulting in only a few copies for each use case. This limitation\nrestricts our ability to personalize content to customers. Contrary to the\nmanual approach, LLMs can generate copies quickly, but the generated content\ndoes not consistently meet all the constraints on the first attempt (similar to\nhumans). While recent studies have shown promise in improving constrained\ngeneration through iterative refinement, they have primarily addressed tasks\nwith only a few simple constraints. Consequently, the effectiveness of\niterative refinement for tasks such as copy generation, which involves many\nintricate constraints, remains unclear. To address this gap, we propose an\nLLM-based end-to-end framework for scalable copy generation using iterative\nrefinement. To the best of our knowledge, this is the first study to address\nmultiple challenging constraints simultaneously in copy generation. Examples of\nthese constraints include length, topics, keywords, preferred lexical ordering,\nand tone of voice. We demonstrate the performance of our framework by creating\ncopies for e-commerce banners for three different use cases of varying\ncomplexity. Our results show that iterative refinement increases the copy\nsuccess rate by $16.25-35.91$% across use cases. Furthermore, the copies\ngenerated using our approach outperformed manually created content in multiple\npilot studies using a multi-armed bandit framework. The winning copy improved\nthe click-through rate by $38.5-45.21$%."
                },
                "authors": [
                    {
                        "name": "Varun Vasudevan"
                    },
                    {
                        "name": "Faezeh Akhavizadegan"
                    },
                    {
                        "name": "Abhinav Prakash"
                    },
                    {
                        "name": "Yokila Arora"
                    },
                    {
                        "name": "Jason Cho"
                    },
                    {
                        "name": "Tanya Mendiratta"
                    },
                    {
                        "name": "Sushant Kumar"
                    },
                    {
                        "name": "Kannan Achan"
                    }
                ],
                "author_detail": {
                    "name": "Kannan Achan"
                },
                "author": "Kannan Achan",
                "arxiv_comment": "10 pages, 2 figures, 7 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10388v1",
                "updated": "2025-04-14T16:32:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    32,
                    17,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T16:32:17Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    32,
                    17,
                    0,
                    104,
                    0
                ],
                "title": "Inferring genotype-phenotype maps using attention models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring genotype-phenotype maps using attention models"
                },
                "summary": "Predicting phenotype from genotype is a central challenge in genetics.\nTraditional approaches in quantitative genetics typically analyze this problem\nusing methods based on linear regression. These methods generally assume that\nthe genetic architecture of complex traits can be parameterized in terms of an\nadditive model, where the effects of loci are independent, plus (in some cases)\npairwise epistatic interactions between loci. However, these models struggle to\nanalyze more complex patterns of epistasis or subtle gene-environment\ninteractions. Recent advances in machine learning, particularly attention-based\nmodels, offer a promising alternative. Initially developed for natural language\nprocessing, attention-based models excel at capturing context-dependent\ninteractions and have shown exceptional performance in predicting protein\nstructure and function. Here, we apply attention-based models to quantitative\ngenetics. We analyze the performance of this attention-based approach in\npredicting phenotype from genotype using simulated data across a range of\nmodels with increasing epistatic complexity, and using experimental data from a\nrecent quantitative trait locus mapping study in budding yeast. We find that\nour model demonstrates superior out-of-sample predictions in epistatic regimes\ncompared to standard methods. We also explore a more general multi-environment\nattention-based model to jointly analyze genotype-phenotype maps across\nmultiple environments and show that such architectures can be used for\n\"transfer learning\" - predicting phenotypes in novel environments with limited\ntraining data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting phenotype from genotype is a central challenge in genetics.\nTraditional approaches in quantitative genetics typically analyze this problem\nusing methods based on linear regression. These methods generally assume that\nthe genetic architecture of complex traits can be parameterized in terms of an\nadditive model, where the effects of loci are independent, plus (in some cases)\npairwise epistatic interactions between loci. However, these models struggle to\nanalyze more complex patterns of epistasis or subtle gene-environment\ninteractions. Recent advances in machine learning, particularly attention-based\nmodels, offer a promising alternative. Initially developed for natural language\nprocessing, attention-based models excel at capturing context-dependent\ninteractions and have shown exceptional performance in predicting protein\nstructure and function. Here, we apply attention-based models to quantitative\ngenetics. We analyze the performance of this attention-based approach in\npredicting phenotype from genotype using simulated data across a range of\nmodels with increasing epistatic complexity, and using experimental data from a\nrecent quantitative trait locus mapping study in budding yeast. We find that\nour model demonstrates superior out-of-sample predictions in epistatic regimes\ncompared to standard methods. We also explore a more general multi-environment\nattention-based model to jointly analyze genotype-phenotype maps across\nmultiple environments and show that such architectures can be used for\n\"transfer learning\" - predicting phenotypes in novel environments with limited\ntraining data."
                },
                "authors": [
                    {
                        "name": "Krishna Rijal"
                    },
                    {
                        "name": "Caroline M. Holmes"
                    },
                    {
                        "name": "Samantha Petti"
                    },
                    {
                        "name": "Gautam Reddy"
                    },
                    {
                        "name": "Michael M. Desai"
                    },
                    {
                        "name": "Pankaj Mehta"
                    }
                ],
                "author_detail": {
                    "name": "Pankaj Mehta"
                },
                "author": "Pankaj Mehta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.08365v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.08365v2",
                "updated": "2025-04-14T16:25:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    25,
                    25,
                    0,
                    104,
                    0
                ],
                "published": "2023-11-14T18:17:56Z",
                "published_parsed": [
                    2023,
                    11,
                    14,
                    18,
                    17,
                    56,
                    1,
                    318,
                    0
                ],
                "title": "Local asymptotics of selection models with applications in Bayesian\n  selective inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local asymptotics of selection models with applications in Bayesian\n  selective inference"
                },
                "summary": "Contemporary focus on selective inference provokes interest in the asymptotic\nproperties of selection models, as the working inferential models in the\nconditional approach to inference after selection. In this paper, we derive an\nasymptotic expansion of the local likelihood ratios of selection models. We\nshow that under mild regularity conditions, they behave asymptotically like a\nsequence of Gaussian selection models. This generalizes the Local Asymptotic\nNormality framework of Le Cam (1960) to a class of non-regular models, and\nindicates a notion of local asymptotic selective normality as the appropriate\nsimplifying theoretical framework for analysis of selective inference.\nFurthermore, we establish practical consequences for Bayesian selective\ninference. Specifically, we derive the asymptotic shape of Bayesian posterior\ndistributions constructed from selection models, and show that they will\ntypically be significantly miscalibrated in a frequentist sense, demonstrating\nthat the familiar asymptotic equivalence between Bayesian and frequentist\napproaches does not hold under selection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary focus on selective inference provokes interest in the asymptotic\nproperties of selection models, as the working inferential models in the\nconditional approach to inference after selection. In this paper, we derive an\nasymptotic expansion of the local likelihood ratios of selection models. We\nshow that under mild regularity conditions, they behave asymptotically like a\nsequence of Gaussian selection models. This generalizes the Local Asymptotic\nNormality framework of Le Cam (1960) to a class of non-regular models, and\nindicates a notion of local asymptotic selective normality as the appropriate\nsimplifying theoretical framework for analysis of selective inference.\nFurthermore, we establish practical consequences for Bayesian selective\ninference. Specifically, we derive the asymptotic shape of Bayesian posterior\ndistributions constructed from selection models, and show that they will\ntypically be significantly miscalibrated in a frequentist sense, demonstrating\nthat the familiar asymptotic equivalence between Bayesian and frequentist\napproaches does not hold under selection."
                },
                "authors": [
                    {
                        "name": "Daniel G. Rasines"
                    },
                    {
                        "name": "G. Alastair Young"
                    }
                ],
                "author_detail": {
                    "name": "G. Alastair Young"
                },
                "author": "G. Alastair Young",
                "arxiv_comment": "28 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.08365v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.08365v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62E20 (Primary) 62F15 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07282v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07282v2",
                "updated": "2025-04-14T16:23:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    23,
                    29,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-09T21:17:52Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    21,
                    17,
                    52,
                    2,
                    99,
                    0
                ],
                "title": "RAISE: Reinforenced Adaptive Instruction Selection For Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAISE: Reinforenced Adaptive Instruction Selection For Large Language\n  Models"
                },
                "summary": "In the instruction fine-tuning of large language models (LLMs), it has become\na consensus that a few high-quality instructions are superior to a large number\nof low-quality instructions. At present, many instruction selection methods\nhave been proposed, but most of these methods select instruction based on\nheuristic quality metrics, and only consider data selection before training.\nThese designs lead to insufficient optimization of instruction fine-tuning, and\nfixed heuristic indicators are often difficult to optimize for specific tasks.\nSo we designed a dynamic, task-objective-driven instruction selection framework\nRAISE(Reinforenced Adaptive Instruction SElection), which incorporates the\nentire instruction fine-tuning process into optimization, selecting instruction\nat each step based on the expected impact of instruction on model performance\nimprovement. Our approach is well interpretable and has strong task-specific\noptimization capabilities. By modeling dynamic instruction selection as a\nsequential decision-making process, we use RL to train our selection strategy.\nExtensive experiments and result analysis prove the superiority of our method\ncompared with other instruction selection methods. Notably, RAISE achieves\nsuperior performance by updating only 1\\% of the training steps compared to\nfull-data training, demonstrating its efficiency and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the instruction fine-tuning of large language models (LLMs), it has become\na consensus that a few high-quality instructions are superior to a large number\nof low-quality instructions. At present, many instruction selection methods\nhave been proposed, but most of these methods select instruction based on\nheuristic quality metrics, and only consider data selection before training.\nThese designs lead to insufficient optimization of instruction fine-tuning, and\nfixed heuristic indicators are often difficult to optimize for specific tasks.\nSo we designed a dynamic, task-objective-driven instruction selection framework\nRAISE(Reinforenced Adaptive Instruction SElection), which incorporates the\nentire instruction fine-tuning process into optimization, selecting instruction\nat each step based on the expected impact of instruction on model performance\nimprovement. Our approach is well interpretable and has strong task-specific\noptimization capabilities. By modeling dynamic instruction selection as a\nsequential decision-making process, we use RL to train our selection strategy.\nExtensive experiments and result analysis prove the superiority of our method\ncompared with other instruction selection methods. Notably, RAISE achieves\nsuperior performance by updating only 1\\% of the training steps compared to\nfull-data training, demonstrating its efficiency and effectiveness."
                },
                "authors": [
                    {
                        "name": "Lv Qingsong"
                    },
                    {
                        "name": "Yangning Li"
                    },
                    {
                        "name": "Zihua Lan"
                    },
                    {
                        "name": "Zishan Xu"
                    },
                    {
                        "name": "Jiwei Tang"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Wenhao Jiang"
                    },
                    {
                        "name": "Hai-Tao Zheng"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07282v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07282v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10369v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10369v1",
                "updated": "2025-04-14T16:15:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    15,
                    55,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T16:15:55Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    15,
                    55,
                    0,
                    104,
                    0
                ],
                "title": "SymRTLO: Enhancing RTL Code Optimization with LLMs and Neuron-Inspired\n  Symbolic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SymRTLO: Enhancing RTL Code Optimization with LLMs and Neuron-Inspired\n  Symbolic Reasoning"
                },
                "summary": "Optimizing Register Transfer Level (RTL) code is crucial for improving the\npower, performance, and area (PPA) of digital circuits in the early stages of\nsynthesis. Manual rewriting, guided by synthesis feedback, can yield\nhigh-quality results but is time-consuming and error-prone. Most existing\ncompiler-based approaches have difficulty handling complex design constraints.\nLarge Language Model (LLM)-based methods have emerged as a promising\nalternative to address these challenges. However, LLM-based approaches often\nface difficulties in ensuring alignment between the generated code and the\nprovided prompts. This paper presents SymRTLO, a novel neuron-symbolic RTL\noptimization framework that seamlessly integrates LLM-based code rewriting with\nsymbolic reasoning techniques. Our method incorporates a retrieval-augmented\ngeneration (RAG) system of optimization rules and Abstract Syntax Tree\n(AST)-based templates, enabling LLM-based rewriting that maintains syntactic\ncorrectness while minimizing undesired circuit behaviors. A symbolic module is\nproposed for analyzing and optimizing finite state machine (FSM) logic,\nallowing fine-grained state merging and partial specification handling beyond\nthe scope of pattern-based compilers. Furthermore, a fast verification\npipeline, combining formal equivalence checks with test-driven validation,\nfurther reduces the complexity of verification. Experiments on the RTL-Rewriter\nbenchmark with Synopsys Design Compiler and Yosys show that SymRTLO improves\npower, performance, and area (PPA) by up to 43.9%, 62.5%, and 51.1%,\nrespectively, compared to the state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Register Transfer Level (RTL) code is crucial for improving the\npower, performance, and area (PPA) of digital circuits in the early stages of\nsynthesis. Manual rewriting, guided by synthesis feedback, can yield\nhigh-quality results but is time-consuming and error-prone. Most existing\ncompiler-based approaches have difficulty handling complex design constraints.\nLarge Language Model (LLM)-based methods have emerged as a promising\nalternative to address these challenges. However, LLM-based approaches often\nface difficulties in ensuring alignment between the generated code and the\nprovided prompts. This paper presents SymRTLO, a novel neuron-symbolic RTL\noptimization framework that seamlessly integrates LLM-based code rewriting with\nsymbolic reasoning techniques. Our method incorporates a retrieval-augmented\ngeneration (RAG) system of optimization rules and Abstract Syntax Tree\n(AST)-based templates, enabling LLM-based rewriting that maintains syntactic\ncorrectness while minimizing undesired circuit behaviors. A symbolic module is\nproposed for analyzing and optimizing finite state machine (FSM) logic,\nallowing fine-grained state merging and partial specification handling beyond\nthe scope of pattern-based compilers. Furthermore, a fast verification\npipeline, combining formal equivalence checks with test-driven validation,\nfurther reduces the complexity of verification. Experiments on the RTL-Rewriter\nbenchmark with Synopsys Design Compiler and Yosys show that SymRTLO improves\npower, performance, and area (PPA) by up to 43.9%, 62.5%, and 51.1%,\nrespectively, compared to the state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yiting Wang"
                    },
                    {
                        "name": "Wanghao Ye"
                    },
                    {
                        "name": "Ping Guo"
                    },
                    {
                        "name": "Yexiao He"
                    },
                    {
                        "name": "Ziyao Wang"
                    },
                    {
                        "name": "Yexiao He"
                    },
                    {
                        "name": "Bowei Tian"
                    },
                    {
                        "name": "Shwai He"
                    },
                    {
                        "name": "Guoheng Sun"
                    },
                    {
                        "name": "Zheyu Shen"
                    },
                    {
                        "name": "Sihan Chen"
                    },
                    {
                        "name": "Ankur Srivastava"
                    },
                    {
                        "name": "Qingfu Zhang"
                    },
                    {
                        "name": "Gang Qu"
                    },
                    {
                        "name": "Ang Li"
                    }
                ],
                "author_detail": {
                    "name": "Ang Li"
                },
                "author": "Ang Li",
                "arxiv_comment": "16 pages, 8 figures, 7 tables. Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10369v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10369v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10368v1",
                "updated": "2025-04-14T16:13:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    13,
                    23,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T16:13:23Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    13,
                    23,
                    0,
                    104,
                    0
                ],
                "title": "S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability\n  of Large Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability\n  of Large Reasoning Models"
                },
                "summary": "We introduce S1-Bench, a novel benchmark designed to evaluate Large Reasoning\nModels' (LRMs) performance on simple tasks that favor intuitive system 1\nthinking rather than deliberative system 2 reasoning. While LRMs have achieved\nsignificant breakthroughs in complex reasoning tasks through explicit chains of\nthought, their reliance on deep analytical thinking may limit their system 1\nthinking capabilities. Moreover, a lack of benchmark currently exists to\nevaluate LRMs' performance in tasks that require such capabilities. To fill\nthis gap, S1-Bench presents a set of simple, diverse, and naturally clear\nquestions across multiple domains and languages, specifically designed to\nassess LRMs' performance in such tasks. Our comprehensive evaluation of 22 LRMs\nreveals significant lower efficiency tendencies, with outputs averaging 15.5\ntimes longer than those of traditional small LLMs. Additionally, LRMs often\nidentify correct answers early but continue unnecessary deliberation, with some\nmodels even producing numerous errors. These findings highlight the rigid\nreasoning patterns of current LRMs and underscore the substantial development\nneeded to achieve balanced dual-system thinking capabilities that can adapt\nappropriately to task complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce S1-Bench, a novel benchmark designed to evaluate Large Reasoning\nModels' (LRMs) performance on simple tasks that favor intuitive system 1\nthinking rather than deliberative system 2 reasoning. While LRMs have achieved\nsignificant breakthroughs in complex reasoning tasks through explicit chains of\nthought, their reliance on deep analytical thinking may limit their system 1\nthinking capabilities. Moreover, a lack of benchmark currently exists to\nevaluate LRMs' performance in tasks that require such capabilities. To fill\nthis gap, S1-Bench presents a set of simple, diverse, and naturally clear\nquestions across multiple domains and languages, specifically designed to\nassess LRMs' performance in such tasks. Our comprehensive evaluation of 22 LRMs\nreveals significant lower efficiency tendencies, with outputs averaging 15.5\ntimes longer than those of traditional small LLMs. Additionally, LRMs often\nidentify correct answers early but continue unnecessary deliberation, with some\nmodels even producing numerous errors. These findings highlight the rigid\nreasoning patterns of current LRMs and underscore the substantial development\nneeded to achieve balanced dual-system thinking capabilities that can adapt\nappropriately to task complexity."
                },
                "authors": [
                    {
                        "name": "Wenyuan Zhang"
                    },
                    {
                        "name": "Shuaiyi Nie"
                    },
                    {
                        "name": "Xinghua Zhang"
                    },
                    {
                        "name": "Zefeng Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tingwen Liu"
                },
                "author": "Tingwen Liu",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10358v1",
                "updated": "2025-04-14T16:07:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    7,
                    16,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T16:07:16Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    7,
                    16,
                    0,
                    104,
                    0
                ],
                "title": "FingER: Content Aware Fine-grained Evaluation with Reasoning for\n  AI-Generated Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FingER: Content Aware Fine-grained Evaluation with Reasoning for\n  AI-Generated Videos"
                },
                "summary": "Recent advances in video generation have posed great challenges in the\nassessment of AI-generated content, particularly with the emergence of\nincreasingly sophisticated models. The various inconsistencies and defects\nobserved in such videos are inherently complex, making overall scoring\nnotoriously difficult. In this paper, we emphasize the critical importance of\nintegrating fine-grained reasoning into video evaluation, and we propose\n$\\textbf{F}$ing$\\textbf{ER}$, a novel entity-level reasoning evaluation\nframework that first automatically generates $\\textbf{F}$ine-grained\n$\\textbf{E}$ntity-level questions, and then answers those questions by a\n$\\textbf{R}$easoning model with scores, which can be subsequently weighted\nsummed to an overall score for different applications. Specifically, we\nleverage LLMs to derive entity-level questions across five distinct\nperspectives, which (i) often focus on some specific entities of the content,\nthereby making answering or scoring much easier by MLLMs, and (ii) are more\ninterpretable. Then we construct a FingER dataset, consisting of approximately\n3.3k videos and corresponding 60k fine-grained QA annotations, each with\ndetailed reasons. Based on that, we further investigate various training\nprotocols to best incentivize the reasoning capability of MLLMs for correct\nanswer prediction. Extensive experiments demonstrate that a reasoning model\ntrained using Group Relative Policy Optimization (GRPO) with a cold-start\nstrategy achieves the best performance. Notably, our model surpasses existing\nmethods by a relative margin of $11.8\\%$ on GenAI-Bench and $5.5\\%$ on\nMonetBench with only 3.3k training videos, which is at most one-tenth of the\ntraining samples utilized by other methods. Our code and dataset will be\nreleased soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in video generation have posed great challenges in the\nassessment of AI-generated content, particularly with the emergence of\nincreasingly sophisticated models. The various inconsistencies and defects\nobserved in such videos are inherently complex, making overall scoring\nnotoriously difficult. In this paper, we emphasize the critical importance of\nintegrating fine-grained reasoning into video evaluation, and we propose\n$\\textbf{F}$ing$\\textbf{ER}$, a novel entity-level reasoning evaluation\nframework that first automatically generates $\\textbf{F}$ine-grained\n$\\textbf{E}$ntity-level questions, and then answers those questions by a\n$\\textbf{R}$easoning model with scores, which can be subsequently weighted\nsummed to an overall score for different applications. Specifically, we\nleverage LLMs to derive entity-level questions across five distinct\nperspectives, which (i) often focus on some specific entities of the content,\nthereby making answering or scoring much easier by MLLMs, and (ii) are more\ninterpretable. Then we construct a FingER dataset, consisting of approximately\n3.3k videos and corresponding 60k fine-grained QA annotations, each with\ndetailed reasons. Based on that, we further investigate various training\nprotocols to best incentivize the reasoning capability of MLLMs for correct\nanswer prediction. Extensive experiments demonstrate that a reasoning model\ntrained using Group Relative Policy Optimization (GRPO) with a cold-start\nstrategy achieves the best performance. Notably, our model surpasses existing\nmethods by a relative margin of $11.8\\%$ on GenAI-Bench and $5.5\\%$ on\nMonetBench with only 3.3k training videos, which is at most one-tenth of the\ntraining samples utilized by other methods. Our code and dataset will be\nreleased soon."
                },
                "authors": [
                    {
                        "name": "Rui Chen"
                    },
                    {
                        "name": "Lei Sun"
                    },
                    {
                        "name": "Jing Tang"
                    },
                    {
                        "name": "Geng Li"
                    },
                    {
                        "name": "Xiangxiang Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangxiang Chu"
                },
                "author": "Xiangxiang Chu",
                "arxiv_comment": "10 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10356v1",
                "updated": "2025-04-14T16:05:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    5,
                    59,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T16:05:59Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    5,
                    59,
                    0,
                    104,
                    0
                ],
                "title": "MultiLoKo: a multilingual local knowledge benchmark for LLMs spanning 31\n  languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiLoKo: a multilingual local knowledge benchmark for LLMs spanning 31\n  languages"
                },
                "summary": "We present MultiLoKo, a new benchmark for evaluating multilinguality in LLMs\ncovering 31 languages. MultiLoKo consists of three partitions: a main partition\nconsisting of 500 questions per language, separately sourced to be locally\nrelevant to the specific language, and two translated partitions, containing\nhuman-authored translations from 30 non-English languages to English and vice\nversa. For comparison, we also release corresponding machine-authored\ntranslations. The data is equally distributed over two splits: a dev split and\na blind, out-of-distribution test split. MultiLoKo can be used to study a\nvariety of questions regarding the multilinguality of LLMs as well as\nmeta-questions about multilingual benchmark creation. We compute MultiLoKo\nscores for 11 base and chat models marketed to be multilingual and study their\naverage performance, their performance parity across languages, how much their\nability to answer questions depends on the question language, and which\nlanguages are most difficult. None of the models we studied performs well on\nMultiLoKo, as indicated by low average scores as well as large differences\nbetween the best and worst scoring languages. Furthermore, we find a\nsubstantial effect of the question language, indicating sub-optimal knowledge\ntransfer between languages. Lastly, we find that using local vs\nEnglish-translated data can result in differences more than 20 points for the\nbest performing models, drastically change the estimated difficulty of some\nlanguages. For using machines instead of human translations, we find a weaker\neffect on ordering of language difficulty, a larger difference in model\nrankings, and a substantial drop in estimated performance for all models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present MultiLoKo, a new benchmark for evaluating multilinguality in LLMs\ncovering 31 languages. MultiLoKo consists of three partitions: a main partition\nconsisting of 500 questions per language, separately sourced to be locally\nrelevant to the specific language, and two translated partitions, containing\nhuman-authored translations from 30 non-English languages to English and vice\nversa. For comparison, we also release corresponding machine-authored\ntranslations. The data is equally distributed over two splits: a dev split and\na blind, out-of-distribution test split. MultiLoKo can be used to study a\nvariety of questions regarding the multilinguality of LLMs as well as\nmeta-questions about multilingual benchmark creation. We compute MultiLoKo\nscores for 11 base and chat models marketed to be multilingual and study their\naverage performance, their performance parity across languages, how much their\nability to answer questions depends on the question language, and which\nlanguages are most difficult. None of the models we studied performs well on\nMultiLoKo, as indicated by low average scores as well as large differences\nbetween the best and worst scoring languages. Furthermore, we find a\nsubstantial effect of the question language, indicating sub-optimal knowledge\ntransfer between languages. Lastly, we find that using local vs\nEnglish-translated data can result in differences more than 20 points for the\nbest performing models, drastically change the estimated difficulty of some\nlanguages. For using machines instead of human translations, we find a weaker\neffect on ordering of language difficulty, a larger difference in model\nrankings, and a substantial drop in estimated performance for all models."
                },
                "authors": [
                    {
                        "name": "Dieuwke Hupkes"
                    },
                    {
                        "name": "Nikolay Bogoychev"
                    }
                ],
                "author_detail": {
                    "name": "Nikolay Bogoychev"
                },
                "author": "Nikolay Bogoychev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10352v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10352v1",
                "updated": "2025-04-14T16:03:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    3,
                    21,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T16:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    3,
                    21,
                    0,
                    104,
                    0
                ],
                "title": "Pseudo-Autoregressive Neural Codec Language Models for Efficient\n  Zero-Shot Text-to-Speech Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pseudo-Autoregressive Neural Codec Language Models for Efficient\n  Zero-Shot Text-to-Speech Synthesis"
                },
                "summary": "Recent zero-shot text-to-speech (TTS) systems face a common dilemma:\nautoregressive (AR) models suffer from slow generation and lack duration\ncontrollability, while non-autoregressive (NAR) models lack temporal modeling\nand typically require complex designs. In this paper, we introduce a novel\npseudo-autoregressive (PAR) codec language modeling approach that unifies AR\nand NAR modeling. Combining explicit temporal modeling from AR with parallel\ngeneration from NAR, PAR generates dynamic-length spans at fixed time steps.\nBuilding on PAR, we propose PALLE, a two-stage TTS system that leverages PAR\nfor initial generation followed by NAR refinement. In the first stage, PAR\nprogressively generates speech tokens along the time dimension, with each step\npredicting all positions in parallel but only retaining the left-most span. In\nthe second stage, low-confidence tokens are iteratively refined in parallel,\nleveraging the global contextual information. Experiments demonstrate that\nPALLE, trained on LibriTTS, outperforms state-of-the-art systems trained on\nlarge-scale data, including F5-TTS, E2-TTS, and MaskGCT, on the LibriSpeech\ntest-clean set in terms of speech quality, speaker similarity, and\nintelligibility, while achieving up to ten times faster inference speed. Audio\nsamples are available at https://anonymous-palle.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent zero-shot text-to-speech (TTS) systems face a common dilemma:\nautoregressive (AR) models suffer from slow generation and lack duration\ncontrollability, while non-autoregressive (NAR) models lack temporal modeling\nand typically require complex designs. In this paper, we introduce a novel\npseudo-autoregressive (PAR) codec language modeling approach that unifies AR\nand NAR modeling. Combining explicit temporal modeling from AR with parallel\ngeneration from NAR, PAR generates dynamic-length spans at fixed time steps.\nBuilding on PAR, we propose PALLE, a two-stage TTS system that leverages PAR\nfor initial generation followed by NAR refinement. In the first stage, PAR\nprogressively generates speech tokens along the time dimension, with each step\npredicting all positions in parallel but only retaining the left-most span. In\nthe second stage, low-confidence tokens are iteratively refined in parallel,\nleveraging the global contextual information. Experiments demonstrate that\nPALLE, trained on LibriTTS, outperforms state-of-the-art systems trained on\nlarge-scale data, including F5-TTS, E2-TTS, and MaskGCT, on the LibriSpeech\ntest-clean set in terms of speech quality, speaker similarity, and\nintelligibility, while achieving up to ten times faster inference speed. Audio\nsamples are available at https://anonymous-palle.github.io."
                },
                "authors": [
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Shujie Liu"
                    },
                    {
                        "name": "Jinyu Li"
                    },
                    {
                        "name": "Yuxuan Hu"
                    },
                    {
                        "name": "Haibin Wu"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Jianwei Yu"
                    },
                    {
                        "name": "Lingwei Meng"
                    },
                    {
                        "name": "Haiyang Sun"
                    },
                    {
                        "name": "Yanqing Liu"
                    },
                    {
                        "name": "Yan Lu"
                    },
                    {
                        "name": "Kai Yu"
                    },
                    {
                        "name": "Xie Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xie Chen"
                },
                "author": "Xie Chen",
                "arxiv_comment": "Submitted to ACM MM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10352v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10352v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08696v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08696v2",
                "updated": "2025-04-14T16:02:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    2,
                    38,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-11T17:03:58Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    3,
                    58,
                    4,
                    101,
                    0
                ],
                "title": "SeaView: Software Engineering Agent Visual Interface for Enhanced\n  Workflow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeaView: Software Engineering Agent Visual Interface for Enhanced\n  Workflow"
                },
                "summary": "Auto-regressive LLM-based software engineering (SWE) agents, henceforth SWE\nagents, have made tremendous progress (>60% on SWE-Bench Verified) on\nreal-world coding challenges including GitHub issue resolution. SWE agents use\na combination of reasoning, environment interaction and self-reflection to\nresolve issues thereby generating \"trajectories\". Analysis of SWE agent\ntrajectories is difficult, not only as they exceed LLM sequence length\n(sometimes, greater than 128k) but also because it involves a relatively\nprolonged interaction between an LLM and the environment managed by the agent.\nIn case of an agent error, it can be hard to decipher, locate and understand\nits scope. Similarly, it can be hard to track improvements or regression over\nmultiple runs or experiments. While a lot of research has gone into making\nthese SWE agents reach state-of-the-art, much less focus has been put into\ncreating tools to help analyze and visualize agent output. We propose a novel\ntool called SeaView: Software Engineering Agent Visual Interface for Enhanced\nWorkflow, with a vision to assist SWE-agent researchers to visualize and\ninspect their experiments. SeaView's novel mechanisms help compare experimental\nruns with varying hyper-parameters or LLMs, and quickly get an understanding of\nLLM or environment related problems. Based on our user study, experienced\nresearchers spend between 10 and 30 minutes to gather the information provided\nby SeaView, while researchers with little experience can spend between 30\nminutes to 1 hour to diagnose their experiment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive LLM-based software engineering (SWE) agents, henceforth SWE\nagents, have made tremendous progress (>60% on SWE-Bench Verified) on\nreal-world coding challenges including GitHub issue resolution. SWE agents use\na combination of reasoning, environment interaction and self-reflection to\nresolve issues thereby generating \"trajectories\". Analysis of SWE agent\ntrajectories is difficult, not only as they exceed LLM sequence length\n(sometimes, greater than 128k) but also because it involves a relatively\nprolonged interaction between an LLM and the environment managed by the agent.\nIn case of an agent error, it can be hard to decipher, locate and understand\nits scope. Similarly, it can be hard to track improvements or regression over\nmultiple runs or experiments. While a lot of research has gone into making\nthese SWE agents reach state-of-the-art, much less focus has been put into\ncreating tools to help analyze and visualize agent output. We propose a novel\ntool called SeaView: Software Engineering Agent Visual Interface for Enhanced\nWorkflow, with a vision to assist SWE-agent researchers to visualize and\ninspect their experiments. SeaView's novel mechanisms help compare experimental\nruns with varying hyper-parameters or LLMs, and quickly get an understanding of\nLLM or environment related problems. Based on our user study, experienced\nresearchers spend between 10 and 30 minutes to gather the information provided\nby SeaView, while researchers with little experience can spend between 30\nminutes to 1 hour to diagnose their experiment."
                },
                "authors": [
                    {
                        "name": "Timothy Bula"
                    },
                    {
                        "name": "Saurabh Pujar"
                    },
                    {
                        "name": "Luca Buratti"
                    },
                    {
                        "name": "Mihaela Bornea"
                    },
                    {
                        "name": "Avirup Sil"
                    }
                ],
                "author_detail": {
                    "name": "Avirup Sil"
                },
                "author": "Avirup Sil",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08696v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08696v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12044v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12044v2",
                "updated": "2025-04-14T16:02:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    2,
                    15,
                    0,
                    104,
                    0
                ],
                "published": "2024-11-18T20:31:38Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    20,
                    31,
                    38,
                    0,
                    323,
                    0
                ],
                "title": "ITACLIP: Boosting Training-Free Semantic Segmentation with Image, Text,\n  and Architectural Enhancements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ITACLIP: Boosting Training-Free Semantic Segmentation with Image, Text,\n  and Architectural Enhancements"
                },
                "summary": "Recent advances in foundational Vision Language Models (VLMs) have reshaped\nthe evaluation paradigm in computer vision tasks. These foundational models,\nespecially CLIP, have accelerated research in open-vocabulary computer vision\ntasks, including Open-Vocabulary Semantic Segmentation (OVSS). Although the\ninitial results are promising, the dense prediction capabilities of VLMs still\nrequire further improvement. In this study, we enhance the semantic\nsegmentation performance of CLIP by introducing new modules and modifications:\n1) architectural changes in the last layer of ViT and the incorporation of\nattention maps from the middle layers with the last layer, 2) Image\nEngineering: applying data augmentations to enrich input image representations,\nand 3) using Large Language Models (LLMs) to generate definitions and synonyms\nfor each class name to leverage CLIP's open-vocabulary capabilities. Our\ntraining-free method, ITACLIP, outperforms current state-of-the-art approaches\non segmentation benchmarks such as COCO-Stuff, COCO-Object, Pascal Context, and\nPascal VOC. Our code is available at https://github.com/m-arda-aydn/ITACLIP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in foundational Vision Language Models (VLMs) have reshaped\nthe evaluation paradigm in computer vision tasks. These foundational models,\nespecially CLIP, have accelerated research in open-vocabulary computer vision\ntasks, including Open-Vocabulary Semantic Segmentation (OVSS). Although the\ninitial results are promising, the dense prediction capabilities of VLMs still\nrequire further improvement. In this study, we enhance the semantic\nsegmentation performance of CLIP by introducing new modules and modifications:\n1) architectural changes in the last layer of ViT and the incorporation of\nattention maps from the middle layers with the last layer, 2) Image\nEngineering: applying data augmentations to enrich input image representations,\nand 3) using Large Language Models (LLMs) to generate definitions and synonyms\nfor each class name to leverage CLIP's open-vocabulary capabilities. Our\ntraining-free method, ITACLIP, outperforms current state-of-the-art approaches\non segmentation benchmarks such as COCO-Stuff, COCO-Object, Pascal Context, and\nPascal VOC. Our code is available at https://github.com/m-arda-aydn/ITACLIP."
                },
                "authors": [
                    {
                        "name": "M. Arda Aydn"
                    },
                    {
                        "name": "Efe Mert rpar"
                    },
                    {
                        "name": "Elvin Abdinli"
                    },
                    {
                        "name": "Gozde Unal"
                    },
                    {
                        "name": "Yusuf H. Sahin"
                    }
                ],
                "author_detail": {
                    "name": "Yusuf H. Sahin"
                },
                "author": "Yusuf H. Sahin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12044v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12044v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10342v1",
                "updated": "2025-04-14T15:50:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    50,
                    39,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:50:39Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    50,
                    39,
                    0,
                    104,
                    0
                ],
                "title": "VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain\n  Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain\n  Knowledge"
                },
                "summary": "Current multimodal benchmarks often conflate reasoning with domain-specific\nknowledge, making it difficult to isolate and evaluate general reasoning\nabilities in non-expert settings. To address this, we introduce VisualPuzzles,\na benchmark that targets visual reasoning while deliberately minimizing\nreliance on specialized knowledge. VisualPuzzles consists of diverse questions\nspanning five categories: algorithmic, analogical, deductive, inductive, and\nspatial reasoning. One major source of our questions is manually translated\nlogical reasoning questions from the Chinese Civil Service Examination.\nExperiments show that VisualPuzzles requires significantly less intensive\ndomain-specific knowledge and more complex reasoning compared to benchmarks\nlike MMMU, enabling us to better evaluate genuine multimodal reasoning.\nEvaluations show that state-of-the-art multimodal large language models\nconsistently lag behind human performance on VisualPuzzles, and that strong\nperformance on knowledge-intensive benchmarks does not necessarily translate to\nsuccess on reasoning-focused, knowledge-light tasks. Additionally, reasoning\nenhancements such as scaling up inference compute (with \"thinking\" modes) yield\ninconsistent gains across models and task types, and we observe no clear\ncorrelation between model size and performance. We also found that models\nexhibit different reasoning and answering patterns on VisualPuzzles compared to\nbenchmarks with heavier emphasis on knowledge. VisualPuzzles offers a clearer\nlens through which to evaluate reasoning capabilities beyond factual recall and\ndomain knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current multimodal benchmarks often conflate reasoning with domain-specific\nknowledge, making it difficult to isolate and evaluate general reasoning\nabilities in non-expert settings. To address this, we introduce VisualPuzzles,\na benchmark that targets visual reasoning while deliberately minimizing\nreliance on specialized knowledge. VisualPuzzles consists of diverse questions\nspanning five categories: algorithmic, analogical, deductive, inductive, and\nspatial reasoning. One major source of our questions is manually translated\nlogical reasoning questions from the Chinese Civil Service Examination.\nExperiments show that VisualPuzzles requires significantly less intensive\ndomain-specific knowledge and more complex reasoning compared to benchmarks\nlike MMMU, enabling us to better evaluate genuine multimodal reasoning.\nEvaluations show that state-of-the-art multimodal large language models\nconsistently lag behind human performance on VisualPuzzles, and that strong\nperformance on knowledge-intensive benchmarks does not necessarily translate to\nsuccess on reasoning-focused, knowledge-light tasks. Additionally, reasoning\nenhancements such as scaling up inference compute (with \"thinking\" modes) yield\ninconsistent gains across models and task types, and we observe no clear\ncorrelation between model size and performance. We also found that models\nexhibit different reasoning and answering patterns on VisualPuzzles compared to\nbenchmarks with heavier emphasis on knowledge. VisualPuzzles offers a clearer\nlens through which to evaluate reasoning capabilities beyond factual recall and\ndomain knowledge."
                },
                "authors": [
                    {
                        "name": "Yueqi Song"
                    },
                    {
                        "name": "Tianyue Ou"
                    },
                    {
                        "name": "Yibo Kong"
                    },
                    {
                        "name": "Zecheng Li"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Xiang Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Yue"
                },
                "author": "Xiang Yue",
                "arxiv_comment": "56 pages, 43 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10340v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10340v1",
                "updated": "2025-04-14T15:48:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    48,
                    56,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:48:56Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    48,
                    56,
                    0,
                    104,
                    0
                ],
                "title": "Forecasting from Clinical Textual Time Series: Adaptations of the\n  Encoder and Decoder Language Model Families",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting from Clinical Textual Time Series: Adaptations of the\n  Encoder and Decoder Language Model Families"
                },
                "summary": "Clinical case reports encode rich, temporal patient trajectories that are\noften underexploited by traditional machine learning methods relying on\nstructured data. In this work, we introduce the forecasting problem from\ntextual time series, where timestamped clinical findings--extracted via an\nLLM-assisted annotation pipeline--serve as the primary input for prediction. We\nsystematically evaluate a diverse suite of models, including fine-tuned\ndecoder-based large language models and encoder-based transformers, on tasks of\nevent occurrence prediction, temporal ordering, and survival analysis. Our\nexperiments reveal that encoder-based models consistently achieve higher F1\nscores and superior temporal concordance for short- and long-horizon event\nforecasting, while fine-tuned masking approaches enhance ranking performance.\nIn contrast, instruction-tuned decoder models demonstrate a relative advantage\nin survival analysis, especially in early prognosis settings. Our sensitivity\nanalyses further demonstrate the importance of time ordering, which requires\nclinical time series construction, as compared to text ordering, the format of\nthe text inputs that LLMs are classically trained on. This highlights the\nadditional benefit that can be ascertained from time-ordered corpora, with\nimplications for temporal tasks in the era of widespread LLM use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical case reports encode rich, temporal patient trajectories that are\noften underexploited by traditional machine learning methods relying on\nstructured data. In this work, we introduce the forecasting problem from\ntextual time series, where timestamped clinical findings--extracted via an\nLLM-assisted annotation pipeline--serve as the primary input for prediction. We\nsystematically evaluate a diverse suite of models, including fine-tuned\ndecoder-based large language models and encoder-based transformers, on tasks of\nevent occurrence prediction, temporal ordering, and survival analysis. Our\nexperiments reveal that encoder-based models consistently achieve higher F1\nscores and superior temporal concordance for short- and long-horizon event\nforecasting, while fine-tuned masking approaches enhance ranking performance.\nIn contrast, instruction-tuned decoder models demonstrate a relative advantage\nin survival analysis, especially in early prognosis settings. Our sensitivity\nanalyses further demonstrate the importance of time ordering, which requires\nclinical time series construction, as compared to text ordering, the format of\nthe text inputs that LLMs are classically trained on. This highlights the\nadditional benefit that can be ascertained from time-ordered corpora, with\nimplications for temporal tasks in the era of widespread LLM use."
                },
                "authors": [
                    {
                        "name": "Shahriar Noroozizadeh"
                    },
                    {
                        "name": "Sayantan Kumar"
                    },
                    {
                        "name": "Jeremy C. Weiss"
                    }
                ],
                "author_detail": {
                    "name": "Jeremy C. Weiss"
                },
                "author": "Jeremy C. Weiss",
                "arxiv_comment": "Machine Learning for Healthcare (MLHC 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10340v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10340v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10337v1",
                "updated": "2025-04-14T15:46:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    46,
                    33,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:46:33Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    46,
                    33,
                    0,
                    104,
                    0
                ],
                "title": "Heimdall: test-time scaling on the generative verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heimdall: test-time scaling on the generative verification"
                },
                "summary": "An AI system can create and maintain knowledge only to the extent that it can\nverify that knowledge itself. Recent work on long Chain-of-Thought reasoning\nhas demonstrated great potential of LLMs on solving competitive problems, but\ntheir verification ability remains to be weak and not sufficiently\ninvestigated. In this paper, we propose Heimdall, the long CoT verification LLM\nthat can accurately judge the correctness of solutions. With pure reinforcement\nlearning, we boost the verification accuracy from 62.5% to 94.5% on competitive\nmath problems. By scaling with repeated sampling, the accuracy further\nincreases to 97.5%. Through human evaluation, Heimdall demonstrates impressive\ngeneralization capabilities, successfully detecting most issues in challenging\nmath proofs, the type of which is not included during training. Furthermore, we\npropose Pessimistic Verification to extend the functionality of Heimdall to\nscaling up the problem solving. It calls Heimdall to judge the solutions from a\nsolver model and based on the pessimistic principle, selects the most likely\ncorrect solution with the least uncertainty. Taking\nDeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification\nimproves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute\nbudget and to 83.3% with more compute budget. With the stronger solver Gemini\n2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge\ndiscovery system, a ternary system where one poses questions, another provides\nsolutions, and the third verifies the solutions. Using the data synthesis work\nNuminaMath for the first two components, Heimdall effectively identifies\nproblematic records within the dataset and reveals that nearly half of the data\nis flawed, which interestingly aligns with the recent ablation studies from\nNuminaMath.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An AI system can create and maintain knowledge only to the extent that it can\nverify that knowledge itself. Recent work on long Chain-of-Thought reasoning\nhas demonstrated great potential of LLMs on solving competitive problems, but\ntheir verification ability remains to be weak and not sufficiently\ninvestigated. In this paper, we propose Heimdall, the long CoT verification LLM\nthat can accurately judge the correctness of solutions. With pure reinforcement\nlearning, we boost the verification accuracy from 62.5% to 94.5% on competitive\nmath problems. By scaling with repeated sampling, the accuracy further\nincreases to 97.5%. Through human evaluation, Heimdall demonstrates impressive\ngeneralization capabilities, successfully detecting most issues in challenging\nmath proofs, the type of which is not included during training. Furthermore, we\npropose Pessimistic Verification to extend the functionality of Heimdall to\nscaling up the problem solving. It calls Heimdall to judge the solutions from a\nsolver model and based on the pessimistic principle, selects the most likely\ncorrect solution with the least uncertainty. Taking\nDeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification\nimproves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute\nbudget and to 83.3% with more compute budget. With the stronger solver Gemini\n2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge\ndiscovery system, a ternary system where one poses questions, another provides\nsolutions, and the third verifies the solutions. Using the data synthesis work\nNuminaMath for the first two components, Heimdall effectively identifies\nproblematic records within the dataset and reveals that nearly half of the data\nis flawed, which interestingly aligns with the recent ablation studies from\nNuminaMath."
                },
                "authors": [
                    {
                        "name": "Wenlei Shi"
                    },
                    {
                        "name": "Xing Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xing Jin"
                },
                "author": "Xing Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10335v1",
                "updated": "2025-04-14T15:44:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    44,
                    45,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:44:45Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    44,
                    45,
                    0,
                    104,
                    0
                ],
                "title": "MorphTok: Morphologically Grounded Tokenization for Indian Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MorphTok: Morphologically Grounded Tokenization for Indian Languages"
                },
                "summary": "Tokenization is a crucial step in NLP, especially with the rise of large\nlanguage models (LLMs), impacting downstream performance, computational cost,\nand efficiency. Existing LLMs rely on the classical Byte-pair Encoding (BPE)\nalgorithm for subword tokenization that greedily merges frequent character\nbigrams. This often leads to segmentation that does not align with\nlinguistically meaningful units. To address this, we propose morphology-aware\nsegmentation as a pre-tokenization step prior to applying BPE. To facilitate\nmorphology-aware segmentation, we create a novel dataset for Hindi and Marathi,\nincorporating sandhi splitting to enhance the subword tokenization. Experiments\non downstream tasks show that morphologically grounded tokenization improves\nperformance for machine translation and language modeling. Additionally, to\nhandle the ambiguity in the Unicode characters for diacritics, particularly\ndependent vowels in syllable-based writing systems, we introduce Constrained\nBPE (CBPE), an extension to the traditional BPE algorithm that incorporates\nscript-specific constraints. Specifically, CBPE handles dependent vowels. Our\nresults show that CBPE achieves a 1.68\\% reduction in fertility scores while\nmaintaining comparable or improved downstream performance in machine\ntranslation, offering a computationally efficient alternative to standard BPE.\nMoreover, to evaluate segmentation across different tokenization algorithms, we\nintroduce a new human evaluation metric, \\textit{EvalTok}, enabling more\nhuman-grounded assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization is a crucial step in NLP, especially with the rise of large\nlanguage models (LLMs), impacting downstream performance, computational cost,\nand efficiency. Existing LLMs rely on the classical Byte-pair Encoding (BPE)\nalgorithm for subword tokenization that greedily merges frequent character\nbigrams. This often leads to segmentation that does not align with\nlinguistically meaningful units. To address this, we propose morphology-aware\nsegmentation as a pre-tokenization step prior to applying BPE. To facilitate\nmorphology-aware segmentation, we create a novel dataset for Hindi and Marathi,\nincorporating sandhi splitting to enhance the subword tokenization. Experiments\non downstream tasks show that morphologically grounded tokenization improves\nperformance for machine translation and language modeling. Additionally, to\nhandle the ambiguity in the Unicode characters for diacritics, particularly\ndependent vowels in syllable-based writing systems, we introduce Constrained\nBPE (CBPE), an extension to the traditional BPE algorithm that incorporates\nscript-specific constraints. Specifically, CBPE handles dependent vowels. Our\nresults show that CBPE achieves a 1.68\\% reduction in fertility scores while\nmaintaining comparable or improved downstream performance in machine\ntranslation, offering a computationally efficient alternative to standard BPE.\nMoreover, to evaluate segmentation across different tokenization algorithms, we\nintroduce a new human evaluation metric, \\textit{EvalTok}, enabling more\nhuman-grounded assessment."
                },
                "authors": [
                    {
                        "name": "Maharaj Brahma"
                    },
                    {
                        "name": "N J Karthika"
                    },
                    {
                        "name": "Atul Singh"
                    },
                    {
                        "name": "Devaraj Adiga"
                    },
                    {
                        "name": "Smruti Bhate"
                    },
                    {
                        "name": "Ganesh Ramakrishnan"
                    },
                    {
                        "name": "Rohit Saluja"
                    },
                    {
                        "name": "Maunendra Sankar Desarkar"
                    }
                ],
                "author_detail": {
                    "name": "Maunendra Sankar Desarkar"
                },
                "author": "Maunendra Sankar Desarkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06560v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06560v2",
                "updated": "2025-04-14T15:44:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    44,
                    36,
                    0,
                    104,
                    0
                ],
                "published": "2024-09-10T14:43:03Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    14,
                    43,
                    3,
                    1,
                    254,
                    0
                ],
                "title": "A Primer on Variational Inference for Physics-Informed Deep Generative\n  Modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Primer on Variational Inference for Physics-Informed Deep Generative\n  Modelling"
                },
                "summary": "Variational inference (VI) is a computationally efficient and scalable\nmethodology for approximate Bayesian inference. It strikes a balance between\naccuracy of uncertainty quantification and practical tractability. It excels at\ngenerative modelling and inversion tasks due to its built-in Bayesian\nregularisation and flexibility, essential qualities for physics related\nproblems. For such problems, the underlying physical model determines the\ndependence between variables of interest, which in turn will require a tailored\nderivation for the central VI learning objective. Furthermore, in many physical\ninference applications this structure has rich meaning and is essential for\naccurately capturing the dynamics of interest. In this paper, we provide an\naccessible and thorough technical introduction to VI for forward and inverse\nproblems, guiding the reader through standard derivations of the VI framework\nand how it can best be realized through deep learning. We then review and unify\nrecent literature exemplifying the flexibility allowed by VI. This paper is\ndesigned for a general scientific audience looking to solve physics-based\nproblems with an emphasis on uncertainty quantification",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational inference (VI) is a computationally efficient and scalable\nmethodology for approximate Bayesian inference. It strikes a balance between\naccuracy of uncertainty quantification and practical tractability. It excels at\ngenerative modelling and inversion tasks due to its built-in Bayesian\nregularisation and flexibility, essential qualities for physics related\nproblems. For such problems, the underlying physical model determines the\ndependence between variables of interest, which in turn will require a tailored\nderivation for the central VI learning objective. Furthermore, in many physical\ninference applications this structure has rich meaning and is essential for\naccurately capturing the dynamics of interest. In this paper, we provide an\naccessible and thorough technical introduction to VI for forward and inverse\nproblems, guiding the reader through standard derivations of the VI framework\nand how it can best be realized through deep learning. We then review and unify\nrecent literature exemplifying the flexibility allowed by VI. This paper is\ndesigned for a general scientific audience looking to solve physics-based\nproblems with an emphasis on uncertainty quantification"
                },
                "authors": [
                    {
                        "name": "Alex Glyn-Davies"
                    },
                    {
                        "name": "Arnaud Vadeboncoeur"
                    },
                    {
                        "name": "O. Deniz Akyildiz"
                    },
                    {
                        "name": "Ieva Kazlauskaite"
                    },
                    {
                        "name": "Mark Girolami"
                    }
                ],
                "author_detail": {
                    "name": "Mark Girolami"
                },
                "author": "Mark Girolami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06560v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06560v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10331v1",
                "updated": "2025-04-14T15:39:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    39,
                    31,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:39:31Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    39,
                    31,
                    0,
                    104,
                    0
                ],
                "title": "LL-Gaussian: Low-Light Scene Reconstruction and Enhancement via Gaussian\n  Splatting for Novel View Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LL-Gaussian: Low-Light Scene Reconstruction and Enhancement via Gaussian\n  Splatting for Novel View Synthesis"
                },
                "summary": "Novel view synthesis (NVS) in low-light scenes remains a significant\nchallenge due to degraded inputs characterized by severe noise, low dynamic\nrange (LDR) and unreliable initialization. While recent NeRF-based approaches\nhave shown promising results, most suffer from high computational costs, and\nsome rely on carefully captured or pre-processed data--such as RAW sensor\ninputs or multi-exposure sequences--which severely limits their practicality.\nIn contrast, 3D Gaussian Splatting (3DGS) enables real-time rendering with\ncompetitive visual fidelity; however, existing 3DGS-based methods struggle with\nlow-light sRGB inputs, resulting in unstable Gaussian initialization and\nineffective noise suppression. To address these challenges, we propose\nLL-Gaussian, a novel framework for 3D reconstruction and enhancement from\nlow-light sRGB images, enabling pseudo normal-light novel view synthesis. Our\nmethod introduces three key innovations: 1) an end-to-end Low-Light Gaussian\nInitialization Module (LLGIM) that leverages dense priors from learning-based\nMVS approach to generate high-quality initial point clouds; 2) a dual-branch\nGaussian decomposition model that disentangles intrinsic scene properties\n(reflectance and illumination) from transient interference, enabling stable and\ninterpretable optimization; 3) an unsupervised optimization strategy guided by\nboth physical constrains and diffusion prior to jointly steer decomposition and\nenhancement. Additionally, we contribute a challenging dataset collected in\nextreme low-light environments and demonstrate the effectiveness of\nLL-Gaussian. Compared to state-of-the-art NeRF-based methods, LL-Gaussian\nachieves up to 2,000 times faster inference and reduces training time to just\n2%, while delivering superior reconstruction and rendering quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novel view synthesis (NVS) in low-light scenes remains a significant\nchallenge due to degraded inputs characterized by severe noise, low dynamic\nrange (LDR) and unreliable initialization. While recent NeRF-based approaches\nhave shown promising results, most suffer from high computational costs, and\nsome rely on carefully captured or pre-processed data--such as RAW sensor\ninputs or multi-exposure sequences--which severely limits their practicality.\nIn contrast, 3D Gaussian Splatting (3DGS) enables real-time rendering with\ncompetitive visual fidelity; however, existing 3DGS-based methods struggle with\nlow-light sRGB inputs, resulting in unstable Gaussian initialization and\nineffective noise suppression. To address these challenges, we propose\nLL-Gaussian, a novel framework for 3D reconstruction and enhancement from\nlow-light sRGB images, enabling pseudo normal-light novel view synthesis. Our\nmethod introduces three key innovations: 1) an end-to-end Low-Light Gaussian\nInitialization Module (LLGIM) that leverages dense priors from learning-based\nMVS approach to generate high-quality initial point clouds; 2) a dual-branch\nGaussian decomposition model that disentangles intrinsic scene properties\n(reflectance and illumination) from transient interference, enabling stable and\ninterpretable optimization; 3) an unsupervised optimization strategy guided by\nboth physical constrains and diffusion prior to jointly steer decomposition and\nenhancement. Additionally, we contribute a challenging dataset collected in\nextreme low-light environments and demonstrate the effectiveness of\nLL-Gaussian. Compared to state-of-the-art NeRF-based methods, LL-Gaussian\nachieves up to 2,000 times faster inference and reduces training time to just\n2%, while delivering superior reconstruction and rendering quality."
                },
                "authors": [
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Fenggen Yu"
                    },
                    {
                        "name": "Huiyao Xu"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Changqing Zou"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zou"
                },
                "author": "Changqing Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10326v1",
                "updated": "2025-04-14T15:34:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    34,
                    26,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:34:26Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    34,
                    26,
                    0,
                    104,
                    0
                ],
                "title": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference"
                },
                "summary": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks."
                },
                "authors": [
                    {
                        "name": "Yangshen Deng"
                    },
                    {
                        "name": "Zhengxin You"
                    },
                    {
                        "name": "Long Xiang"
                    },
                    {
                        "name": "Qilong Li"
                    },
                    {
                        "name": "Peiqi Yuan"
                    },
                    {
                        "name": "Zhaoyang Hong"
                    },
                    {
                        "name": "Yitao Zheng"
                    },
                    {
                        "name": "Wanting Li"
                    },
                    {
                        "name": "Runzhong Li"
                    },
                    {
                        "name": "Haotian Liu"
                    },
                    {
                        "name": "Kyriakos Mouratidis"
                    },
                    {
                        "name": "Man Lung Yiu"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Qiaomu Shen"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Bo Tang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Tang"
                },
                "author": "Bo Tang",
                "arxiv_comment": "14 pages, 12 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.1; H.3.2; H.3.3; H.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10320v1",
                "updated": "2025-04-14T15:30:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    30,
                    3,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:30:03Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    30,
                    3,
                    0,
                    104,
                    0
                ],
                "title": "SlowFastVAD: Video Anomaly Detection via Integrating Simple Detector and\n  RAG-Enhanced Vision-Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlowFastVAD: Video Anomaly Detection via Integrating Simple Detector and\n  RAG-Enhanced Vision-Language Model"
                },
                "summary": "Video anomaly detection (VAD) aims to identify unexpected events in videos\nand has wide applications in safety-critical domains. While semi-supervised\nmethods trained on only normal samples have gained traction, they often suffer\nfrom high false alarm rates and poor interpretability. Recently,\nvision-language models (VLMs) have demonstrated strong multimodal reasoning\ncapabilities, offering new opportunities for explainable anomaly detection.\nHowever, their high computational cost and lack of domain adaptation hinder\nreal-time deployment and reliability. Inspired by dual complementary pathways\nin human visual perception, we propose SlowFastVAD, a hybrid framework that\nintegrates a fast anomaly detector with a slow anomaly detector (namely a\nretrieval augmented generation (RAG) enhanced VLM), to address these\nlimitations. Specifically, the fast detector first provides coarse anomaly\nconfidence scores, and only a small subset of ambiguous segments, rather than\nthe entire video, is further analyzed by the slower yet more interpretable VLM\nfor elaborate detection and reasoning. Furthermore, to adapt VLMs to\ndomain-specific VAD scenarios, we construct a knowledge base including normal\npatterns based on few normal samples and abnormal patterns inferred by VLMs.\nDuring inference, relevant patterns are retrieved and used to augment prompts\nfor anomaly reasoning. Finally, we smoothly fuse the anomaly confidence of fast\nand slow detectors to enhance robustness of anomaly detection. Extensive\nexperiments on four benchmarks demonstrate that SlowFastVAD effectively\ncombines the strengths of both fast and slow detectors, and achieves remarkable\ndetection accuracy and interpretability with significantly reduced\ncomputational overhead, making it well-suited for real-world VAD applications\nwith high reliability requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video anomaly detection (VAD) aims to identify unexpected events in videos\nand has wide applications in safety-critical domains. While semi-supervised\nmethods trained on only normal samples have gained traction, they often suffer\nfrom high false alarm rates and poor interpretability. Recently,\nvision-language models (VLMs) have demonstrated strong multimodal reasoning\ncapabilities, offering new opportunities for explainable anomaly detection.\nHowever, their high computational cost and lack of domain adaptation hinder\nreal-time deployment and reliability. Inspired by dual complementary pathways\nin human visual perception, we propose SlowFastVAD, a hybrid framework that\nintegrates a fast anomaly detector with a slow anomaly detector (namely a\nretrieval augmented generation (RAG) enhanced VLM), to address these\nlimitations. Specifically, the fast detector first provides coarse anomaly\nconfidence scores, and only a small subset of ambiguous segments, rather than\nthe entire video, is further analyzed by the slower yet more interpretable VLM\nfor elaborate detection and reasoning. Furthermore, to adapt VLMs to\ndomain-specific VAD scenarios, we construct a knowledge base including normal\npatterns based on few normal samples and abnormal patterns inferred by VLMs.\nDuring inference, relevant patterns are retrieved and used to augment prompts\nfor anomaly reasoning. Finally, we smoothly fuse the anomaly confidence of fast\nand slow detectors to enhance robustness of anomaly detection. Extensive\nexperiments on four benchmarks demonstrate that SlowFastVAD effectively\ncombines the strengths of both fast and slow detectors, and achieves remarkable\ndetection accuracy and interpretability with significantly reduced\ncomputational overhead, making it well-suited for real-world VAD applications\nwith high reliability requirements."
                },
                "authors": [
                    {
                        "name": "Zongcan Ding"
                    },
                    {
                        "name": "Haodong Zhang"
                    },
                    {
                        "name": "Peng Wu"
                    },
                    {
                        "name": "Guansong Pang"
                    },
                    {
                        "name": "Zhiwei Yang"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Yanning Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yanning Zhang"
                },
                "author": "Yanning Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12110v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12110v4",
                "updated": "2025-04-14T15:21:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    21,
                    49,
                    0,
                    104,
                    0
                ],
                "published": "2025-02-17T18:36:14Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    18,
                    36,
                    14,
                    0,
                    48,
                    0
                ],
                "title": "A-MEM: Agentic Memory for LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A-MEM: Agentic Memory for LLM Agents"
                },
                "summary": "While large language model (LLM) agents can effectively use external tools\nfor complex real-world tasks, they require memory systems to leverage\nhistorical experiences. Current memory systems enable basic storage and\nretrieval but lack sophisticated memory organization, despite recent attempts\nto incorporate graph databases. Moreover, these systems' fixed operations and\nstructures limit their adaptability across diverse tasks. To address this\nlimitation, this paper proposes a novel agentic memory system for LLM agents\nthat can dynamically organize memories in an agentic way. Following the basic\nprinciples of the Zettelkasten method, we designed our memory system to create\ninterconnected knowledge networks through dynamic indexing and linking. When a\nnew memory is added, we generate a comprehensive note containing multiple\nstructured attributes, including contextual descriptions, keywords, and tags.\nThe system then analyzes historical memories to identify relevant connections,\nestablishing links where meaningful similarities exist. Additionally, this\nprocess enables memory evolution - as new memories are integrated, they can\ntrigger updates to the contextual representations and attributes of existing\nhistorical memories, allowing the memory network to continuously refine its\nunderstanding. Our approach combines the structured organization principles of\nZettelkasten with the flexibility of agent-driven decision making, allowing for\nmore adaptive and context-aware memory management. Empirical experiments on six\nfoundation models show superior improvement against existing SOTA baselines.\nThe source code for evaluating performance is available at\nhttps://github.com/WujiangXu/AgenticMemory, while the source code of agentic\nmemory system is available at https://github.com/agiresearch/A-mem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language model (LLM) agents can effectively use external tools\nfor complex real-world tasks, they require memory systems to leverage\nhistorical experiences. Current memory systems enable basic storage and\nretrieval but lack sophisticated memory organization, despite recent attempts\nto incorporate graph databases. Moreover, these systems' fixed operations and\nstructures limit their adaptability across diverse tasks. To address this\nlimitation, this paper proposes a novel agentic memory system for LLM agents\nthat can dynamically organize memories in an agentic way. Following the basic\nprinciples of the Zettelkasten method, we designed our memory system to create\ninterconnected knowledge networks through dynamic indexing and linking. When a\nnew memory is added, we generate a comprehensive note containing multiple\nstructured attributes, including contextual descriptions, keywords, and tags.\nThe system then analyzes historical memories to identify relevant connections,\nestablishing links where meaningful similarities exist. Additionally, this\nprocess enables memory evolution - as new memories are integrated, they can\ntrigger updates to the contextual representations and attributes of existing\nhistorical memories, allowing the memory network to continuously refine its\nunderstanding. Our approach combines the structured organization principles of\nZettelkasten with the flexibility of agent-driven decision making, allowing for\nmore adaptive and context-aware memory management. Empirical experiments on six\nfoundation models show superior improvement against existing SOTA baselines.\nThe source code for evaluating performance is available at\nhttps://github.com/WujiangXu/AgenticMemory, while the source code of agentic\nmemory system is available at https://github.com/agiresearch/A-mem."
                },
                "authors": [
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Zujie Liang"
                    },
                    {
                        "name": "Kai Mei"
                    },
                    {
                        "name": "Hang Gao"
                    },
                    {
                        "name": "Juntao Tan"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12110v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12110v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10309v1",
                "updated": "2025-04-14T15:18:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    18,
                    59,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:18:59Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    18,
                    59,
                    0,
                    104,
                    0
                ],
                "title": "AutoStyle-TTS: Retrieval-Augmented Generation based Automatic Style\n  Matching Text-to-Speech Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoStyle-TTS: Retrieval-Augmented Generation based Automatic Style\n  Matching Text-to-Speech Synthesis"
                },
                "summary": "With the advancement of speech synthesis technology, users have higher\nexpectations for the naturalness and expressiveness of synthesized speech. But\nprevious research ignores the importance of prompt selection. This study\nproposes a text-to-speech (TTS) framework based on Retrieval-Augmented\nGeneration (RAG) technology, which can dynamically adjust the speech style\naccording to the text content to achieve more natural and vivid communication\neffects. We have constructed a speech style knowledge database containing\nhigh-quality speech samples in various contexts and developed a style matching\nscheme. This scheme uses embeddings, extracted by Llama, PER-LLM-Embedder,and\nMoka, to match with samples in the knowledge database, selecting the most\nappropriate speech style for synthesis. Furthermore, our empirical research\nvalidates the effectiveness of the proposed method. Our demo can be viewed at:\nhttps://thuhcsi.github.io/icme2025-AutoStyle-TTS",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancement of speech synthesis technology, users have higher\nexpectations for the naturalness and expressiveness of synthesized speech. But\nprevious research ignores the importance of prompt selection. This study\nproposes a text-to-speech (TTS) framework based on Retrieval-Augmented\nGeneration (RAG) technology, which can dynamically adjust the speech style\naccording to the text content to achieve more natural and vivid communication\neffects. We have constructed a speech style knowledge database containing\nhigh-quality speech samples in various contexts and developed a style matching\nscheme. This scheme uses embeddings, extracted by Llama, PER-LLM-Embedder,and\nMoka, to match with samples in the knowledge database, selecting the most\nappropriate speech style for synthesis. Furthermore, our empirical research\nvalidates the effectiveness of the proposed method. Our demo can be viewed at:\nhttps://thuhcsi.github.io/icme2025-AutoStyle-TTS"
                },
                "authors": [
                    {
                        "name": "Dan Luo"
                    },
                    {
                        "name": "Chengyuan Ma"
                    },
                    {
                        "name": "Weiqin Li"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Zhiyong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Wu"
                },
                "author": "Zhiyong Wu",
                "arxiv_comment": "accepted by ICME25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07615v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07615v2",
                "updated": "2025-04-14T15:15:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    15,
                    54,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-10T10:05:15Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    5,
                    15,
                    3,
                    100,
                    0
                ],
                "title": "VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model"
                },
                "summary": "Recently DeepSeek R1 has shown that reinforcement learning (RL) can\nsubstantially improve the reasoning capabilities of Large Language Models\n(LLMs) through a simple yet effective design. The core of R1 lies in its\nrule-based reward formulation, which leverages tasks with deterministic\nground-truth answers to enable precise and stable reward computation. In the\nvisual domain, we similarly observe that a wide range of visual understanding\ntasks are inherently equipped with well-defined ground-truth annotations. This\nproperty makes them naturally compatible with rule-based reward mechanisms.\nMotivated by this observation, we investigate the extension of R1-style\nreinforcement learning to Vision-Language Models (VLMs), aiming to enhance\ntheir visual reasoning capabilities. To this end, we develop VLM-R1, a\ndedicated framework designed to harness RL for improving VLMs' performance on\ngeneral vision-language tasks. Using this framework, we further explore the\nfeasibility of applying RL to visual domain. Experimental results indicate that\nthe RL-based model not only delivers competitive performance on visual\nunderstanding tasks but also surpasses Supervised Fine-Tuning (SFT) in\ngeneralization ability. Furthermore, we conduct comprehensive ablation studies\nthat uncover a series of noteworthy insights, including the presence of reward\nhacking in object detection, the emergence of the \"OD aha moment\", the impact\nof training data quality, and the scaling behavior of RL across different model\nsizes. Through these analyses, we aim to deepen the understanding of how\nreinforcement learning enhances the capabilities of vision-language models, and\nwe hope our findings and open-source contributions will support continued\nprogress in the vision-language RL community. Our code and model are available\nat https://github.com/om-ai-lab/VLM-R1",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently DeepSeek R1 has shown that reinforcement learning (RL) can\nsubstantially improve the reasoning capabilities of Large Language Models\n(LLMs) through a simple yet effective design. The core of R1 lies in its\nrule-based reward formulation, which leverages tasks with deterministic\nground-truth answers to enable precise and stable reward computation. In the\nvisual domain, we similarly observe that a wide range of visual understanding\ntasks are inherently equipped with well-defined ground-truth annotations. This\nproperty makes them naturally compatible with rule-based reward mechanisms.\nMotivated by this observation, we investigate the extension of R1-style\nreinforcement learning to Vision-Language Models (VLMs), aiming to enhance\ntheir visual reasoning capabilities. To this end, we develop VLM-R1, a\ndedicated framework designed to harness RL for improving VLMs' performance on\ngeneral vision-language tasks. Using this framework, we further explore the\nfeasibility of applying RL to visual domain. Experimental results indicate that\nthe RL-based model not only delivers competitive performance on visual\nunderstanding tasks but also surpasses Supervised Fine-Tuning (SFT) in\ngeneralization ability. Furthermore, we conduct comprehensive ablation studies\nthat uncover a series of noteworthy insights, including the presence of reward\nhacking in object detection, the emergence of the \"OD aha moment\", the impact\nof training data quality, and the scaling behavior of RL across different model\nsizes. Through these analyses, we aim to deepen the understanding of how\nreinforcement learning enhances the capabilities of vision-language models, and\nwe hope our findings and open-source contributions will support continued\nprogress in the vision-language RL community. Our code and model are available\nat https://github.com/om-ai-lab/VLM-R1"
                },
                "authors": [
                    {
                        "name": "Haozhan Shen"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Jingcheng Li"
                    },
                    {
                        "name": "Chunxin Fang"
                    },
                    {
                        "name": "Yibo Ma"
                    },
                    {
                        "name": "Jiajia Liao"
                    },
                    {
                        "name": "Qiaoli Shen"
                    },
                    {
                        "name": "Zilun Zhang"
                    },
                    {
                        "name": "Kangjia Zhao"
                    },
                    {
                        "name": "Qianqian Zhang"
                    },
                    {
                        "name": "Ruochen Xu"
                    },
                    {
                        "name": "Tiancheng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tiancheng Zhao"
                },
                "author": "Tiancheng Zhao",
                "arxiv_comment": "11 pages, fix some minor typos in the previous version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07615v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07615v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03338v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03338v2",
                "updated": "2025-04-14T15:12:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    12,
                    17,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-04T10:42:56Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    10,
                    42,
                    56,
                    4,
                    94,
                    0
                ],
                "title": "BabyLM's First Words: Word Segmentation as a Phonological Probing Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BabyLM's First Words: Word Segmentation as a Phonological Probing Task"
                },
                "summary": "Language models provide a key framework for studying linguistic theories\nbased on prediction, but phonological analysis using large language models\n(LLMs) is difficult; there are few phonological benchmarks beyond English and\nthe standard input representation used in LLMs (subwords of graphemes) is not\nsuitable for analyzing the representation of phonemes. In this work, we\ndemonstrate how word segmentation can be used as a phonological probing task,\nallowing us to study the representations learned by phoneme-based language\nmodels trained on child-directed speech across 31 languages. Following\ncomputational models of word segmentation, we present unsupervised methods for\nextracting word boundaries from a trained model using the observation that\nprediction-error peaks at the start of words. We also use linear probes to\nidentify that these models implicitly track word boundaries, even when they do\nnot appear in training. This cross-lingual work corroborates statistical\nlearning theories of acquisition and empirically motivates new methods for\ntraining subword tokenizers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models provide a key framework for studying linguistic theories\nbased on prediction, but phonological analysis using large language models\n(LLMs) is difficult; there are few phonological benchmarks beyond English and\nthe standard input representation used in LLMs (subwords of graphemes) is not\nsuitable for analyzing the representation of phonemes. In this work, we\ndemonstrate how word segmentation can be used as a phonological probing task,\nallowing us to study the representations learned by phoneme-based language\nmodels trained on child-directed speech across 31 languages. Following\ncomputational models of word segmentation, we present unsupervised methods for\nextracting word boundaries from a trained model using the observation that\nprediction-error peaks at the start of words. We also use linear probes to\nidentify that these models implicitly track word boundaries, even when they do\nnot appear in training. This cross-lingual work corroborates statistical\nlearning theories of acquisition and empirically motivates new methods for\ntraining subword tokenizers."
                },
                "authors": [
                    {
                        "name": "Zbulon Goriely"
                    },
                    {
                        "name": "Paula Buttery"
                    }
                ],
                "author_detail": {
                    "name": "Paula Buttery"
                },
                "author": "Paula Buttery",
                "arxiv_comment": "17 pages, 10 figures, submitted to CoNLL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03338v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03338v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13843v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13843v2",
                "updated": "2025-04-14T15:10:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    10,
                    31,
                    0,
                    104,
                    0
                ],
                "published": "2024-03-17T17:45:04Z",
                "published_parsed": [
                    2024,
                    3,
                    17,
                    17,
                    45,
                    4,
                    6,
                    77,
                    0
                ],
                "title": "Machine Learning and Transformers for Thyroid Carcinoma Diagnosis: A\n  Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Learning and Transformers for Thyroid Carcinoma Diagnosis: A\n  Review"
                },
                "summary": "The growing interest in developing smart diagnostic systems to help medical\nexperts process extensive data for treating incurable diseases has been\nnotable. In particular, the challenge of identifying thyroid cancer (TC) has\nseen progress with the use of machine learning (ML) and big data analysis,\nincorporating Transformers to evaluate TC prognosis and determine the risk of\nmalignancy in individuals. This review article presents a summary of various\nstudies on AI-based approaches, especially those employing Transformers, for\ndiagnosing TC. It introduces a new categorization system for these methods\nbased on artificial intelligence (AI) algorithms, the goals of the framework,\nand the computing environments used. Additionally, it scrutinizes and contrasts\nthe available TC datasets by their features. The paper highlights the\nimportance of AI instruments in aiding the diagnosis and treatment of TC\nthrough supervised, unsupervised, or mixed approaches, with a special focus on\nthe ongoing importance of Transformers and large language models (LLMs) in\nmedical diagnostics and disease management. It further discusses the progress\nmade and the continuing obstacles in this area. Lastly, it explores future\ndirections and focuses within this research field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing interest in developing smart diagnostic systems to help medical\nexperts process extensive data for treating incurable diseases has been\nnotable. In particular, the challenge of identifying thyroid cancer (TC) has\nseen progress with the use of machine learning (ML) and big data analysis,\nincorporating Transformers to evaluate TC prognosis and determine the risk of\nmalignancy in individuals. This review article presents a summary of various\nstudies on AI-based approaches, especially those employing Transformers, for\ndiagnosing TC. It introduces a new categorization system for these methods\nbased on artificial intelligence (AI) algorithms, the goals of the framework,\nand the computing environments used. Additionally, it scrutinizes and contrasts\nthe available TC datasets by their features. The paper highlights the\nimportance of AI instruments in aiding the diagnosis and treatment of TC\nthrough supervised, unsupervised, or mixed approaches, with a special focus on\nthe ongoing importance of Transformers and large language models (LLMs) in\nmedical diagnostics and disease management. It further discusses the progress\nmade and the continuing obstacles in this area. Lastly, it explores future\ndirections and focuses within this research field."
                },
                "authors": [
                    {
                        "name": "Yassine Habchi"
                    },
                    {
                        "name": "Hamza Kheddar"
                    },
                    {
                        "name": "Yassine Himeur"
                    },
                    {
                        "name": "Mohamed Chahine Ghanem"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Chahine Ghanem"
                },
                "author": "Mohamed Chahine Ghanem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13843v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13843v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10299v1",
                "updated": "2025-04-14T15:08:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    8,
                    42,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:08:42Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    8,
                    42,
                    0,
                    104,
                    0
                ],
                "title": "IRR-Based AS Type of Relationship Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IRR-Based AS Type of Relationship Inference"
                },
                "summary": "The Internet comprises tens of thousands of autonomous systems (ASes) whose\ncommercial relationships are not publicly announced. The classification of the\nType of Relationship (ToR) between ASes has been extensively studied over the\npast two decades due to its relevance in network routing management and\nsecurity.\n  This paper presents a new approach to ToR classification, leveraging publicly\navailable BGP data from the Internet Routing Registry (IRR). We show how the\nIRR can be mined and the results refined to achieve a large and accurate ToR\ndatabase. Using a ground truth database with hundreds of entries we show that\nwe indeed manage to obtain high accuracy. About two-thirds of our ToRs are new,\nnamely, they were not obtained by previous works, which means that we enrich\nour ToR knowledge with links that are otherwise missed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Internet comprises tens of thousands of autonomous systems (ASes) whose\ncommercial relationships are not publicly announced. The classification of the\nType of Relationship (ToR) between ASes has been extensively studied over the\npast two decades due to its relevance in network routing management and\nsecurity.\n  This paper presents a new approach to ToR classification, leveraging publicly\navailable BGP data from the Internet Routing Registry (IRR). We show how the\nIRR can be mined and the results refined to achieve a large and accurate ToR\ndatabase. Using a ground truth database with hundreds of entries we show that\nwe indeed manage to obtain high accuracy. About two-thirds of our ToRs are new,\nnamely, they were not obtained by previous works, which means that we enrich\nour ToR knowledge with links that are otherwise missed."
                },
                "authors": [
                    {
                        "name": "Amit Zulan"
                    },
                    {
                        "name": "Omer Miron"
                    },
                    {
                        "name": "Tal Shapira"
                    },
                    {
                        "name": "Yuval Shavitt"
                    }
                ],
                "author_detail": {
                    "name": "Yuval Shavitt"
                },
                "author": "Yuval Shavitt",
                "arxiv_comment": "19 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06334v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06334v2",
                "updated": "2025-04-14T15:07:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    7,
                    15,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-08T18:00:00Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    18,
                    0,
                    0,
                    1,
                    98,
                    0
                ],
                "title": "UNCOVER/MegaScience: No Evidence of Environmental Quenching in a\n  z$\\sim$2.6 Proto-cluster",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNCOVER/MegaScience: No Evidence of Environmental Quenching in a\n  z$\\sim$2.6 Proto-cluster"
                },
                "summary": "Environmental quenching -- where interactions with other galaxies and/or the\nintra-cluster medium (ICM) suppress star formation in low-mass galaxies -- has\nlong been proposed as the primary driver to establish the red sequence for\nlow-mass galaxies within clusters at low redshift ($z<1$). However, we still do\nnot know whether these environmental quenching mechanisms are also active at\nhigher redshifts in proto-cluster environments that have yet to fully\nvirialize. In large part, this regime has remained unexplored due to\nobservational limitations; however, the James Webb Space Telescope has recently\nopened a new window into the role of environmental quenching on low-mass\n(log(M$_{\\star}$/M$_{\\odot}$)$<$9.5) galaxies at cosmic noon ($2 < z < 3$).\nHere, we use data from the JWST UNCOVER and MegaScience programs to directly\nprobe the role of environmental quenching on low-mass galaxies in a\nnewly-discovered $z\\approx 2.6$ overdensity. Leveraging the deep imaging and R\n$\\sim$ 15 spectrophotometry enabled by these JWST/NIRCam data, we analyze the\nstellar populations and inferred star formation histories (SFHs) of 20 low-mass\n(8.5$<$log(M$_{\\star}$/M$_{\\odot}$)$\\leq$9.0) quiescent galaxies in the\noverdense environment and compare to a similar sample of 18 such galaxies in\nthe field. The SFHs of quiescent galaxies in the proto-cluster and field across\nthe entire probed stellar mass regime\n(8.5$<$log(M$_{\\star}$/M$_{\\odot}$)$\\leq$11.0) are indistinguishable,\ndemonstrating that the environment at cosmic noon is not yet accelerating\nquenching compared to the field. This is consistent with expectations that\nproto-clusters at $z>2$ have yet to virialize and develop a dense enough\nenvironment to efficiently quench low-mass galaxies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Environmental quenching -- where interactions with other galaxies and/or the\nintra-cluster medium (ICM) suppress star formation in low-mass galaxies -- has\nlong been proposed as the primary driver to establish the red sequence for\nlow-mass galaxies within clusters at low redshift ($z<1$). However, we still do\nnot know whether these environmental quenching mechanisms are also active at\nhigher redshifts in proto-cluster environments that have yet to fully\nvirialize. In large part, this regime has remained unexplored due to\nobservational limitations; however, the James Webb Space Telescope has recently\nopened a new window into the role of environmental quenching on low-mass\n(log(M$_{\\star}$/M$_{\\odot}$)$<$9.5) galaxies at cosmic noon ($2 < z < 3$).\nHere, we use data from the JWST UNCOVER and MegaScience programs to directly\nprobe the role of environmental quenching on low-mass galaxies in a\nnewly-discovered $z\\approx 2.6$ overdensity. Leveraging the deep imaging and R\n$\\sim$ 15 spectrophotometry enabled by these JWST/NIRCam data, we analyze the\nstellar populations and inferred star formation histories (SFHs) of 20 low-mass\n(8.5$<$log(M$_{\\star}$/M$_{\\odot}$)$\\leq$9.0) quiescent galaxies in the\noverdense environment and compare to a similar sample of 18 such galaxies in\nthe field. The SFHs of quiescent galaxies in the proto-cluster and field across\nthe entire probed stellar mass regime\n(8.5$<$log(M$_{\\star}$/M$_{\\odot}$)$\\leq$11.0) are indistinguishable,\ndemonstrating that the environment at cosmic noon is not yet accelerating\nquenching compared to the field. This is consistent with expectations that\nproto-clusters at $z>2$ have yet to virialize and develop a dense enough\nenvironment to efficiently quench low-mass galaxies."
                },
                "authors": [
                    {
                        "name": "Richard Pan"
                    },
                    {
                        "name": "Katherine A. Suess"
                    },
                    {
                        "name": "Danilo Marchesini"
                    },
                    {
                        "name": "Bingjie Wang"
                    },
                    {
                        "name": "Joel Leja"
                    },
                    {
                        "name": "Sam E. Cutler"
                    },
                    {
                        "name": "Katherine E. Whitaker"
                    },
                    {
                        "name": "Rachel Bezanson"
                    },
                    {
                        "name": "Sedona H. Price"
                    },
                    {
                        "name": "Lukas J. Furtak"
                    },
                    {
                        "name": "John R. Weaver"
                    },
                    {
                        "name": "Ivo Labb"
                    },
                    {
                        "name": "Gabriel Brammer"
                    },
                    {
                        "name": "Yunchong Zhang"
                    },
                    {
                        "name": "Pratika Dayal"
                    },
                    {
                        "name": "Robert Feldmann"
                    },
                    {
                        "name": "Jenny E. Greene"
                    },
                    {
                        "name": "Tim B. Miller"
                    },
                    {
                        "name": "Ikki Mitsuhashi"
                    },
                    {
                        "name": "Themiya Nanayakkara"
                    },
                    {
                        "name": "Erica J. Nelson"
                    },
                    {
                        "name": "David J. Setton"
                    },
                    {
                        "name": "Adi Zitrin"
                    }
                ],
                "author_detail": {
                    "name": "Adi Zitrin"
                },
                "author": "Adi Zitrin",
                "arxiv_comment": "12 pages, 6 figures, Submitted to ApJL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06334v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05168v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05168v4",
                "updated": "2025-04-14T14:58:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    58,
                    1,
                    0,
                    104,
                    0
                ],
                "published": "2024-10-07T16:25:39Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    25,
                    39,
                    0,
                    281,
                    0
                ],
                "title": "ReasoningRank: Teaching Student Models to Rank through Reasoning-Based\n  Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReasoningRank: Teaching Student Models to Rank through Reasoning-Based\n  Knowledge Distillation"
                },
                "summary": "Reranking documents based on their relevance to a given query is a critical\ntask in information retrieval. Traditional reranking methods often lack\ntransparency and rely on proprietary models, hindering reproducibility and\ninterpretability. We propose Reason-to-Rank (R2R), a novel open-source\nreranking approach that enhances transparency by generating two types of\nreasoning: direct relevance reasoning, which explains how a document addresses\nthe query, and comparison reasoning, which justifies the relevance of one\ndocument over another. We leverage large language models (LLMs) as teacher\nmodels to generate these explanations and distill this knowledge into smaller,\nopenly available student models. Our student models are trained to generate\nmeaningful reasoning and rerank documents, achieving competitive performance\nacross multiple datasets, including MSMARCO and BRIGHT. Experiments demonstrate\nthat R2R not only improves reranking accuracy but also provides valuable\ninsights into the decision-making process. By offering a structured and\ninterpretable solution with openly accessible resources, R2R aims to bridge the\ngap between effectiveness and transparency in information retrieval, fostering\nreproducibility and further research in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reranking documents based on their relevance to a given query is a critical\ntask in information retrieval. Traditional reranking methods often lack\ntransparency and rely on proprietary models, hindering reproducibility and\ninterpretability. We propose Reason-to-Rank (R2R), a novel open-source\nreranking approach that enhances transparency by generating two types of\nreasoning: direct relevance reasoning, which explains how a document addresses\nthe query, and comparison reasoning, which justifies the relevance of one\ndocument over another. We leverage large language models (LLMs) as teacher\nmodels to generate these explanations and distill this knowledge into smaller,\nopenly available student models. Our student models are trained to generate\nmeaningful reasoning and rerank documents, achieving competitive performance\nacross multiple datasets, including MSMARCO and BRIGHT. Experiments demonstrate\nthat R2R not only improves reranking accuracy but also provides valuable\ninsights into the decision-making process. By offering a structured and\ninterpretable solution with openly accessible resources, R2R aims to bridge the\ngap between effectiveness and transparency in information retrieval, fostering\nreproducibility and further research in the field."
                },
                "authors": [
                    {
                        "name": "Yuelyu Ji"
                    },
                    {
                        "name": "Zhuochun Li"
                    },
                    {
                        "name": "Rui Meng"
                    },
                    {
                        "name": "Daqing He"
                    }
                ],
                "author_detail": {
                    "name": "Daqing He"
                },
                "author": "Daqing He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05168v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05168v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11736v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11736v2",
                "updated": "2025-04-14T14:57:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    57,
                    10,
                    0,
                    104,
                    0
                ],
                "published": "2025-03-14T14:02:12Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    2,
                    12,
                    4,
                    73,
                    0
                ],
                "title": "A Smooth Analytical Formulation of Collision Detection and Rigid Body\n  Dynamics With Contact",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Smooth Analytical Formulation of Collision Detection and Rigid Body\n  Dynamics With Contact"
                },
                "summary": "Generating intelligent robot behavior in contact-rich settings is a research\nproblem where zeroth-order methods currently prevail. A major contributor to\nthe success of such methods is their robustness in the face of non-smooth and\ndiscontinuous optimization landscapes that are characteristic of contact\ninteractions, yet zeroth-order methods remain computationally inefficient. It\nis therefore desirable to develop methods for perception, planning and control\nin contact-rich settings that can achieve further efficiency by making use of\nfirst and second order information (i.e., gradients and Hessians). To\nfacilitate this, we present a joint formulation of collision detection and\ncontact modelling which, compared to existing differentiable simulation\napproaches, provides the following benefits: i) it results in forward and\ninverse dynamics that are entirely analytical (i.e. do not require solving\noptimization or root-finding problems with iterative methods) and smooth (i.e.\ntwice differentiable), ii) it supports arbitrary collision geometries without\nneeding a convex decomposition, and iii) its runtime is independent of the\nnumber of contacts. Through simulation experiments, we demonstrate the validity\nof the proposed formulation as a \"physics for inference\" that can facilitate\nfuture development of efficient methods to generate intelligent contact-rich\nbehavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating intelligent robot behavior in contact-rich settings is a research\nproblem where zeroth-order methods currently prevail. A major contributor to\nthe success of such methods is their robustness in the face of non-smooth and\ndiscontinuous optimization landscapes that are characteristic of contact\ninteractions, yet zeroth-order methods remain computationally inefficient. It\nis therefore desirable to develop methods for perception, planning and control\nin contact-rich settings that can achieve further efficiency by making use of\nfirst and second order information (i.e., gradients and Hessians). To\nfacilitate this, we present a joint formulation of collision detection and\ncontact modelling which, compared to existing differentiable simulation\napproaches, provides the following benefits: i) it results in forward and\ninverse dynamics that are entirely analytical (i.e. do not require solving\noptimization or root-finding problems with iterative methods) and smooth (i.e.\ntwice differentiable), ii) it supports arbitrary collision geometries without\nneeding a convex decomposition, and iii) its runtime is independent of the\nnumber of contacts. Through simulation experiments, we demonstrate the validity\nof the proposed formulation as a \"physics for inference\" that can facilitate\nfuture development of efficient methods to generate intelligent contact-rich\nbehavior."
                },
                "authors": [
                    {
                        "name": "Onur Beker"
                    },
                    {
                        "name": "Nico Grtler"
                    },
                    {
                        "name": "Ji Shi"
                    },
                    {
                        "name": "A. Ren Geist"
                    },
                    {
                        "name": "Amirreza Razmjoo"
                    },
                    {
                        "name": "Georg Martius"
                    },
                    {
                        "name": "Sylvain Calinon"
                    }
                ],
                "author_detail": {
                    "name": "Sylvain Calinon"
                },
                "author": "Sylvain Calinon",
                "arxiv_comment": "Added references to point-based implicit surface representations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11736v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11736v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10286v1",
                "updated": "2025-04-14T14:53:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    53,
                    31,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T14:53:31Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    53,
                    31,
                    0,
                    104,
                    0
                ],
                "title": "Characterizing LLM-driven Social Network: The Chirper.ai Case",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing LLM-driven Social Network: The Chirper.ai Case"
                },
                "summary": "Large language models (LLMs) demonstrate the ability to simulate human\ndecision-making processes, enabling their use as agents in modeling\nsophisticated social networks, both offline and online. Recent research has\nexplored collective behavioral patterns and structural characteristics of LLM\nagents within simulated networks. However, empirical comparisons between\nLLM-driven and human-driven online social networks remain scarce, limiting our\nunderstanding of how LLM agents differ from human users. This paper presents a\nlarge-scale analysis of Chirper.ai, an X/Twitter-like social network entirely\npopulated by LLM agents, comprising over 65,000 agents and 7.7 million\nAI-generated posts. For comparison, we collect a parallel dataset from\nMastodon, a human-driven decentralized social network, with over 117,000 users\nand 16 million posts. We examine key differences between LLM agents and humans\nin posting behaviors, abusive content, and social network structures. Our\nfindings provide critical insights into the evolving landscape of online social\nnetwork analysis in the AI era, offering a comprehensive profile of LLM agents\nin social simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate the ability to simulate human\ndecision-making processes, enabling their use as agents in modeling\nsophisticated social networks, both offline and online. Recent research has\nexplored collective behavioral patterns and structural characteristics of LLM\nagents within simulated networks. However, empirical comparisons between\nLLM-driven and human-driven online social networks remain scarce, limiting our\nunderstanding of how LLM agents differ from human users. This paper presents a\nlarge-scale analysis of Chirper.ai, an X/Twitter-like social network entirely\npopulated by LLM agents, comprising over 65,000 agents and 7.7 million\nAI-generated posts. For comparison, we collect a parallel dataset from\nMastodon, a human-driven decentralized social network, with over 117,000 users\nand 16 million posts. We examine key differences between LLM agents and humans\nin posting behaviors, abusive content, and social network structures. Our\nfindings provide critical insights into the evolving landscape of online social\nnetwork analysis in the AI era, offering a comprehensive profile of LLM agents\nin social simulations."
                },
                "authors": [
                    {
                        "name": "Yiming Zhu"
                    },
                    {
                        "name": "Yupeng He"
                    },
                    {
                        "name": "Ehsan-Ul Haq"
                    },
                    {
                        "name": "Gareth Tyson"
                    },
                    {
                        "name": "Pan Hui"
                    }
                ],
                "author_detail": {
                    "name": "Pan Hui"
                },
                "author": "Pan Hui",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18865v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18865v3",
                "updated": "2025-04-14T14:52:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    52,
                    38,
                    0,
                    104,
                    0
                ],
                "published": "2025-03-24T16:41:17Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    41,
                    17,
                    0,
                    83,
                    0
                ],
                "title": "Structuring Scientific Innovation: A Framework for Modeling and\n  Discovering Impactful Knowledge Combinations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structuring Scientific Innovation: A Framework for Modeling and\n  Discovering Impactful Knowledge Combinations"
                },
                "summary": "The emergence of large language models offers new possibilities for\nstructured exploration of scientific knowledge. Rather than viewing scientific\ndiscovery as isolated ideas or content, we propose a structured approach that\nemphasizes the role of method combinations in shaping disruptive insights.\nSpecifically, we investigate how knowledge unit--especially those tied to\nmethodological design--can be modeled and recombined to yield research\nbreakthroughs. Our proposed framework addresses two key challenges. First, we\nintroduce a contrastive learning-based mechanism to identify distinguishing\nfeatures of historically disruptive method combinations within problem-driven\ncontexts. Second, we propose a reasoning-guided Monte Carlo search algorithm\nthat leverages the chain-of-thought capability of LLMs to identify promising\nknowledge recombinations for new problem statements.Empirical studies across\nmultiple domains show that the framework is capable of modeling the structural\ndynamics of innovation and successfully highlights combinations with high\ndisruptive potential. This research provides a new path for computationally\nguided scientific ideation grounded in structured reasoning and historical data\nmodeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models offers new possibilities for\nstructured exploration of scientific knowledge. Rather than viewing scientific\ndiscovery as isolated ideas or content, we propose a structured approach that\nemphasizes the role of method combinations in shaping disruptive insights.\nSpecifically, we investigate how knowledge unit--especially those tied to\nmethodological design--can be modeled and recombined to yield research\nbreakthroughs. Our proposed framework addresses two key challenges. First, we\nintroduce a contrastive learning-based mechanism to identify distinguishing\nfeatures of historically disruptive method combinations within problem-driven\ncontexts. Second, we propose a reasoning-guided Monte Carlo search algorithm\nthat leverages the chain-of-thought capability of LLMs to identify promising\nknowledge recombinations for new problem statements.Empirical studies across\nmultiple domains show that the framework is capable of modeling the structural\ndynamics of innovation and successfully highlights combinations with high\ndisruptive potential. This research provides a new path for computationally\nguided scientific ideation grounded in structured reasoning and historical data\nmodeling."
                },
                "authors": [
                    {
                        "name": "Junlan Chen"
                    },
                    {
                        "name": "Kexin Zhang"
                    },
                    {
                        "name": "Daifeng Li"
                    },
                    {
                        "name": "Yangyang Feng"
                    },
                    {
                        "name": "Yuxuan Zhang"
                    },
                    {
                        "name": "Bowen Deng"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Deng"
                },
                "author": "Bowen Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18865v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18865v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10284v1",
                "updated": "2025-04-14T14:52:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    52,
                    28,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T14:52:28Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    52,
                    28,
                    0,
                    104,
                    0
                ],
                "title": "Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the\n  Evaluation Protocol",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the\n  Evaluation Protocol"
                },
                "summary": "Literature review tables are essential for summarizing and comparing\ncollections of scientific papers. We explore the task of generating tables that\nbest fulfill a user's informational needs given a collection of scientific\npapers. Building on recent work (Newman et al., 2024), we extend prior\napproaches to address real-world complexities through a combination of\nLLM-based methods and human annotations. Our contributions focus on three key\nchallenges encountered in real-world use: (i) User prompts are often\nunder-specified; (ii) Retrieved candidate papers frequently contain irrelevant\ncontent; and (iii) Task evaluation should move beyond shallow text similarity\ntechniques and instead assess the utility of inferred tables for\ninformation-seeking tasks (e.g., comparing papers). To support reproducible\nevaluation, we introduce ARXIV2TABLE, a more realistic and challenging\nbenchmark for this task, along with a novel approach to improve literature\nreview table generation in real-world scenarios. Our extensive experiments on\nthis benchmark show that both open-weight and proprietary LLMs struggle with\nthe task, highlighting its difficulty and the need for further advancements.\nOur dataset and code are available at https://github.com/JHU-CLSP/arXiv2Table.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Literature review tables are essential for summarizing and comparing\ncollections of scientific papers. We explore the task of generating tables that\nbest fulfill a user's informational needs given a collection of scientific\npapers. Building on recent work (Newman et al., 2024), we extend prior\napproaches to address real-world complexities through a combination of\nLLM-based methods and human annotations. Our contributions focus on three key\nchallenges encountered in real-world use: (i) User prompts are often\nunder-specified; (ii) Retrieved candidate papers frequently contain irrelevant\ncontent; and (iii) Task evaluation should move beyond shallow text similarity\ntechniques and instead assess the utility of inferred tables for\ninformation-seeking tasks (e.g., comparing papers). To support reproducible\nevaluation, we introduce ARXIV2TABLE, a more realistic and challenging\nbenchmark for this task, along with a novel approach to improve literature\nreview table generation in real-world scenarios. Our extensive experiments on\nthis benchmark show that both open-weight and proprietary LLMs struggle with\nthe task, highlighting its difficulty and the need for further advancements.\nOur dataset and code are available at https://github.com/JHU-CLSP/arXiv2Table."
                },
                "authors": [
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Jiefu Ou"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    },
                    {
                        "name": "Daniel Khashabi"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Khashabi"
                },
                "author": "Daniel Khashabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07029v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07029v2",
                "updated": "2025-04-14T14:47:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    47,
                    35,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-09T16:44:19Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    16,
                    44,
                    19,
                    2,
                    99,
                    0
                ],
                "title": "Distilling Textual Priors from LLM to Efficient Image Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling Textual Priors from LLM to Efficient Image Fusion"
                },
                "summary": "Multi-modality image fusion aims to synthesize a single, comprehensive image\nfrom multiple source inputs. Traditional approaches, such as CNNs and GANs,\noffer efficiency but struggle to handle low-quality or complex inputs. Recent\nadvances in text-guided methods leverage large model priors to overcome these\nlimitations, but at the cost of significant computational overhead, both in\nmemory and inference time. To address this challenge, we propose a novel\nframework for distilling large model priors, eliminating the need for text\nguidance during inference while dramatically reducing model size. Our framework\nutilizes a teacher-student architecture, where the teacher network incorporates\nlarge model priors and transfers this knowledge to a smaller student network\nvia a tailored distillation process. Additionally, we introduce spatial-channel\ncross-fusion module to enhance the model's ability to leverage textual priors\nacross both spatial and channel dimensions. Our method achieves a favorable\ntrade-off between computational efficiency and fusion quality. The distilled\nnetwork, requiring only 10% of the parameters and inference time of the teacher\nnetwork, retains 90% of its performance and outperforms existing SOTA methods.\nExtensive experiments demonstrate the effectiveness of our approach. The\nimplementation will be made publicly available as an open-source resource.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modality image fusion aims to synthesize a single, comprehensive image\nfrom multiple source inputs. Traditional approaches, such as CNNs and GANs,\noffer efficiency but struggle to handle low-quality or complex inputs. Recent\nadvances in text-guided methods leverage large model priors to overcome these\nlimitations, but at the cost of significant computational overhead, both in\nmemory and inference time. To address this challenge, we propose a novel\nframework for distilling large model priors, eliminating the need for text\nguidance during inference while dramatically reducing model size. Our framework\nutilizes a teacher-student architecture, where the teacher network incorporates\nlarge model priors and transfers this knowledge to a smaller student network\nvia a tailored distillation process. Additionally, we introduce spatial-channel\ncross-fusion module to enhance the model's ability to leverage textual priors\nacross both spatial and channel dimensions. Our method achieves a favorable\ntrade-off between computational efficiency and fusion quality. The distilled\nnetwork, requiring only 10% of the parameters and inference time of the teacher\nnetwork, retains 90% of its performance and outperforms existing SOTA methods.\nExtensive experiments demonstrate the effectiveness of our approach. The\nimplementation will be made publicly available as an open-source resource."
                },
                "authors": [
                    {
                        "name": "Ran Zhang"
                    },
                    {
                        "name": "Xuanhua He"
                    },
                    {
                        "name": "Ke Cao"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Man Zhou"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07029v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07029v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23633v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23633v5",
                "updated": "2025-04-14T14:21:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    21,
                    34,
                    0,
                    104,
                    0
                ],
                "published": "2025-03-31T00:12:48Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    0,
                    12,
                    48,
                    0,
                    90,
                    0
                ],
                "title": "GIScience in the Era of Artificial Intelligence: A Research Agenda\n  Towards Autonomous GIS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GIScience in the Era of Artificial Intelligence: A Research Agenda\n  Towards Autonomous GIS"
                },
                "summary": "The advent of generative AI exemplified by large language models (LLMs) opens\nnew ways to represent and compute geographic information and transcends the\nprocess of geographic knowledge production, driving geographic information\nsystems (GIS) towards autonomous GIS. Leveraging LLMs as the decision core,\nautonomous GIS can independently generate and execute geoprocessing workflows\nto perform spatial analysis. In this vision paper, we further elaborate on the\nconcept of autonomous GIS and present a conceptual framework that defines its\nfive autonomous goals, five autonomous levels, five core functions, and three\noperational scales. We demonstrate how autonomous GIS could perform geospatial\ndata retrieval, spatial analysis, and map making with four proof-of-concept GIS\nagents. We conclude by identifying critical challenges and future research\ndirections, including fine-tuning and self-growing decision-cores, autonomous\nmodeling, and examining the societal and practical implications of autonomous\nGIS. By establishing the groundwork for a paradigm shift in GIScience, this\npaper envisions a future where GIS moves beyond traditional workflows to\nautonomously reason, derive, innovate, and advance geospatial solutions to\npressing global challenges. Meanwhile, as we design and deploy increasingly\nintelligent geospatial systems, we carry a responsibility to ensure they are\ndeveloped in socially responsible ways, serve the public good, and support the\ncontinued value of human geographic insight in an AI-augmented future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of generative AI exemplified by large language models (LLMs) opens\nnew ways to represent and compute geographic information and transcends the\nprocess of geographic knowledge production, driving geographic information\nsystems (GIS) towards autonomous GIS. Leveraging LLMs as the decision core,\nautonomous GIS can independently generate and execute geoprocessing workflows\nto perform spatial analysis. In this vision paper, we further elaborate on the\nconcept of autonomous GIS and present a conceptual framework that defines its\nfive autonomous goals, five autonomous levels, five core functions, and three\noperational scales. We demonstrate how autonomous GIS could perform geospatial\ndata retrieval, spatial analysis, and map making with four proof-of-concept GIS\nagents. We conclude by identifying critical challenges and future research\ndirections, including fine-tuning and self-growing decision-cores, autonomous\nmodeling, and examining the societal and practical implications of autonomous\nGIS. By establishing the groundwork for a paradigm shift in GIScience, this\npaper envisions a future where GIS moves beyond traditional workflows to\nautonomously reason, derive, innovate, and advance geospatial solutions to\npressing global challenges. Meanwhile, as we design and deploy increasingly\nintelligent geospatial systems, we carry a responsibility to ensure they are\ndeveloped in socially responsible ways, serve the public good, and support the\ncontinued value of human geographic insight in an AI-augmented future."
                },
                "authors": [
                    {
                        "name": "Zhenlong Li"
                    },
                    {
                        "name": "Huan Ning"
                    },
                    {
                        "name": "Song Gao"
                    },
                    {
                        "name": "Krzysztof Janowicz"
                    },
                    {
                        "name": "Wenwen Li"
                    },
                    {
                        "name": "Samantha T. Arundel"
                    },
                    {
                        "name": "Chaowei Yang"
                    },
                    {
                        "name": "Budhendra Bhaduri"
                    },
                    {
                        "name": "Shaowen Wang"
                    },
                    {
                        "name": "A-Xing Zhu"
                    },
                    {
                        "name": "Mark Gahegan"
                    },
                    {
                        "name": "Shashi Shekhar"
                    },
                    {
                        "name": "Xinyue Ye"
                    },
                    {
                        "name": "Grant McKenzie"
                    },
                    {
                        "name": "Guido Cervone"
                    },
                    {
                        "name": "Michael E. Hodgson"
                    }
                ],
                "author_detail": {
                    "name": "Michael E. Hodgson"
                },
                "author": "Michael E. Hodgson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23633v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23633v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10258v1",
                "updated": "2025-04-14T14:19:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    19,
                    57,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T14:19:57Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    19,
                    57,
                    0,
                    104,
                    0
                ],
                "title": "XY-Cut++: Advanced Layout Ordering via Hierarchical Mask Mechanism on a\n  Novel Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XY-Cut++: Advanced Layout Ordering via Hierarchical Mask Mechanism on a\n  Novel Benchmark"
                },
                "summary": "Document Reading Order Recovery is a fundamental task in document image\nunderstanding, playing a pivotal role in enhancing Retrieval-Augmented\nGeneration (RAG) and serving as a critical preprocessing step for large\nlanguage models (LLMs). Existing methods often struggle with complex\nlayouts(e.g., multi-column newspapers), high-overhead interactions between\ncross-modal elements (visual regions and textual semantics), and a lack of\nrobust evaluation benchmarks. We introduce XY-Cut++, an advanced layout\nordering method that integrates pre-mask processing, multi-granularity\nsegmentation, and cross-modal matching to address these challenges. Our method\nsignificantly enhances layout ordering accuracy compared to traditional XY-Cut\ntechniques. Specifically, XY-Cut++ achieves state-of-the-art performance (98.8\nBLEU overall) while maintaining simplicity and efficiency. It outperforms\nexisting baselines by up to 24\\% and demonstrates consistent accuracy across\nsimple and complex layouts on the newly introduced DocBench-100 dataset. This\nadvancement establishes a reliable foundation for document structure recovery,\nsetting a new standard for layout ordering tasks and facilitating more\neffective RAG and LLM preprocessing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Document Reading Order Recovery is a fundamental task in document image\nunderstanding, playing a pivotal role in enhancing Retrieval-Augmented\nGeneration (RAG) and serving as a critical preprocessing step for large\nlanguage models (LLMs). Existing methods often struggle with complex\nlayouts(e.g., multi-column newspapers), high-overhead interactions between\ncross-modal elements (visual regions and textual semantics), and a lack of\nrobust evaluation benchmarks. We introduce XY-Cut++, an advanced layout\nordering method that integrates pre-mask processing, multi-granularity\nsegmentation, and cross-modal matching to address these challenges. Our method\nsignificantly enhances layout ordering accuracy compared to traditional XY-Cut\ntechniques. Specifically, XY-Cut++ achieves state-of-the-art performance (98.8\nBLEU overall) while maintaining simplicity and efficiency. It outperforms\nexisting baselines by up to 24\\% and demonstrates consistent accuracy across\nsimple and complex layouts on the newly introduced DocBench-100 dataset. This\nadvancement establishes a reliable foundation for document structure recovery,\nsetting a new standard for layout ordering tasks and facilitating more\neffective RAG and LLM preprocessing."
                },
                "authors": [
                    {
                        "name": "Shuai Liu"
                    },
                    {
                        "name": "Youmeng Li"
                    },
                    {
                        "name": "Jizeng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Jizeng Wei"
                },
                "author": "Jizeng Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10254v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10254v1",
                "updated": "2025-04-14T14:15:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    15,
                    46,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T14:15:46Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    15,
                    46,
                    0,
                    104,
                    0
                ],
                "title": "MASSeg : 2nd Technical Report for 4th PVUW MOSE Track",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MASSeg : 2nd Technical Report for 4th PVUW MOSE Track"
                },
                "summary": "Complex video object segmentation continues to face significant challenges in\nsmall object recognition, occlusion handling, and dynamic scene modeling. This\nreport presents our solution, which ranked second in the MOSE track of CVPR\n2025 PVUW Challenge. Based on an existing segmentation framework, we propose an\nimproved model named MASSeg for complex video object segmentation, and\nconstruct an enhanced dataset, MOSE+, which includes typical scenarios with\nocclusions, cluttered backgrounds, and small target instances. During training,\nwe incorporate a combination of inter-frame consistent and inconsistent data\naugmentation strategies to improve robustness and generalization. During\ninference, we design a mask output scaling strategy to better adapt to varying\nobject sizes and occlusion levels. As a result, MASSeg achieves a J score of\n0.8250, F score of 0.9007, and a J&F score of 0.8628 on the MOSE test set.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex video object segmentation continues to face significant challenges in\nsmall object recognition, occlusion handling, and dynamic scene modeling. This\nreport presents our solution, which ranked second in the MOSE track of CVPR\n2025 PVUW Challenge. Based on an existing segmentation framework, we propose an\nimproved model named MASSeg for complex video object segmentation, and\nconstruct an enhanced dataset, MOSE+, which includes typical scenarios with\nocclusions, cluttered backgrounds, and small target instances. During training,\nwe incorporate a combination of inter-frame consistent and inconsistent data\naugmentation strategies to improve robustness and generalization. During\ninference, we design a mask output scaling strategy to better adapt to varying\nobject sizes and occlusion levels. As a result, MASSeg achieves a J score of\n0.8250, F score of 0.9007, and a J&F score of 0.8628 on the MOSE test set."
                },
                "authors": [
                    {
                        "name": "Xuqiang Cao"
                    },
                    {
                        "name": "Linnan Zhao"
                    },
                    {
                        "name": "Jiaxuan Zhao"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Puhua Chen"
                    },
                    {
                        "name": "Wenping Ma"
                    }
                ],
                "author_detail": {
                    "name": "Wenping Ma"
                },
                "author": "Wenping Ma",
                "arxiv_comment": "5 pages,4 figures,Technical report on Complex Video Object\n  Segmentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10254v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10254v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10242v1",
                "updated": "2025-04-14T14:04:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    4,
                    55,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T14:04:55Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    4,
                    55,
                    0,
                    104,
                    0
                ],
                "title": "CAT: A Conditional Adaptation Tailor for Efficient and Effective\n  Instance-Specific Pansharpening on Real-World Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAT: A Conditional Adaptation Tailor for Efficient and Effective\n  Instance-Specific Pansharpening on Real-World Data"
                },
                "summary": "Pansharpening is a crucial remote sensing technique that fuses low-resolution\nmultispectral (LRMS) images with high-resolution panchromatic (PAN) images to\ngenerate high-resolution multispectral (HRMS) imagery. Although deep learning\ntechniques have significantly advanced pansharpening, many existing methods\nsuffer from limited cross-sensor generalization and high computational\noverhead, restricting their real-time applications. To address these\nchallenges, we propose an efficient framework that quickly adapts to a specific\ninput instance, completing both training and inference in a short time. Our\nframework splits the input image into multiple patches, selects a subset for\nunsupervised CAT training, and then performs inference on all patches,\nstitching them into the final output. The CAT module, integrated between the\nfeature extraction and channel transformation stages of a pre-trained network,\ntailors the fused features and fixes the parameters for efficient inference,\ngenerating improved results. Our approach offers two key advantages: (1)\n$\\textit{Improved Generalization Ability}$: by mitigating cross-sensor\ndegradation, our model--although pre-trained on a specific dataset--achieves\nsuperior performance on datasets captured by other sensors; (2)\n$\\textit{Enhanced Computational Efficiency}$: the CAT-enhanced network can\nswiftly adapt to the test sample using the single LRMS-PAN pair input, without\nrequiring extensive large-scale data retraining. Experiments on the real-world\ndata from WorldView-3 and WorldView-2 datasets demonstrate that our method\nachieves state-of-the-art performance on cross-sensor real-world data, while\nachieving both training and inference of $512\\times512$ image within\n$\\textit{0.4 seconds}$ and $4000\\times4000$ image within $\\textit{3 seconds}$\nat the fastest setting on a commonly used RTX 3090 GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pansharpening is a crucial remote sensing technique that fuses low-resolution\nmultispectral (LRMS) images with high-resolution panchromatic (PAN) images to\ngenerate high-resolution multispectral (HRMS) imagery. Although deep learning\ntechniques have significantly advanced pansharpening, many existing methods\nsuffer from limited cross-sensor generalization and high computational\noverhead, restricting their real-time applications. To address these\nchallenges, we propose an efficient framework that quickly adapts to a specific\ninput instance, completing both training and inference in a short time. Our\nframework splits the input image into multiple patches, selects a subset for\nunsupervised CAT training, and then performs inference on all patches,\nstitching them into the final output. The CAT module, integrated between the\nfeature extraction and channel transformation stages of a pre-trained network,\ntailors the fused features and fixes the parameters for efficient inference,\ngenerating improved results. Our approach offers two key advantages: (1)\n$\\textit{Improved Generalization Ability}$: by mitigating cross-sensor\ndegradation, our model--although pre-trained on a specific dataset--achieves\nsuperior performance on datasets captured by other sensors; (2)\n$\\textit{Enhanced Computational Efficiency}$: the CAT-enhanced network can\nswiftly adapt to the test sample using the single LRMS-PAN pair input, without\nrequiring extensive large-scale data retraining. Experiments on the real-world\ndata from WorldView-3 and WorldView-2 datasets demonstrate that our method\nachieves state-of-the-art performance on cross-sensor real-world data, while\nachieving both training and inference of $512\\times512$ image within\n$\\textit{0.4 seconds}$ and $4000\\times4000$ image within $\\textit{3 seconds}$\nat the fastest setting on a commonly used RTX 3090 GPU."
                },
                "authors": [
                    {
                        "name": "Tianyu Xin"
                    },
                    {
                        "name": "Jin-Liang Xiao"
                    },
                    {
                        "name": "Zeyu Xia"
                    },
                    {
                        "name": "Shan Yin"
                    },
                    {
                        "name": "Liang-Jian Deng"
                    }
                ],
                "author_detail": {
                    "name": "Liang-Jian Deng"
                },
                "author": "Liang-Jian Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10240v1",
                "updated": "2025-04-14T14:02:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    2,
                    9,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T14:02:09Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    2,
                    9,
                    0,
                    104,
                    0
                ],
                "title": "GNN-ACLP: Graph Neural Networks based Analog Circuit Link Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GNN-ACLP: Graph Neural Networks based Analog Circuit Link Prediction"
                },
                "summary": "Circuit link prediction identifying missing component connections from\nincomplete netlists is crucial in automating analog circuit design. However,\nexisting methods face three main challenges: 1) Insufficient use of topological\npatterns in circuit graphs reduces prediction accuracy; 2) Data scarcity due to\nthe complexity of annotations hinders model generalization; 3) Limited\nadaptability to various netlist formats. We propose GNN-ACLP, a Graph Neural\nNetworks (GNNs) based framework featuring three innovations to tackle these\nchallenges. First, we introduce the SEAL (Subgraphs, Embeddings, and Attributes\nfor Link Prediction) framework and achieve port-level accuracy in circuit link\nprediction. Second, we propose Netlist Babel Fish, a netlist format conversion\ntool leveraging retrieval-augmented generation (RAG) with large language model\n(LLM) to enhance the compatibility of netlist formats. Finally, we construct\nSpiceNetlist, a comprehensive dataset that contains 775 annotated circuits\nacross 10 different classes of components. The experimental results demonstrate\nan improvement of 15.05% on the SpiceNetlist dataset and 12.01% on the\nImage2Net dataset over the existing approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Circuit link prediction identifying missing component connections from\nincomplete netlists is crucial in automating analog circuit design. However,\nexisting methods face three main challenges: 1) Insufficient use of topological\npatterns in circuit graphs reduces prediction accuracy; 2) Data scarcity due to\nthe complexity of annotations hinders model generalization; 3) Limited\nadaptability to various netlist formats. We propose GNN-ACLP, a Graph Neural\nNetworks (GNNs) based framework featuring three innovations to tackle these\nchallenges. First, we introduce the SEAL (Subgraphs, Embeddings, and Attributes\nfor Link Prediction) framework and achieve port-level accuracy in circuit link\nprediction. Second, we propose Netlist Babel Fish, a netlist format conversion\ntool leveraging retrieval-augmented generation (RAG) with large language model\n(LLM) to enhance the compatibility of netlist formats. Finally, we construct\nSpiceNetlist, a comprehensive dataset that contains 775 annotated circuits\nacross 10 different classes of components. The experimental results demonstrate\nan improvement of 15.05% on the SpiceNetlist dataset and 12.01% on the\nImage2Net dataset over the existing approach."
                },
                "authors": [
                    {
                        "name": "Guanyuan Pan"
                    },
                    {
                        "name": "Tiansheng Zhou"
                    },
                    {
                        "name": "Bingtao Ma"
                    },
                    {
                        "name": "Yaqi Wang"
                    },
                    {
                        "name": "Jianxiang Zhao"
                    },
                    {
                        "name": "Shuai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shuai Wang"
                },
                "author": "Shuai Wang",
                "arxiv_comment": "Data will be made available on request",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10329v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10329v3",
                "updated": "2025-04-14T13:58:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    13,
                    58,
                    8,
                    0,
                    104,
                    0
                ],
                "published": "2024-06-14T18:00:00Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    18,
                    0,
                    0,
                    4,
                    166,
                    0
                ],
                "title": "Exploring the AGN Fraction of a Sample of JWST's Little Red Dots at $5 <\n  z < 8$: Overmassive Black Holes Are Strongly Favored",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the AGN Fraction of a Sample of JWST's Little Red Dots at $5 <\n  z < 8$: Overmassive Black Holes Are Strongly Favored"
                },
                "summary": "JWST is revolutionizing our view of the early Universe by pushing the\nboundaries of detectable galaxies and black holes in redshift (upward) and mass\n(downward). The Little Red Dots (LRDs), detected by several surveys at $z > 4$,\npresent a significant interpretational challenge, as their Spectral Energy\nDistributions (SED) can mimic both AGN and stellar population templates. This\nstudy analyzes 19 LRDs from the JADES survey, utilizing NIRCam and MIRI\nphotometry. By performing SED fitting across a vast parameter space, we explore\na broad range of AGN fractions, defined as the ratio of the monochromatic\nluminosities (AGN, galaxy, and dust) over a specified wavelength range, 0.4 -\n0.7 $\\mu m$ rest-frame. We find that 17 of the 19 LRDs investigated are\nconsistent with having significant AGN contributions, with best-fitting AGN\nfractions ranging between 20% and 70%, while one galaxy shows a low AGN\ncontribution (2%) and another appears to be purely star-forming. Moreover,\nassuming these LRDs do indeed host AGN, we can place limits on their black hole\nmasses using the inferred AGN bolometric luminosities and adopting the\nEddington limit. We find that, independent of the specific AGN fraction\nadopted, the LRDs' black holes are significantly overmassive relative to their\nhost galaxies (by $\\sim 1$ dex, and up to $\\sim 4$ dex in the most extreme\ncases) compared to the local $M_{\\bullet} - M_{\\star}$ relation. The presence\nof overmassive black holes in the high-$z$ Universe may provide the strongest\nevidence yet of heavy black hole seeding occurring during the cosmic dark ages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JWST is revolutionizing our view of the early Universe by pushing the\nboundaries of detectable galaxies and black holes in redshift (upward) and mass\n(downward). The Little Red Dots (LRDs), detected by several surveys at $z > 4$,\npresent a significant interpretational challenge, as their Spectral Energy\nDistributions (SED) can mimic both AGN and stellar population templates. This\nstudy analyzes 19 LRDs from the JADES survey, utilizing NIRCam and MIRI\nphotometry. By performing SED fitting across a vast parameter space, we explore\na broad range of AGN fractions, defined as the ratio of the monochromatic\nluminosities (AGN, galaxy, and dust) over a specified wavelength range, 0.4 -\n0.7 $\\mu m$ rest-frame. We find that 17 of the 19 LRDs investigated are\nconsistent with having significant AGN contributions, with best-fitting AGN\nfractions ranging between 20% and 70%, while one galaxy shows a low AGN\ncontribution (2%) and another appears to be purely star-forming. Moreover,\nassuming these LRDs do indeed host AGN, we can place limits on their black hole\nmasses using the inferred AGN bolometric luminosities and adopting the\nEddington limit. We find that, independent of the specific AGN fraction\nadopted, the LRDs' black holes are significantly overmassive relative to their\nhost galaxies (by $\\sim 1$ dex, and up to $\\sim 4$ dex in the most extreme\ncases) compared to the local $M_{\\bullet} - M_{\\star}$ relation. The presence\nof overmassive black holes in the high-$z$ Universe may provide the strongest\nevidence yet of heavy black hole seeding occurring during the cosmic dark ages."
                },
                "authors": [
                    {
                        "name": "Emmanuel Durodola"
                    },
                    {
                        "name": "Fabio Pacucci"
                    },
                    {
                        "name": "Ryan C. Hickox"
                    }
                ],
                "author_detail": {
                    "name": "Ryan C. Hickox"
                },
                "author": "Ryan C. Hickox",
                "arxiv_comment": "Accepted for publication in The Astrophysical Journal. This is the\n  final version. 14 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10329v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10329v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12899v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12899v2",
                "updated": "2025-04-14T13:57:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    13,
                    57,
                    28,
                    0,
                    104,
                    0
                ],
                "published": "2025-03-17T07:59:42Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    7,
                    59,
                    42,
                    0,
                    76,
                    0
                ],
                "title": "A Semantic-based Optimization Approach for Repairing LLMs: Case Study on\n  Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Semantic-based Optimization Approach for Repairing LLMs: Case Study on\n  Code Generation"
                },
                "summary": "Language Models (LMs) are widely used in software engineering for code\ngeneration, but they may produce code with errors. Rather than repairing the\ngenerated code, an alternative way is to address the underlying failures of\nmodels. LM repair offers a lightweight solution to this challenge: it requires\nminimal data, reduces computational costs, and reduces the side effects. Unlike\nretraining, LM repair focuses on applying tailored updates to targeted neurons,\nmaking it ideal for scenarios with limited resources, high-performance demands,\nor strict safety requirements. In this paper, we propose \\ul{S}emantic\n\\ul{T}argeting for \\ul{A}nalytical \\ul{R}epair (\\textsc{STAR}), a pioneering\nand novel semantic-based optimization approach for repairing LLMs.\n\\textsc{STAR} realizes main operations in LM repair methods in an optimization\nprocess, including locating ``buggy neurons'', solving ``neuron patches'', and\npatching ``buggy neurons''. Correspondingly, it computes the deltas of weight\nmatrix as the prior information to guide optimization; and attributes the\ntargeted layers and neurons leveraging statistical insights. The neuron patches\nare computed with a solid semantic-based analytical formula, which directly\nbridges the changes to logits with the deltas of neurons, by steering latent\nrepresentations. Compared to the prior work of LM repair (\\textsc{MINT}) and\noptimization methods (\\textsc{SGD}), \\textsc{STAR} integrates their strengths\nwhile mitigating their limitations. \\textsc{STAR} supports solving multiple\nfailures together, significantly improving the usefulness. Evaluated on three\ncode generation tasks using popular code LMs, \\textsc{STAR} demonstrates\nsuperior effectiveness. Additionally, \\textsc{STAR} exhibits better efficiency.\nIn terms of side effects, namely the balance between generalization and\nspecificity, \\textsc{STAR} outperforms prior work by a significant margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models (LMs) are widely used in software engineering for code\ngeneration, but they may produce code with errors. Rather than repairing the\ngenerated code, an alternative way is to address the underlying failures of\nmodels. LM repair offers a lightweight solution to this challenge: it requires\nminimal data, reduces computational costs, and reduces the side effects. Unlike\nretraining, LM repair focuses on applying tailored updates to targeted neurons,\nmaking it ideal for scenarios with limited resources, high-performance demands,\nor strict safety requirements. In this paper, we propose \\ul{S}emantic\n\\ul{T}argeting for \\ul{A}nalytical \\ul{R}epair (\\textsc{STAR}), a pioneering\nand novel semantic-based optimization approach for repairing LLMs.\n\\textsc{STAR} realizes main operations in LM repair methods in an optimization\nprocess, including locating ``buggy neurons'', solving ``neuron patches'', and\npatching ``buggy neurons''. Correspondingly, it computes the deltas of weight\nmatrix as the prior information to guide optimization; and attributes the\ntargeted layers and neurons leveraging statistical insights. The neuron patches\nare computed with a solid semantic-based analytical formula, which directly\nbridges the changes to logits with the deltas of neurons, by steering latent\nrepresentations. Compared to the prior work of LM repair (\\textsc{MINT}) and\noptimization methods (\\textsc{SGD}), \\textsc{STAR} integrates their strengths\nwhile mitigating their limitations. \\textsc{STAR} supports solving multiple\nfailures together, significantly improving the usefulness. Evaluated on three\ncode generation tasks using popular code LMs, \\textsc{STAR} demonstrates\nsuperior effectiveness. Additionally, \\textsc{STAR} exhibits better efficiency.\nIn terms of side effects, namely the balance between generalization and\nspecificity, \\textsc{STAR} outperforms prior work by a significant margin."
                },
                "authors": [
                    {
                        "name": "Jian Gu"
                    },
                    {
                        "name": "Aldeida Aleti"
                    },
                    {
                        "name": "Chunyang Chen"
                    },
                    {
                        "name": "Hongyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hongyu Zhang"
                },
                "author": "Hongyu Zhang",
                "arxiv_comment": "12 pages, 6 figure, 6 tables, under peer-review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12899v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12899v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04285v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04285v2",
                "updated": "2025-04-14T13:55:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    13,
                    55,
                    42,
                    0,
                    104,
                    0
                ],
                "published": "2025-01-08T05:17:09Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    5,
                    17,
                    9,
                    2,
                    8,
                    0
                ],
                "title": "Separate Source Channel Coding Is Still What You Need: An LLM-based\n  Rethinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Separate Source Channel Coding Is Still What You Need: An LLM-based\n  Rethinking"
                },
                "summary": "Along with the proliferating research interest in Semantic Communication\n(SemCom), Joint Source Channel Coding (JSCC) has dominated the attention due to\nthe widely assumed existence in efficiently delivering information semantics.\n%has emerged as a pivotal area of research, aiming to enhance the efficiency\nand reliability of information transmission through deep learning-based\nmethods. Nevertheless, this paper challenges the conventional JSCC paradigm,\nand advocates for adoption of Separate Source Channel Coding (SSCC) to enjoy\nthe underlying more degree of freedom for optimization. We demonstrate that\nSSCC, after leveraging the strengths of Large Language Model (LLM) for source\ncoding and Error Correction Code Transformer (ECCT) complemented for channel\ndecoding, offers superior performance over JSCC. Our proposed framework also\neffectively highlights the compatibility challenges between SemCom approaches\nand digital communication systems, particularly concerning the resource costs\nassociated with the transmission of high precision floating point numbers.\nThrough comprehensive evaluations, we establish that empowered by LLM-based\ncompression and ECCT-enhanced error correction, SSCC remains a viable and\neffective solution for modern communication systems. In other words, separate\nsource and channel coding is still what we need!",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Along with the proliferating research interest in Semantic Communication\n(SemCom), Joint Source Channel Coding (JSCC) has dominated the attention due to\nthe widely assumed existence in efficiently delivering information semantics.\n%has emerged as a pivotal area of research, aiming to enhance the efficiency\nand reliability of information transmission through deep learning-based\nmethods. Nevertheless, this paper challenges the conventional JSCC paradigm,\nand advocates for adoption of Separate Source Channel Coding (SSCC) to enjoy\nthe underlying more degree of freedom for optimization. We demonstrate that\nSSCC, after leveraging the strengths of Large Language Model (LLM) for source\ncoding and Error Correction Code Transformer (ECCT) complemented for channel\ndecoding, offers superior performance over JSCC. Our proposed framework also\neffectively highlights the compatibility challenges between SemCom approaches\nand digital communication systems, particularly concerning the resource costs\nassociated with the transmission of high precision floating point numbers.\nThrough comprehensive evaluations, we establish that empowered by LLM-based\ncompression and ECCT-enhanced error correction, SSCC remains a viable and\neffective solution for modern communication systems. In other words, separate\nsource and channel coding is still what we need!"
                },
                "authors": [
                    {
                        "name": "Tianqi Ren"
                    },
                    {
                        "name": "Rongpeng Li"
                    },
                    {
                        "name": "Ming-min Zhao"
                    },
                    {
                        "name": "Xianfu Chen"
                    },
                    {
                        "name": "Guangyi Liu"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Zhifeng Zhao"
                    },
                    {
                        "name": "Honggang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Honggang Zhang"
                },
                "author": "Honggang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04285v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04285v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10230v1",
                "updated": "2025-04-14T13:51:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    13,
                    51,
                    55,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T13:51:55Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    13,
                    51,
                    55,
                    0,
                    104,
                    0
                ],
                "title": "Extracting cosmological information from the abundance of galaxy\n  clusters with simulation-based inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting cosmological information from the abundance of galaxy\n  clusters with simulation-based inference"
                },
                "summary": "The abundance of galaxy clusters as a function of mass and redshift is a\nwell-established and powerful cosmological probe. Cosmological analyses based\non galaxy cluster number counts have traditionally relied on explicitly\ncomputed likelihoods, which are often challenging to develop with the required\naccuracy and expensive to evaluate. In this work, we implement an alternative\napproach based on simulation-based inference (SBI) methods that relies solely\non synthetic galaxy cluster catalogues generated under a given model. These\ncatalogues are much easier to produce than it is to develop and validate a\nlikelihood. We validate this approach in the context of the galaxy cluster\nsurvey of the upcoming Simons Observatory for a setup in which we can also\nevaluate an exact explicit likelihood. We find that our SBI-based approach\nyields cosmological parameter posterior means that are within $0.2\\,\\sigma$ of\nthose obtained with the explicit likelihood and with biases smaller than\n$0.1\\,\\sigma$. We also introduce and validate a procedure to assess the\ngoodness of fit using only synthetic catalogues similar to those used for\ntraining. This demonstrates, for the first time, that a galaxy cluster number\ncount cosmological analysis can be performed fully without resorting to a\nlikelihood at any stage. Finally, we apply our SBI-based approach to the real\nPlanck MMF3 cosmology sample, obtaining cosmological parameter constraints that\nare within $0.1\\,\\sigma$ of their likelihood-based counterparts. This\nconstitutes the first SBI-based number count cosmological analysis of a real\ngalaxy cluster catalogue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The abundance of galaxy clusters as a function of mass and redshift is a\nwell-established and powerful cosmological probe. Cosmological analyses based\non galaxy cluster number counts have traditionally relied on explicitly\ncomputed likelihoods, which are often challenging to develop with the required\naccuracy and expensive to evaluate. In this work, we implement an alternative\napproach based on simulation-based inference (SBI) methods that relies solely\non synthetic galaxy cluster catalogues generated under a given model. These\ncatalogues are much easier to produce than it is to develop and validate a\nlikelihood. We validate this approach in the context of the galaxy cluster\nsurvey of the upcoming Simons Observatory for a setup in which we can also\nevaluate an exact explicit likelihood. We find that our SBI-based approach\nyields cosmological parameter posterior means that are within $0.2\\,\\sigma$ of\nthose obtained with the explicit likelihood and with biases smaller than\n$0.1\\,\\sigma$. We also introduce and validate a procedure to assess the\ngoodness of fit using only synthetic catalogues similar to those used for\ntraining. This demonstrates, for the first time, that a galaxy cluster number\ncount cosmological analysis can be performed fully without resorting to a\nlikelihood at any stage. Finally, we apply our SBI-based approach to the real\nPlanck MMF3 cosmology sample, obtaining cosmological parameter constraints that\nare within $0.1\\,\\sigma$ of their likelihood-based counterparts. This\nconstitutes the first SBI-based number count cosmological analysis of a real\ngalaxy cluster catalogue."
                },
                "authors": [
                    {
                        "name": "igo Zubeldia"
                    },
                    {
                        "name": "Boris Bolliet"
                    },
                    {
                        "name": "Anthony Challinor"
                    },
                    {
                        "name": "William Handley"
                    }
                ],
                "author_detail": {
                    "name": "William Handley"
                },
                "author": "William Handley",
                "arxiv_comment": "13 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10227v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10227v1",
                "updated": "2025-04-14T13:46:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    13,
                    46,
                    35,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T13:46:35Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    13,
                    46,
                    35,
                    0,
                    104,
                    0
                ],
                "title": "Probing then Editing Response Personality of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing then Editing Response Personality of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated promising capabilities to\ngenerate responses that exhibit consistent personality traits. Despite the\nmajor attempts to analyze personality expression through output-based\nevaluations, little is known about how such traits are internally encoded\nwithin LLM parameters. In this paper, we introduce a layer-wise probing\nframework to systematically investigate the layer-wise capability of LLMs in\nencoding personality for responding. We conduct probing experiments on 11\nopen-source LLMs over the PersonalityEdit benchmark and find that LLMs\npredominantly encode personality for responding in their middle and upper\nlayers, with instruction-tuned models demonstrating a slightly clearer\nseparation of personality traits. Furthermore, by interpreting the trained\nprobing hyperplane as a layer-wise boundary for each personality category, we\npropose a layer-wise perturbation method to edit the personality expressed by\nLLMs during inference. Our results show that even when the prompt explicitly\nspecifies a particular personality, our method can still successfully alter the\nresponse personality of LLMs. Interestingly, the difficulty of converting\nbetween certain personality traits varies substantially, which aligns with the\nrepresentational distances in our probing experiments. Finally, we conduct a\ncomprehensive MMLU benchmark evaluation and time overhead analysis,\ndemonstrating that our proposed personality editing method incurs only minimal\ndegradation in general capabilities while maintaining low training costs and\nacceptable inference latency. Our code is publicly available at\nhttps://github.com/universe-sky/probing-then-editing-personality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated promising capabilities to\ngenerate responses that exhibit consistent personality traits. Despite the\nmajor attempts to analyze personality expression through output-based\nevaluations, little is known about how such traits are internally encoded\nwithin LLM parameters. In this paper, we introduce a layer-wise probing\nframework to systematically investigate the layer-wise capability of LLMs in\nencoding personality for responding. We conduct probing experiments on 11\nopen-source LLMs over the PersonalityEdit benchmark and find that LLMs\npredominantly encode personality for responding in their middle and upper\nlayers, with instruction-tuned models demonstrating a slightly clearer\nseparation of personality traits. Furthermore, by interpreting the trained\nprobing hyperplane as a layer-wise boundary for each personality category, we\npropose a layer-wise perturbation method to edit the personality expressed by\nLLMs during inference. Our results show that even when the prompt explicitly\nspecifies a particular personality, our method can still successfully alter the\nresponse personality of LLMs. Interestingly, the difficulty of converting\nbetween certain personality traits varies substantially, which aligns with the\nrepresentational distances in our probing experiments. Finally, we conduct a\ncomprehensive MMLU benchmark evaluation and time overhead analysis,\ndemonstrating that our proposed personality editing method incurs only minimal\ndegradation in general capabilities while maintaining low training costs and\nacceptable inference latency. Our code is publicly available at\nhttps://github.com/universe-sky/probing-then-editing-personality."
                },
                "authors": [
                    {
                        "name": "Tianjie Ju"
                    },
                    {
                        "name": "Zhenyu Shao"
                    },
                    {
                        "name": "Bowen Wang"
                    },
                    {
                        "name": "Yujia Chen"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Mong-Li Lee"
                    },
                    {
                        "name": "Wynne Hsu"
                    },
                    {
                        "name": "Sufeng Duan"
                    },
                    {
                        "name": "Gongshen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Gongshen Liu"
                },
                "author": "Gongshen Liu",
                "arxiv_comment": "Working in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10227v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10227v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08619v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08619v2",
                "updated": "2025-04-14T13:45:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    13,
                    45,
                    49,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-11T15:24:23Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    24,
                    23,
                    4,
                    101,
                    0
                ],
                "title": "Analyzing 16,193 LLM Papers for Fun and Profits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing 16,193 LLM Papers for Fun and Profits"
                },
                "summary": "Large Language Models (LLMs) are reshaping the landscape of computer science\nresearch, driving significant shifts in research priorities across diverse\nconferences and fields. This study provides a comprehensive analysis of the\npublication trend of LLM-related papers in 77 top-tier computer science\nconferences over the past six years (2019-2024). We approach this analysis from\nfour distinct perspectives: (1) We investigate how LLM research is driving\ntopic shifts within major conferences. (2) We adopt a topic modeling approach\nto identify various areas of LLM-related topic growth and reveal the topics of\nconcern at different conferences. (3) We explore distinct contribution patterns\nof academic and industrial institutions. (4) We study the influence of national\norigins on LLM development trajectories. Synthesizing the findings from these\ndiverse analytical angles, we derive ten key insights that illuminate the\ndynamics and evolution of the LLM research ecosystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are reshaping the landscape of computer science\nresearch, driving significant shifts in research priorities across diverse\nconferences and fields. This study provides a comprehensive analysis of the\npublication trend of LLM-related papers in 77 top-tier computer science\nconferences over the past six years (2019-2024). We approach this analysis from\nfour distinct perspectives: (1) We investigate how LLM research is driving\ntopic shifts within major conferences. (2) We adopt a topic modeling approach\nto identify various areas of LLM-related topic growth and reveal the topics of\nconcern at different conferences. (3) We explore distinct contribution patterns\nof academic and industrial institutions. (4) We study the influence of national\norigins on LLM development trajectories. Synthesizing the findings from these\ndiverse analytical angles, we derive ten key insights that illuminate the\ndynamics and evolution of the LLM research ecosystem."
                },
                "authors": [
                    {
                        "name": "Zhiqiu Xia"
                    },
                    {
                        "name": "Lang Zhu"
                    },
                    {
                        "name": "Bingzhe Li"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Qiannan Li"
                    },
                    {
                        "name": "Hang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Liu"
                },
                "author": "Hang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08619v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08619v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24157v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24157v2",
                "updated": "2025-04-14T13:31:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    13,
                    31,
                    18,
                    0,
                    104,
                    0
                ],
                "published": "2025-03-31T14:40:31Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    14,
                    40,
                    31,
                    0,
                    90,
                    0
                ],
                "title": "LLM4FS: Leveraging Large Language Models for Feature Selection and How\n  to Improve It",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4FS: Leveraging Large Language Models for Feature Selection and How\n  to Improve It"
                },
                "summary": "Recent advances in large language models (LLMs) have provided new\nopportunities for decision-making, particularly in the task of automated\nfeature selection. In this paper, we first comprehensively evaluate LLM-based\nfeature selection methods, covering the state-of-the-art DeepSeek-R1,\nGPT-o3-mini, and GPT-4.5. Then, we propose a novel hybrid strategy called\nLLM4FS that integrates LLMs with traditional data-driven methods. Specifically,\ninput data samples into LLMs, and directly call traditional data-driven\ntechniques such as random forest and forward sequential selection. Notably, our\nanalysis reveals that the hybrid strategy leverages the contextual\nunderstanding of LLMs and the high statistical reliability of traditional\ndata-driven methods to achieve excellent feature selection performance, even\nsurpassing LLMs and traditional data-driven methods. Finally, we point out the\nlimitations of its application in decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have provided new\nopportunities for decision-making, particularly in the task of automated\nfeature selection. In this paper, we first comprehensively evaluate LLM-based\nfeature selection methods, covering the state-of-the-art DeepSeek-R1,\nGPT-o3-mini, and GPT-4.5. Then, we propose a novel hybrid strategy called\nLLM4FS that integrates LLMs with traditional data-driven methods. Specifically,\ninput data samples into LLMs, and directly call traditional data-driven\ntechniques such as random forest and forward sequential selection. Notably, our\nanalysis reveals that the hybrid strategy leverages the contextual\nunderstanding of LLMs and the high statistical reliability of traditional\ndata-driven methods to achieve excellent feature selection performance, even\nsurpassing LLMs and traditional data-driven methods. Finally, we point out the\nlimitations of its application in decision-making."
                },
                "authors": [
                    {
                        "name": "Jianhao Li"
                    },
                    {
                        "name": "Xianchao Xiu"
                    }
                ],
                "author_detail": {
                    "name": "Xianchao Xiu"
                },
                "author": "Xianchao Xiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24157v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24157v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10210v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10210v1",
                "updated": "2025-04-14T13:25:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    13,
                    25,
                    50,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T13:25:50Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    13,
                    25,
                    50,
                    0,
                    104,
                    0
                ],
                "title": "Can Competition Enhance the Proficiency of Agents Powered by Large\n  Language Models in the Realm of News-driven Time Series Forecasting?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Competition Enhance the Proficiency of Agents Powered by Large\n  Language Models in the Realm of News-driven Time Series Forecasting?"
                },
                "summary": "Multi-agents-based news-driven time series forecasting is considered as a\npotential paradigm shift in the era of large language models (LLMs). The\nchallenge of this task lies in measuring the influences of different news\nevents towards the fluctuations of time series. This requires agents to possess\nstronger abilities of innovative thinking and the identifying misleading logic.\nHowever, the existing multi-agent discussion framework has limited enhancement\non time series prediction in terms of optimizing these two capabilities.\nInspired by the role of competition in fostering innovation, this study embeds\na competition mechanism within the multi-agent discussion to enhance agents'\ncapability of generating innovative thoughts. Furthermore, to bolster the\nmodel's proficiency in identifying misleading information, we incorporate a\nfine-tuned small-scale LLM model within the reflective stage, offering\nauxiliary decision-making support. Experimental results confirm that the\ncompetition can boost agents' capacity for innovative thinking, which can\nsignificantly improve the performances of time series prediction. Similar to\nthe findings of social science, the intensity of competition within this\nframework can influence the performances of agents, providing a new perspective\nfor studying LLMs-based multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agents-based news-driven time series forecasting is considered as a\npotential paradigm shift in the era of large language models (LLMs). The\nchallenge of this task lies in measuring the influences of different news\nevents towards the fluctuations of time series. This requires agents to possess\nstronger abilities of innovative thinking and the identifying misleading logic.\nHowever, the existing multi-agent discussion framework has limited enhancement\non time series prediction in terms of optimizing these two capabilities.\nInspired by the role of competition in fostering innovation, this study embeds\na competition mechanism within the multi-agent discussion to enhance agents'\ncapability of generating innovative thoughts. Furthermore, to bolster the\nmodel's proficiency in identifying misleading information, we incorporate a\nfine-tuned small-scale LLM model within the reflective stage, offering\nauxiliary decision-making support. Experimental results confirm that the\ncompetition can boost agents' capacity for innovative thinking, which can\nsignificantly improve the performances of time series prediction. Similar to\nthe findings of social science, the intensity of competition within this\nframework can influence the performances of agents, providing a new perspective\nfor studying LLMs-based multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhang"
                    },
                    {
                        "name": "Yangyang Feng"
                    },
                    {
                        "name": "Daifeng Li"
                    },
                    {
                        "name": "Kexin Zhang"
                    },
                    {
                        "name": "Junlan Chen"
                    },
                    {
                        "name": "Bowen Deng"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Deng"
                },
                "author": "Bowen Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10210v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10208v1",
                "updated": "2025-04-14T13:21:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    13,
                    21,
                    29,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T13:21:29Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    13,
                    21,
                    29,
                    0,
                    104,
                    0
                ],
                "title": "From Prompting to Alignment: A Generative Framework for Query\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Prompting to Alignment: A Generative Framework for Query\n  Recommendation"
                },
                "summary": "In modern search systems, search engines often suggest relevant queries to\nusers through various panels or components, helping refine their information\nneeds. Traditionally, these recommendations heavily rely on historical search\nlogs to build models, which suffer from cold-start or long-tail issues.\nFurthermore, tasks such as query suggestion, completion or clarification are\nstudied separately by specific design, which lacks generalizability and hinders\nadaptation to novel applications. Despite recent attempts to explore the use of\nLLMs for query recommendation, these methods mainly rely on the inherent\nknowledge of LLMs or external sources like few-shot examples, retrieved\ndocuments, or knowledge bases, neglecting the importance of the calibration and\nalignment with user feedback, thus limiting their practical utility. To address\nthese challenges, we first propose a general Generative Query Recommendation\n(GQR) framework that aligns LLM-based query generation with user preference.\nSpecifically, we unify diverse query recommendation tasks by a universal prompt\nframework, leveraging the instruct-following capability of LLMs for effective\ngeneration. Secondly, we align LLMs with user feedback via presenting a\nCTR-alignment framework, which involves training a query-wise CTR predictor as\na process reward model and employing list-wise preference alignment to maximize\nthe click probability of the generated query list. Furthermore, recognizing the\ninconsistency between LLM knowledge and proactive search intents arising from\nthe separation of user-initiated queries from models, we align LLMs with user\ninitiative via retrieving co-occurrence queries as side information when\nhistorical logs are available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern search systems, search engines often suggest relevant queries to\nusers through various panels or components, helping refine their information\nneeds. Traditionally, these recommendations heavily rely on historical search\nlogs to build models, which suffer from cold-start or long-tail issues.\nFurthermore, tasks such as query suggestion, completion or clarification are\nstudied separately by specific design, which lacks generalizability and hinders\nadaptation to novel applications. Despite recent attempts to explore the use of\nLLMs for query recommendation, these methods mainly rely on the inherent\nknowledge of LLMs or external sources like few-shot examples, retrieved\ndocuments, or knowledge bases, neglecting the importance of the calibration and\nalignment with user feedback, thus limiting their practical utility. To address\nthese challenges, we first propose a general Generative Query Recommendation\n(GQR) framework that aligns LLM-based query generation with user preference.\nSpecifically, we unify diverse query recommendation tasks by a universal prompt\nframework, leveraging the instruct-following capability of LLMs for effective\ngeneration. Secondly, we align LLMs with user feedback via presenting a\nCTR-alignment framework, which involves training a query-wise CTR predictor as\na process reward model and employing list-wise preference alignment to maximize\nthe click probability of the generated query list. Furthermore, recognizing the\ninconsistency between LLM knowledge and proactive search intents arising from\nthe separation of user-initiated queries from models, we align LLMs with user\ninitiative via retrieving co-occurrence queries as side information when\nhistorical logs are available."
                },
                "authors": [
                    {
                        "name": "Erxue Min"
                    },
                    {
                        "name": "Hsiu-Yuan Huang"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Xihong Yang"
                    },
                    {
                        "name": "Xin Jia"
                    },
                    {
                        "name": "Yunfang Wu"
                    },
                    {
                        "name": "Hengyi Cai"
                    },
                    {
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "name": "Dawei Yin"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Yin"
                },
                "author": "Dawei Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03983v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03983v2",
                "updated": "2025-04-14T13:16:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    13,
                    16,
                    42,
                    0,
                    104,
                    0
                ],
                "published": "2025-02-06T11:31:21Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    11,
                    31,
                    21,
                    3,
                    37,
                    0
                ],
                "title": "Time delay interferometry with minimal null frequencies and shortened\n  time span",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time delay interferometry with minimal null frequencies and shortened\n  time span"
                },
                "summary": "In Paper I, we introduced the hybrid Relay, an alternative second-generation\ntime-delay interferometry (TDI) configuration designed to minimize null\nfrequencies and enhance gravitational wave (GW) analysis for massive binary\nblack holes (MBBHs). In Paper II, we further improved its noise\ncharacterization performance by replacing its null stream with a more stable\nchannel, $C^{12}_3$. In this work, we present a novel TDI scheme, PD4L, which\nfeatures minimal null frequencies and a reduced time span. Unlike the hybrid\nRelay or the second-generation Michelson which require delays up to $7L$ (with\n$L$ denoting the light-travel time between spacecraft), the PD4L uses delays no\nlonger than $3L$, corresponding to a total time span of $4L$. This compact\nstructure yields several advantages: 1) reducing data margins at segment\nboundaries, 2) mitigating aliasing in the high frequency regime, and 3)\nshortening the signal tails caused by long span. To evaluate PD4L's\nperformance, we perform parameter inference for chirping GW signals from\ncoalescing MBBHs. Our results show that the PD4L outperforms the hybrid Relay\nin the high frequency band and performs comparably at low frequencies.\nMoreover, PD4L's null stream exhibits the same minimal null frequencies as its\nscience channels and maintains a more stable noise spectrum than $C^{12}_3$.\nWhile the noise spectra of its science channels are slightly less stable than\nthose of the hybrid Relay, PD4L can still reliably infer noise parameters for\ndata durations of up to four months. These results suggest PD4L as a promising\nTDI scheme, particularly well-suited for analyzing GW signal in the\nhigher-frequency domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Paper I, we introduced the hybrid Relay, an alternative second-generation\ntime-delay interferometry (TDI) configuration designed to minimize null\nfrequencies and enhance gravitational wave (GW) analysis for massive binary\nblack holes (MBBHs). In Paper II, we further improved its noise\ncharacterization performance by replacing its null stream with a more stable\nchannel, $C^{12}_3$. In this work, we present a novel TDI scheme, PD4L, which\nfeatures minimal null frequencies and a reduced time span. Unlike the hybrid\nRelay or the second-generation Michelson which require delays up to $7L$ (with\n$L$ denoting the light-travel time between spacecraft), the PD4L uses delays no\nlonger than $3L$, corresponding to a total time span of $4L$. This compact\nstructure yields several advantages: 1) reducing data margins at segment\nboundaries, 2) mitigating aliasing in the high frequency regime, and 3)\nshortening the signal tails caused by long span. To evaluate PD4L's\nperformance, we perform parameter inference for chirping GW signals from\ncoalescing MBBHs. Our results show that the PD4L outperforms the hybrid Relay\nin the high frequency band and performs comparably at low frequencies.\nMoreover, PD4L's null stream exhibits the same minimal null frequencies as its\nscience channels and maintains a more stable noise spectrum than $C^{12}_3$.\nWhile the noise spectra of its science channels are slightly less stable than\nthose of the hybrid Relay, PD4L can still reliably infer noise parameters for\ndata durations of up to four months. These results suggest PD4L as a promising\nTDI scheme, particularly well-suited for analyzing GW signal in the\nhigher-frequency domain."
                },
                "authors": [
                    {
                        "name": "Gang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Gang Wang"
                },
                "author": "Gang Wang",
                "arxiv_comment": "17 pages, 11 figures (update results in v2), a follow-up to the works\n  arXiv:2403.01490 and arXiv:2406.11305",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03983v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03983v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10198v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10198v1",
                "updated": "2025-04-14T13:02:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    13,
                    2,
                    53,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T13:02:53Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    13,
                    2,
                    53,
                    0,
                    104,
                    0
                ],
                "title": "DioR: Adaptive Cognitive Detection and Contextual Retrieval Optimization\n  for Dynamic Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DioR: Adaptive Cognitive Detection and Contextual Retrieval Optimization\n  for Dynamic Retrieval-Augmented Generation"
                },
                "summary": "Dynamic Retrieval-augmented Generation (RAG) has shown great success in\nmitigating hallucinations in large language models (LLMs) during generation.\nHowever, existing dynamic RAG methods face significant limitations in two key\naspects: 1) Lack of an effective mechanism to control retrieval triggers, and\n2) Lack of effective scrutiny of retrieval content. To address these\nlimitations, we propose an innovative dynamic RAG method, DioR (Adaptive\nCognitive Detection and Contextual Retrieval Optimization), which consists of\ntwo main components: adaptive cognitive detection and contextual retrieval\noptimization, specifically designed to determine when retrieval is needed and\nwhat to retrieve for LLMs is useful. Experimental results demonstrate that DioR\nachieves superior performance on all tasks, demonstrating the effectiveness of\nour work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Retrieval-augmented Generation (RAG) has shown great success in\nmitigating hallucinations in large language models (LLMs) during generation.\nHowever, existing dynamic RAG methods face significant limitations in two key\naspects: 1) Lack of an effective mechanism to control retrieval triggers, and\n2) Lack of effective scrutiny of retrieval content. To address these\nlimitations, we propose an innovative dynamic RAG method, DioR (Adaptive\nCognitive Detection and Contextual Retrieval Optimization), which consists of\ntwo main components: adaptive cognitive detection and contextual retrieval\noptimization, specifically designed to determine when retrieval is needed and\nwhat to retrieve for LLMs is useful. Experimental results demonstrate that DioR\nachieves superior performance on all tasks, demonstrating the effectiveness of\nour work."
                },
                "authors": [
                    {
                        "name": "Hanghui Guo"
                    },
                    {
                        "name": "Jia Zhu"
                    },
                    {
                        "name": "Shimin Di"
                    },
                    {
                        "name": "Weijie Shi"
                    },
                    {
                        "name": "Zhangze Chen"
                    },
                    {
                        "name": "Jiajie Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jiajie Xu"
                },
                "author": "Jiajie Xu",
                "arxiv_comment": "24 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10198v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18188v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18188v2",
                "updated": "2025-04-14T12:55:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    55,
                    38,
                    0,
                    104,
                    0
                ],
                "published": "2024-04-28T13:56:35Z",
                "published_parsed": [
                    2024,
                    4,
                    28,
                    13,
                    56,
                    35,
                    6,
                    119,
                    0
                ],
                "title": "Enhancing dark siren cosmology through multi-band gravitational wave\n  synergetic observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing dark siren cosmology through multi-band gravitational wave\n  synergetic observations"
                },
                "summary": "Multi-band gravitational-wave (GW) standard siren observations are poised to\nherald a new era in the study of cosmic evolution. These observations offer\nhigher signal-to-noise ratios and improved localizations compared to those\nachieved with single-band GW detection, which are crucial for the cosmological\napplications of dark sirens. In this work, we explore the role multi-band GW\nsynergetic observations will play in measuring cosmological parameters,\nparticularly in comparison with single GW observatory data. We used mock\nmulti-band dark siren data from third-generation GW detectors and the baseline\nDecihertz Interferometer Gravitational-Wave Observatory to infer cosmological\nparameters. Our analysis shows that multi-band GW observations significantly\nimprove sky localization accuracy by two to three orders of magnitude over\nsingle-band observations, although their impact on luminosity distance error\nremains limited. This results in a substantial improvement in the constraints\non matter density and the Hubble constant, enhancing their constraint precision\nby $60\\%$-$90\\%$ and $52\\%$-$85\\%$, respectively. We conclude that the\nsignificant potential of multi-band GW synergistic observations for detecting\nGW signals and resolving the Hubble tension is highly promising and warrants\nanticipation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-band gravitational-wave (GW) standard siren observations are poised to\nherald a new era in the study of cosmic evolution. These observations offer\nhigher signal-to-noise ratios and improved localizations compared to those\nachieved with single-band GW detection, which are crucial for the cosmological\napplications of dark sirens. In this work, we explore the role multi-band GW\nsynergetic observations will play in measuring cosmological parameters,\nparticularly in comparison with single GW observatory data. We used mock\nmulti-band dark siren data from third-generation GW detectors and the baseline\nDecihertz Interferometer Gravitational-Wave Observatory to infer cosmological\nparameters. Our analysis shows that multi-band GW observations significantly\nimprove sky localization accuracy by two to three orders of magnitude over\nsingle-band observations, although their impact on luminosity distance error\nremains limited. This results in a substantial improvement in the constraints\non matter density and the Hubble constant, enhancing their constraint precision\nby $60\\%$-$90\\%$ and $52\\%$-$85\\%$, respectively. We conclude that the\nsignificant potential of multi-band GW synergistic observations for detecting\nGW signals and resolving the Hubble tension is highly promising and warrants\nanticipation."
                },
                "authors": [
                    {
                        "name": "Yue-Yan Dong"
                    },
                    {
                        "name": "Ji-Yu Song"
                    },
                    {
                        "name": "Shang-Jie Jin"
                    },
                    {
                        "name": "Jing-Fei Zhang"
                    },
                    {
                        "name": "Xin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Zhang"
                },
                "author": "Xin Zhang",
                "arxiv_comment": "28 pages, 7 figures; accepted for publication in JCAP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.18188v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18188v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10191v1",
                "updated": "2025-04-14T12:53:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    53,
                    58,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T12:53:58Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    53,
                    58,
                    0,
                    104,
                    0
                ],
                "title": "Localized Cultural Knowledge is Conserved and Controllable in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localized Cultural Knowledge is Conserved and Controllable in Large\n  Language Models"
                },
                "summary": "Just as humans display language patterns influenced by their native tongue\nwhen speaking new languages, LLMs often default to English-centric responses\neven when generating in other languages. Nevertheless, we observe that local\ncultural information persists within the models and can be readily activated\nfor cultural customization. We first demonstrate that explicitly providing\ncultural context in prompts significantly improves the models' ability to\ngenerate culturally localized responses. We term the disparity in model\nperformance with versus without explicit cultural context the explicit-implicit\nlocalization gap, indicating that while cultural knowledge exists within LLMs,\nit may not naturally surface in multilingual interactions if cultural context\nis not explicitly provided. Despite the explicit prompting benefit, however,\nthe answers reduce in diversity and tend toward stereotypes. Second, we\nidentify an explicit cultural customization vector, conserved across all\nnon-English languages we explore, which enables LLMs to be steered from the\nsynthetic English cultural world-model toward each non-English cultural world.\nSteered responses retain the diversity of implicit prompting and reduce\nstereotypes to dramatically improve the potential for customization. We discuss\nthe implications of explicit cultural customization for understanding the\nconservation of alternative cultural world models within LLMs, and their\ncontrollable utility for translation, cultural customization, and the\npossibility of making the explicit implicit through soft control for expanded\nLLM function and appeal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Just as humans display language patterns influenced by their native tongue\nwhen speaking new languages, LLMs often default to English-centric responses\neven when generating in other languages. Nevertheless, we observe that local\ncultural information persists within the models and can be readily activated\nfor cultural customization. We first demonstrate that explicitly providing\ncultural context in prompts significantly improves the models' ability to\ngenerate culturally localized responses. We term the disparity in model\nperformance with versus without explicit cultural context the explicit-implicit\nlocalization gap, indicating that while cultural knowledge exists within LLMs,\nit may not naturally surface in multilingual interactions if cultural context\nis not explicitly provided. Despite the explicit prompting benefit, however,\nthe answers reduce in diversity and tend toward stereotypes. Second, we\nidentify an explicit cultural customization vector, conserved across all\nnon-English languages we explore, which enables LLMs to be steered from the\nsynthetic English cultural world-model toward each non-English cultural world.\nSteered responses retain the diversity of implicit prompting and reduce\nstereotypes to dramatically improve the potential for customization. We discuss\nthe implications of explicit cultural customization for understanding the\nconservation of alternative cultural world models within LLMs, and their\ncontrollable utility for translation, cultural customization, and the\npossibility of making the explicit implicit through soft control for expanded\nLLM function and appeal."
                },
                "authors": [
                    {
                        "name": "Veniamin Veselovsky"
                    },
                    {
                        "name": "Berke Argin"
                    },
                    {
                        "name": "Benedikt Stroebl"
                    },
                    {
                        "name": "Chris Wendler"
                    },
                    {
                        "name": "Robert West"
                    },
                    {
                        "name": "James Evans"
                    },
                    {
                        "name": "Thomas L. Griffiths"
                    },
                    {
                        "name": "Arvind Narayanan"
                    }
                ],
                "author_detail": {
                    "name": "Arvind Narayanan"
                },
                "author": "Arvind Narayanan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10423v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10423v2",
                "updated": "2025-04-14T12:52:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    52,
                    24,
                    0,
                    104,
                    0
                ],
                "published": "2024-12-10T12:42:33Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    12,
                    42,
                    33,
                    1,
                    345,
                    0
                ],
                "title": "Look Before You Leap: Enhancing Attention and Vigilance Regarding\n  Harmful Content with GuidelineLLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Look Before You Leap: Enhancing Attention and Vigilance Regarding\n  Harmful Content with GuidelineLLM"
                },
                "summary": "Despite being empowered with alignment mechanisms, large language models\n(LLMs) are increasingly vulnerable to emerging jailbreak attacks that can\ncompromise their alignment mechanisms. This vulnerability poses significant\nrisks to real-world applications. Existing work faces challenges in both\ntraining efficiency and generalization capabilities (i.e., Reinforcement\nLearning from Human Feedback and Red-Teaming). Developing effective strategies\nto enable LLMs to resist continuously evolving jailbreak attempts represents a\nsignificant challenge. To address this challenge, we propose a novel defensive\nparadigm called GuidelineLLM, which assists LLMs in recognizing queries that\nmay have harmful content. Before LLMs respond to a query, GuidelineLLM first\nidentifies potential risks associated with the query, summarizes these risks\ninto guideline suggestions, and then feeds these guidelines to the responding\nLLMs. Importantly, our approach eliminates the necessity for additional safety\nfine-tuning of the LLMs themselves; only the GuidelineLLM requires fine-tuning.\nThis characteristic enhances the general applicability of GuidelineLLM across\nvarious LLMs. Experimental results demonstrate that GuidelineLLM can\nsignificantly reduce the attack success rate (ASR) against LLM (an average\nreduction of 34.17\\% ASR) while maintaining the usefulness of LLM in handling\nbenign queries. The code is available at\nhttps://github.com/sqzhang-lazy/GuidelineLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite being empowered with alignment mechanisms, large language models\n(LLMs) are increasingly vulnerable to emerging jailbreak attacks that can\ncompromise their alignment mechanisms. This vulnerability poses significant\nrisks to real-world applications. Existing work faces challenges in both\ntraining efficiency and generalization capabilities (i.e., Reinforcement\nLearning from Human Feedback and Red-Teaming). Developing effective strategies\nto enable LLMs to resist continuously evolving jailbreak attempts represents a\nsignificant challenge. To address this challenge, we propose a novel defensive\nparadigm called GuidelineLLM, which assists LLMs in recognizing queries that\nmay have harmful content. Before LLMs respond to a query, GuidelineLLM first\nidentifies potential risks associated with the query, summarizes these risks\ninto guideline suggestions, and then feeds these guidelines to the responding\nLLMs. Importantly, our approach eliminates the necessity for additional safety\nfine-tuning of the LLMs themselves; only the GuidelineLLM requires fine-tuning.\nThis characteristic enhances the general applicability of GuidelineLLM across\nvarious LLMs. Experimental results demonstrate that GuidelineLLM can\nsignificantly reduce the attack success rate (ASR) against LLM (an average\nreduction of 34.17\\% ASR) while maintaining the usefulness of LLM in handling\nbenign queries. The code is available at\nhttps://github.com/sqzhang-lazy/GuidelineLLM."
                },
                "authors": [
                    {
                        "name": "Shaoqing Zhang"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Rongxiang Weng"
                    },
                    {
                        "name": "Muyun Yang"
                    },
                    {
                        "name": "Tiejun Zhao"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10423v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10423v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17274v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17274v5",
                "updated": "2025-04-14T12:50:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    50,
                    55,
                    0,
                    104,
                    0
                ],
                "published": "2024-11-26T09:51:55Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    51,
                    55,
                    1,
                    331,
                    0
                ],
                "title": "CleanVul: Automatic Function-Level Vulnerability Detection in Code\n  Commits Using LLM Heuristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CleanVul: Automatic Function-Level Vulnerability Detection in Code\n  Commits Using LLM Heuristics"
                },
                "summary": "Accurate identification of software vulnerabilities is crucial for system\nintegrity. Vulnerability datasets, often derived from the National\nVulnerability Database (NVD) or directly from GitHub, are essential for\ntraining machine learning models to detect these security flaws. However, these\ndatasets frequently suffer from significant noise, typically 40% to 75%, due\nprimarily to the automatic and indiscriminate labeling of all changes in\nvulnerability-fixing commits (VFCs) as vulnerability-related. This\nmisclassification occurs because not all changes in a commit aimed at fixing\nvulnerabilities pertain to security threats; many are routine updates like bug\nfixes or test improvements.\n  This paper introduces the first methodology that uses the Large Language\nModel (LLM) with a heuristic enhancement to automatically identify\nvulnerability-fixing changes from VFCs, achieving an F1-score of 0.82.\nVulSifter was applied to a large-scale study, where we conducted a crawl of\n127,063 repositories on GitHub, resulting in the acquisition of 5,352,105\ncommits. VulSifter involves utilizing an LLM to comprehend code semantics and\ncontextual information, while applying heuristics to filter out unrelated\nchanges. We then developed CleanVul, a high-quality dataset comprising 8,203\nfunctions using our LLM heuristic enhancement approach, demonstrating\nCorrectness (90.6%) comparable to established datasets such as SVEN and\nPrimeVul.\n  To evaluate the CleanVul dataset, we conducted experiments focusing on\nfine-tuning various LLMs on CleanVul and other high-quality datasets.\nEvaluation results reveal that LLMs fine-tuned on CleanVul not only exhibit\nenhanced accuracy but also superior generalization capabilities compared to\nthose trained on uncleaned datasets. Specifically, models trained on CleanVul\nand tested on PrimeVul achieve accuracy higher than those trained and tested\nexclusively on PrimeVul.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate identification of software vulnerabilities is crucial for system\nintegrity. Vulnerability datasets, often derived from the National\nVulnerability Database (NVD) or directly from GitHub, are essential for\ntraining machine learning models to detect these security flaws. However, these\ndatasets frequently suffer from significant noise, typically 40% to 75%, due\nprimarily to the automatic and indiscriminate labeling of all changes in\nvulnerability-fixing commits (VFCs) as vulnerability-related. This\nmisclassification occurs because not all changes in a commit aimed at fixing\nvulnerabilities pertain to security threats; many are routine updates like bug\nfixes or test improvements.\n  This paper introduces the first methodology that uses the Large Language\nModel (LLM) with a heuristic enhancement to automatically identify\nvulnerability-fixing changes from VFCs, achieving an F1-score of 0.82.\nVulSifter was applied to a large-scale study, where we conducted a crawl of\n127,063 repositories on GitHub, resulting in the acquisition of 5,352,105\ncommits. VulSifter involves utilizing an LLM to comprehend code semantics and\ncontextual information, while applying heuristics to filter out unrelated\nchanges. We then developed CleanVul, a high-quality dataset comprising 8,203\nfunctions using our LLM heuristic enhancement approach, demonstrating\nCorrectness (90.6%) comparable to established datasets such as SVEN and\nPrimeVul.\n  To evaluate the CleanVul dataset, we conducted experiments focusing on\nfine-tuning various LLMs on CleanVul and other high-quality datasets.\nEvaluation results reveal that LLMs fine-tuned on CleanVul not only exhibit\nenhanced accuracy but also superior generalization capabilities compared to\nthose trained on uncleaned datasets. Specifically, models trained on CleanVul\nand tested on PrimeVul achieve accuracy higher than those trained and tested\nexclusively on PrimeVul."
                },
                "authors": [
                    {
                        "name": "Yikun Li"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "Ratnadira Widyasari"
                    },
                    {
                        "name": "Yan Naing Tun"
                    },
                    {
                        "name": "Huu Hung Nguyen"
                    },
                    {
                        "name": "Tan Bui"
                    },
                    {
                        "name": "Ivana Clairine Irsan"
                    },
                    {
                        "name": "Yiran Cheng"
                    },
                    {
                        "name": "Xiang Lan"
                    },
                    {
                        "name": "Han Wei Ang"
                    },
                    {
                        "name": "Frank Liauw"
                    },
                    {
                        "name": "Martin Weyssow"
                    },
                    {
                        "name": "Hong Jin Kang"
                    },
                    {
                        "name": "Eng Lieh Ouh"
                    },
                    {
                        "name": "Lwin Khin Shar"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17274v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17274v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10187v1",
                "updated": "2025-04-14T12:40:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    40,
                    39,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T12:40:39Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    40,
                    39,
                    0,
                    104,
                    0
                ],
                "title": "Deep Reasoning Translation via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Reasoning Translation via Reinforcement Learning"
                },
                "summary": "Recently, deep reasoning LLMs (e.g., OpenAI o1/o3 and DeepSeek-R1) have shown\npromising performance in various complex tasks. Free translation is an\nimportant and interesting task in the multilingual world, which requires going\nbeyond word-for-word translation and taking cultural differences into account.\nThis task is still under-explored in deep reasoning LLMs. In this paper, we\nintroduce DeepTrans, a deep reasoning translation model that learns free\ntranslation via reinforcement learning. Specifically, we carefully build a\nreward model with pre-defined scoring criteria on both the translation results\nand the thought process. Given the source sentences, the reward model teaches\nthe deep translation model how to think and free-translate them during\nreinforcement learning. In this way, training DeepTrans does not need any\nlabeled translations, avoiding the human-intensive annotation or\nresource-intensive data synthesis. Experimental results show the effectiveness\nof DeepTrans. Using Qwen2.5-7B as the backbone, DeepTrans improves performance\nby 16.3% in literature translation, and outperforms strong deep reasoning\nbaselines as well as baselines that are fine-tuned with synthesized data.\nMoreover, we summarize the failures and interesting findings during our RL\nexploration. We hope this work could inspire other researchers in free\ntranslation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, deep reasoning LLMs (e.g., OpenAI o1/o3 and DeepSeek-R1) have shown\npromising performance in various complex tasks. Free translation is an\nimportant and interesting task in the multilingual world, which requires going\nbeyond word-for-word translation and taking cultural differences into account.\nThis task is still under-explored in deep reasoning LLMs. In this paper, we\nintroduce DeepTrans, a deep reasoning translation model that learns free\ntranslation via reinforcement learning. Specifically, we carefully build a\nreward model with pre-defined scoring criteria on both the translation results\nand the thought process. Given the source sentences, the reward model teaches\nthe deep translation model how to think and free-translate them during\nreinforcement learning. In this way, training DeepTrans does not need any\nlabeled translations, avoiding the human-intensive annotation or\nresource-intensive data synthesis. Experimental results show the effectiveness\nof DeepTrans. Using Qwen2.5-7B as the backbone, DeepTrans improves performance\nby 16.3% in literature translation, and outperforms strong deep reasoning\nbaselines as well as baselines that are fine-tuned with synthesized data.\nMoreover, we summarize the failures and interesting findings during our RL\nexploration. We hope this work could inspire other researchers in free\ntranslation."
                },
                "authors": [
                    {
                        "name": "Jiaan Wang"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhou"
                },
                "author": "Jie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10185v1",
                "updated": "2025-04-14T12:38:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    38,
                    37,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T12:38:37Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    38,
                    37,
                    0,
                    104,
                    0
                ],
                "title": "LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in\n  Current Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in\n  Current Benchmarks"
                },
                "summary": "Large language model unlearning has become a critical challenge in ensuring\nsafety and controlled model behavior by removing undesired data-model\ninfluences from the pretrained model while preserving general utility.\nSignificant recent efforts have been dedicated to developing LLM unlearning\nbenchmarks such as WMDP (Weapons of Mass Destruction Proxy) and MUSE (Machine\nUnlearning Six-way Evaluation), facilitating standardized unlearning\nperformance assessment and method comparison. Despite their usefulness, we\nuncover for the first time a novel coreset effect within these benchmarks.\nSpecifically, we find that LLM unlearning achieved with the original (full)\nforget set can be effectively maintained using a significantly smaller subset\n(functioning as a \"coreset\"), e.g., as little as 5% of the forget set, even\nwhen selected at random. This suggests that LLM unlearning in these benchmarks\ncan be performed surprisingly easily, even in an extremely low-data regime. We\ndemonstrate that this coreset effect remains strong, regardless of the LLM\nunlearning method used, such as NPO (Negative Preference Optimization) and RMU\n(Representation Misdirection Unlearning), the popular ones in these benchmarks.\nThe surprisingly strong coreset effect is also robust across various data\nselection methods, ranging from random selection to more sophisticated\nheuristic approaches. We explain the coreset effect in LLM unlearning through a\nkeyword-based perspective, showing that keywords extracted from the forget set\nalone contribute significantly to unlearning effectiveness and indicating that\ncurrent unlearning is driven by a compact set of high-impact tokens rather than\nthe entire dataset. We further justify the faithfulness of coreset-unlearned\nmodels along additional dimensions, such as mode connectivity and robustness to\njailbreaking attacks. Codes are available at\nhttps://github.com/OPTML-Group/MU-Coreset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model unlearning has become a critical challenge in ensuring\nsafety and controlled model behavior by removing undesired data-model\ninfluences from the pretrained model while preserving general utility.\nSignificant recent efforts have been dedicated to developing LLM unlearning\nbenchmarks such as WMDP (Weapons of Mass Destruction Proxy) and MUSE (Machine\nUnlearning Six-way Evaluation), facilitating standardized unlearning\nperformance assessment and method comparison. Despite their usefulness, we\nuncover for the first time a novel coreset effect within these benchmarks.\nSpecifically, we find that LLM unlearning achieved with the original (full)\nforget set can be effectively maintained using a significantly smaller subset\n(functioning as a \"coreset\"), e.g., as little as 5% of the forget set, even\nwhen selected at random. This suggests that LLM unlearning in these benchmarks\ncan be performed surprisingly easily, even in an extremely low-data regime. We\ndemonstrate that this coreset effect remains strong, regardless of the LLM\nunlearning method used, such as NPO (Negative Preference Optimization) and RMU\n(Representation Misdirection Unlearning), the popular ones in these benchmarks.\nThe surprisingly strong coreset effect is also robust across various data\nselection methods, ranging from random selection to more sophisticated\nheuristic approaches. We explain the coreset effect in LLM unlearning through a\nkeyword-based perspective, showing that keywords extracted from the forget set\nalone contribute significantly to unlearning effectiveness and indicating that\ncurrent unlearning is driven by a compact set of high-impact tokens rather than\nthe entire dataset. We further justify the faithfulness of coreset-unlearned\nmodels along additional dimensions, such as mode connectivity and robustness to\njailbreaking attacks. Codes are available at\nhttps://github.com/OPTML-Group/MU-Coreset."
                },
                "authors": [
                    {
                        "name": "Soumyadeep Pal"
                    },
                    {
                        "name": "Changsheng Wang"
                    },
                    {
                        "name": "James Diffenderfer"
                    },
                    {
                        "name": "Bhavya Kailkhura"
                    },
                    {
                        "name": "Sijia Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sijia Liu"
                },
                "author": "Sijia Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05045v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05045v3",
                "updated": "2025-04-14T12:33:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    33,
                    32,
                    0,
                    104,
                    0
                ],
                "published": "2024-09-08T10:06:54Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    10,
                    6,
                    54,
                    6,
                    252,
                    0
                ],
                "title": "Using Large Language Models for Template Detection from Security Event\n  Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Large Language Models for Template Detection from Security Event\n  Logs"
                },
                "summary": "In modern IT systems and computer networks, real-time and offline event log\nanalysis is a crucial part of cyber security monitoring. In particular, event\nlog analysis techniques are essential for the timely detection of cyber attacks\nand for assisting security experts with the analysis of past security\nincidents. The detection of line patterns or templates from unstructured\ntextual event logs has been identified as an important task of event log\nanalysis since detected templates represent event types in the event log and\nprepare the logs for downstream online or offline security monitoring tasks.\nDuring the last two decades, a number of template mining algorithms have been\nproposed. However, many proposed algorithms rely on traditional data mining\ntechniques, and the usage of Large Language Models (LLMs) has received less\nattention so far. Also, most approaches that harness LLMs are supervised, and\nunsupervised LLM-based template mining remains an understudied area. The\ncurrent paper addresses this research gap and investigates the application of\nLLMs for unsupervised detection of templates from unstructured security event\nlogs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern IT systems and computer networks, real-time and offline event log\nanalysis is a crucial part of cyber security monitoring. In particular, event\nlog analysis techniques are essential for the timely detection of cyber attacks\nand for assisting security experts with the analysis of past security\nincidents. The detection of line patterns or templates from unstructured\ntextual event logs has been identified as an important task of event log\nanalysis since detected templates represent event types in the event log and\nprepare the logs for downstream online or offline security monitoring tasks.\nDuring the last two decades, a number of template mining algorithms have been\nproposed. However, many proposed algorithms rely on traditional data mining\ntechniques, and the usage of Large Language Models (LLMs) has received less\nattention so far. Also, most approaches that harness LLMs are supervised, and\nunsupervised LLM-based template mining remains an understudied area. The\ncurrent paper addresses this research gap and investigates the application of\nLLMs for unsupervised detection of templates from unstructured security event\nlogs."
                },
                "authors": [
                    {
                        "name": "Risto Vaarandi"
                    },
                    {
                        "name": "Hayretdin Bahsi"
                    }
                ],
                "author_detail": {
                    "name": "Hayretdin Bahsi"
                },
                "author": "Hayretdin Bahsi",
                "arxiv_doi": "10.1007/s10207-025-01018-y",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10207-025-01018-y",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.05045v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05045v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication in International Journal of Information\n  Security",
                "arxiv_journal_ref": "Int. J. Inf. Secur. 24, 104 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05946v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05946v2",
                "updated": "2025-04-14T12:28:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    28,
                    2,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-08T11:59:00Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    11,
                    59,
                    0,
                    1,
                    98,
                    0
                ],
                "title": "InstructMPC: A Human-LLM-in-the-Loop Framework for Context-Aware Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstructMPC: A Human-LLM-in-the-Loop Framework for Context-Aware Control"
                },
                "summary": "Model Predictive Control (MPC) is a powerful control strategy widely utilized\nin domains like energy management, building control, and autonomous systems.\nHowever, its effectiveness in real-world settings is challenged by the need to\nincorporate context-specific predictions and expert instructions, which\ntraditional MPC often neglects. We propose InstructMPC, a novel framework that\naddresses this gap by integrating real-time human instructions through a Large\nLanguage Model (LLM) to produce context-aware predictions for MPC. Our method\nemploys a Language-to-Distribution (L2D) module to translate contextual\ninformation into predictive disturbance trajectories, which are then\nincorporated into the MPC optimization. Unlike existing context-aware and\nlanguage-based MPC models, InstructMPC enables dynamic human-LLM interaction\nand fine-tunes the L2D module in a closed loop with theoretical performance\nguarantees, achieving a regret bound of $O(\\sqrt{T\\log T})$ for linear dynamics\nwhen optimized via advanced fine-tuning methods such as Direct Preference\nOptimization (DPO) using a tailored loss function.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Predictive Control (MPC) is a powerful control strategy widely utilized\nin domains like energy management, building control, and autonomous systems.\nHowever, its effectiveness in real-world settings is challenged by the need to\nincorporate context-specific predictions and expert instructions, which\ntraditional MPC often neglects. We propose InstructMPC, a novel framework that\naddresses this gap by integrating real-time human instructions through a Large\nLanguage Model (LLM) to produce context-aware predictions for MPC. Our method\nemploys a Language-to-Distribution (L2D) module to translate contextual\ninformation into predictive disturbance trajectories, which are then\nincorporated into the MPC optimization. Unlike existing context-aware and\nlanguage-based MPC models, InstructMPC enables dynamic human-LLM interaction\nand fine-tunes the L2D module in a closed loop with theoretical performance\nguarantees, achieving a regret bound of $O(\\sqrt{T\\log T})$ for linear dynamics\nwhen optimized via advanced fine-tuning methods such as Direct Preference\nOptimization (DPO) using a tailored loss function."
                },
                "authors": [
                    {
                        "name": "Ruixiang Wu"
                    },
                    {
                        "name": "Jiahao Ai"
                    },
                    {
                        "name": "Tongxin Li"
                    }
                ],
                "author_detail": {
                    "name": "Tongxin Li"
                },
                "author": "Tongxin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05946v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05946v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13929v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13929v6",
                "updated": "2025-04-14T12:23:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    23,
                    27,
                    0,
                    104,
                    0
                ],
                "published": "2024-05-22T18:58:58Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    18,
                    58,
                    58,
                    2,
                    143,
                    0
                ],
                "title": "Vikhr: The Family of Open-Source Instruction-Tuned Large Language Models\n  for Russian",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vikhr: The Family of Open-Source Instruction-Tuned Large Language Models\n  for Russian"
                },
                "summary": "There has been a surge in the development of various Large Language Models\n(LLMs). However, text generation for languages other than English often faces\nsignificant challenges, including poor generation quality and reduced\ncomputational performance due to the disproportionate representation of tokens\nin the model's vocabulary. In this work, we address these issues by developing\na pipeline for the adaptation of English-oriented pre-trained models to other\nlanguages and constructing efficient bilingual LLMs. Using this pipeline, we\nconstruct Vikhr, a series of bilingual open-source instruction-following LLMs\ndesigned specifically for the Russian language. ``Vikhr'' refers to the name of\nthe Mistral LLM series and means a ``strong gust of wind.'' Unlike previous\nRussian-language models that typically rely on LoRA adapters on top of\nEnglish-oriented models, sacrificing performance for lower training costs,\nVikhr features an adapted tokenizer vocabulary and undergoes the continued\npre-training and instruction tuning of all weights. This not only enhances the\nmodel's performance but also significantly improves its computational and\ncontextual efficiency. We also expanded the instruction datasets and corpora\nfor continued pre-training. The model weights, instruction sets, and code are\npublicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There has been a surge in the development of various Large Language Models\n(LLMs). However, text generation for languages other than English often faces\nsignificant challenges, including poor generation quality and reduced\ncomputational performance due to the disproportionate representation of tokens\nin the model's vocabulary. In this work, we address these issues by developing\na pipeline for the adaptation of English-oriented pre-trained models to other\nlanguages and constructing efficient bilingual LLMs. Using this pipeline, we\nconstruct Vikhr, a series of bilingual open-source instruction-following LLMs\ndesigned specifically for the Russian language. ``Vikhr'' refers to the name of\nthe Mistral LLM series and means a ``strong gust of wind.'' Unlike previous\nRussian-language models that typically rely on LoRA adapters on top of\nEnglish-oriented models, sacrificing performance for lower training costs,\nVikhr features an adapted tokenizer vocabulary and undergoes the continued\npre-training and instruction tuning of all weights. This not only enhances the\nmodel's performance but also significantly improves its computational and\ncontextual efficiency. We also expanded the instruction datasets and corpora\nfor continued pre-training. The model weights, instruction sets, and code are\npublicly available."
                },
                "authors": [
                    {
                        "name": "Aleksandr Nikolich"
                    },
                    {
                        "name": "Konstantin Korolev"
                    },
                    {
                        "name": "Sergei Bratchikov"
                    },
                    {
                        "name": "Igor Kiselev"
                    },
                    {
                        "name": "Artem Shelmanov"
                    }
                ],
                "author_detail": {
                    "name": "Artem Shelmanov"
                },
                "author": "Artem Shelmanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13929v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13929v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10168v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10168v1",
                "updated": "2025-04-14T12:22:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    22,
                    30,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T12:22:30Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    22,
                    30,
                    0,
                    104,
                    0
                ],
                "title": "HalluSearch at SemEval-2025 Task 3: A Search-Enhanced RAG Pipeline for\n  Hallucination Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HalluSearch at SemEval-2025 Task 3: A Search-Enhanced RAG Pipeline for\n  Hallucination Detection"
                },
                "summary": "In this paper, we present HalluSearch, a multilingual pipeline designed to\ndetect fabricated text spans in Large Language Model (LLM) outputs. Developed\nas part of Mu-SHROOM, the Multilingual Shared-task on Hallucinations and\nRelated Observable Overgeneration Mistakes, HalluSearch couples\nretrieval-augmented verification with fine-grained factual splitting to\nidentify and localize hallucinations in fourteen different languages. Empirical\nevaluations show that HalluSearch performs competitively, placing fourth in\nboth English (within the top ten percent) and Czech. While the system's\nretrieval-based strategy generally proves robust, it faces challenges in\nlanguages with limited online coverage, underscoring the need for further\nresearch to ensure consistent hallucination detection across diverse linguistic\ncontexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present HalluSearch, a multilingual pipeline designed to\ndetect fabricated text spans in Large Language Model (LLM) outputs. Developed\nas part of Mu-SHROOM, the Multilingual Shared-task on Hallucinations and\nRelated Observable Overgeneration Mistakes, HalluSearch couples\nretrieval-augmented verification with fine-grained factual splitting to\nidentify and localize hallucinations in fourteen different languages. Empirical\nevaluations show that HalluSearch performs competitively, placing fourth in\nboth English (within the top ten percent) and Czech. While the system's\nretrieval-based strategy generally proves robust, it faces challenges in\nlanguages with limited online coverage, underscoring the need for further\nresearch to ensure consistent hallucination detection across diverse linguistic\ncontexts."
                },
                "authors": [
                    {
                        "name": "Mohamed A. Abdallah"
                    },
                    {
                        "name": "Samhaa R. El-Beltagy"
                    }
                ],
                "author_detail": {
                    "name": "Samhaa R. El-Beltagy"
                },
                "author": "Samhaa R. El-Beltagy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10168v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19460v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19460v2",
                "updated": "2025-04-14T12:22:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    22,
                    6,
                    0,
                    104,
                    0
                ],
                "published": "2025-02-26T10:28:44Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    10,
                    28,
                    44,
                    2,
                    57,
                    0
                ],
                "title": "Practical Evaluation of Copula-based Survival Metrics: Beyond the\n  Independent Censoring Assumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practical Evaluation of Copula-based Survival Metrics: Beyond the\n  Independent Censoring Assumption"
                },
                "summary": "Conventional survival metrics, such as Harrell's concordance index and the\nBrier Score, rely on the independent censoring assumption for valid inference\nin the presence of right-censored data. However, when instances are censored\nfor reasons related to the event of interest, this assumption no longer holds,\nas this kind of dependent censoring biases the marginal survival estimates of\npopular nonparametric estimators. In this paper, we propose three copula-based\nmetrics to evaluate survival models in the presence of dependent censoring, and\ndesign a framework to create realistic, semi-synthetic datasets with dependent\ncensoring to facilitate the evaluation of the metrics. Our empirical analyses\nin synthetic and semi-synthetic datasets show that our metrics can give error\nestimates that are closer to the true error, mainly in terms of prediction\naccuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional survival metrics, such as Harrell's concordance index and the\nBrier Score, rely on the independent censoring assumption for valid inference\nin the presence of right-censored data. However, when instances are censored\nfor reasons related to the event of interest, this assumption no longer holds,\nas this kind of dependent censoring biases the marginal survival estimates of\npopular nonparametric estimators. In this paper, we propose three copula-based\nmetrics to evaluate survival models in the presence of dependent censoring, and\ndesign a framework to create realistic, semi-synthetic datasets with dependent\ncensoring to facilitate the evaluation of the metrics. Our empirical analyses\nin synthetic and semi-synthetic datasets show that our metrics can give error\nestimates that are closer to the true error, mainly in terms of prediction\naccuracy."
                },
                "authors": [
                    {
                        "name": "Christian Marius Lillelund"
                    },
                    {
                        "name": "Shi-ang Qi"
                    },
                    {
                        "name": "Russell Greiner"
                    }
                ],
                "author_detail": {
                    "name": "Russell Greiner"
                },
                "author": "Russell Greiner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19460v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19460v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10167v1",
                "updated": "2025-04-14T12:21:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    21,
                    55,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T12:21:55Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    21,
                    55,
                    0,
                    104,
                    0
                ],
                "title": "C-FAITH: A Chinese Fine-Grained Benchmark for Automated Hallucination\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C-FAITH: A Chinese Fine-Grained Benchmark for Automated Hallucination\n  Evaluation"
                },
                "summary": "Despite the rapid advancement of large language models, they remain highly\nsusceptible to generating hallucinations, which significantly hinders their\nwidespread application. Hallucination research requires dynamic and\nfine-grained evaluation. However, most existing hallucination benchmarks\n(especially in Chinese language) rely on human annotations, making automatical\nand cost-effective hallucination evaluation challenging. To address this, we\nintroduce HaluAgent, an agentic framework that automatically constructs\nfine-grained QA dataset based on some knowledge documents. Our experiments\ndemonstrate that the manually designed rules and prompt optimization can\nimprove the quality of generated data. Using HaluAgent, we construct C-FAITH, a\nChinese QA hallucination benchmark created from 1,399 knowledge documents\nobtained from web scraping, totaling 60,702 entries. We comprehensively\nevaluate 16 mainstream LLMs with our proposed C-FAITH, providing detailed\nexperimental results and analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the rapid advancement of large language models, they remain highly\nsusceptible to generating hallucinations, which significantly hinders their\nwidespread application. Hallucination research requires dynamic and\nfine-grained evaluation. However, most existing hallucination benchmarks\n(especially in Chinese language) rely on human annotations, making automatical\nand cost-effective hallucination evaluation challenging. To address this, we\nintroduce HaluAgent, an agentic framework that automatically constructs\nfine-grained QA dataset based on some knowledge documents. Our experiments\ndemonstrate that the manually designed rules and prompt optimization can\nimprove the quality of generated data. Using HaluAgent, we construct C-FAITH, a\nChinese QA hallucination benchmark created from 1,399 knowledge documents\nobtained from web scraping, totaling 60,702 entries. We comprehensively\nevaluate 16 mainstream LLMs with our proposed C-FAITH, providing detailed\nexperimental results and analysis."
                },
                "authors": [
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Zhifei Liu"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Huixuan Zhang"
                    },
                    {
                        "name": "Fan Xu"
                    },
                    {
                        "name": "Junzhe Zhang"
                    },
                    {
                        "name": "Xiaojun Wan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Wan"
                },
                "author": "Xiaojun Wan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10166v1",
                "updated": "2025-04-14T12:21:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    21,
                    27,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T12:21:27Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    21,
                    27,
                    0,
                    104,
                    0
                ],
                "title": "Fact-Checking with Contextual Narratives: Leveraging Retrieval-Augmented\n  LLMs for Social Media Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fact-Checking with Contextual Narratives: Leveraging Retrieval-Augmented\n  LLMs for Social Media Analysis"
                },
                "summary": "We propose CRAVE (Cluster-based Retrieval Augmented Verification with\nExplanation); a novel framework that integrates retrieval-augmented Large\nLanguage Models (LLMs) with clustering techniques to address fact-checking\nchallenges on social media. CRAVE automatically retrieves multimodal evidence\nfrom diverse, often contradictory, sources. Evidence is clustered into coherent\nnarratives, and evaluated via an LLM-based judge to deliver fact-checking\nverdicts explained by evidence summaries. By synthesizing evidence from both\ntext and image modalities and incorporating agent-based refinement, CRAVE\nensures consistency and diversity in evidence representation. Comprehensive\nexperiments demonstrate CRAVE's efficacy in retrieval precision, clustering\nquality, and judgment accuracy, showcasing its potential as a robust\ndecision-support tool for fact-checkers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose CRAVE (Cluster-based Retrieval Augmented Verification with\nExplanation); a novel framework that integrates retrieval-augmented Large\nLanguage Models (LLMs) with clustering techniques to address fact-checking\nchallenges on social media. CRAVE automatically retrieves multimodal evidence\nfrom diverse, often contradictory, sources. Evidence is clustered into coherent\nnarratives, and evaluated via an LLM-based judge to deliver fact-checking\nverdicts explained by evidence summaries. By synthesizing evidence from both\ntext and image modalities and incorporating agent-based refinement, CRAVE\nensures consistency and diversity in evidence representation. Comprehensive\nexperiments demonstrate CRAVE's efficacy in retrieval precision, clustering\nquality, and judgment accuracy, showcasing its potential as a robust\ndecision-support tool for fact-checkers."
                },
                "authors": [
                    {
                        "name": "Arka Ujjal Dey"
                    },
                    {
                        "name": "Muhammad Junaid Awan"
                    },
                    {
                        "name": "Georgia Channing"
                    },
                    {
                        "name": "Christian Schroeder de Witt"
                    },
                    {
                        "name": "John Collomosse"
                    }
                ],
                "author_detail": {
                    "name": "John Collomosse"
                },
                "author": "John Collomosse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10160v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10160v1",
                "updated": "2025-04-14T12:14:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    14,
                    18,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T12:14:18Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    14,
                    18,
                    0,
                    104,
                    0
                ],
                "title": "MT-R1-Zero: Advancing LLM-based Machine Translation via R1-Zero-like\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MT-R1-Zero: Advancing LLM-based Machine Translation via R1-Zero-like\n  Reinforcement Learning"
                },
                "summary": "Large-scale reinforcement learning (RL) methods have proven highly effective\nin enhancing the reasoning abilities of large language models (LLMs),\nparticularly for tasks with verifiable solutions such as mathematics and\ncoding. However, applying this idea to machine translation (MT), where outputs\nare flexibly formatted and difficult to automatically evaluate with explicit\nrules, remains underexplored. In this work, we introduce MT-R1-Zero, the first\nopen-source adaptation of the R1-Zero RL framework for MT without supervised\nfine-tuning or cold-start. We propose a rule-metric mixed reward mechanism to\nguide LLMs towards improved translation quality via emergent reasoning. On the\nWMT 24 English-Chinese benchmark, our MT-R1-Zero-3B-Mix achieves competitive\nperformance, surpassing TowerInstruct-7B-v0.2 by an average of 1.26 points.\nMeanwhile, our MT-R1-Zero-7B-Mix attains a high average score of 62.25 across\nall metrics, placing it on par with advanced proprietary models such as GPT-4o\nand Claude-3.5-Sonnet, while the MT-R1-Zero-7B-Sem variant achieves\nstate-of-the-art scores on semantic metrics. Moreover, our work exhibits strong\ngeneralization capabilities on out-of-distribution MT tasks, robustly\nsupporting multilingual and low-resource settings. Extensive analysis of model\nbehavior across different initializations and reward metrics offers pioneering\ninsight into the critical role of reward design, LLM adaptability, training\ndynamics, and emergent reasoning patterns within the R1-Zero paradigm for MT.\nOur code is available at https://github.com/fzp0424/MT-R1-Zero.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale reinforcement learning (RL) methods have proven highly effective\nin enhancing the reasoning abilities of large language models (LLMs),\nparticularly for tasks with verifiable solutions such as mathematics and\ncoding. However, applying this idea to machine translation (MT), where outputs\nare flexibly formatted and difficult to automatically evaluate with explicit\nrules, remains underexplored. In this work, we introduce MT-R1-Zero, the first\nopen-source adaptation of the R1-Zero RL framework for MT without supervised\nfine-tuning or cold-start. We propose a rule-metric mixed reward mechanism to\nguide LLMs towards improved translation quality via emergent reasoning. On the\nWMT 24 English-Chinese benchmark, our MT-R1-Zero-3B-Mix achieves competitive\nperformance, surpassing TowerInstruct-7B-v0.2 by an average of 1.26 points.\nMeanwhile, our MT-R1-Zero-7B-Mix attains a high average score of 62.25 across\nall metrics, placing it on par with advanced proprietary models such as GPT-4o\nand Claude-3.5-Sonnet, while the MT-R1-Zero-7B-Sem variant achieves\nstate-of-the-art scores on semantic metrics. Moreover, our work exhibits strong\ngeneralization capabilities on out-of-distribution MT tasks, robustly\nsupporting multilingual and low-resource settings. Extensive analysis of model\nbehavior across different initializations and reward metrics offers pioneering\ninsight into the critical role of reward design, LLM adaptability, training\ndynamics, and emergent reasoning patterns within the R1-Zero paradigm for MT.\nOur code is available at https://github.com/fzp0424/MT-R1-Zero."
                },
                "authors": [
                    {
                        "name": "Zhaopeng Feng"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Jiahan Ren"
                    },
                    {
                        "name": "Jiayuan Su"
                    },
                    {
                        "name": "Ruizhe Chen"
                    },
                    {
                        "name": "Yan Zhang"
                    },
                    {
                        "name": "Zhe Xu"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Jian Wu"
                    },
                    {
                        "name": "Zuozhu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zuozhu Liu"
                },
                "author": "Zuozhu Liu",
                "arxiv_comment": "Work in progress. Our code is available at\n  https://github.com/fzp0424/MT-R1-Zero",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10160v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10160v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10157v1",
                "updated": "2025-04-14T12:12:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    12,
                    52,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T12:12:52Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    12,
                    52,
                    0,
                    104,
                    0
                ],
                "title": "SocioVerse: A World Model for Social Simulation Powered by LLM Agents\n  and A Pool of 10 Million Real-World Users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SocioVerse: A World Model for Social Simulation Powered by LLM Agents\n  and A Pool of 10 Million Real-World Users"
                },
                "summary": "Social simulation is transforming traditional social science research by\nmodeling human behavior through interactions between virtual individuals and\ntheir environments. With recent advances in large language models (LLMs), this\napproach has shown growing potential in capturing individual differences and\npredicting group behaviors. However, existing methods face alignment challenges\nrelated to the environment, target users, interaction mechanisms, and\nbehavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven\nworld model for social simulation. Our framework features four powerful\nalignment components and a user pool of 10 million real individuals. To\nvalidate its effectiveness, we conducted large-scale simulation experiments\nacross three distinct domains: politics, news, and economics. Results\ndemonstrate that SocioVerse can reflect large-scale population dynamics while\nensuring diversity, credibility, and representativeness through standardized\nprocedures and minimal manual adjustments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social simulation is transforming traditional social science research by\nmodeling human behavior through interactions between virtual individuals and\ntheir environments. With recent advances in large language models (LLMs), this\napproach has shown growing potential in capturing individual differences and\npredicting group behaviors. However, existing methods face alignment challenges\nrelated to the environment, target users, interaction mechanisms, and\nbehavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven\nworld model for social simulation. Our framework features four powerful\nalignment components and a user pool of 10 million real individuals. To\nvalidate its effectiveness, we conducted large-scale simulation experiments\nacross three distinct domains: politics, news, and economics. Results\ndemonstrate that SocioVerse can reflect large-scale population dynamics while\nensuring diversity, credibility, and representativeness through standardized\nprocedures and minimal manual adjustments."
                },
                "authors": [
                    {
                        "name": "Xinnong Zhang"
                    },
                    {
                        "name": "Jiayu Lin"
                    },
                    {
                        "name": "Xinyi Mou"
                    },
                    {
                        "name": "Shiyue Yang"
                    },
                    {
                        "name": "Xiawei Liu"
                    },
                    {
                        "name": "Libo Sun"
                    },
                    {
                        "name": "Hanjia Lyu"
                    },
                    {
                        "name": "Yihang Yang"
                    },
                    {
                        "name": "Weihong Qi"
                    },
                    {
                        "name": "Yue Chen"
                    },
                    {
                        "name": "Guanying Li"
                    },
                    {
                        "name": "Ling Yan"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Siming Chen"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Jingxuan Huang"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Shiping Tang"
                    },
                    {
                        "name": "Libo Wu"
                    },
                    {
                        "name": "Baohua Zhou"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10150v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10150v1",
                "updated": "2025-04-14T12:01:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    1,
                    11,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T12:01:11Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    1,
                    11,
                    0,
                    104,
                    0
                ],
                "title": "HistLLM: A Unified Framework for LLM-Based Multimodal Recommendation\n  with User History Encoding and Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HistLLM: A Unified Framework for LLM-Based Multimodal Recommendation\n  with User History Encoding and Compression"
                },
                "summary": "While large language models (LLMs) have proven effective in leveraging\ntextual data for recommendations, their application to multimodal\nrecommendation tasks remains relatively underexplored. Although LLMs can\nprocess multimodal information through projection functions that map visual\nfeatures into their semantic space, recommendation tasks often require\nrepresenting users' history interactions through lengthy prompts combining text\nand visual elements, which not only hampers training and inference efficiency\nbut also makes it difficult for the model to accurately capture user\npreferences from complex and extended prompts, leading to reduced\nrecommendation performance. To address this challenge, we introduce HistLLM, an\ninnovative multimodal recommendation framework that integrates textual and\nvisual features through a User History Encoding Module (UHEM), compressing\nmultimodal user history interactions into a single token representation,\neffectively facilitating LLMs in processing user preferences. Extensive\nexperiments demonstrate the effectiveness and efficiency of our proposed\nmechanism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have proven effective in leveraging\ntextual data for recommendations, their application to multimodal\nrecommendation tasks remains relatively underexplored. Although LLMs can\nprocess multimodal information through projection functions that map visual\nfeatures into their semantic space, recommendation tasks often require\nrepresenting users' history interactions through lengthy prompts combining text\nand visual elements, which not only hampers training and inference efficiency\nbut also makes it difficult for the model to accurately capture user\npreferences from complex and extended prompts, leading to reduced\nrecommendation performance. To address this challenge, we introduce HistLLM, an\ninnovative multimodal recommendation framework that integrates textual and\nvisual features through a User History Encoding Module (UHEM), compressing\nmultimodal user history interactions into a single token representation,\neffectively facilitating LLMs in processing user preferences. Extensive\nexperiments demonstrate the effectiveness and efficiency of our proposed\nmechanism."
                },
                "authors": [
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Bo Hu"
                    },
                    {
                        "name": "Weidong Chen"
                    },
                    {
                        "name": "Zhendong Mao"
                    }
                ],
                "author_detail": {
                    "name": "Zhendong Mao"
                },
                "author": "Zhendong Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10150v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10150v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10149v1",
                "updated": "2025-04-14T12:00:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    0,
                    0,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T12:00:00Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    0,
                    0,
                    0,
                    104,
                    0
                ],
                "title": "BoTTA: Benchmarking on-device Test Time Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BoTTA: Benchmarking on-device Test Time Adaptation"
                },
                "summary": "The performance of deep learning models depends heavily on test samples at\nruntime, and shifts from the training data distribution can significantly\nreduce accuracy. Test-time adaptation (TTA) addresses this by adapting models\nduring inference without requiring labeled test data or access to the original\ntraining set. While research has explored TTA from various perspectives like\nalgorithmic complexity, data and class distribution shifts, model\narchitectures, and offline versus continuous learning, constraints specific to\nmobile and edge devices remain underexplored. We propose BoTTA, a benchmark\ndesigned to evaluate TTA methods under practical constraints on mobile and edge\ndevices. Our evaluation targets four key challenges caused by limited resources\nand usage conditions: (i) limited test samples, (ii) limited exposure to\ncategories, (iii) diverse distribution shifts, and (iv) overlapping shifts\nwithin a sample. We assess state-of-the-art TTA methods under these scenarios\nusing benchmark datasets and report system-level metrics on a real testbed.\nFurthermore, unlike prior work, we align with on-device requirements by\nadvocating periodic adaptation instead of continuous inference-time adaptation.\nExperiments reveal key insights: many recent TTA algorithms struggle with small\ndatasets, fail to generalize to unseen categories, and depend on the diversity\nand complexity of distribution shifts. BoTTA also reports device-specific\nresource use. For example, while SHOT improves accuracy by $2.25\\times$ with\n$512$ adaptation samples, it uses $1.08\\times$ peak memory on Raspberry Pi\nversus the base model. BoTTA offers actionable guidance for TTA in real-world,\nresource-constrained deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of deep learning models depends heavily on test samples at\nruntime, and shifts from the training data distribution can significantly\nreduce accuracy. Test-time adaptation (TTA) addresses this by adapting models\nduring inference without requiring labeled test data or access to the original\ntraining set. While research has explored TTA from various perspectives like\nalgorithmic complexity, data and class distribution shifts, model\narchitectures, and offline versus continuous learning, constraints specific to\nmobile and edge devices remain underexplored. We propose BoTTA, a benchmark\ndesigned to evaluate TTA methods under practical constraints on mobile and edge\ndevices. Our evaluation targets four key challenges caused by limited resources\nand usage conditions: (i) limited test samples, (ii) limited exposure to\ncategories, (iii) diverse distribution shifts, and (iv) overlapping shifts\nwithin a sample. We assess state-of-the-art TTA methods under these scenarios\nusing benchmark datasets and report system-level metrics on a real testbed.\nFurthermore, unlike prior work, we align with on-device requirements by\nadvocating periodic adaptation instead of continuous inference-time adaptation.\nExperiments reveal key insights: many recent TTA algorithms struggle with small\ndatasets, fail to generalize to unseen categories, and depend on the diversity\nand complexity of distribution shifts. BoTTA also reports device-specific\nresource use. For example, while SHOT improves accuracy by $2.25\\times$ with\n$512$ adaptation samples, it uses $1.08\\times$ peak memory on Raspberry Pi\nversus the base model. BoTTA offers actionable guidance for TTA in real-world,\nresource-constrained deployments."
                },
                "authors": [
                    {
                        "name": "Michal Danilowski"
                    },
                    {
                        "name": "Soumyajit Chatterjee"
                    },
                    {
                        "name": "Abhirup Ghosh"
                    }
                ],
                "author_detail": {
                    "name": "Abhirup Ghosh"
                },
                "author": "Abhirup Ghosh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10147v1",
                "updated": "2025-04-14T11:57:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    57,
                    52,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T11:57:52Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    57,
                    52,
                    0,
                    104,
                    0
                ],
                "title": "A Survey of Personalization: From RAG to Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Personalization: From RAG to Agent"
                },
                "summary": "Personalization has become an essential capability in modern AI systems,\nenabling customized interactions that align with individual user preferences,\ncontexts, and goals. Recent research has increasingly concentrated on\nRetrieval-Augmented Generation (RAG) frameworks and their evolution into more\nadvanced agent-based architectures within personalized settings to enhance user\nsatisfaction. Building on this foundation, this survey systematically examines\npersonalization across the three core stages of RAG: pre-retrieval, retrieval,\nand generation. Beyond RAG, we further extend its capabilities into the realm\nof Personalized LLM-based Agents, which enhance traditional RAG systems with\nagentic functionalities, including user understanding, personalized planning\nand execution, and dynamic generation. For both personalization in RAG and\nagent-based personalization, we provide formal definitions, conduct a\ncomprehensive review of recent literature, and summarize key datasets and\nevaluation metrics. Additionally, we discuss fundamental challenges,\nlimitations, and promising research directions in this evolving field. Relevant\npapers and resources are continuously updated at\nhttps://github.com/Applied-Machine-Learning-Lab/Awesome-Personalized-RAG-Agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalization has become an essential capability in modern AI systems,\nenabling customized interactions that align with individual user preferences,\ncontexts, and goals. Recent research has increasingly concentrated on\nRetrieval-Augmented Generation (RAG) frameworks and their evolution into more\nadvanced agent-based architectures within personalized settings to enhance user\nsatisfaction. Building on this foundation, this survey systematically examines\npersonalization across the three core stages of RAG: pre-retrieval, retrieval,\nand generation. Beyond RAG, we further extend its capabilities into the realm\nof Personalized LLM-based Agents, which enhance traditional RAG systems with\nagentic functionalities, including user understanding, personalized planning\nand execution, and dynamic generation. For both personalization in RAG and\nagent-based personalization, we provide formal definitions, conduct a\ncomprehensive review of recent literature, and summarize key datasets and\nevaluation metrics. Additionally, we discuss fundamental challenges,\nlimitations, and promising research directions in this evolving field. Relevant\npapers and resources are continuously updated at\nhttps://github.com/Applied-Machine-Learning-Lab/Awesome-Personalized-RAG-Agent."
                },
                "authors": [
                    {
                        "name": "Xiaopeng Li"
                    },
                    {
                        "name": "Pengyue Jia"
                    },
                    {
                        "name": "Derong Xu"
                    },
                    {
                        "name": "Yi Wen"
                    },
                    {
                        "name": "Yingyi Zhang"
                    },
                    {
                        "name": "Wenlin Zhang"
                    },
                    {
                        "name": "Wanyu Wang"
                    },
                    {
                        "name": "Yichao Wang"
                    },
                    {
                        "name": "Zhaocheng Du"
                    },
                    {
                        "name": "Xiangyang Li"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Huifeng Guo"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhao"
                },
                "author": "Xiangyu Zhao",
                "arxiv_comment": "18 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10145v1",
                "updated": "2025-04-14T11:56:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    56,
                    40,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T11:56:40Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    56,
                    40,
                    0,
                    104,
                    0
                ],
                "title": "Estimating the dense gas mass of molecular clouds using spatially\n  unresolved 3 mm line observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating the dense gas mass of molecular clouds using spatially\n  unresolved 3 mm line observations"
                },
                "summary": "We aim to develop a new method to infer the sub-beam probability density\nfunction (PDF) of H2 column densities and the dense gas mass within molecular\nclouds using spatially unresolved observations of molecular emission lines in\nthe 3 mm band. We model spatially unresolved line integrated intensity\nmeasurements as the average of an emission function weighted by the sub-beam\ncolumn density PDF. The emission function, which expresses the line integrated\nintensity as a function of the gas column density, is an empirical fit to high\nresolution (< 0.05 pc) multi-line observations of the Orion B molecular cloud.\nThe column density PDF is assumed to be parametric, composed of a lognormal\ndistribution at moderate column densities and a power law distribution at\nhigher column densities. To estimate the sub-beam column density PDF, the\nemission model is combined with a Bayesian inversion algorithm (the Beetroots\ncode), which takes account of thermal noise and calibration errors. We validate\nour method by demonstrating that it recovers the true column density PDF of the\nOrion B cloud, reproducing the observed emission line integrated intensities.\nWe apply the method to 12CO(J=1-0), 13CO(J=1-0), C18O(J=1-0), HCN(J=1-0),\nHCO+(J=1-0) and N2H+(J=1-0) observations of a 700 x 700 pc2 field of view (FoV)\nin the nearby galaxy M51. On average, the model reproduces the observed\nintensities within 30%. The column density PDFs obtained for the spiral arm\nregion within our test FoV are dominated by a power-law tail at high column\ndensities, with slopes that are consistent with gravitational collapse. Outside\nthe spiral arm, the column density PDFs are predominantly lognormal, consistent\nwith supersonic isothermal turbulence. We calculate the mass associated with\nthe powerlaw tail of the column density PDFs and observe a strong, linear\ncorrelation between this mass and the 24$\\mu$m surface brightness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We aim to develop a new method to infer the sub-beam probability density\nfunction (PDF) of H2 column densities and the dense gas mass within molecular\nclouds using spatially unresolved observations of molecular emission lines in\nthe 3 mm band. We model spatially unresolved line integrated intensity\nmeasurements as the average of an emission function weighted by the sub-beam\ncolumn density PDF. The emission function, which expresses the line integrated\nintensity as a function of the gas column density, is an empirical fit to high\nresolution (< 0.05 pc) multi-line observations of the Orion B molecular cloud.\nThe column density PDF is assumed to be parametric, composed of a lognormal\ndistribution at moderate column densities and a power law distribution at\nhigher column densities. To estimate the sub-beam column density PDF, the\nemission model is combined with a Bayesian inversion algorithm (the Beetroots\ncode), which takes account of thermal noise and calibration errors. We validate\nour method by demonstrating that it recovers the true column density PDF of the\nOrion B cloud, reproducing the observed emission line integrated intensities.\nWe apply the method to 12CO(J=1-0), 13CO(J=1-0), C18O(J=1-0), HCN(J=1-0),\nHCO+(J=1-0) and N2H+(J=1-0) observations of a 700 x 700 pc2 field of view (FoV)\nin the nearby galaxy M51. On average, the model reproduces the observed\nintensities within 30%. The column density PDFs obtained for the spiral arm\nregion within our test FoV are dominated by a power-law tail at high column\ndensities, with slopes that are consistent with gravitational collapse. Outside\nthe spiral arm, the column density PDFs are predominantly lognormal, consistent\nwith supersonic isothermal turbulence. We calculate the mass associated with\nthe powerlaw tail of the column density PDFs and observe a strong, linear\ncorrelation between this mass and the 24$\\mu$m surface brightness."
                },
                "authors": [
                    {
                        "name": "Antoine Zakardjian"
                    },
                    {
                        "name": "Annie Hughes"
                    },
                    {
                        "name": "Jrme Pety"
                    },
                    {
                        "name": "Maryvonne Gerin"
                    },
                    {
                        "name": "Pierre Palud"
                    },
                    {
                        "name": "Ivana Beslic"
                    },
                    {
                        "name": "Simon Coud"
                    },
                    {
                        "name": "Lucas Einig"
                    },
                    {
                        "name": "Helena Mazurek"
                    },
                    {
                        "name": "Jan H. Orkisz"
                    },
                    {
                        "name": "Miriam G. Santa-Maria"
                    },
                    {
                        "name": "Lontine Sgal"
                    },
                    {
                        "name": "Sophia K. Stuber"
                    },
                    {
                        "name": "Sbastien Bardeau"
                    },
                    {
                        "name": "Emeric Bron"
                    },
                    {
                        "name": "Pierre Chainais"
                    },
                    {
                        "name": "Karine Demyk"
                    },
                    {
                        "name": "Victor de Souza Magalhaes"
                    },
                    {
                        "name": "Javier R. Goicoechea"
                    },
                    {
                        "name": "Pierre Gratier"
                    },
                    {
                        "name": "Viviana V. Guzman"
                    },
                    {
                        "name": "David Languignon"
                    },
                    {
                        "name": "Franois Levrier"
                    },
                    {
                        "name": "Franck Le Petit"
                    },
                    {
                        "name": "Dariusz C. Lis"
                    },
                    {
                        "name": "Harvey S. Liszt"
                    },
                    {
                        "name": "Nicolas Peretto"
                    },
                    {
                        "name": "Antoine Roueff"
                    },
                    {
                        "name": "Evelyne Roueff"
                    },
                    {
                        "name": "Albrecht Sievers"
                    },
                    {
                        "name": "Pierre-Antoine Thouvenin"
                    }
                ],
                "author_detail": {
                    "name": "Pierre-Antoine Thouvenin"
                },
                "author": "Pierre-Antoine Thouvenin",
                "arxiv_comment": "18 pages, 16 figures, submitted to A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10136v1",
                "updated": "2025-04-14T11:47:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    47,
                    42,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T11:47:42Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    47,
                    42,
                    0,
                    104,
                    0
                ],
                "title": "Uncertainty Propagation in the Fast Fourier Transform",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty Propagation in the Fast Fourier Transform"
                },
                "summary": "We address the problem of uncertainty propagation in the discrete Fourier\ntransform by modeling the fast Fourier transform as a factor graph. Building on\nthis representation, we propose an efficient framework for approximate Bayesian\ninference using belief propagation (BP) and expectation propagation, extending\nits applicability beyond Gaussian assumptions. By leveraging an appropriate BP\nmessage representation and a suitable schedule, our method achieves stable\nconvergence with accurate mean and variance estimates. Numerical experiments in\nrepresentative scenarios from communications demonstrate the practical\npotential of the proposed framework for uncertainty-aware inference in\nprobabilistic systems operating across both time and frequency domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the problem of uncertainty propagation in the discrete Fourier\ntransform by modeling the fast Fourier transform as a factor graph. Building on\nthis representation, we propose an efficient framework for approximate Bayesian\ninference using belief propagation (BP) and expectation propagation, extending\nits applicability beyond Gaussian assumptions. By leveraging an appropriate BP\nmessage representation and a suitable schedule, our method achieves stable\nconvergence with accurate mean and variance estimates. Numerical experiments in\nrepresentative scenarios from communications demonstrate the practical\npotential of the proposed framework for uncertainty-aware inference in\nprobabilistic systems operating across both time and frequency domain."
                },
                "authors": [
                    {
                        "name": "Luca Schmid"
                    },
                    {
                        "name": "Charlotte Muth"
                    },
                    {
                        "name": "Laurent Schmalen"
                    }
                ],
                "author_detail": {
                    "name": "Laurent Schmalen"
                },
                "author": "Laurent Schmalen",
                "arxiv_comment": "Submitted to IEEE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20384v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20384v2",
                "updated": "2025-04-14T11:39:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    39,
                    39,
                    0,
                    104,
                    0
                ],
                "published": "2025-03-26T10:05:38Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    10,
                    5,
                    38,
                    2,
                    85,
                    0
                ],
                "title": "MoLe-VLA: Dynamic Layer-skipping Vision Language Action Model via\n  Mixture-of-Layers for Efficient Robot Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoLe-VLA: Dynamic Layer-skipping Vision Language Action Model via\n  Mixture-of-Layers for Efficient Robot Manipulation"
                },
                "summary": "Multimodal Large Language Models (MLLMs) excel in understanding complex\nlanguage and visual data, enabling generalist robotic systems to interpret\ninstructions and perform embodied tasks. Nevertheless, their real-world\ndeployment is hindered by substantial computational and storage demands. Recent\ninsights into the homogeneous patterns in the LLM layer have inspired\nsparsification techniques to address these challenges, such as early exit and\ntoken pruning. However, these methods often neglect the critical role of the\nfinal layers that encode the semantic information most relevant to downstream\nrobotic tasks. Aligning with the recent breakthrough of the Shallow Brain\nHypothesis (SBH) in neuroscience and the mixture of experts in model\nsparsification, we conceptualize each LLM layer as an expert and propose a\nMixture-of-Layers Vision-Language-Action model (MoLe-VLA, or simply MoLe)\narchitecture for dynamic LLM layer activation. We introduce a Spatial-Temporal\nAware Router (STAR) for MoLe to selectively activate only parts of the layers\nbased on the robot's current state, mimicking the brain's distinct signal\npathways specialized for cognition and causal reasoning. Additionally, to\ncompensate for the cognitive ability of LLMs lost in MoLe, we devise a\nCognition Self-Knowledge Distillation (CogKD) framework. CogKD enhances the\nunderstanding of task demands and improves the generation of task-relevant\naction sequences by leveraging cognitive features. Extensive experiments\nconducted in both RLBench simulation and real-world environments demonstrate\nthe superiority of MoLe-VLA in both efficiency and performance. Specifically,\nMoLe-VLA achieves an 8% improvement in the mean success rate across ten tasks\nwhile reducing computational costs by up to x5.6 compared to standard LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) excel in understanding complex\nlanguage and visual data, enabling generalist robotic systems to interpret\ninstructions and perform embodied tasks. Nevertheless, their real-world\ndeployment is hindered by substantial computational and storage demands. Recent\ninsights into the homogeneous patterns in the LLM layer have inspired\nsparsification techniques to address these challenges, such as early exit and\ntoken pruning. However, these methods often neglect the critical role of the\nfinal layers that encode the semantic information most relevant to downstream\nrobotic tasks. Aligning with the recent breakthrough of the Shallow Brain\nHypothesis (SBH) in neuroscience and the mixture of experts in model\nsparsification, we conceptualize each LLM layer as an expert and propose a\nMixture-of-Layers Vision-Language-Action model (MoLe-VLA, or simply MoLe)\narchitecture for dynamic LLM layer activation. We introduce a Spatial-Temporal\nAware Router (STAR) for MoLe to selectively activate only parts of the layers\nbased on the robot's current state, mimicking the brain's distinct signal\npathways specialized for cognition and causal reasoning. Additionally, to\ncompensate for the cognitive ability of LLMs lost in MoLe, we devise a\nCognition Self-Knowledge Distillation (CogKD) framework. CogKD enhances the\nunderstanding of task demands and improves the generation of task-relevant\naction sequences by leveraging cognitive features. Extensive experiments\nconducted in both RLBench simulation and real-world environments demonstrate\nthe superiority of MoLe-VLA in both efficiency and performance. Specifically,\nMoLe-VLA achieves an 8% improvement in the mean success rate across ten tasks\nwhile reducing computational costs by up to x5.6 compared to standard LLMs."
                },
                "authors": [
                    {
                        "name": "Rongyu Zhang"
                    },
                    {
                        "name": "Menghang Dong"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Liang Heng"
                    },
                    {
                        "name": "Xiaowei Chi"
                    },
                    {
                        "name": "Gaole Dai"
                    },
                    {
                        "name": "Li Du"
                    },
                    {
                        "name": "Yuan Du"
                    },
                    {
                        "name": "Shanghang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shanghang Zhang"
                },
                "author": "Shanghang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20384v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20384v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13116v2",
                "updated": "2025-04-14T11:28:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    28,
                    43,
                    0,
                    104,
                    0
                ],
                "published": "2025-03-17T12:38:03Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    38,
                    3,
                    0,
                    76,
                    0
                ],
                "title": "VeriLeaky: Navigating IP Protection vs Utility in Fine-Tuning for\n  LLM-Driven Verilog Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeriLeaky: Navigating IP Protection vs Utility in Fine-Tuning for\n  LLM-Driven Verilog Coding"
                },
                "summary": "Large language models (LLMs) offer significant potential for coding, yet\nfine-tuning (FT) with curated data is essential for niche languages like\nVerilog. Using proprietary intellectual property (IP) for FT presents a serious\nrisk, as FT data can be leaked through LLM inference. This leads to a critical\ndilemma for design houses: seeking to build externally accessible LLMs offering\ncompetitive Verilog coding, how can they leverage in-house IP to enhance FT\nutility while ensuring IP protection?\n  For the first time in the literature, we study this dilemma. Using LLaMA\n3.1-8B, we conduct in-house FT on a baseline Verilog dataset (RTLCoder)\nsupplemented with our own in-house IP, which is validated through multiple\ntape-outs. To rigorously assess IP leakage, we quantify structural similarity\n(AST/Dolos) and functional equivalence (Synopsys Formality) between generated\ncodes and our in-house IP. We show that our IP can indeed be leaked, confirming\nthe threat. As defense, we evaluate logic locking of Verilog codes (ASSURE).\nThis offers some level of protection, yet reduces the IP's utility for FT and\ndegrades the LLM's performance. Our study shows the need for novel strategies\nthat are both effective and minimally disruptive to FT, an essential effort for\nenabling design houses to fully utilize their proprietary IP toward LLM-driven\nVerilog coding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) offer significant potential for coding, yet\nfine-tuning (FT) with curated data is essential for niche languages like\nVerilog. Using proprietary intellectual property (IP) for FT presents a serious\nrisk, as FT data can be leaked through LLM inference. This leads to a critical\ndilemma for design houses: seeking to build externally accessible LLMs offering\ncompetitive Verilog coding, how can they leverage in-house IP to enhance FT\nutility while ensuring IP protection?\n  For the first time in the literature, we study this dilemma. Using LLaMA\n3.1-8B, we conduct in-house FT on a baseline Verilog dataset (RTLCoder)\nsupplemented with our own in-house IP, which is validated through multiple\ntape-outs. To rigorously assess IP leakage, we quantify structural similarity\n(AST/Dolos) and functional equivalence (Synopsys Formality) between generated\ncodes and our in-house IP. We show that our IP can indeed be leaked, confirming\nthe threat. As defense, we evaluate logic locking of Verilog codes (ASSURE).\nThis offers some level of protection, yet reduces the IP's utility for FT and\ndegrades the LLM's performance. Our study shows the need for novel strategies\nthat are both effective and minimally disruptive to FT, an essential effort for\nenabling design houses to fully utilize their proprietary IP toward LLM-driven\nVerilog coding."
                },
                "authors": [
                    {
                        "name": "Zeng Wang"
                    },
                    {
                        "name": "Minghao Shao"
                    },
                    {
                        "name": "Mohammed Nabeel"
                    },
                    {
                        "name": "Prithwish Basu Roy"
                    },
                    {
                        "name": "Likhitha Mankali"
                    },
                    {
                        "name": "Jitendra Bhandari"
                    },
                    {
                        "name": "Ramesh Karri"
                    },
                    {
                        "name": "Ozgur Sinanoglu"
                    },
                    {
                        "name": "Muhammad Shafique"
                    },
                    {
                        "name": "Johann Knechtel"
                    }
                ],
                "author_detail": {
                    "name": "Johann Knechtel"
                },
                "author": "Johann Knechtel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10112v1",
                "updated": "2025-04-14T11:21:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    21,
                    33,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T11:21:33Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    21,
                    33,
                    0,
                    104,
                    0
                ],
                "title": "Benchmarking Practices in LLM-driven Offensive Security: Testbeds,\n  Metrics, and Experiment Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Practices in LLM-driven Offensive Security: Testbeds,\n  Metrics, and Experiment Design"
                },
                "summary": "Large Language Models (LLMs) have emerged as a powerful approach for driving\noffensive penetration-testing tooling. This paper analyzes the methodology and\nbenchmarking practices used for evaluating Large Language Model (LLM)-driven\nattacks, focusing on offensive uses of LLMs in cybersecurity. We review 16\nresearch papers detailing 15 prototypes and their respective testbeds.\n  We detail our findings and provide actionable recommendations for future\nresearch, emphasizing the importance of extending existing testbeds, creating\nbaselines, and including comprehensive metrics and qualitative analysis. We\nalso note the distinction between security research and practice, suggesting\nthat CTF-based challenges may not fully represent real-world penetration\ntesting scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as a powerful approach for driving\noffensive penetration-testing tooling. This paper analyzes the methodology and\nbenchmarking practices used for evaluating Large Language Model (LLM)-driven\nattacks, focusing on offensive uses of LLMs in cybersecurity. We review 16\nresearch papers detailing 15 prototypes and their respective testbeds.\n  We detail our findings and provide actionable recommendations for future\nresearch, emphasizing the importance of extending existing testbeds, creating\nbaselines, and including comprehensive metrics and qualitative analysis. We\nalso note the distinction between security research and practice, suggesting\nthat CTF-based challenges may not fully represent real-world penetration\ntesting scenarios."
                },
                "authors": [
                    {
                        "name": "Andreas Happe"
                    },
                    {
                        "name": "Jrgen Cito"
                    }
                ],
                "author_detail": {
                    "name": "Jrgen Cito"
                },
                "author": "Jrgen Cito",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21491v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21491v2",
                "updated": "2025-04-14T11:19:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    19,
                    21,
                    0,
                    104,
                    0
                ],
                "published": "2024-10-28T20:02:05Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    20,
                    2,
                    5,
                    0,
                    302,
                    0
                ],
                "title": "Trustworthiness of Stochastic Gradient Descent in Distributed Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthiness of Stochastic Gradient Descent in Distributed Learning"
                },
                "summary": "Distributed learning (DL) uses multiple nodes to accelerate training,\nenabling efficient optimization of large-scale models. Stochastic Gradient\nDescent (SGD), a key optimization algorithm, plays a central role in this\nprocess. However, communication bottlenecks often limit scalability and\nefficiency, leading to increasing adoption of compressed SGD techniques to\nalleviate these challenges. Despite addressing communication overheads,\ncompressed SGD introduces trustworthiness concerns, as gradient exchanges among\nnodes are vulnerable to attacks like gradient inversion (GradInv) and\nmembership inference attacks (MIA). The trustworthiness of compressed SGD\nremains unexplored, leaving important questions about its reliability\nunanswered.\n  In this paper, we provide a trustworthiness evaluation of compressed versus\nuncompressed SGD. Specifically, we conducted empirical studies using GradInv\nattacks, revealing that compressed SGD demonstrates significantly higher\nresistance to privacy leakage compared to uncompressed SGD. In addition, our\nfindings suggest that MIA may not be a reliable metric for assessing privacy\nrisks in distributed learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed learning (DL) uses multiple nodes to accelerate training,\nenabling efficient optimization of large-scale models. Stochastic Gradient\nDescent (SGD), a key optimization algorithm, plays a central role in this\nprocess. However, communication bottlenecks often limit scalability and\nefficiency, leading to increasing adoption of compressed SGD techniques to\nalleviate these challenges. Despite addressing communication overheads,\ncompressed SGD introduces trustworthiness concerns, as gradient exchanges among\nnodes are vulnerable to attacks like gradient inversion (GradInv) and\nmembership inference attacks (MIA). The trustworthiness of compressed SGD\nremains unexplored, leaving important questions about its reliability\nunanswered.\n  In this paper, we provide a trustworthiness evaluation of compressed versus\nuncompressed SGD. Specifically, we conducted empirical studies using GradInv\nattacks, revealing that compressed SGD demonstrates significantly higher\nresistance to privacy leakage compared to uncompressed SGD. In addition, our\nfindings suggest that MIA may not be a reliable metric for assessing privacy\nrisks in distributed learning."
                },
                "authors": [
                    {
                        "name": "Hongyang Li"
                    },
                    {
                        "name": "Caesar Wu"
                    },
                    {
                        "name": "Mohammed Chadli"
                    },
                    {
                        "name": "Said Mammar"
                    },
                    {
                        "name": "Pascal Bouvry"
                    }
                ],
                "author_detail": {
                    "name": "Pascal Bouvry"
                },
                "author": "Pascal Bouvry",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21491v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21491v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10110v1",
                "updated": "2025-04-14T11:18:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    18,
                    2,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T11:18:02Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    18,
                    2,
                    0,
                    104,
                    0
                ],
                "title": "Eigengap Sparsity for Covariance Parsimony",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigengap Sparsity for Covariance Parsimony"
                },
                "summary": "Covariance estimation is a central problem in statistics. An important issue\nis that there are rarely enough samples $n$ to accurately estimate the $p (p+1)\n/ 2$ coefficients in dimension $p$. Parsimonious covariance models are\ntherefore preferred, but the discrete nature of model selection makes inference\ncomputationally challenging. In this paper, we propose a relaxation of\ncovariance parsimony termed \"eigengap sparsity\" and motivated by the good\naccuracy-parsimony tradeoff of eigenvalue-equalization in covariance matrices.\nThis new penalty can be included in a penalized-likelihood framework that we\npropose to solve with a projected gradient descent on a monotone cone. The\nalgorithm turns out to resemble an isotonic regression of mutually-attracted\nsample eigenvalues, drawing an interesting link between covariance parsimony\nand shrinkage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Covariance estimation is a central problem in statistics. An important issue\nis that there are rarely enough samples $n$ to accurately estimate the $p (p+1)\n/ 2$ coefficients in dimension $p$. Parsimonious covariance models are\ntherefore preferred, but the discrete nature of model selection makes inference\ncomputationally challenging. In this paper, we propose a relaxation of\ncovariance parsimony termed \"eigengap sparsity\" and motivated by the good\naccuracy-parsimony tradeoff of eigenvalue-equalization in covariance matrices.\nThis new penalty can be included in a penalized-likelihood framework that we\npropose to solve with a projected gradient descent on a monotone cone. The\nalgorithm turns out to resemble an isotonic regression of mutually-attracted\nsample eigenvalues, drawing an interesting link between covariance parsimony\nand shrinkage."
                },
                "authors": [
                    {
                        "name": "Tom Szwagier"
                    },
                    {
                        "name": "Guillaume Olikier"
                    },
                    {
                        "name": "Xavier Pennec"
                    }
                ],
                "author_detail": {
                    "name": "Xavier Pennec"
                },
                "author": "Xavier Pennec",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10107v1",
                "updated": "2025-04-14T11:15:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    15,
                    30,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T11:15:30Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    15,
                    30,
                    0,
                    104,
                    0
                ],
                "title": "Enhancing LLM-based Recommendation through Semantic-Aligned\n  Collaborative Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM-based Recommendation through Semantic-Aligned\n  Collaborative Knowledge"
                },
                "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities in\nleveraging comprehensive world knowledge and sophisticated reasoning mechanisms\nfor recommendation tasks. However, a notable limitation lies in their inability\nto effectively model sparse identifiers (e.g., user and item IDs), unlike\nconventional collaborative filtering models (Collabs.), thus hindering LLM to\nlearn distinctive user-item representations and creating a performance\nbottleneck. Prior studies indicate that integrating collaborative knowledge\nfrom Collabs. into LLMs can mitigate the above limitations and enhance their\nrecommendation performance. Nevertheless, the significant discrepancy in\nknowledge distribution and semantic space between LLMs and Collab. presents\nsubstantial challenges for effective knowledge transfer. To tackle these\nchallenges, we propose a novel framework, SeLLa-Rec, which focuses on achieving\nalignment between the semantic spaces of Collabs. and LLMs. This alignment\nfosters effective knowledge fusion, mitigating the influence of discriminative\nnoise and facilitating the deep integration of knowledge from diverse models.\nSpecifically, three special tokens with collaborative knowledge are embedded\ninto the LLM's semantic space through a hybrid projection layer and integrated\ninto task-specific prompts to guide the recommendation process. Experiments\nconducted on two public benchmark datasets (MovieLens-1M and Amazon Book)\ndemonstrate that SeLLa-Rec achieves state-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate remarkable capabilities in\nleveraging comprehensive world knowledge and sophisticated reasoning mechanisms\nfor recommendation tasks. However, a notable limitation lies in their inability\nto effectively model sparse identifiers (e.g., user and item IDs), unlike\nconventional collaborative filtering models (Collabs.), thus hindering LLM to\nlearn distinctive user-item representations and creating a performance\nbottleneck. Prior studies indicate that integrating collaborative knowledge\nfrom Collabs. into LLMs can mitigate the above limitations and enhance their\nrecommendation performance. Nevertheless, the significant discrepancy in\nknowledge distribution and semantic space between LLMs and Collab. presents\nsubstantial challenges for effective knowledge transfer. To tackle these\nchallenges, we propose a novel framework, SeLLa-Rec, which focuses on achieving\nalignment between the semantic spaces of Collabs. and LLMs. This alignment\nfosters effective knowledge fusion, mitigating the influence of discriminative\nnoise and facilitating the deep integration of knowledge from diverse models.\nSpecifically, three special tokens with collaborative knowledge are embedded\ninto the LLM's semantic space through a hybrid projection layer and integrated\ninto task-specific prompts to guide the recommendation process. Experiments\nconducted on two public benchmark datasets (MovieLens-1M and Amazon Book)\ndemonstrate that SeLLa-Rec achieves state-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Jinghao Lin"
                    },
                    {
                        "name": "Xiaocui Yang"
                    },
                    {
                        "name": "Yongkang Liu"
                    },
                    {
                        "name": "Shi Feng"
                    },
                    {
                        "name": "Daling Wang"
                    },
                    {
                        "name": "Yifei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yifei Zhang"
                },
                "author": "Yifei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10101v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10101v1",
                "updated": "2025-04-14T11:07:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    7,
                    11,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T11:07:11Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    7,
                    11,
                    0,
                    104,
                    0
                ],
                "title": "The Human Visual System Can Inspire New Interaction Paradigms for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Human Visual System Can Inspire New Interaction Paradigms for LLMs"
                },
                "summary": "The dominant metaphor of LLMs-as-minds leads to misleading conceptions of\nmachine agency and is limited in its ability to help both users and developers\nbuild the right degree of trust and understanding for outputs from LLMs. It\nmakes it harder to disentangle hallucinations from useful model interactions.\nThis position paper argues that there are fundamental similarities between\nvisual perception and the way LLMs process and present language. These\nsimilarities inspire a metaphor for LLMs which could open new avenues for\nresearch into interaction paradigms and shared representations. Our visual\nsystem metaphor introduces possibilities for addressing these challenges by\nunderstanding the information landscape assimilated by LLMs. In this paper we\nmotivate our proposal, introduce the interrelating theories from the fields\nthat inspired this view and discuss research directions that stem from this\nabstraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dominant metaphor of LLMs-as-minds leads to misleading conceptions of\nmachine agency and is limited in its ability to help both users and developers\nbuild the right degree of trust and understanding for outputs from LLMs. It\nmakes it harder to disentangle hallucinations from useful model interactions.\nThis position paper argues that there are fundamental similarities between\nvisual perception and the way LLMs process and present language. These\nsimilarities inspire a metaphor for LLMs which could open new avenues for\nresearch into interaction paradigms and shared representations. Our visual\nsystem metaphor introduces possibilities for addressing these challenges by\nunderstanding the information landscape assimilated by LLMs. In this paper we\nmotivate our proposal, introduce the interrelating theories from the fields\nthat inspired this view and discuss research directions that stem from this\nabstraction."
                },
                "authors": [
                    {
                        "name": "Diana Robinson"
                    },
                    {
                        "name": "Neil Lawrence"
                    }
                ],
                "author_detail": {
                    "name": "Neil Lawrence"
                },
                "author": "Neil Lawrence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10101v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10101v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08980v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08980v2",
                "updated": "2025-04-14T11:00:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    0,
                    31,
                    0,
                    104,
                    0
                ],
                "published": "2025-03-12T01:21:17Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    1,
                    21,
                    17,
                    2,
                    71,
                    0
                ],
                "title": "I Predict Therefore I Am: Is Next Token Prediction Enough to Learn\n  Human-Interpretable Concepts from Data?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Predict Therefore I Am: Is Next Token Prediction Enough to Learn\n  Human-Interpretable Concepts from Data?"
                },
                "summary": "The remarkable achievements of large language models (LLMs) have led many to\nconclude that they exhibit a form of intelligence. This is as opposed to\nexplanations of their capabilities based on their ability to perform relatively\nsimple manipulations of vast volumes of data. To illuminate the distinction\nbetween these explanations, we introduce a novel generative model that\ngenerates tokens on the basis of human interpretable concepts represented as\nlatent discrete variables. Under mild conditions, even when the mapping from\nthe latent space to the observed space is non-invertible, we establish an\nidentifiability result: the representations learned by LLMs through next-token\nprediction can be approximately modeled as the logarithm of the posterior\nprobabilities of these latent discrete concepts, up to an invertible linear\ntransformation. This theoretical finding not only provides evidence that LLMs\ncapture underlying generative factors, but also strongly reinforces the linear\nrepresentation hypothesis, which posits that LLMs learn linear representations\nof human-interpretable concepts. Empirically, we validate our theoretical\nresults through evaluations on both simulation data and the Pythia, Llama, and\nDeepSeek model families.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable achievements of large language models (LLMs) have led many to\nconclude that they exhibit a form of intelligence. This is as opposed to\nexplanations of their capabilities based on their ability to perform relatively\nsimple manipulations of vast volumes of data. To illuminate the distinction\nbetween these explanations, we introduce a novel generative model that\ngenerates tokens on the basis of human interpretable concepts represented as\nlatent discrete variables. Under mild conditions, even when the mapping from\nthe latent space to the observed space is non-invertible, we establish an\nidentifiability result: the representations learned by LLMs through next-token\nprediction can be approximately modeled as the logarithm of the posterior\nprobabilities of these latent discrete concepts, up to an invertible linear\ntransformation. This theoretical finding not only provides evidence that LLMs\ncapture underlying generative factors, but also strongly reinforces the linear\nrepresentation hypothesis, which posits that LLMs learn linear representations\nof human-interpretable concepts. Empirically, we validate our theoretical\nresults through evaluations on both simulation data and the Pythia, Llama, and\nDeepSeek model families."
                },
                "authors": [
                    {
                        "name": "Yuhang Liu"
                    },
                    {
                        "name": "Dong Gong"
                    },
                    {
                        "name": "Erdun Gao"
                    },
                    {
                        "name": "Zhen Zhang"
                    },
                    {
                        "name": "Biwei Huang"
                    },
                    {
                        "name": "Mingming Gong"
                    },
                    {
                        "name": "Anton van den Hengel"
                    },
                    {
                        "name": "Javen Qinfeng Shi"
                    }
                ],
                "author_detail": {
                    "name": "Javen Qinfeng Shi"
                },
                "author": "Javen Qinfeng Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08980v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08980v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10092v1",
                "updated": "2025-04-14T10:56:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    10,
                    56,
                    42,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T10:56:42Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    10,
                    56,
                    42,
                    0,
                    104,
                    0
                ],
                "title": "Bayesian optimal experimental design with Wasserstein information\n  criteria",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian optimal experimental design with Wasserstein information\n  criteria"
                },
                "summary": "Bayesian optimal experimental design (OED) provides a principled framework\nfor selecting the most informative observational settings in experiments. With\nrapid advances in computational power, Bayesian OED has become increasingly\nfeasible for inference problems involving large-scale simulations, attracting\ngrowing interest in fields such as inverse problems. In this paper, we\nintroduce a novel design criterion based on the expected Wasserstein-$p$\ndistance between the prior and posterior distributions. Especially, for $p=2$,\nthis criterion shares key parallels with the widely used expected information\ngain (EIG), which relies on the Kullback--Leibler divergence instead. First,\nthe Wasserstein-2 criterion admits a closed-form solution for Gaussian\nregression, a property which can be also leveraged for approximative schemes.\nSecond, it can be interpreted as maximizing the information gain measured by\nthe transport cost incurred when updating the prior to the posterior. Our main\ncontribution is a stability analysis of the Wasserstein-1 criterion, where we\nprovide a rigorous error analysis under perturbations of the prior or\nlikelihood. We partially extend this study also to the Wasserstein-2 criterion.\nIn particular, these results yield error rates when empirical approximations of\npriors are used. Finally, we demonstrate the computability of the Wasserstein-2\ncriterion and demonstrate our approximation rates through simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian optimal experimental design (OED) provides a principled framework\nfor selecting the most informative observational settings in experiments. With\nrapid advances in computational power, Bayesian OED has become increasingly\nfeasible for inference problems involving large-scale simulations, attracting\ngrowing interest in fields such as inverse problems. In this paper, we\nintroduce a novel design criterion based on the expected Wasserstein-$p$\ndistance between the prior and posterior distributions. Especially, for $p=2$,\nthis criterion shares key parallels with the widely used expected information\ngain (EIG), which relies on the Kullback--Leibler divergence instead. First,\nthe Wasserstein-2 criterion admits a closed-form solution for Gaussian\nregression, a property which can be also leveraged for approximative schemes.\nSecond, it can be interpreted as maximizing the information gain measured by\nthe transport cost incurred when updating the prior to the posterior. Our main\ncontribution is a stability analysis of the Wasserstein-1 criterion, where we\nprovide a rigorous error analysis under perturbations of the prior or\nlikelihood. We partially extend this study also to the Wasserstein-2 criterion.\nIn particular, these results yield error rates when empirical approximations of\npriors are used. Finally, we demonstrate the computability of the Wasserstein-2\ncriterion and demonstrate our approximation rates through simulations."
                },
                "authors": [
                    {
                        "name": "Tapio Helin"
                    },
                    {
                        "name": "Youssef Marzouk"
                    },
                    {
                        "name": "Jose Rodrigo Rojo-Garcia"
                    }
                ],
                "author_detail": {
                    "name": "Jose Rodrigo Rojo-Garcia"
                },
                "author": "Jose Rodrigo Rojo-Garcia",
                "arxiv_comment": "27 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.10481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10481v1",
                "updated": "2025-04-14T17:59:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    59,
                    36,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T17:59:36Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    59,
                    36,
                    0,
                    104,
                    0
                ],
                "title": "xVerify: Efficient Answer Verifier for Reasoning Model Evaluations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xVerify: Efficient Answer Verifier for Reasoning Model Evaluations"
                },
                "summary": "With the release of the o1 model by OpenAI, reasoning models adopting slow\nthinking strategies have gradually emerged. As the responses generated by such\nmodels often include complex reasoning, intermediate steps, and\nself-reflection, existing evaluation methods are often inadequate. They\nstruggle to determine whether the LLM output is truly equivalent to the\nreference answer, and also have difficulty identifying and extracting the final\nanswer from long, complex responses. To address this issue, we propose xVerify,\nan efficient answer verifier for reasoning model evaluations. xVerify\ndemonstrates strong capability in equivalence judgment, enabling it to\neffectively determine whether the answers produced by reasoning models are\nequivalent to reference answers across various types of objective questions. To\ntrain and evaluate xVerify, we construct the VAR dataset by collecting\nquestion-answer pairs generated by multiple LLMs across various datasets,\nleveraging multiple reasoning models and challenging evaluation sets designed\nspecifically for reasoning model assessment. A multi-round annotation process\nis employed to ensure label accuracy. Based on the VAR dataset, we train\nmultiple xVerify models of different scales. In evaluation experiments\nconducted on both the test set and generalization set, all xVerify models\nachieve overall F1 scores and accuracy exceeding 95\\%. Notably, the smallest\nvariant, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o,\nwhile xVerify-3B-Ib surpasses GPT-4o in overall performance. These results\nvalidate the effectiveness and generalizability of xVerify.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the release of the o1 model by OpenAI, reasoning models adopting slow\nthinking strategies have gradually emerged. As the responses generated by such\nmodels often include complex reasoning, intermediate steps, and\nself-reflection, existing evaluation methods are often inadequate. They\nstruggle to determine whether the LLM output is truly equivalent to the\nreference answer, and also have difficulty identifying and extracting the final\nanswer from long, complex responses. To address this issue, we propose xVerify,\nan efficient answer verifier for reasoning model evaluations. xVerify\ndemonstrates strong capability in equivalence judgment, enabling it to\neffectively determine whether the answers produced by reasoning models are\nequivalent to reference answers across various types of objective questions. To\ntrain and evaluate xVerify, we construct the VAR dataset by collecting\nquestion-answer pairs generated by multiple LLMs across various datasets,\nleveraging multiple reasoning models and challenging evaluation sets designed\nspecifically for reasoning model assessment. A multi-round annotation process\nis employed to ensure label accuracy. Based on the VAR dataset, we train\nmultiple xVerify models of different scales. In evaluation experiments\nconducted on both the test set and generalization set, all xVerify models\nachieve overall F1 scores and accuracy exceeding 95\\%. Notably, the smallest\nvariant, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o,\nwhile xVerify-3B-Ib surpasses GPT-4o in overall performance. These results\nvalidate the effectiveness and generalizability of xVerify."
                },
                "authors": [
                    {
                        "name": "Ding Chen"
                    },
                    {
                        "name": "Qingchen Yu"
                    },
                    {
                        "name": "Pengyuan Wang"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Xinchi Li"
                    },
                    {
                        "name": "Minchuan Yang"
                    },
                    {
                        "name": "Zhiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Li"
                },
                "author": "Zhiyu Li",
                "arxiv_comment": "32 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10479v1",
                "updated": "2025-04-14T17:59:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    59,
                    25,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T17:59:25Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    59,
                    25,
                    0,
                    104,
                    0
                ],
                "title": "InternVL3: Exploring Advanced Training and Test-Time Recipes for\n  Open-Source Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InternVL3: Exploring Advanced Training and Test-Time Recipes for\n  Open-Source Multimodal Models"
                },
                "summary": "We introduce InternVL3, a significant advancement in the InternVL series\nfeaturing a native multimodal pre-training paradigm. Rather than adapting a\ntext-only large language model (LLM) into a multimodal large language model\n(MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and\nlinguistic capabilities from both diverse multimodal data and pure-text corpora\nduring a single pre-training stage. This unified training paradigm effectively\naddresses the complexities and alignment challenges commonly encountered in\nconventional post-hoc training pipelines for MLLMs. To further improve\nperformance and scalability, InternVL3 incorporates variable visual position\nencoding (V2PE) to support extended multimodal contexts, employs advanced\npost-training techniques such as supervised fine-tuning (SFT) and mixed\npreference optimization (MPO), and adopts test-time scaling strategies\nalongside an optimized training infrastructure. Extensive empirical evaluations\ndemonstrate that InternVL3 delivers superior performance across a wide range of\nmulti-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the\nMMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its\ncapabilities remain highly competitive with leading proprietary models,\nincluding ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also\nmaintaining strong pure-language proficiency. In pursuit of open-science\nprinciples, we will publicly release both the training data and model weights\nto foster further research and development in next-generation MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce InternVL3, a significant advancement in the InternVL series\nfeaturing a native multimodal pre-training paradigm. Rather than adapting a\ntext-only large language model (LLM) into a multimodal large language model\n(MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and\nlinguistic capabilities from both diverse multimodal data and pure-text corpora\nduring a single pre-training stage. This unified training paradigm effectively\naddresses the complexities and alignment challenges commonly encountered in\nconventional post-hoc training pipelines for MLLMs. To further improve\nperformance and scalability, InternVL3 incorporates variable visual position\nencoding (V2PE) to support extended multimodal contexts, employs advanced\npost-training techniques such as supervised fine-tuning (SFT) and mixed\npreference optimization (MPO), and adopts test-time scaling strategies\nalongside an optimized training infrastructure. Extensive empirical evaluations\ndemonstrate that InternVL3 delivers superior performance across a wide range of\nmulti-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the\nMMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its\ncapabilities remain highly competitive with leading proprietary models,\nincluding ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also\nmaintaining strong pure-language proficiency. In pursuit of open-science\nprinciples, we will publicly release both the training data and model weights\nto foster further research and development in next-generation MLLMs."
                },
                "authors": [
                    {
                        "name": "Jinguo Zhu"
                    },
                    {
                        "name": "Weiyun Wang"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Zhaoyang Liu"
                    },
                    {
                        "name": "Shenglong Ye"
                    },
                    {
                        "name": "Lixin Gu"
                    },
                    {
                        "name": "Yuchen Duan"
                    },
                    {
                        "name": "Hao Tian"
                    },
                    {
                        "name": "Weijie Su"
                    },
                    {
                        "name": "Jie Shao"
                    },
                    {
                        "name": "Zhangwei Gao"
                    },
                    {
                        "name": "Erfei Cui"
                    },
                    {
                        "name": "Yue Cao"
                    },
                    {
                        "name": "Yangzhou Liu"
                    },
                    {
                        "name": "Weiye Xu"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Han Lv"
                    },
                    {
                        "name": "Dengnian Chen"
                    },
                    {
                        "name": "Songze Li"
                    },
                    {
                        "name": "Yinan He"
                    },
                    {
                        "name": "Tan Jiang"
                    },
                    {
                        "name": "Jiapeng Luo"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Botian Shi"
                    },
                    {
                        "name": "Xingcheng Zhang"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Yingtong Xiong"
                    },
                    {
                        "name": "Wenwen Qu"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Penglong Jiao"
                    },
                    {
                        "name": "Lijun Wu"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Huipeng Deng"
                    },
                    {
                        "name": "Jiaye Ge"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Limin Wang"
                    },
                    {
                        "name": "Min Dou"
                    },
                    {
                        "name": "Lewei Lu"
                    },
                    {
                        "name": "Xizhou Zhu"
                    },
                    {
                        "name": "Tong Lu"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Jifeng Dai"
                    },
                    {
                        "name": "Wenhai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhai Wang"
                },
                "author": "Wenhai Wang",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07288v2",
                "updated": "2025-04-14T17:48:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    48,
                    8,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-09T21:28:17Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    21,
                    28,
                    17,
                    2,
                    99,
                    0
                ],
                "title": "MDIT: A Model-free Data Interpolation Method for Diverse Instruction\n  Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MDIT: A Model-free Data Interpolation Method for Diverse Instruction\n  Tuning"
                },
                "summary": "As Large Language Models (LLMs) are increasingly applied across various\ntasks, instruction tuning has emerged as a critical method for enhancing model\nperformance. However, current data management strategies face substantial\nchallenges in generating diverse and comprehensive data, restricting further\nimprovements in model performance. To address this gap, we propose MDIT, a\nnovel model-free data interpolation method for diverse instruction tuning,\nwhich generates varied and high-quality instruction data by performing task\ninterpolation. Moreover, it contains diversity-based clustering strategies to\nensure the diversity of the training data. Extensive experiments show that our\nmethod achieves superior performance in multiple benchmark tasks. The LLMs\nfinetuned with MDIT show significant improvements in numerous tasks such as\ngeneral question answering, math reasoning, and code generation. MDIT offers an\nefficient and automatic data synthetic method, generating diverse instruction\ndata without depending on external resources while expanding the application\npotential of LLMs in complex environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) are increasingly applied across various\ntasks, instruction tuning has emerged as a critical method for enhancing model\nperformance. However, current data management strategies face substantial\nchallenges in generating diverse and comprehensive data, restricting further\nimprovements in model performance. To address this gap, we propose MDIT, a\nnovel model-free data interpolation method for diverse instruction tuning,\nwhich generates varied and high-quality instruction data by performing task\ninterpolation. Moreover, it contains diversity-based clustering strategies to\nensure the diversity of the training data. Extensive experiments show that our\nmethod achieves superior performance in multiple benchmark tasks. The LLMs\nfinetuned with MDIT show significant improvements in numerous tasks such as\ngeneral question answering, math reasoning, and code generation. MDIT offers an\nefficient and automatic data synthetic method, generating diverse instruction\ndata without depending on external resources while expanding the application\npotential of LLMs in complex environments."
                },
                "authors": [
                    {
                        "name": "Yangning Li"
                    },
                    {
                        "name": "Zihua Lan"
                    },
                    {
                        "name": "Lv Qingsong"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Hai-Tao Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Hai-Tao Zheng"
                },
                "author": "Hai-Tao Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15583v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15583v2",
                "updated": "2025-04-14T17:42:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    42,
                    35,
                    0,
                    104,
                    0
                ],
                "published": "2024-06-21T18:31:49Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    18,
                    31,
                    49,
                    4,
                    173,
                    0
                ],
                "title": "Detecting AI-Generated Text: Factors Influencing Detectability with\n  Current Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting AI-Generated Text: Factors Influencing Detectability with\n  Current Methods"
                },
                "summary": "Large language models (LLMs) have advanced to a point that even humans have\ndifficulty discerning whether a text was generated by another human, or by a\ncomputer. However, knowing whether a text was produced by human or artificial\nintelligence (AI) is important to determining its trustworthiness, and has\napplications in many domains including detecting fraud and academic dishonesty,\nas well as combating the spread of misinformation and political propaganda. The\ntask of AI-generated text (AIGT) detection is therefore both very challenging,\nand highly critical. In this survey, we summarize state-of-the art approaches\nto AIGT detection, including watermarking, statistical and stylistic analysis,\nand machine learning classification. We also provide information about existing\ndatasets for this task. Synthesizing the research findings, we aim to provide\ninsight into the salient factors that combine to determine how \"detectable\"\nAIGT text is under different scenarios, and to make practical recommendations\nfor future work towards this significant technical and societal challenge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have advanced to a point that even humans have\ndifficulty discerning whether a text was generated by another human, or by a\ncomputer. However, knowing whether a text was produced by human or artificial\nintelligence (AI) is important to determining its trustworthiness, and has\napplications in many domains including detecting fraud and academic dishonesty,\nas well as combating the spread of misinformation and political propaganda. The\ntask of AI-generated text (AIGT) detection is therefore both very challenging,\nand highly critical. In this survey, we summarize state-of-the art approaches\nto AIGT detection, including watermarking, statistical and stylistic analysis,\nand machine learning classification. We also provide information about existing\ndatasets for this task. Synthesizing the research findings, we aim to provide\ninsight into the salient factors that combine to determine how \"detectable\"\nAIGT text is under different scenarios, and to make practical recommendations\nfor future work towards this significant technical and societal challenge."
                },
                "authors": [
                    {
                        "name": "Kathleen C. Fraser"
                    },
                    {
                        "name": "Hillary Dawkins"
                    },
                    {
                        "name": "Svetlana Kiritchenko"
                    }
                ],
                "author_detail": {
                    "name": "Svetlana Kiritchenko"
                },
                "author": "Svetlana Kiritchenko",
                "arxiv_doi": "10.1613/jair.1.16665",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1613/jair.1.16665",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.15583v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15583v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Journal of Artificial Intelligence Research Vol. 82 (2025)\n  2233-2278",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10449v1",
                "updated": "2025-04-14T17:38:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    38,
                    25,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T17:38:25Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    38,
                    25,
                    0,
                    104,
                    0
                ],
                "title": "M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models"
                },
                "summary": "Effective reasoning is crucial to solving complex mathematical problems.\nRecent large language models (LLMs) have boosted performance by scaling\ntest-time computation through long chain-of-thought reasoning. However,\ntransformer-based models are inherently limited in extending context length due\nto their quadratic computational complexity and linear memory requirements. In\nthis paper, we introduce a novel hybrid linear RNN reasoning model, M1, built\non the Mamba architecture, which allows memory-efficient inference. Our\napproach leverages a distillation process from existing reasoning models and is\nfurther enhanced through RL training. Experimental results on the AIME and MATH\nbenchmarks show that M1 not only outperforms previous linear RNN models but\nalso matches the performance of state-of-the-art Deepseek R1 distilled\nreasoning models at a similar scale. We also compare our generation speed with\na highly performant general purpose inference engine, vLLM, and observe more\nthan a 3x speedup compared to a same size transformer. With throughput speedup,\nwe are able to achieve higher accuracy compared to DeepSeek R1 distilled\ntransformer reasoning models under a fixed generation time budget using\nself-consistency voting. Overall, we introduce a hybrid Mamba reasoning model\nand provide a more effective approach to scaling test-time generation using\nself-consistency or long chain of thought reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective reasoning is crucial to solving complex mathematical problems.\nRecent large language models (LLMs) have boosted performance by scaling\ntest-time computation through long chain-of-thought reasoning. However,\ntransformer-based models are inherently limited in extending context length due\nto their quadratic computational complexity and linear memory requirements. In\nthis paper, we introduce a novel hybrid linear RNN reasoning model, M1, built\non the Mamba architecture, which allows memory-efficient inference. Our\napproach leverages a distillation process from existing reasoning models and is\nfurther enhanced through RL training. Experimental results on the AIME and MATH\nbenchmarks show that M1 not only outperforms previous linear RNN models but\nalso matches the performance of state-of-the-art Deepseek R1 distilled\nreasoning models at a similar scale. We also compare our generation speed with\na highly performant general purpose inference engine, vLLM, and observe more\nthan a 3x speedup compared to a same size transformer. With throughput speedup,\nwe are able to achieve higher accuracy compared to DeepSeek R1 distilled\ntransformer reasoning models under a fixed generation time budget using\nself-consistency voting. Overall, we introduce a hybrid Mamba reasoning model\nand provide a more effective approach to scaling test-time generation using\nself-consistency or long chain of thought reasoning."
                },
                "authors": [
                    {
                        "name": "Junxiong Wang"
                    },
                    {
                        "name": "Wen-Ding Li"
                    },
                    {
                        "name": "Daniele Paliotta"
                    },
                    {
                        "name": "Daniel Ritter"
                    },
                    {
                        "name": "Alexander M. Rush"
                    },
                    {
                        "name": "Tri Dao"
                    }
                ],
                "author_detail": {
                    "name": "Tri Dao"
                },
                "author": "Tri Dao",
                "arxiv_comment": "Code is available https://github.com/jxiw/M1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17391v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17391v2",
                "updated": "2025-04-14T17:34:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    34,
                    19,
                    0,
                    104,
                    0
                ],
                "published": "2025-01-29T02:52:32Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    2,
                    52,
                    32,
                    2,
                    29,
                    0
                ],
                "title": "Learning Free Token Reduction for Multi-Modal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Free Token Reduction for Multi-Modal Large Language Models"
                },
                "summary": "Vision-Language Models (VLMs) have achieved remarkable success across a range\nof multimodal tasks; however, their practical deployment is often constrained\nby high computational costs and prolonged inference times. Since the vision\nmodality typically carries more information than the text modality, compressing\nvisual prompts offers a promising solution to alleviate these challenges.\nExisting approaches predominantly focus on refining model architectures or\ndirectly reducing the number of visual tokens. However, these methods often\ncompromise inference performance due to a lack of consideration for the unique\nspatial and temporal characteristics of visual data. In this work, we propose a\ntoken compression paradigm that operates on both spatial and temporal\ndimensions. Our approach includes a learning-free, plug-and-play compression\npipeline that can be seamlessly integrated into most Multimodal Large Language\nModel (MLLM) frameworks. By leveraging this method, we enhance the model\ninference capability while simultaneously reducing its computational cost.\nExperimental results on the Video-QA task demonstrate the effectiveness of the\nproposed approach, showcasing significant improvements in efficiency without\nsacrificing performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have achieved remarkable success across a range\nof multimodal tasks; however, their practical deployment is often constrained\nby high computational costs and prolonged inference times. Since the vision\nmodality typically carries more information than the text modality, compressing\nvisual prompts offers a promising solution to alleviate these challenges.\nExisting approaches predominantly focus on refining model architectures or\ndirectly reducing the number of visual tokens. However, these methods often\ncompromise inference performance due to a lack of consideration for the unique\nspatial and temporal characteristics of visual data. In this work, we propose a\ntoken compression paradigm that operates on both spatial and temporal\ndimensions. Our approach includes a learning-free, plug-and-play compression\npipeline that can be seamlessly integrated into most Multimodal Large Language\nModel (MLLM) frameworks. By leveraging this method, we enhance the model\ninference capability while simultaneously reducing its computational cost.\nExperimental results on the Video-QA task demonstrate the effectiveness of the\nproposed approach, showcasing significant improvements in efficiency without\nsacrificing performance."
                },
                "authors": [
                    {
                        "name": "Zihui Zhao"
                    },
                    {
                        "name": "Yingxin Li"
                    },
                    {
                        "name": "Yang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yang Li"
                },
                "author": "Yang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17391v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17391v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10443v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10443v1",
                "updated": "2025-04-14T17:34:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    34,
                    6,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T17:34:06Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    34,
                    6,
                    0,
                    104,
                    0
                ],
                "title": "Multimodal Long Video Modeling Based on Temporal Dynamic Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Long Video Modeling Based on Temporal Dynamic Context"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have led to significant\nbreakthroughs in video understanding. However, existing models still struggle\nwith long video processing due to the context length constraint of LLMs and the\nvast amount of information within the video. Although some recent methods are\ndesigned for long video understanding, they often lose crucial information\nduring token compression and struggle with additional modality like audio. In\nthis work, we propose a dynamic long video encoding method utilizing the\ntemporal relationship between frames, named Temporal Dynamic Context (TDC).\nFirstly, we segment the video into semantically consistent scenes based on\ninter-frame similarities, then encode each frame into tokens using visual-audio\nencoders. Secondly, we propose a novel temporal context compressor to reduce\nthe number of tokens within each segment. Specifically, we employ a query-based\nTransformer to aggregate video, audio, and instruction text tokens into a\nlimited set of temporal context tokens. Finally, we feed the static frame\ntokens and the temporal context tokens into the LLM for video understanding.\nFurthermore, to handle extremely long videos, we propose a training-free\nchain-of-thought strategy that progressively extracts answers from multiple\nvideo segments. These intermediate answers serve as part of the reasoning\nprocess and contribute to the final answer. We conduct extensive experiments on\ngeneral video understanding and audio-video understanding benchmarks, where our\nmethod demonstrates strong performance. The code and models are available at\nhttps://github.com/Hoar012/TDC-Video.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have led to significant\nbreakthroughs in video understanding. However, existing models still struggle\nwith long video processing due to the context length constraint of LLMs and the\nvast amount of information within the video. Although some recent methods are\ndesigned for long video understanding, they often lose crucial information\nduring token compression and struggle with additional modality like audio. In\nthis work, we propose a dynamic long video encoding method utilizing the\ntemporal relationship between frames, named Temporal Dynamic Context (TDC).\nFirstly, we segment the video into semantically consistent scenes based on\ninter-frame similarities, then encode each frame into tokens using visual-audio\nencoders. Secondly, we propose a novel temporal context compressor to reduce\nthe number of tokens within each segment. Specifically, we employ a query-based\nTransformer to aggregate video, audio, and instruction text tokens into a\nlimited set of temporal context tokens. Finally, we feed the static frame\ntokens and the temporal context tokens into the LLM for video understanding.\nFurthermore, to handle extremely long videos, we propose a training-free\nchain-of-thought strategy that progressively extracts answers from multiple\nvideo segments. These intermediate answers serve as part of the reasoning\nprocess and contribute to the final answer. We conduct extensive experiments on\ngeneral video understanding and audio-video understanding benchmarks, where our\nmethod demonstrates strong performance. The code and models are available at\nhttps://github.com/Hoar012/TDC-Video."
                },
                "authors": [
                    {
                        "name": "Haoran Hao"
                    },
                    {
                        "name": "Jiaming Han"
                    },
                    {
                        "name": "Yiyuan Zhang"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10443v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08727v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08727v2",
                "updated": "2025-04-14T17:30:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    30,
                    56,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-11T17:55:45Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    55,
                    45,
                    4,
                    101,
                    0
                ],
                "title": "Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections\n  of Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections\n  of Images"
                },
                "summary": "We present a system using Multimodal LLMs (MLLMs) to analyze a large database\nwith tens of millions of images captured at different times, with the aim of\ndiscovering patterns in temporal changes. Specifically, we aim to capture\nfrequent co-occurring changes (\"trends\") across a city over a certain period.\nUnlike previous visual analyses, our analysis answers open-ended queries (e.g.,\n\"what are the frequent types of changes in the city?\") without any\npredetermined target subjects or training labels. These properties cast prior\nlearning-based or unsupervised visual analysis tools unsuitable. We identify\nMLLMs as a novel tool for their open-ended semantic understanding capabilities.\nYet, our datasets are four orders of magnitude too large for an MLLM to ingest\nas context. So we introduce a bottom-up procedure that decomposes the massive\nvisual analysis problem into more tractable sub-problems. We carefully design\nMLLM-based solutions to each sub-problem. During experiments and ablation\nstudies with our system, we find it significantly outperforms baselines and is\nable to discover interesting trends from images captured in large cities (e.g.,\n\"addition of outdoor dining,\", \"overpass was painted blue,\" etc.). See more\nresults and interactive demos at https://boyangdeng.com/visual-chronicles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a system using Multimodal LLMs (MLLMs) to analyze a large database\nwith tens of millions of images captured at different times, with the aim of\ndiscovering patterns in temporal changes. Specifically, we aim to capture\nfrequent co-occurring changes (\"trends\") across a city over a certain period.\nUnlike previous visual analyses, our analysis answers open-ended queries (e.g.,\n\"what are the frequent types of changes in the city?\") without any\npredetermined target subjects or training labels. These properties cast prior\nlearning-based or unsupervised visual analysis tools unsuitable. We identify\nMLLMs as a novel tool for their open-ended semantic understanding capabilities.\nYet, our datasets are four orders of magnitude too large for an MLLM to ingest\nas context. So we introduce a bottom-up procedure that decomposes the massive\nvisual analysis problem into more tractable sub-problems. We carefully design\nMLLM-based solutions to each sub-problem. During experiments and ablation\nstudies with our system, we find it significantly outperforms baselines and is\nable to discover interesting trends from images captured in large cities (e.g.,\n\"addition of outdoor dining,\", \"overpass was painted blue,\" etc.). See more\nresults and interactive demos at https://boyangdeng.com/visual-chronicles."
                },
                "authors": [
                    {
                        "name": "Boyang Deng"
                    },
                    {
                        "name": "Songyou Peng"
                    },
                    {
                        "name": "Kyle Genova"
                    },
                    {
                        "name": "Gordon Wetzstein"
                    },
                    {
                        "name": "Noah Snavely"
                    },
                    {
                        "name": "Leonidas Guibas"
                    },
                    {
                        "name": "Thomas Funkhouser"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Funkhouser"
                },
                "author": "Thomas Funkhouser",
                "arxiv_comment": "Project page: https://boyangdeng.com/visual-chronicles , second and\n  third listed authors have equal contributions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08727v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08727v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10430v1",
                "updated": "2025-04-14T17:20:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    20,
                    34,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T17:20:34Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    20,
                    34,
                    0,
                    104,
                    0
                ],
                "title": "LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety\n  in Large Language Models"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have enabled them to\napproach human-level persuasion capabilities. However, such potential also\nraises concerns about the safety risks of LLM-driven persuasion, particularly\ntheir potential for unethical influence through manipulation, deception,\nexploitation of vulnerabilities, and many other harmful tactics. In this work,\nwe present a systematic investigation of LLM persuasion safety through two\ncritical aspects: (1) whether LLMs appropriately reject unethical persuasion\ntasks and avoid unethical strategies during execution, including cases where\nthe initial persuasion goal appears ethically neutral, and (2) how influencing\nfactors like personality traits and external pressures affect their behavior.\nTo this end, we introduce PersuSafety, the first comprehensive framework for\nthe assessment of persuasion safety which consists of three stages, i.e.,\npersuasion scene creation, persuasive conversation simulation, and persuasion\nsafety assessment. PersuSafety covers 6 diverse unethical persuasion topics and\n15 common unethical strategies. Through extensive experiments across 8 widely\nused LLMs, we observe significant safety concerns in most LLMs, including\nfailing to identify harmful persuasion tasks and leveraging various unethical\npersuasion strategies. Our study calls for more attention to improve safety\nalignment in progressive and goal-driven conversations such as persuasion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have enabled them to\napproach human-level persuasion capabilities. However, such potential also\nraises concerns about the safety risks of LLM-driven persuasion, particularly\ntheir potential for unethical influence through manipulation, deception,\nexploitation of vulnerabilities, and many other harmful tactics. In this work,\nwe present a systematic investigation of LLM persuasion safety through two\ncritical aspects: (1) whether LLMs appropriately reject unethical persuasion\ntasks and avoid unethical strategies during execution, including cases where\nthe initial persuasion goal appears ethically neutral, and (2) how influencing\nfactors like personality traits and external pressures affect their behavior.\nTo this end, we introduce PersuSafety, the first comprehensive framework for\nthe assessment of persuasion safety which consists of three stages, i.e.,\npersuasion scene creation, persuasive conversation simulation, and persuasion\nsafety assessment. PersuSafety covers 6 diverse unethical persuasion topics and\n15 common unethical strategies. Through extensive experiments across 8 widely\nused LLMs, we observe significant safety concerns in most LLMs, including\nfailing to identify harmful persuasion tasks and leveraging various unethical\npersuasion strategies. Our study calls for more attention to improve safety\nalignment in progressive and goal-driven conversations such as persuasion."
                },
                "authors": [
                    {
                        "name": "Minqian Liu"
                    },
                    {
                        "name": "Zhiyang Xu"
                    },
                    {
                        "name": "Xinyi Zhang"
                    },
                    {
                        "name": "Heajun An"
                    },
                    {
                        "name": "Sarvech Qadir"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Pamela J. Wisniewski"
                    },
                    {
                        "name": "Jin-Hee Cho"
                    },
                    {
                        "name": "Sang Won Lee"
                    },
                    {
                        "name": "Ruoxi Jia"
                    },
                    {
                        "name": "Lifu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Lifu Huang"
                },
                "author": "Lifu Huang",
                "arxiv_comment": "20 pages, 7 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10421v1",
                "updated": "2025-04-14T17:08:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    8,
                    20,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T17:08:20Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    8,
                    20,
                    0,
                    104,
                    0
                ],
                "title": "Can We Edit LLMs for Long-Tail Biomedical Knowledge?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can We Edit LLMs for Long-Tail Biomedical Knowledge?"
                },
                "summary": "Knowledge editing has emerged as an effective approach for updating large\nlanguage models (LLMs) by modifying their internal knowledge. However, their\napplication to the biomedical domain faces unique challenges due to the\nlong-tailed distribution of biomedical knowledge, where rare and infrequent\ninformation is prevalent. In this paper, we conduct the first comprehensive\nstudy to investigate the effectiveness of knowledge editing methods for editing\nlong-tail biomedical knowledge. Our results indicate that, while existing\nediting methods can enhance LLMs' performance on long-tail biomedical\nknowledge, their performance on long-tail knowledge remains inferior to that on\nhigh-frequency popular knowledge, even after editing. Our further analysis\nreveals that long-tail biomedical knowledge contains a significant amount of\none-to-many knowledge, where one subject and relation link to multiple objects.\nThis high prevalence of one-to-many knowledge limits the effectiveness of\nknowledge editing in improving LLMs' understanding of long-tail biomedical\nknowledge, highlighting the need for tailored strategies to bridge this\nperformance gap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge editing has emerged as an effective approach for updating large\nlanguage models (LLMs) by modifying their internal knowledge. However, their\napplication to the biomedical domain faces unique challenges due to the\nlong-tailed distribution of biomedical knowledge, where rare and infrequent\ninformation is prevalent. In this paper, we conduct the first comprehensive\nstudy to investigate the effectiveness of knowledge editing methods for editing\nlong-tail biomedical knowledge. Our results indicate that, while existing\nediting methods can enhance LLMs' performance on long-tail biomedical\nknowledge, their performance on long-tail knowledge remains inferior to that on\nhigh-frequency popular knowledge, even after editing. Our further analysis\nreveals that long-tail biomedical knowledge contains a significant amount of\none-to-many knowledge, where one subject and relation link to multiple objects.\nThis high prevalence of one-to-many knowledge limits the effectiveness of\nknowledge editing in improving LLMs' understanding of long-tail biomedical\nknowledge, highlighting the need for tailored strategies to bridge this\nperformance gap."
                },
                "authors": [
                    {
                        "name": "Xinhao Yi"
                    },
                    {
                        "name": "Jake Lever"
                    },
                    {
                        "name": "Kevin Bryson"
                    },
                    {
                        "name": "Zaiqiao Meng"
                    }
                ],
                "author_detail": {
                    "name": "Zaiqiao Meng"
                },
                "author": "Zaiqiao Meng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10418v1",
                "updated": "2025-04-14T17:06:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    6,
                    47,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T17:06:47Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    6,
                    47,
                    0,
                    104,
                    0
                ],
                "title": "CliniChat: A Multi-Source Knowledge-Driven Framework for Clinical\n  Interview Dialogue Reconstruction and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CliniChat: A Multi-Source Knowledge-Driven Framework for Clinical\n  Interview Dialogue Reconstruction and Evaluation"
                },
                "summary": "Large language models (LLMs) hold great promise for assisting clinical\ninterviews due to their fluent interactive capabilities and extensive medical\nknowledge. However, the lack of high-quality interview dialogue data and widely\naccepted evaluation methods has significantly impeded this process. So we\npropose CliniChat, a framework that integrates multi-source knowledge to enable\nLLMs to simulate real-world clinical interviews. It consists of two modules:\nClini-Recon and Clini-Eval, each responsible for reconstructing and evaluating\ninterview dialogues, respectively. By incorporating three sources of knowledge,\nClini-Recon transforms clinical notes into systematic, professional, and\nempathetic interview dialogues. Clini-Eval combines a comprehensive evaluation\nmetric system with a two-phase automatic evaluation approach, enabling LLMs to\nassess interview performance like experts. We contribute MedQA-Dialog, a\nhigh-quality synthetic interview dialogue dataset, and CliniChatGLM, a model\nspecialized for clinical interviews. Experimental results demonstrate that\nCliniChatGLM's interview capabilities undergo a comprehensive upgrade,\nparticularly in history-taking, achieving state-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) hold great promise for assisting clinical\ninterviews due to their fluent interactive capabilities and extensive medical\nknowledge. However, the lack of high-quality interview dialogue data and widely\naccepted evaluation methods has significantly impeded this process. So we\npropose CliniChat, a framework that integrates multi-source knowledge to enable\nLLMs to simulate real-world clinical interviews. It consists of two modules:\nClini-Recon and Clini-Eval, each responsible for reconstructing and evaluating\ninterview dialogues, respectively. By incorporating three sources of knowledge,\nClini-Recon transforms clinical notes into systematic, professional, and\nempathetic interview dialogues. Clini-Eval combines a comprehensive evaluation\nmetric system with a two-phase automatic evaluation approach, enabling LLMs to\nassess interview performance like experts. We contribute MedQA-Dialog, a\nhigh-quality synthetic interview dialogue dataset, and CliniChatGLM, a model\nspecialized for clinical interviews. Experimental results demonstrate that\nCliniChatGLM's interview capabilities undergo a comprehensive upgrade,\nparticularly in history-taking, achieving state-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Jing Chen"
                    },
                    {
                        "name": "Zhihua Wei"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Yingying Hu"
                    },
                    {
                        "name": "Qiong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qiong Zhang"
                },
                "author": "Qiong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10415v1",
                "updated": "2025-04-14T17:00:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    0,
                    13,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T17:00:13Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    0,
                    13,
                    0,
                    104,
                    0
                ],
                "title": "LLM-SRBench: A New Benchmark for Scientific Equation Discovery with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-SRBench: A New Benchmark for Scientific Equation Discovery with\n  Large Language Models"
                },
                "summary": "Scientific equation discovery is a fundamental task in the history of\nscientific progress, enabling the derivation of laws governing natural\nphenomena. Recently, Large Language Models (LLMs) have gained interest for this\ntask due to their potential to leverage embedded scientific knowledge for\nhypothesis generation. However, evaluating the true discovery capabilities of\nthese methods remains challenging, as existing benchmarks often rely on common\nequations that are susceptible to memorization by LLMs, leading to inflated\nperformance metrics that do not reflect discovery. In this paper, we introduce\nLLM-SRBench, a comprehensive benchmark with 239 challenging problems across\nfour scientific domains specifically designed to evaluate LLM-based scientific\nequation discovery methods while preventing trivial memorization. Our benchmark\ncomprises two main categories: LSR-Transform, which transforms common physical\nmodels into less common mathematical representations to test reasoning beyond\nmemorized forms, and LSR-Synth, which introduces synthetic, discovery-driven\nproblems requiring data-driven reasoning. Through extensive evaluation of\nseveral state-of-the-art methods, using both open and closed LLMs, we find that\nthe best-performing system so far achieves only 31.5% symbolic accuracy. These\nfindings highlight the challenges of scientific equation discovery, positioning\nLLM-SRBench as a valuable resource for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific equation discovery is a fundamental task in the history of\nscientific progress, enabling the derivation of laws governing natural\nphenomena. Recently, Large Language Models (LLMs) have gained interest for this\ntask due to their potential to leverage embedded scientific knowledge for\nhypothesis generation. However, evaluating the true discovery capabilities of\nthese methods remains challenging, as existing benchmarks often rely on common\nequations that are susceptible to memorization by LLMs, leading to inflated\nperformance metrics that do not reflect discovery. In this paper, we introduce\nLLM-SRBench, a comprehensive benchmark with 239 challenging problems across\nfour scientific domains specifically designed to evaluate LLM-based scientific\nequation discovery methods while preventing trivial memorization. Our benchmark\ncomprises two main categories: LSR-Transform, which transforms common physical\nmodels into less common mathematical representations to test reasoning beyond\nmemorized forms, and LSR-Synth, which introduces synthetic, discovery-driven\nproblems requiring data-driven reasoning. Through extensive evaluation of\nseveral state-of-the-art methods, using both open and closed LLMs, we find that\nthe best-performing system so far achieves only 31.5% symbolic accuracy. These\nfindings highlight the challenges of scientific equation discovery, positioning\nLLM-SRBench as a valuable resource for future research."
                },
                "authors": [
                    {
                        "name": "Parshin Shojaee"
                    },
                    {
                        "name": "Ngoc-Hieu Nguyen"
                    },
                    {
                        "name": "Kazem Meidani"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    },
                    {
                        "name": "Khoa D Doan"
                    },
                    {
                        "name": "Chandan K Reddy"
                    }
                ],
                "author_detail": {
                    "name": "Chandan K Reddy"
                },
                "author": "Chandan K Reddy",
                "arxiv_comment": "Project page:\n  https://github.com/deep-symbolic-mathematics/llm-srbench , Benchmark page:\n  https://huggingface.co/datasets/nnheui/llm-srbench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10414v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10414v1",
                "updated": "2025-04-14T16:59:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    59,
                    29,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T16:59:29Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    59,
                    29,
                    0,
                    104,
                    0
                ],
                "title": "HUMOTO: A 4D Dataset of Mocap Human Object Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HUMOTO: A 4D Dataset of Mocap Human Object Interactions"
                },
                "summary": "We present Human Motions with Objects (HUMOTO), a high-fidelity dataset of\nhuman-object interactions for motion generation, computer vision, and robotics\napplications. Featuring 736 sequences (7,875 seconds at 30 fps), HUMOTO\ncaptures interactions with 63 precisely modeled objects and 72 articulated\nparts. Our innovations include a scene-driven LLM scripting pipeline creating\ncomplete, purposeful tasks with natural progression, and a mocap-and-camera\nrecording setup to effectively handle occlusions. Spanning diverse activities\nfrom cooking to outdoor picnics, HUMOTO preserves both physical accuracy and\nlogical task flow. Professional artists rigorously clean and verify each\nsequence, minimizing foot sliding and object penetrations. We also provide\nbenchmarks compared to other datasets. HUMOTO's comprehensive full-body motion\nand simultaneous multi-object interactions address key data-capturing\nchallenges and provide opportunities to advance realistic human-object\ninteraction modeling across research domains with practical applications in\nanimation, robotics, and embodied AI systems. Project:\nhttps://jiaxin-lu.github.io/humoto/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Human Motions with Objects (HUMOTO), a high-fidelity dataset of\nhuman-object interactions for motion generation, computer vision, and robotics\napplications. Featuring 736 sequences (7,875 seconds at 30 fps), HUMOTO\ncaptures interactions with 63 precisely modeled objects and 72 articulated\nparts. Our innovations include a scene-driven LLM scripting pipeline creating\ncomplete, purposeful tasks with natural progression, and a mocap-and-camera\nrecording setup to effectively handle occlusions. Spanning diverse activities\nfrom cooking to outdoor picnics, HUMOTO preserves both physical accuracy and\nlogical task flow. Professional artists rigorously clean and verify each\nsequence, minimizing foot sliding and object penetrations. We also provide\nbenchmarks compared to other datasets. HUMOTO's comprehensive full-body motion\nand simultaneous multi-object interactions address key data-capturing\nchallenges and provide opportunities to advance realistic human-object\ninteraction modeling across research domains with practical applications in\nanimation, robotics, and embodied AI systems. Project:\nhttps://jiaxin-lu.github.io/humoto/ ."
                },
                "authors": [
                    {
                        "name": "Jiaxin Lu"
                    },
                    {
                        "name": "Chun-Hao Paul Huang"
                    },
                    {
                        "name": "Uttaran Bhattacharya"
                    },
                    {
                        "name": "Qixing Huang"
                    },
                    {
                        "name": "Yi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zhou"
                },
                "author": "Yi Zhou",
                "arxiv_comment": "19 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10414v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10414v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01436v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01436v2",
                "updated": "2025-04-14T16:58:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    58,
                    48,
                    0,
                    104,
                    0
                ],
                "published": "2025-02-03T15:19:28Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    19,
                    28,
                    0,
                    34,
                    0
                ],
                "title": "Towards Safer Chatbots: A Framework for Policy Compliance Evaluation of\n  Custom GPTs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Safer Chatbots: A Framework for Policy Compliance Evaluation of\n  Custom GPTs"
                },
                "summary": "Large Language Models (LLMs) have gained unprecedented prominence, achieving\nwidespread adoption across diverse domains and integrating deeply into society.\nThe capability to fine-tune general-purpose LLMs, such as Generative\nPre-trained Transformers (GPT), for specific tasks has facilitated the\nemergence of numerous Custom GPTs. These tailored models are increasingly made\navailable through dedicated marketplaces, such as OpenAI's GPT Store. However,\ntheir black-box nature introduces significant safety and compliance risks. In\nthis work, we present a scalable framework for the automated evaluation of\nCustom GPTs against OpenAI's usage policies, which define the permissible\nbehaviors of these systems. Our framework integrates three core components: (1)\nautomated discovery and data collection of models from the GPT store, (2) a\nred-teaming prompt generator tailored to specific policy categories and the\ncharacteristics of each target GPT, and (3) an LLM-as-a-judge technique to\nanalyze each prompt-response pair for potential policy violations. We validate\nour framework with a manually annotated ground truth, and evaluate it through a\nlarge-scale study with 782 Custom GPTs across three categories: Romantic,\nCybersecurity, and Academic GPTs. Our manual annotation process achieved an F1\nscore of 0.975 in identifying policy violations, confirming the reliability of\nthe framework's assessments. The results reveal that 58.7% of the analyzed\nmodels exhibit indications of non-compliance, exposing weaknesses in the GPT\nstore's review and approval processes. Furthermore, our findings indicate that\na model's popularity does not correlate with compliance, and non-compliance\nissues largely stem from behaviors inherited from base models rather than\nuser-driven customizations. We believe this approach is extendable to other\nchatbot platforms and policy domains, improving LLM-based systems safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained unprecedented prominence, achieving\nwidespread adoption across diverse domains and integrating deeply into society.\nThe capability to fine-tune general-purpose LLMs, such as Generative\nPre-trained Transformers (GPT), for specific tasks has facilitated the\nemergence of numerous Custom GPTs. These tailored models are increasingly made\navailable through dedicated marketplaces, such as OpenAI's GPT Store. However,\ntheir black-box nature introduces significant safety and compliance risks. In\nthis work, we present a scalable framework for the automated evaluation of\nCustom GPTs against OpenAI's usage policies, which define the permissible\nbehaviors of these systems. Our framework integrates three core components: (1)\nautomated discovery and data collection of models from the GPT store, (2) a\nred-teaming prompt generator tailored to specific policy categories and the\ncharacteristics of each target GPT, and (3) an LLM-as-a-judge technique to\nanalyze each prompt-response pair for potential policy violations. We validate\nour framework with a manually annotated ground truth, and evaluate it through a\nlarge-scale study with 782 Custom GPTs across three categories: Romantic,\nCybersecurity, and Academic GPTs. Our manual annotation process achieved an F1\nscore of 0.975 in identifying policy violations, confirming the reliability of\nthe framework's assessments. The results reveal that 58.7% of the analyzed\nmodels exhibit indications of non-compliance, exposing weaknesses in the GPT\nstore's review and approval processes. Furthermore, our findings indicate that\na model's popularity does not correlate with compliance, and non-compliance\nissues largely stem from behaviors inherited from base models rather than\nuser-driven customizations. We believe this approach is extendable to other\nchatbot platforms and policy domains, improving LLM-based systems safety."
                },
                "authors": [
                    {
                        "name": "David Rodriguez"
                    },
                    {
                        "name": "William Seymour"
                    },
                    {
                        "name": "Jose M. Del Alamo"
                    },
                    {
                        "name": "Jose Such"
                    }
                ],
                "author_detail": {
                    "name": "Jose Such"
                },
                "author": "Jose Such",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01436v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01436v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14701v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14701v3",
                "updated": "2025-04-14T16:58:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    58,
                    34,
                    0,
                    104,
                    0
                ],
                "published": "2024-02-22T16:56:44Z",
                "published_parsed": [
                    2024,
                    2,
                    22,
                    16,
                    56,
                    44,
                    3,
                    53,
                    0
                ],
                "title": "COMPASS: Computational Mapping of Patient-Therapist Alliance Strategies\n  with Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COMPASS: Computational Mapping of Patient-Therapist Alliance Strategies\n  with Language Modeling"
                },
                "summary": "The therapeutic working alliance is a critical predictor of psychotherapy\nsuccess. Traditionally, working alliance assessment relies on questionnaires\ncompleted by both therapists and patients. In this paper, we present COMPASS, a\nnovel framework to directly infer the therapeutic working alliance from the\nnatural language used in psychotherapy sessions. Our approach leverages\nadvanced large language models (LLMs) to analyze session transcripts and map\nthem to distributed representations. These representations capture the semantic\nsimilarities between the dialogues and psychometric instruments, such as the\nWorking Alliance Inventory. Analyzing a dataset of over 950 sessions spanning\ndiverse psychiatric conditions -- including anxiety (N=498), depression\n(N=377), schizophrenia (N=71), and suicidal tendencies (N=12) -- collected\nbetween 1970 and 2012, we demonstrate the effectiveness of our method in\nproviding fine-grained mapping of patient-therapist alignment trajectories,\noffering interpretable insights for clinical practice, and identifying emerging\npatterns related to the condition being treated. By employing various deep\nlearning-based topic modeling techniques in combination with prompting\ngenerative language models, we analyze the topical characteristics of different\npsychiatric conditions and how these topics evolve during each turn of the\nconversation. This integrated framework enhances the understanding of\ntherapeutic interactions, enables timely feedback for therapists on the quality\nof therapeutic relationships, and provides clear, actionable insights to\nimprove the effectiveness of psychotherapy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The therapeutic working alliance is a critical predictor of psychotherapy\nsuccess. Traditionally, working alliance assessment relies on questionnaires\ncompleted by both therapists and patients. In this paper, we present COMPASS, a\nnovel framework to directly infer the therapeutic working alliance from the\nnatural language used in psychotherapy sessions. Our approach leverages\nadvanced large language models (LLMs) to analyze session transcripts and map\nthem to distributed representations. These representations capture the semantic\nsimilarities between the dialogues and psychometric instruments, such as the\nWorking Alliance Inventory. Analyzing a dataset of over 950 sessions spanning\ndiverse psychiatric conditions -- including anxiety (N=498), depression\n(N=377), schizophrenia (N=71), and suicidal tendencies (N=12) -- collected\nbetween 1970 and 2012, we demonstrate the effectiveness of our method in\nproviding fine-grained mapping of patient-therapist alignment trajectories,\noffering interpretable insights for clinical practice, and identifying emerging\npatterns related to the condition being treated. By employing various deep\nlearning-based topic modeling techniques in combination with prompting\ngenerative language models, we analyze the topical characteristics of different\npsychiatric conditions and how these topics evolve during each turn of the\nconversation. This integrated framework enhances the understanding of\ntherapeutic interactions, enables timely feedback for therapists on the quality\nof therapeutic relationships, and provides clear, actionable insights to\nimprove the effectiveness of psychotherapy."
                },
                "authors": [
                    {
                        "name": "Baihan Lin"
                    },
                    {
                        "name": "Djallel Bouneffouf"
                    },
                    {
                        "name": "Yulia Landa"
                    },
                    {
                        "name": "Rachel Jespersen"
                    },
                    {
                        "name": "Cheryl Corcoran"
                    },
                    {
                        "name": "Guillermo Cecchi"
                    }
                ],
                "author_detail": {
                    "name": "Guillermo Cecchi"
                },
                "author": "Guillermo Cecchi",
                "arxiv_comment": "Translational Psychiatry, in press. This work extends our research\n  series in computational psychiatry (e.g auto annotation in arXiv:2204.05522,\n  topic extraction in arXiv:2204.10189, and diagnosis in arXiv:2210.15603) with\n  the introduction of LLMs to complete the full cycle of interpreting and\n  understanding psychotherapy strategies as a comprehensive analytical\n  framework",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14701v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14701v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10405v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10405v1",
                "updated": "2025-04-14T16:53:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    53,
                    59,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T16:53:59Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    53,
                    59,
                    0,
                    104,
                    0
                ],
                "title": "Performance of Large Language Models in Supporting Medical Diagnosis and\n  Treatment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance of Large Language Models in Supporting Medical Diagnosis and\n  Treatment"
                },
                "summary": "The integration of Large Language Models (LLMs) into healthcare holds\nsignificant potential to enhance diagnostic accuracy and support medical\ntreatment planning. These AI-driven systems can analyze vast datasets,\nassisting clinicians in identifying diseases, recommending treatments, and\npredicting patient outcomes. This study evaluates the performance of a range of\ncontemporary LLMs, including both open-source and closed-source models, on the\n2024 Portuguese National Exam for medical specialty access (PNA), a\nstandardized medical knowledge assessment. Our results highlight considerable\nvariation in accuracy and cost-effectiveness, with several models demonstrating\nperformance exceeding human benchmarks for medical students on this specific\ntask. We identify leading models based on a combined score of accuracy and\ncost, discuss the implications of reasoning methodologies like\nChain-of-Thought, and underscore the potential for LLMs to function as valuable\ncomplementary tools aiding medical professionals in complex clinical\ndecision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into healthcare holds\nsignificant potential to enhance diagnostic accuracy and support medical\ntreatment planning. These AI-driven systems can analyze vast datasets,\nassisting clinicians in identifying diseases, recommending treatments, and\npredicting patient outcomes. This study evaluates the performance of a range of\ncontemporary LLMs, including both open-source and closed-source models, on the\n2024 Portuguese National Exam for medical specialty access (PNA), a\nstandardized medical knowledge assessment. Our results highlight considerable\nvariation in accuracy and cost-effectiveness, with several models demonstrating\nperformance exceeding human benchmarks for medical students on this specific\ntask. We identify leading models based on a combined score of accuracy and\ncost, discuss the implications of reasoning methodologies like\nChain-of-Thought, and underscore the potential for LLMs to function as valuable\ncomplementary tools aiding medical professionals in complex clinical\ndecision-making."
                },
                "authors": [
                    {
                        "name": "Diogo Sousa"
                    },
                    {
                        "name": "Guilherme Barbosa"
                    },
                    {
                        "name": "Catarina Rocha"
                    },
                    {
                        "name": "Dulce Oliveira"
                    }
                ],
                "author_detail": {
                    "name": "Dulce Oliveira"
                },
                "author": "Dulce Oliveira",
                "arxiv_comment": "21 pages, 6 figures, 4 tables. Acknowledgements: The authors\n  acknowledge the support of the AITriage4SU Project (2024.07400.IACDC/2024),\n  funded by the FCT (Foundation for Science and Technology), Portugal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10405v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10400v1",
                "updated": "2025-04-14T16:51:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    51,
                    10,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T16:51:10Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    51,
                    10,
                    0,
                    104,
                    0
                ],
                "title": "Towards Low-Latency Event-based Obstacle Avoidance on a FPGA-Drone",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Low-Latency Event-based Obstacle Avoidance on a FPGA-Drone"
                },
                "summary": "This work quantitatively evaluates the performance of event-based vision\nsystems (EVS) against conventional RGB-based models for action prediction in\ncollision avoidance on an FPGA accelerator. Our experiments demonstrate that\nthe EVS model achieves a significantly higher effective frame rate (1 kHz) and\nlower temporal (-20 ms) and spatial prediction errors (-20 mm) compared to the\nRGB-based model, particularly when tested on out-of-distribution data. The EVS\nmodel also exhibits superior robustness in selecting optimal evasion maneuvers.\nIn particular, in distinguishing between movement and stationary states, it\nachieves a 59 percentage point advantage in precision (78% vs. 19%) and a\nsubstantially higher F1 score (0.73 vs. 0.06), highlighting the susceptibility\nof the RGB model to overfitting. Further analysis in different combinations of\nspatial classes confirms the consistent performance of the EVS model in both\ntest data sets. Finally, we evaluated the system end-to-end and achieved a\nlatency of approximately 2.14 ms, with event aggregation (1 ms) and inference\non the processing unit (0.94 ms) accounting for the largest components. These\nresults underscore the advantages of event-based vision for real-time collision\navoidance and demonstrate its potential for deployment in resource-constrained\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work quantitatively evaluates the performance of event-based vision\nsystems (EVS) against conventional RGB-based models for action prediction in\ncollision avoidance on an FPGA accelerator. Our experiments demonstrate that\nthe EVS model achieves a significantly higher effective frame rate (1 kHz) and\nlower temporal (-20 ms) and spatial prediction errors (-20 mm) compared to the\nRGB-based model, particularly when tested on out-of-distribution data. The EVS\nmodel also exhibits superior robustness in selecting optimal evasion maneuvers.\nIn particular, in distinguishing between movement and stationary states, it\nachieves a 59 percentage point advantage in precision (78% vs. 19%) and a\nsubstantially higher F1 score (0.73 vs. 0.06), highlighting the susceptibility\nof the RGB model to overfitting. Further analysis in different combinations of\nspatial classes confirms the consistent performance of the EVS model in both\ntest data sets. Finally, we evaluated the system end-to-end and achieved a\nlatency of approximately 2.14 ms, with event aggregation (1 ms) and inference\non the processing unit (0.94 ms) accounting for the largest components. These\nresults underscore the advantages of event-based vision for real-time collision\navoidance and demonstrate its potential for deployment in resource-constrained\nenvironments."
                },
                "authors": [
                    {
                        "name": "Pietro Bonazzi"
                    },
                    {
                        "name": "Christian Vogt"
                    },
                    {
                        "name": "Michael Jost"
                    },
                    {
                        "name": "Lyes Khacef"
                    },
                    {
                        "name": "Federico Paredes-Valls"
                    },
                    {
                        "name": "Michele Magno"
                    }
                ],
                "author_detail": {
                    "name": "Michele Magno"
                },
                "author": "Michele Magno",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10397v1",
                "updated": "2025-04-14T16:45:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    45,
                    52,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T16:45:52Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    45,
                    52,
                    0,
                    104,
                    0
                ],
                "title": "Can LLMs Assist Expert Elicitation for Probabilistic Causal Modeling?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Assist Expert Elicitation for Probabilistic Causal Modeling?"
                },
                "summary": "Objective: This study investigates the potential of Large Language Models\n(LLMs) as an alternative to human expert elicitation for extracting structured\ncausal knowledge and facilitating causal modeling in biometric and healthcare\napplications.\n  Material and Methods: LLM-generated causal structures, specifically Bayesian\nnetworks (BNs), were benchmarked against traditional statistical methods (e.g.,\nBayesian Information Criterion) using healthcare datasets. Validation\ntechniques included structural equation modeling (SEM) to verifying\nrelationships, and measures such as entropy, predictive accuracy, and\nrobustness to compare network structures.\n  Results and Discussion: LLM-generated BNs demonstrated lower entropy than\nexpert-elicited and statistically generated BNs, suggesting higher confidence\nand precision in predictions. However, limitations such as contextual\nconstraints, hallucinated dependencies, and potential biases inherited from\ntraining data require further investigation.\n  Conclusion: LLMs represent a novel frontier in expert elicitation for\nprobabilistic causal modeling, promising to improve transparency and reduce\nuncertainty in the decision-making using such models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objective: This study investigates the potential of Large Language Models\n(LLMs) as an alternative to human expert elicitation for extracting structured\ncausal knowledge and facilitating causal modeling in biometric and healthcare\napplications.\n  Material and Methods: LLM-generated causal structures, specifically Bayesian\nnetworks (BNs), were benchmarked against traditional statistical methods (e.g.,\nBayesian Information Criterion) using healthcare datasets. Validation\ntechniques included structural equation modeling (SEM) to verifying\nrelationships, and measures such as entropy, predictive accuracy, and\nrobustness to compare network structures.\n  Results and Discussion: LLM-generated BNs demonstrated lower entropy than\nexpert-elicited and statistically generated BNs, suggesting higher confidence\nand precision in predictions. However, limitations such as contextual\nconstraints, hallucinated dependencies, and potential biases inherited from\ntraining data require further investigation.\n  Conclusion: LLMs represent a novel frontier in expert elicitation for\nprobabilistic causal modeling, promising to improve transparency and reduce\nuncertainty in the decision-making using such models."
                },
                "authors": [
                    {
                        "name": "Olha Shaposhnyk"
                    },
                    {
                        "name": "Daria Zahorska"
                    },
                    {
                        "name": "Svetlana Yanushkevich"
                    }
                ],
                "author_detail": {
                    "name": "Svetlana Yanushkevich"
                },
                "author": "Svetlana Yanushkevich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10391v1",
                "updated": "2025-04-14T16:38:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    38,
                    28,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T16:38:28Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    38,
                    28,
                    0,
                    104,
                    0
                ],
                "title": "LLM-driven Constrained Copy Generation through Iterative Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-driven Constrained Copy Generation through Iterative Refinement"
                },
                "summary": "Crafting a marketing message (copy), or copywriting is a challenging\ngeneration task, as the copy must adhere to various constraints. Copy creation\nis inherently iterative for humans, starting with an initial draft followed by\nsuccessive refinements. However, manual copy creation is time-consuming and\nexpensive, resulting in only a few copies for each use case. This limitation\nrestricts our ability to personalize content to customers. Contrary to the\nmanual approach, LLMs can generate copies quickly, but the generated content\ndoes not consistently meet all the constraints on the first attempt (similar to\nhumans). While recent studies have shown promise in improving constrained\ngeneration through iterative refinement, they have primarily addressed tasks\nwith only a few simple constraints. Consequently, the effectiveness of\niterative refinement for tasks such as copy generation, which involves many\nintricate constraints, remains unclear. To address this gap, we propose an\nLLM-based end-to-end framework for scalable copy generation using iterative\nrefinement. To the best of our knowledge, this is the first study to address\nmultiple challenging constraints simultaneously in copy generation. Examples of\nthese constraints include length, topics, keywords, preferred lexical ordering,\nand tone of voice. We demonstrate the performance of our framework by creating\ncopies for e-commerce banners for three different use cases of varying\ncomplexity. Our results show that iterative refinement increases the copy\nsuccess rate by $16.25-35.91$% across use cases. Furthermore, the copies\ngenerated using our approach outperformed manually created content in multiple\npilot studies using a multi-armed bandit framework. The winning copy improved\nthe click-through rate by $38.5-45.21$%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crafting a marketing message (copy), or copywriting is a challenging\ngeneration task, as the copy must adhere to various constraints. Copy creation\nis inherently iterative for humans, starting with an initial draft followed by\nsuccessive refinements. However, manual copy creation is time-consuming and\nexpensive, resulting in only a few copies for each use case. This limitation\nrestricts our ability to personalize content to customers. Contrary to the\nmanual approach, LLMs can generate copies quickly, but the generated content\ndoes not consistently meet all the constraints on the first attempt (similar to\nhumans). While recent studies have shown promise in improving constrained\ngeneration through iterative refinement, they have primarily addressed tasks\nwith only a few simple constraints. Consequently, the effectiveness of\niterative refinement for tasks such as copy generation, which involves many\nintricate constraints, remains unclear. To address this gap, we propose an\nLLM-based end-to-end framework for scalable copy generation using iterative\nrefinement. To the best of our knowledge, this is the first study to address\nmultiple challenging constraints simultaneously in copy generation. Examples of\nthese constraints include length, topics, keywords, preferred lexical ordering,\nand tone of voice. We demonstrate the performance of our framework by creating\ncopies for e-commerce banners for three different use cases of varying\ncomplexity. Our results show that iterative refinement increases the copy\nsuccess rate by $16.25-35.91$% across use cases. Furthermore, the copies\ngenerated using our approach outperformed manually created content in multiple\npilot studies using a multi-armed bandit framework. The winning copy improved\nthe click-through rate by $38.5-45.21$%."
                },
                "authors": [
                    {
                        "name": "Varun Vasudevan"
                    },
                    {
                        "name": "Faezeh Akhavizadegan"
                    },
                    {
                        "name": "Abhinav Prakash"
                    },
                    {
                        "name": "Yokila Arora"
                    },
                    {
                        "name": "Jason Cho"
                    },
                    {
                        "name": "Tanya Mendiratta"
                    },
                    {
                        "name": "Sushant Kumar"
                    },
                    {
                        "name": "Kannan Achan"
                    }
                ],
                "author_detail": {
                    "name": "Kannan Achan"
                },
                "author": "Kannan Achan",
                "arxiv_comment": "10 pages, 2 figures, 7 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10390v1",
                "updated": "2025-04-14T16:36:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    36,
                    56,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T16:36:56Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    36,
                    56,
                    0,
                    104,
                    0
                ],
                "title": "Teacher Motion Priors: Enhancing Robot Locomotion over Challenging\n  Terrain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teacher Motion Priors: Enhancing Robot Locomotion over Challenging\n  Terrain"
                },
                "summary": "Achieving robust locomotion on complex terrains remains a challenge due to\nhigh dimensional control and environmental uncertainties. This paper introduces\na teacher prior framework based on the teacher student paradigm, integrating\nimitation and auxiliary task learning to improve learning efficiency and\ngeneralization. Unlike traditional paradigms that strongly rely on\nencoder-based state embeddings, our framework decouples the network design,\nsimplifying the policy network and deployment. A high performance teacher\npolicy is first trained using privileged information to acquire generalizable\nmotion skills. The teacher's motion distribution is transferred to the student\npolicy, which relies only on noisy proprioceptive data, via a generative\nadversarial mechanism to mitigate performance degradation caused by\ndistributional shifts. Additionally, auxiliary task learning enhances the\nstudent policy's feature representation, speeding up convergence and improving\nadaptability to varying terrains. The framework is validated on a humanoid\nrobot, showing a great improvement in locomotion stability on dynamic terrains\nand significant reductions in development costs. This work provides a practical\nsolution for deploying robust locomotion strategies in humanoid robots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving robust locomotion on complex terrains remains a challenge due to\nhigh dimensional control and environmental uncertainties. This paper introduces\na teacher prior framework based on the teacher student paradigm, integrating\nimitation and auxiliary task learning to improve learning efficiency and\ngeneralization. Unlike traditional paradigms that strongly rely on\nencoder-based state embeddings, our framework decouples the network design,\nsimplifying the policy network and deployment. A high performance teacher\npolicy is first trained using privileged information to acquire generalizable\nmotion skills. The teacher's motion distribution is transferred to the student\npolicy, which relies only on noisy proprioceptive data, via a generative\nadversarial mechanism to mitigate performance degradation caused by\ndistributional shifts. Additionally, auxiliary task learning enhances the\nstudent policy's feature representation, speeding up convergence and improving\nadaptability to varying terrains. The framework is validated on a humanoid\nrobot, showing a great improvement in locomotion stability on dynamic terrains\nand significant reductions in development costs. This work provides a practical\nsolution for deploying robust locomotion strategies in humanoid robots."
                },
                "authors": [
                    {
                        "name": "Fangcheng Jin"
                    },
                    {
                        "name": "Yuqi Wang"
                    },
                    {
                        "name": "Peixin Ma"
                    },
                    {
                        "name": "Guodong Yang"
                    },
                    {
                        "name": "Pan Zhao"
                    },
                    {
                        "name": "En Li"
                    },
                    {
                        "name": "Zhengtao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhengtao Zhang"
                },
                "author": "Zhengtao Zhang",
                "arxiv_comment": "8 pages, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T40",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07282v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07282v2",
                "updated": "2025-04-14T16:23:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    23,
                    29,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-09T21:17:52Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    21,
                    17,
                    52,
                    2,
                    99,
                    0
                ],
                "title": "RAISE: Reinforenced Adaptive Instruction Selection For Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAISE: Reinforenced Adaptive Instruction Selection For Large Language\n  Models"
                },
                "summary": "In the instruction fine-tuning of large language models (LLMs), it has become\na consensus that a few high-quality instructions are superior to a large number\nof low-quality instructions. At present, many instruction selection methods\nhave been proposed, but most of these methods select instruction based on\nheuristic quality metrics, and only consider data selection before training.\nThese designs lead to insufficient optimization of instruction fine-tuning, and\nfixed heuristic indicators are often difficult to optimize for specific tasks.\nSo we designed a dynamic, task-objective-driven instruction selection framework\nRAISE(Reinforenced Adaptive Instruction SElection), which incorporates the\nentire instruction fine-tuning process into optimization, selecting instruction\nat each step based on the expected impact of instruction on model performance\nimprovement. Our approach is well interpretable and has strong task-specific\noptimization capabilities. By modeling dynamic instruction selection as a\nsequential decision-making process, we use RL to train our selection strategy.\nExtensive experiments and result analysis prove the superiority of our method\ncompared with other instruction selection methods. Notably, RAISE achieves\nsuperior performance by updating only 1\\% of the training steps compared to\nfull-data training, demonstrating its efficiency and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the instruction fine-tuning of large language models (LLMs), it has become\na consensus that a few high-quality instructions are superior to a large number\nof low-quality instructions. At present, many instruction selection methods\nhave been proposed, but most of these methods select instruction based on\nheuristic quality metrics, and only consider data selection before training.\nThese designs lead to insufficient optimization of instruction fine-tuning, and\nfixed heuristic indicators are often difficult to optimize for specific tasks.\nSo we designed a dynamic, task-objective-driven instruction selection framework\nRAISE(Reinforenced Adaptive Instruction SElection), which incorporates the\nentire instruction fine-tuning process into optimization, selecting instruction\nat each step based on the expected impact of instruction on model performance\nimprovement. Our approach is well interpretable and has strong task-specific\noptimization capabilities. By modeling dynamic instruction selection as a\nsequential decision-making process, we use RL to train our selection strategy.\nExtensive experiments and result analysis prove the superiority of our method\ncompared with other instruction selection methods. Notably, RAISE achieves\nsuperior performance by updating only 1\\% of the training steps compared to\nfull-data training, demonstrating its efficiency and effectiveness."
                },
                "authors": [
                    {
                        "name": "Lv Qingsong"
                    },
                    {
                        "name": "Yangning Li"
                    },
                    {
                        "name": "Zihua Lan"
                    },
                    {
                        "name": "Zishan Xu"
                    },
                    {
                        "name": "Jiwei Tang"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Wenhao Jiang"
                    },
                    {
                        "name": "Hai-Tao Zheng"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07282v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07282v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10369v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10369v1",
                "updated": "2025-04-14T16:15:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    15,
                    55,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T16:15:55Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    15,
                    55,
                    0,
                    104,
                    0
                ],
                "title": "SymRTLO: Enhancing RTL Code Optimization with LLMs and Neuron-Inspired\n  Symbolic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SymRTLO: Enhancing RTL Code Optimization with LLMs and Neuron-Inspired\n  Symbolic Reasoning"
                },
                "summary": "Optimizing Register Transfer Level (RTL) code is crucial for improving the\npower, performance, and area (PPA) of digital circuits in the early stages of\nsynthesis. Manual rewriting, guided by synthesis feedback, can yield\nhigh-quality results but is time-consuming and error-prone. Most existing\ncompiler-based approaches have difficulty handling complex design constraints.\nLarge Language Model (LLM)-based methods have emerged as a promising\nalternative to address these challenges. However, LLM-based approaches often\nface difficulties in ensuring alignment between the generated code and the\nprovided prompts. This paper presents SymRTLO, a novel neuron-symbolic RTL\noptimization framework that seamlessly integrates LLM-based code rewriting with\nsymbolic reasoning techniques. Our method incorporates a retrieval-augmented\ngeneration (RAG) system of optimization rules and Abstract Syntax Tree\n(AST)-based templates, enabling LLM-based rewriting that maintains syntactic\ncorrectness while minimizing undesired circuit behaviors. A symbolic module is\nproposed for analyzing and optimizing finite state machine (FSM) logic,\nallowing fine-grained state merging and partial specification handling beyond\nthe scope of pattern-based compilers. Furthermore, a fast verification\npipeline, combining formal equivalence checks with test-driven validation,\nfurther reduces the complexity of verification. Experiments on the RTL-Rewriter\nbenchmark with Synopsys Design Compiler and Yosys show that SymRTLO improves\npower, performance, and area (PPA) by up to 43.9%, 62.5%, and 51.1%,\nrespectively, compared to the state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Register Transfer Level (RTL) code is crucial for improving the\npower, performance, and area (PPA) of digital circuits in the early stages of\nsynthesis. Manual rewriting, guided by synthesis feedback, can yield\nhigh-quality results but is time-consuming and error-prone. Most existing\ncompiler-based approaches have difficulty handling complex design constraints.\nLarge Language Model (LLM)-based methods have emerged as a promising\nalternative to address these challenges. However, LLM-based approaches often\nface difficulties in ensuring alignment between the generated code and the\nprovided prompts. This paper presents SymRTLO, a novel neuron-symbolic RTL\noptimization framework that seamlessly integrates LLM-based code rewriting with\nsymbolic reasoning techniques. Our method incorporates a retrieval-augmented\ngeneration (RAG) system of optimization rules and Abstract Syntax Tree\n(AST)-based templates, enabling LLM-based rewriting that maintains syntactic\ncorrectness while minimizing undesired circuit behaviors. A symbolic module is\nproposed for analyzing and optimizing finite state machine (FSM) logic,\nallowing fine-grained state merging and partial specification handling beyond\nthe scope of pattern-based compilers. Furthermore, a fast verification\npipeline, combining formal equivalence checks with test-driven validation,\nfurther reduces the complexity of verification. Experiments on the RTL-Rewriter\nbenchmark with Synopsys Design Compiler and Yosys show that SymRTLO improves\npower, performance, and area (PPA) by up to 43.9%, 62.5%, and 51.1%,\nrespectively, compared to the state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yiting Wang"
                    },
                    {
                        "name": "Wanghao Ye"
                    },
                    {
                        "name": "Ping Guo"
                    },
                    {
                        "name": "Yexiao He"
                    },
                    {
                        "name": "Ziyao Wang"
                    },
                    {
                        "name": "Yexiao He"
                    },
                    {
                        "name": "Bowei Tian"
                    },
                    {
                        "name": "Shwai He"
                    },
                    {
                        "name": "Guoheng Sun"
                    },
                    {
                        "name": "Zheyu Shen"
                    },
                    {
                        "name": "Sihan Chen"
                    },
                    {
                        "name": "Ankur Srivastava"
                    },
                    {
                        "name": "Qingfu Zhang"
                    },
                    {
                        "name": "Gang Qu"
                    },
                    {
                        "name": "Ang Li"
                    }
                ],
                "author_detail": {
                    "name": "Ang Li"
                },
                "author": "Ang Li",
                "arxiv_comment": "16 pages, 8 figures, 7 tables. Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10369v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10369v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10368v1",
                "updated": "2025-04-14T16:13:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    13,
                    23,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T16:13:23Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    13,
                    23,
                    0,
                    104,
                    0
                ],
                "title": "S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability\n  of Large Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability\n  of Large Reasoning Models"
                },
                "summary": "We introduce S1-Bench, a novel benchmark designed to evaluate Large Reasoning\nModels' (LRMs) performance on simple tasks that favor intuitive system 1\nthinking rather than deliberative system 2 reasoning. While LRMs have achieved\nsignificant breakthroughs in complex reasoning tasks through explicit chains of\nthought, their reliance on deep analytical thinking may limit their system 1\nthinking capabilities. Moreover, a lack of benchmark currently exists to\nevaluate LRMs' performance in tasks that require such capabilities. To fill\nthis gap, S1-Bench presents a set of simple, diverse, and naturally clear\nquestions across multiple domains and languages, specifically designed to\nassess LRMs' performance in such tasks. Our comprehensive evaluation of 22 LRMs\nreveals significant lower efficiency tendencies, with outputs averaging 15.5\ntimes longer than those of traditional small LLMs. Additionally, LRMs often\nidentify correct answers early but continue unnecessary deliberation, with some\nmodels even producing numerous errors. These findings highlight the rigid\nreasoning patterns of current LRMs and underscore the substantial development\nneeded to achieve balanced dual-system thinking capabilities that can adapt\nappropriately to task complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce S1-Bench, a novel benchmark designed to evaluate Large Reasoning\nModels' (LRMs) performance on simple tasks that favor intuitive system 1\nthinking rather than deliberative system 2 reasoning. While LRMs have achieved\nsignificant breakthroughs in complex reasoning tasks through explicit chains of\nthought, their reliance on deep analytical thinking may limit their system 1\nthinking capabilities. Moreover, a lack of benchmark currently exists to\nevaluate LRMs' performance in tasks that require such capabilities. To fill\nthis gap, S1-Bench presents a set of simple, diverse, and naturally clear\nquestions across multiple domains and languages, specifically designed to\nassess LRMs' performance in such tasks. Our comprehensive evaluation of 22 LRMs\nreveals significant lower efficiency tendencies, with outputs averaging 15.5\ntimes longer than those of traditional small LLMs. Additionally, LRMs often\nidentify correct answers early but continue unnecessary deliberation, with some\nmodels even producing numerous errors. These findings highlight the rigid\nreasoning patterns of current LRMs and underscore the substantial development\nneeded to achieve balanced dual-system thinking capabilities that can adapt\nappropriately to task complexity."
                },
                "authors": [
                    {
                        "name": "Wenyuan Zhang"
                    },
                    {
                        "name": "Shuaiyi Nie"
                    },
                    {
                        "name": "Xinghua Zhang"
                    },
                    {
                        "name": "Zefeng Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tingwen Liu"
                },
                "author": "Tingwen Liu",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10358v1",
                "updated": "2025-04-14T16:07:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    7,
                    16,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T16:07:16Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    7,
                    16,
                    0,
                    104,
                    0
                ],
                "title": "FingER: Content Aware Fine-grained Evaluation with Reasoning for\n  AI-Generated Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FingER: Content Aware Fine-grained Evaluation with Reasoning for\n  AI-Generated Videos"
                },
                "summary": "Recent advances in video generation have posed great challenges in the\nassessment of AI-generated content, particularly with the emergence of\nincreasingly sophisticated models. The various inconsistencies and defects\nobserved in such videos are inherently complex, making overall scoring\nnotoriously difficult. In this paper, we emphasize the critical importance of\nintegrating fine-grained reasoning into video evaluation, and we propose\n$\\textbf{F}$ing$\\textbf{ER}$, a novel entity-level reasoning evaluation\nframework that first automatically generates $\\textbf{F}$ine-grained\n$\\textbf{E}$ntity-level questions, and then answers those questions by a\n$\\textbf{R}$easoning model with scores, which can be subsequently weighted\nsummed to an overall score for different applications. Specifically, we\nleverage LLMs to derive entity-level questions across five distinct\nperspectives, which (i) often focus on some specific entities of the content,\nthereby making answering or scoring much easier by MLLMs, and (ii) are more\ninterpretable. Then we construct a FingER dataset, consisting of approximately\n3.3k videos and corresponding 60k fine-grained QA annotations, each with\ndetailed reasons. Based on that, we further investigate various training\nprotocols to best incentivize the reasoning capability of MLLMs for correct\nanswer prediction. Extensive experiments demonstrate that a reasoning model\ntrained using Group Relative Policy Optimization (GRPO) with a cold-start\nstrategy achieves the best performance. Notably, our model surpasses existing\nmethods by a relative margin of $11.8\\%$ on GenAI-Bench and $5.5\\%$ on\nMonetBench with only 3.3k training videos, which is at most one-tenth of the\ntraining samples utilized by other methods. Our code and dataset will be\nreleased soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in video generation have posed great challenges in the\nassessment of AI-generated content, particularly with the emergence of\nincreasingly sophisticated models. The various inconsistencies and defects\nobserved in such videos are inherently complex, making overall scoring\nnotoriously difficult. In this paper, we emphasize the critical importance of\nintegrating fine-grained reasoning into video evaluation, and we propose\n$\\textbf{F}$ing$\\textbf{ER}$, a novel entity-level reasoning evaluation\nframework that first automatically generates $\\textbf{F}$ine-grained\n$\\textbf{E}$ntity-level questions, and then answers those questions by a\n$\\textbf{R}$easoning model with scores, which can be subsequently weighted\nsummed to an overall score for different applications. Specifically, we\nleverage LLMs to derive entity-level questions across five distinct\nperspectives, which (i) often focus on some specific entities of the content,\nthereby making answering or scoring much easier by MLLMs, and (ii) are more\ninterpretable. Then we construct a FingER dataset, consisting of approximately\n3.3k videos and corresponding 60k fine-grained QA annotations, each with\ndetailed reasons. Based on that, we further investigate various training\nprotocols to best incentivize the reasoning capability of MLLMs for correct\nanswer prediction. Extensive experiments demonstrate that a reasoning model\ntrained using Group Relative Policy Optimization (GRPO) with a cold-start\nstrategy achieves the best performance. Notably, our model surpasses existing\nmethods by a relative margin of $11.8\\%$ on GenAI-Bench and $5.5\\%$ on\nMonetBench with only 3.3k training videos, which is at most one-tenth of the\ntraining samples utilized by other methods. Our code and dataset will be\nreleased soon."
                },
                "authors": [
                    {
                        "name": "Rui Chen"
                    },
                    {
                        "name": "Lei Sun"
                    },
                    {
                        "name": "Jing Tang"
                    },
                    {
                        "name": "Geng Li"
                    },
                    {
                        "name": "Xiangxiang Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangxiang Chu"
                },
                "author": "Xiangxiang Chu",
                "arxiv_comment": "10 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10356v1",
                "updated": "2025-04-14T16:05:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    5,
                    59,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T16:05:59Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    5,
                    59,
                    0,
                    104,
                    0
                ],
                "title": "MultiLoKo: a multilingual local knowledge benchmark for LLMs spanning 31\n  languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiLoKo: a multilingual local knowledge benchmark for LLMs spanning 31\n  languages"
                },
                "summary": "We present MultiLoKo, a new benchmark for evaluating multilinguality in LLMs\ncovering 31 languages. MultiLoKo consists of three partitions: a main partition\nconsisting of 500 questions per language, separately sourced to be locally\nrelevant to the specific language, and two translated partitions, containing\nhuman-authored translations from 30 non-English languages to English and vice\nversa. For comparison, we also release corresponding machine-authored\ntranslations. The data is equally distributed over two splits: a dev split and\na blind, out-of-distribution test split. MultiLoKo can be used to study a\nvariety of questions regarding the multilinguality of LLMs as well as\nmeta-questions about multilingual benchmark creation. We compute MultiLoKo\nscores for 11 base and chat models marketed to be multilingual and study their\naverage performance, their performance parity across languages, how much their\nability to answer questions depends on the question language, and which\nlanguages are most difficult. None of the models we studied performs well on\nMultiLoKo, as indicated by low average scores as well as large differences\nbetween the best and worst scoring languages. Furthermore, we find a\nsubstantial effect of the question language, indicating sub-optimal knowledge\ntransfer between languages. Lastly, we find that using local vs\nEnglish-translated data can result in differences more than 20 points for the\nbest performing models, drastically change the estimated difficulty of some\nlanguages. For using machines instead of human translations, we find a weaker\neffect on ordering of language difficulty, a larger difference in model\nrankings, and a substantial drop in estimated performance for all models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present MultiLoKo, a new benchmark for evaluating multilinguality in LLMs\ncovering 31 languages. MultiLoKo consists of three partitions: a main partition\nconsisting of 500 questions per language, separately sourced to be locally\nrelevant to the specific language, and two translated partitions, containing\nhuman-authored translations from 30 non-English languages to English and vice\nversa. For comparison, we also release corresponding machine-authored\ntranslations. The data is equally distributed over two splits: a dev split and\na blind, out-of-distribution test split. MultiLoKo can be used to study a\nvariety of questions regarding the multilinguality of LLMs as well as\nmeta-questions about multilingual benchmark creation. We compute MultiLoKo\nscores for 11 base and chat models marketed to be multilingual and study their\naverage performance, their performance parity across languages, how much their\nability to answer questions depends on the question language, and which\nlanguages are most difficult. None of the models we studied performs well on\nMultiLoKo, as indicated by low average scores as well as large differences\nbetween the best and worst scoring languages. Furthermore, we find a\nsubstantial effect of the question language, indicating sub-optimal knowledge\ntransfer between languages. Lastly, we find that using local vs\nEnglish-translated data can result in differences more than 20 points for the\nbest performing models, drastically change the estimated difficulty of some\nlanguages. For using machines instead of human translations, we find a weaker\neffect on ordering of language difficulty, a larger difference in model\nrankings, and a substantial drop in estimated performance for all models."
                },
                "authors": [
                    {
                        "name": "Dieuwke Hupkes"
                    },
                    {
                        "name": "Nikolay Bogoychev"
                    }
                ],
                "author_detail": {
                    "name": "Nikolay Bogoychev"
                },
                "author": "Nikolay Bogoychev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08696v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08696v2",
                "updated": "2025-04-14T16:02:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    2,
                    38,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-11T17:03:58Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    3,
                    58,
                    4,
                    101,
                    0
                ],
                "title": "SeaView: Software Engineering Agent Visual Interface for Enhanced\n  Workflow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeaView: Software Engineering Agent Visual Interface for Enhanced\n  Workflow"
                },
                "summary": "Auto-regressive LLM-based software engineering (SWE) agents, henceforth SWE\nagents, have made tremendous progress (>60% on SWE-Bench Verified) on\nreal-world coding challenges including GitHub issue resolution. SWE agents use\na combination of reasoning, environment interaction and self-reflection to\nresolve issues thereby generating \"trajectories\". Analysis of SWE agent\ntrajectories is difficult, not only as they exceed LLM sequence length\n(sometimes, greater than 128k) but also because it involves a relatively\nprolonged interaction between an LLM and the environment managed by the agent.\nIn case of an agent error, it can be hard to decipher, locate and understand\nits scope. Similarly, it can be hard to track improvements or regression over\nmultiple runs or experiments. While a lot of research has gone into making\nthese SWE agents reach state-of-the-art, much less focus has been put into\ncreating tools to help analyze and visualize agent output. We propose a novel\ntool called SeaView: Software Engineering Agent Visual Interface for Enhanced\nWorkflow, with a vision to assist SWE-agent researchers to visualize and\ninspect their experiments. SeaView's novel mechanisms help compare experimental\nruns with varying hyper-parameters or LLMs, and quickly get an understanding of\nLLM or environment related problems. Based on our user study, experienced\nresearchers spend between 10 and 30 minutes to gather the information provided\nby SeaView, while researchers with little experience can spend between 30\nminutes to 1 hour to diagnose their experiment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive LLM-based software engineering (SWE) agents, henceforth SWE\nagents, have made tremendous progress (>60% on SWE-Bench Verified) on\nreal-world coding challenges including GitHub issue resolution. SWE agents use\na combination of reasoning, environment interaction and self-reflection to\nresolve issues thereby generating \"trajectories\". Analysis of SWE agent\ntrajectories is difficult, not only as they exceed LLM sequence length\n(sometimes, greater than 128k) but also because it involves a relatively\nprolonged interaction between an LLM and the environment managed by the agent.\nIn case of an agent error, it can be hard to decipher, locate and understand\nits scope. Similarly, it can be hard to track improvements or regression over\nmultiple runs or experiments. While a lot of research has gone into making\nthese SWE agents reach state-of-the-art, much less focus has been put into\ncreating tools to help analyze and visualize agent output. We propose a novel\ntool called SeaView: Software Engineering Agent Visual Interface for Enhanced\nWorkflow, with a vision to assist SWE-agent researchers to visualize and\ninspect their experiments. SeaView's novel mechanisms help compare experimental\nruns with varying hyper-parameters or LLMs, and quickly get an understanding of\nLLM or environment related problems. Based on our user study, experienced\nresearchers spend between 10 and 30 minutes to gather the information provided\nby SeaView, while researchers with little experience can spend between 30\nminutes to 1 hour to diagnose their experiment."
                },
                "authors": [
                    {
                        "name": "Timothy Bula"
                    },
                    {
                        "name": "Saurabh Pujar"
                    },
                    {
                        "name": "Luca Buratti"
                    },
                    {
                        "name": "Mihaela Bornea"
                    },
                    {
                        "name": "Avirup Sil"
                    }
                ],
                "author_detail": {
                    "name": "Avirup Sil"
                },
                "author": "Avirup Sil",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08696v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08696v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12044v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12044v2",
                "updated": "2025-04-14T16:02:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    16,
                    2,
                    15,
                    0,
                    104,
                    0
                ],
                "published": "2024-11-18T20:31:38Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    20,
                    31,
                    38,
                    0,
                    323,
                    0
                ],
                "title": "ITACLIP: Boosting Training-Free Semantic Segmentation with Image, Text,\n  and Architectural Enhancements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ITACLIP: Boosting Training-Free Semantic Segmentation with Image, Text,\n  and Architectural Enhancements"
                },
                "summary": "Recent advances in foundational Vision Language Models (VLMs) have reshaped\nthe evaluation paradigm in computer vision tasks. These foundational models,\nespecially CLIP, have accelerated research in open-vocabulary computer vision\ntasks, including Open-Vocabulary Semantic Segmentation (OVSS). Although the\ninitial results are promising, the dense prediction capabilities of VLMs still\nrequire further improvement. In this study, we enhance the semantic\nsegmentation performance of CLIP by introducing new modules and modifications:\n1) architectural changes in the last layer of ViT and the incorporation of\nattention maps from the middle layers with the last layer, 2) Image\nEngineering: applying data augmentations to enrich input image representations,\nand 3) using Large Language Models (LLMs) to generate definitions and synonyms\nfor each class name to leverage CLIP's open-vocabulary capabilities. Our\ntraining-free method, ITACLIP, outperforms current state-of-the-art approaches\non segmentation benchmarks such as COCO-Stuff, COCO-Object, Pascal Context, and\nPascal VOC. Our code is available at https://github.com/m-arda-aydn/ITACLIP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in foundational Vision Language Models (VLMs) have reshaped\nthe evaluation paradigm in computer vision tasks. These foundational models,\nespecially CLIP, have accelerated research in open-vocabulary computer vision\ntasks, including Open-Vocabulary Semantic Segmentation (OVSS). Although the\ninitial results are promising, the dense prediction capabilities of VLMs still\nrequire further improvement. In this study, we enhance the semantic\nsegmentation performance of CLIP by introducing new modules and modifications:\n1) architectural changes in the last layer of ViT and the incorporation of\nattention maps from the middle layers with the last layer, 2) Image\nEngineering: applying data augmentations to enrich input image representations,\nand 3) using Large Language Models (LLMs) to generate definitions and synonyms\nfor each class name to leverage CLIP's open-vocabulary capabilities. Our\ntraining-free method, ITACLIP, outperforms current state-of-the-art approaches\non segmentation benchmarks such as COCO-Stuff, COCO-Object, Pascal Context, and\nPascal VOC. Our code is available at https://github.com/m-arda-aydn/ITACLIP."
                },
                "authors": [
                    {
                        "name": "M. Arda Aydn"
                    },
                    {
                        "name": "Efe Mert rpar"
                    },
                    {
                        "name": "Elvin Abdinli"
                    },
                    {
                        "name": "Gozde Unal"
                    },
                    {
                        "name": "Yusuf H. Sahin"
                    }
                ],
                "author_detail": {
                    "name": "Yusuf H. Sahin"
                },
                "author": "Yusuf H. Sahin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12044v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12044v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15199v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15199v2",
                "updated": "2025-04-14T15:52:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    52,
                    38,
                    0,
                    104,
                    0
                ],
                "published": "2025-03-19T13:38:25Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    13,
                    38,
                    25,
                    2,
                    78,
                    0
                ],
                "title": "Radon: a Programming Model and Platform for Computing Continuum Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radon: a Programming Model and Platform for Computing Continuum Systems"
                },
                "summary": "Emerging compute continuum environments pose new challenges that traditional\ncloud-centric architectures struggle to address. Latency, bandwidth\nconstraints, and the heterogeneity of edge environments hinder the efficiency\nof centralized cloud solutions. While major cloud providers extend their\nplatforms to the edge, these approaches often overlook its unique\ncharacteristics, limiting its potential.\n  To tackle these challenges, we introduce Radon, a flexible programming model\nand platform designed for the edge-to-cloud continuum. Radon applications are\nstructured as atoms, isolated stateful entities that communicate through\nmessaging and can be composed into complex systems. The Radon runtime, based on\nWebAssembly (WASM), enables language- and deployment-independent execution,\nensuring portability and adaptability across heterogeneous environments. This\ndecoupling allows developers to focus on application logic while the runtime\noptimizes for diverse infrastructure conditions.\n  We present a prototype implementation of Radon and evaluate its effectiveness\nthrough a distributed key-value store case study. We analyze the implementation\nin terms of code complexity and performance. Our results demonstrate that Radon\nfacilitates the development and operation of scalable applications across the\nedge-to-cloud continuum advancing the current state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging compute continuum environments pose new challenges that traditional\ncloud-centric architectures struggle to address. Latency, bandwidth\nconstraints, and the heterogeneity of edge environments hinder the efficiency\nof centralized cloud solutions. While major cloud providers extend their\nplatforms to the edge, these approaches often overlook its unique\ncharacteristics, limiting its potential.\n  To tackle these challenges, we introduce Radon, a flexible programming model\nand platform designed for the edge-to-cloud continuum. Radon applications are\nstructured as atoms, isolated stateful entities that communicate through\nmessaging and can be composed into complex systems. The Radon runtime, based on\nWebAssembly (WASM), enables language- and deployment-independent execution,\nensuring portability and adaptability across heterogeneous environments. This\ndecoupling allows developers to focus on application logic while the runtime\noptimizes for diverse infrastructure conditions.\n  We present a prototype implementation of Radon and evaluate its effectiveness\nthrough a distributed key-value store case study. We analyze the implementation\nin terms of code complexity and performance. Our results demonstrate that Radon\nfacilitates the development and operation of scalable applications across the\nedge-to-cloud continuum advancing the current state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Luca De Martini"
                    },
                    {
                        "name": "Dario d'Abate"
                    },
                    {
                        "name": "Alessandro Margara"
                    },
                    {
                        "name": "Gianpaolo Cugola"
                    }
                ],
                "author_detail": {
                    "name": "Gianpaolo Cugola"
                },
                "author": "Gianpaolo Cugola",
                "arxiv_comment": "Submitted to EDCCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15199v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15199v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10340v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10340v1",
                "updated": "2025-04-14T15:48:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    48,
                    56,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:48:56Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    48,
                    56,
                    0,
                    104,
                    0
                ],
                "title": "Forecasting from Clinical Textual Time Series: Adaptations of the\n  Encoder and Decoder Language Model Families",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting from Clinical Textual Time Series: Adaptations of the\n  Encoder and Decoder Language Model Families"
                },
                "summary": "Clinical case reports encode rich, temporal patient trajectories that are\noften underexploited by traditional machine learning methods relying on\nstructured data. In this work, we introduce the forecasting problem from\ntextual time series, where timestamped clinical findings--extracted via an\nLLM-assisted annotation pipeline--serve as the primary input for prediction. We\nsystematically evaluate a diverse suite of models, including fine-tuned\ndecoder-based large language models and encoder-based transformers, on tasks of\nevent occurrence prediction, temporal ordering, and survival analysis. Our\nexperiments reveal that encoder-based models consistently achieve higher F1\nscores and superior temporal concordance for short- and long-horizon event\nforecasting, while fine-tuned masking approaches enhance ranking performance.\nIn contrast, instruction-tuned decoder models demonstrate a relative advantage\nin survival analysis, especially in early prognosis settings. Our sensitivity\nanalyses further demonstrate the importance of time ordering, which requires\nclinical time series construction, as compared to text ordering, the format of\nthe text inputs that LLMs are classically trained on. This highlights the\nadditional benefit that can be ascertained from time-ordered corpora, with\nimplications for temporal tasks in the era of widespread LLM use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical case reports encode rich, temporal patient trajectories that are\noften underexploited by traditional machine learning methods relying on\nstructured data. In this work, we introduce the forecasting problem from\ntextual time series, where timestamped clinical findings--extracted via an\nLLM-assisted annotation pipeline--serve as the primary input for prediction. We\nsystematically evaluate a diverse suite of models, including fine-tuned\ndecoder-based large language models and encoder-based transformers, on tasks of\nevent occurrence prediction, temporal ordering, and survival analysis. Our\nexperiments reveal that encoder-based models consistently achieve higher F1\nscores and superior temporal concordance for short- and long-horizon event\nforecasting, while fine-tuned masking approaches enhance ranking performance.\nIn contrast, instruction-tuned decoder models demonstrate a relative advantage\nin survival analysis, especially in early prognosis settings. Our sensitivity\nanalyses further demonstrate the importance of time ordering, which requires\nclinical time series construction, as compared to text ordering, the format of\nthe text inputs that LLMs are classically trained on. This highlights the\nadditional benefit that can be ascertained from time-ordered corpora, with\nimplications for temporal tasks in the era of widespread LLM use."
                },
                "authors": [
                    {
                        "name": "Shahriar Noroozizadeh"
                    },
                    {
                        "name": "Sayantan Kumar"
                    },
                    {
                        "name": "Jeremy C. Weiss"
                    }
                ],
                "author_detail": {
                    "name": "Jeremy C. Weiss"
                },
                "author": "Jeremy C. Weiss",
                "arxiv_comment": "Machine Learning for Healthcare (MLHC 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10340v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10340v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10337v1",
                "updated": "2025-04-14T15:46:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    46,
                    33,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:46:33Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    46,
                    33,
                    0,
                    104,
                    0
                ],
                "title": "Heimdall: test-time scaling on the generative verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heimdall: test-time scaling on the generative verification"
                },
                "summary": "An AI system can create and maintain knowledge only to the extent that it can\nverify that knowledge itself. Recent work on long Chain-of-Thought reasoning\nhas demonstrated great potential of LLMs on solving competitive problems, but\ntheir verification ability remains to be weak and not sufficiently\ninvestigated. In this paper, we propose Heimdall, the long CoT verification LLM\nthat can accurately judge the correctness of solutions. With pure reinforcement\nlearning, we boost the verification accuracy from 62.5% to 94.5% on competitive\nmath problems. By scaling with repeated sampling, the accuracy further\nincreases to 97.5%. Through human evaluation, Heimdall demonstrates impressive\ngeneralization capabilities, successfully detecting most issues in challenging\nmath proofs, the type of which is not included during training. Furthermore, we\npropose Pessimistic Verification to extend the functionality of Heimdall to\nscaling up the problem solving. It calls Heimdall to judge the solutions from a\nsolver model and based on the pessimistic principle, selects the most likely\ncorrect solution with the least uncertainty. Taking\nDeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification\nimproves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute\nbudget and to 83.3% with more compute budget. With the stronger solver Gemini\n2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge\ndiscovery system, a ternary system where one poses questions, another provides\nsolutions, and the third verifies the solutions. Using the data synthesis work\nNuminaMath for the first two components, Heimdall effectively identifies\nproblematic records within the dataset and reveals that nearly half of the data\nis flawed, which interestingly aligns with the recent ablation studies from\nNuminaMath.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An AI system can create and maintain knowledge only to the extent that it can\nverify that knowledge itself. Recent work on long Chain-of-Thought reasoning\nhas demonstrated great potential of LLMs on solving competitive problems, but\ntheir verification ability remains to be weak and not sufficiently\ninvestigated. In this paper, we propose Heimdall, the long CoT verification LLM\nthat can accurately judge the correctness of solutions. With pure reinforcement\nlearning, we boost the verification accuracy from 62.5% to 94.5% on competitive\nmath problems. By scaling with repeated sampling, the accuracy further\nincreases to 97.5%. Through human evaluation, Heimdall demonstrates impressive\ngeneralization capabilities, successfully detecting most issues in challenging\nmath proofs, the type of which is not included during training. Furthermore, we\npropose Pessimistic Verification to extend the functionality of Heimdall to\nscaling up the problem solving. It calls Heimdall to judge the solutions from a\nsolver model and based on the pessimistic principle, selects the most likely\ncorrect solution with the least uncertainty. Taking\nDeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification\nimproves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute\nbudget and to 83.3% with more compute budget. With the stronger solver Gemini\n2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge\ndiscovery system, a ternary system where one poses questions, another provides\nsolutions, and the third verifies the solutions. Using the data synthesis work\nNuminaMath for the first two components, Heimdall effectively identifies\nproblematic records within the dataset and reveals that nearly half of the data\nis flawed, which interestingly aligns with the recent ablation studies from\nNuminaMath."
                },
                "authors": [
                    {
                        "name": "Wenlei Shi"
                    },
                    {
                        "name": "Xing Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xing Jin"
                },
                "author": "Xing Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10335v1",
                "updated": "2025-04-14T15:44:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    44,
                    45,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:44:45Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    44,
                    45,
                    0,
                    104,
                    0
                ],
                "title": "MorphTok: Morphologically Grounded Tokenization for Indian Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MorphTok: Morphologically Grounded Tokenization for Indian Languages"
                },
                "summary": "Tokenization is a crucial step in NLP, especially with the rise of large\nlanguage models (LLMs), impacting downstream performance, computational cost,\nand efficiency. Existing LLMs rely on the classical Byte-pair Encoding (BPE)\nalgorithm for subword tokenization that greedily merges frequent character\nbigrams. This often leads to segmentation that does not align with\nlinguistically meaningful units. To address this, we propose morphology-aware\nsegmentation as a pre-tokenization step prior to applying BPE. To facilitate\nmorphology-aware segmentation, we create a novel dataset for Hindi and Marathi,\nincorporating sandhi splitting to enhance the subword tokenization. Experiments\non downstream tasks show that morphologically grounded tokenization improves\nperformance for machine translation and language modeling. Additionally, to\nhandle the ambiguity in the Unicode characters for diacritics, particularly\ndependent vowels in syllable-based writing systems, we introduce Constrained\nBPE (CBPE), an extension to the traditional BPE algorithm that incorporates\nscript-specific constraints. Specifically, CBPE handles dependent vowels. Our\nresults show that CBPE achieves a 1.68\\% reduction in fertility scores while\nmaintaining comparable or improved downstream performance in machine\ntranslation, offering a computationally efficient alternative to standard BPE.\nMoreover, to evaluate segmentation across different tokenization algorithms, we\nintroduce a new human evaluation metric, \\textit{EvalTok}, enabling more\nhuman-grounded assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization is a crucial step in NLP, especially with the rise of large\nlanguage models (LLMs), impacting downstream performance, computational cost,\nand efficiency. Existing LLMs rely on the classical Byte-pair Encoding (BPE)\nalgorithm for subword tokenization that greedily merges frequent character\nbigrams. This often leads to segmentation that does not align with\nlinguistically meaningful units. To address this, we propose morphology-aware\nsegmentation as a pre-tokenization step prior to applying BPE. To facilitate\nmorphology-aware segmentation, we create a novel dataset for Hindi and Marathi,\nincorporating sandhi splitting to enhance the subword tokenization. Experiments\non downstream tasks show that morphologically grounded tokenization improves\nperformance for machine translation and language modeling. Additionally, to\nhandle the ambiguity in the Unicode characters for diacritics, particularly\ndependent vowels in syllable-based writing systems, we introduce Constrained\nBPE (CBPE), an extension to the traditional BPE algorithm that incorporates\nscript-specific constraints. Specifically, CBPE handles dependent vowels. Our\nresults show that CBPE achieves a 1.68\\% reduction in fertility scores while\nmaintaining comparable or improved downstream performance in machine\ntranslation, offering a computationally efficient alternative to standard BPE.\nMoreover, to evaluate segmentation across different tokenization algorithms, we\nintroduce a new human evaluation metric, \\textit{EvalTok}, enabling more\nhuman-grounded assessment."
                },
                "authors": [
                    {
                        "name": "Maharaj Brahma"
                    },
                    {
                        "name": "N J Karthika"
                    },
                    {
                        "name": "Atul Singh"
                    },
                    {
                        "name": "Devaraj Adiga"
                    },
                    {
                        "name": "Smruti Bhate"
                    },
                    {
                        "name": "Ganesh Ramakrishnan"
                    },
                    {
                        "name": "Rohit Saluja"
                    },
                    {
                        "name": "Maunendra Sankar Desarkar"
                    }
                ],
                "author_detail": {
                    "name": "Maunendra Sankar Desarkar"
                },
                "author": "Maunendra Sankar Desarkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10326v1",
                "updated": "2025-04-14T15:34:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    34,
                    26,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:34:26Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    34,
                    26,
                    0,
                    104,
                    0
                ],
                "title": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference"
                },
                "summary": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks."
                },
                "authors": [
                    {
                        "name": "Yangshen Deng"
                    },
                    {
                        "name": "Zhengxin You"
                    },
                    {
                        "name": "Long Xiang"
                    },
                    {
                        "name": "Qilong Li"
                    },
                    {
                        "name": "Peiqi Yuan"
                    },
                    {
                        "name": "Zhaoyang Hong"
                    },
                    {
                        "name": "Yitao Zheng"
                    },
                    {
                        "name": "Wanting Li"
                    },
                    {
                        "name": "Runzhong Li"
                    },
                    {
                        "name": "Haotian Liu"
                    },
                    {
                        "name": "Kyriakos Mouratidis"
                    },
                    {
                        "name": "Man Lung Yiu"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Qiaomu Shen"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Bo Tang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Tang"
                },
                "author": "Bo Tang",
                "arxiv_comment": "14 pages, 12 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.1; H.3.2; H.3.3; H.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10320v1",
                "updated": "2025-04-14T15:30:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    30,
                    3,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:30:03Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    30,
                    3,
                    0,
                    104,
                    0
                ],
                "title": "SlowFastVAD: Video Anomaly Detection via Integrating Simple Detector and\n  RAG-Enhanced Vision-Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlowFastVAD: Video Anomaly Detection via Integrating Simple Detector and\n  RAG-Enhanced Vision-Language Model"
                },
                "summary": "Video anomaly detection (VAD) aims to identify unexpected events in videos\nand has wide applications in safety-critical domains. While semi-supervised\nmethods trained on only normal samples have gained traction, they often suffer\nfrom high false alarm rates and poor interpretability. Recently,\nvision-language models (VLMs) have demonstrated strong multimodal reasoning\ncapabilities, offering new opportunities for explainable anomaly detection.\nHowever, their high computational cost and lack of domain adaptation hinder\nreal-time deployment and reliability. Inspired by dual complementary pathways\nin human visual perception, we propose SlowFastVAD, a hybrid framework that\nintegrates a fast anomaly detector with a slow anomaly detector (namely a\nretrieval augmented generation (RAG) enhanced VLM), to address these\nlimitations. Specifically, the fast detector first provides coarse anomaly\nconfidence scores, and only a small subset of ambiguous segments, rather than\nthe entire video, is further analyzed by the slower yet more interpretable VLM\nfor elaborate detection and reasoning. Furthermore, to adapt VLMs to\ndomain-specific VAD scenarios, we construct a knowledge base including normal\npatterns based on few normal samples and abnormal patterns inferred by VLMs.\nDuring inference, relevant patterns are retrieved and used to augment prompts\nfor anomaly reasoning. Finally, we smoothly fuse the anomaly confidence of fast\nand slow detectors to enhance robustness of anomaly detection. Extensive\nexperiments on four benchmarks demonstrate that SlowFastVAD effectively\ncombines the strengths of both fast and slow detectors, and achieves remarkable\ndetection accuracy and interpretability with significantly reduced\ncomputational overhead, making it well-suited for real-world VAD applications\nwith high reliability requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video anomaly detection (VAD) aims to identify unexpected events in videos\nand has wide applications in safety-critical domains. While semi-supervised\nmethods trained on only normal samples have gained traction, they often suffer\nfrom high false alarm rates and poor interpretability. Recently,\nvision-language models (VLMs) have demonstrated strong multimodal reasoning\ncapabilities, offering new opportunities for explainable anomaly detection.\nHowever, their high computational cost and lack of domain adaptation hinder\nreal-time deployment and reliability. Inspired by dual complementary pathways\nin human visual perception, we propose SlowFastVAD, a hybrid framework that\nintegrates a fast anomaly detector with a slow anomaly detector (namely a\nretrieval augmented generation (RAG) enhanced VLM), to address these\nlimitations. Specifically, the fast detector first provides coarse anomaly\nconfidence scores, and only a small subset of ambiguous segments, rather than\nthe entire video, is further analyzed by the slower yet more interpretable VLM\nfor elaborate detection and reasoning. Furthermore, to adapt VLMs to\ndomain-specific VAD scenarios, we construct a knowledge base including normal\npatterns based on few normal samples and abnormal patterns inferred by VLMs.\nDuring inference, relevant patterns are retrieved and used to augment prompts\nfor anomaly reasoning. Finally, we smoothly fuse the anomaly confidence of fast\nand slow detectors to enhance robustness of anomaly detection. Extensive\nexperiments on four benchmarks demonstrate that SlowFastVAD effectively\ncombines the strengths of both fast and slow detectors, and achieves remarkable\ndetection accuracy and interpretability with significantly reduced\ncomputational overhead, making it well-suited for real-world VAD applications\nwith high reliability requirements."
                },
                "authors": [
                    {
                        "name": "Zongcan Ding"
                    },
                    {
                        "name": "Haodong Zhang"
                    },
                    {
                        "name": "Peng Wu"
                    },
                    {
                        "name": "Guansong Pang"
                    },
                    {
                        "name": "Zhiwei Yang"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Yanning Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yanning Zhang"
                },
                "author": "Yanning Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12110v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12110v4",
                "updated": "2025-04-14T15:21:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    21,
                    49,
                    0,
                    104,
                    0
                ],
                "published": "2025-02-17T18:36:14Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    18,
                    36,
                    14,
                    0,
                    48,
                    0
                ],
                "title": "A-MEM: Agentic Memory for LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A-MEM: Agentic Memory for LLM Agents"
                },
                "summary": "While large language model (LLM) agents can effectively use external tools\nfor complex real-world tasks, they require memory systems to leverage\nhistorical experiences. Current memory systems enable basic storage and\nretrieval but lack sophisticated memory organization, despite recent attempts\nto incorporate graph databases. Moreover, these systems' fixed operations and\nstructures limit their adaptability across diverse tasks. To address this\nlimitation, this paper proposes a novel agentic memory system for LLM agents\nthat can dynamically organize memories in an agentic way. Following the basic\nprinciples of the Zettelkasten method, we designed our memory system to create\ninterconnected knowledge networks through dynamic indexing and linking. When a\nnew memory is added, we generate a comprehensive note containing multiple\nstructured attributes, including contextual descriptions, keywords, and tags.\nThe system then analyzes historical memories to identify relevant connections,\nestablishing links where meaningful similarities exist. Additionally, this\nprocess enables memory evolution - as new memories are integrated, they can\ntrigger updates to the contextual representations and attributes of existing\nhistorical memories, allowing the memory network to continuously refine its\nunderstanding. Our approach combines the structured organization principles of\nZettelkasten with the flexibility of agent-driven decision making, allowing for\nmore adaptive and context-aware memory management. Empirical experiments on six\nfoundation models show superior improvement against existing SOTA baselines.\nThe source code for evaluating performance is available at\nhttps://github.com/WujiangXu/AgenticMemory, while the source code of agentic\nmemory system is available at https://github.com/agiresearch/A-mem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language model (LLM) agents can effectively use external tools\nfor complex real-world tasks, they require memory systems to leverage\nhistorical experiences. Current memory systems enable basic storage and\nretrieval but lack sophisticated memory organization, despite recent attempts\nto incorporate graph databases. Moreover, these systems' fixed operations and\nstructures limit their adaptability across diverse tasks. To address this\nlimitation, this paper proposes a novel agentic memory system for LLM agents\nthat can dynamically organize memories in an agentic way. Following the basic\nprinciples of the Zettelkasten method, we designed our memory system to create\ninterconnected knowledge networks through dynamic indexing and linking. When a\nnew memory is added, we generate a comprehensive note containing multiple\nstructured attributes, including contextual descriptions, keywords, and tags.\nThe system then analyzes historical memories to identify relevant connections,\nestablishing links where meaningful similarities exist. Additionally, this\nprocess enables memory evolution - as new memories are integrated, they can\ntrigger updates to the contextual representations and attributes of existing\nhistorical memories, allowing the memory network to continuously refine its\nunderstanding. Our approach combines the structured organization principles of\nZettelkasten with the flexibility of agent-driven decision making, allowing for\nmore adaptive and context-aware memory management. Empirical experiments on six\nfoundation models show superior improvement against existing SOTA baselines.\nThe source code for evaluating performance is available at\nhttps://github.com/WujiangXu/AgenticMemory, while the source code of agentic\nmemory system is available at https://github.com/agiresearch/A-mem."
                },
                "authors": [
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Zujie Liang"
                    },
                    {
                        "name": "Kai Mei"
                    },
                    {
                        "name": "Hang Gao"
                    },
                    {
                        "name": "Juntao Tan"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12110v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12110v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10309v1",
                "updated": "2025-04-14T15:18:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    18,
                    59,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:18:59Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    18,
                    59,
                    0,
                    104,
                    0
                ],
                "title": "AutoStyle-TTS: Retrieval-Augmented Generation based Automatic Style\n  Matching Text-to-Speech Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoStyle-TTS: Retrieval-Augmented Generation based Automatic Style\n  Matching Text-to-Speech Synthesis"
                },
                "summary": "With the advancement of speech synthesis technology, users have higher\nexpectations for the naturalness and expressiveness of synthesized speech. But\nprevious research ignores the importance of prompt selection. This study\nproposes a text-to-speech (TTS) framework based on Retrieval-Augmented\nGeneration (RAG) technology, which can dynamically adjust the speech style\naccording to the text content to achieve more natural and vivid communication\neffects. We have constructed a speech style knowledge database containing\nhigh-quality speech samples in various contexts and developed a style matching\nscheme. This scheme uses embeddings, extracted by Llama, PER-LLM-Embedder,and\nMoka, to match with samples in the knowledge database, selecting the most\nappropriate speech style for synthesis. Furthermore, our empirical research\nvalidates the effectiveness of the proposed method. Our demo can be viewed at:\nhttps://thuhcsi.github.io/icme2025-AutoStyle-TTS",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancement of speech synthesis technology, users have higher\nexpectations for the naturalness and expressiveness of synthesized speech. But\nprevious research ignores the importance of prompt selection. This study\nproposes a text-to-speech (TTS) framework based on Retrieval-Augmented\nGeneration (RAG) technology, which can dynamically adjust the speech style\naccording to the text content to achieve more natural and vivid communication\neffects. We have constructed a speech style knowledge database containing\nhigh-quality speech samples in various contexts and developed a style matching\nscheme. This scheme uses embeddings, extracted by Llama, PER-LLM-Embedder,and\nMoka, to match with samples in the knowledge database, selecting the most\nappropriate speech style for synthesis. Furthermore, our empirical research\nvalidates the effectiveness of the proposed method. Our demo can be viewed at:\nhttps://thuhcsi.github.io/icme2025-AutoStyle-TTS"
                },
                "authors": [
                    {
                        "name": "Dan Luo"
                    },
                    {
                        "name": "Chengyuan Ma"
                    },
                    {
                        "name": "Weiqin Li"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Zhiyong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Wu"
                },
                "author": "Zhiyong Wu",
                "arxiv_comment": "accepted by ICME25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07615v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07615v2",
                "updated": "2025-04-14T15:15:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    15,
                    54,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-10T10:05:15Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    5,
                    15,
                    3,
                    100,
                    0
                ],
                "title": "VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model"
                },
                "summary": "Recently DeepSeek R1 has shown that reinforcement learning (RL) can\nsubstantially improve the reasoning capabilities of Large Language Models\n(LLMs) through a simple yet effective design. The core of R1 lies in its\nrule-based reward formulation, which leverages tasks with deterministic\nground-truth answers to enable precise and stable reward computation. In the\nvisual domain, we similarly observe that a wide range of visual understanding\ntasks are inherently equipped with well-defined ground-truth annotations. This\nproperty makes them naturally compatible with rule-based reward mechanisms.\nMotivated by this observation, we investigate the extension of R1-style\nreinforcement learning to Vision-Language Models (VLMs), aiming to enhance\ntheir visual reasoning capabilities. To this end, we develop VLM-R1, a\ndedicated framework designed to harness RL for improving VLMs' performance on\ngeneral vision-language tasks. Using this framework, we further explore the\nfeasibility of applying RL to visual domain. Experimental results indicate that\nthe RL-based model not only delivers competitive performance on visual\nunderstanding tasks but also surpasses Supervised Fine-Tuning (SFT) in\ngeneralization ability. Furthermore, we conduct comprehensive ablation studies\nthat uncover a series of noteworthy insights, including the presence of reward\nhacking in object detection, the emergence of the \"OD aha moment\", the impact\nof training data quality, and the scaling behavior of RL across different model\nsizes. Through these analyses, we aim to deepen the understanding of how\nreinforcement learning enhances the capabilities of vision-language models, and\nwe hope our findings and open-source contributions will support continued\nprogress in the vision-language RL community. Our code and model are available\nat https://github.com/om-ai-lab/VLM-R1",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently DeepSeek R1 has shown that reinforcement learning (RL) can\nsubstantially improve the reasoning capabilities of Large Language Models\n(LLMs) through a simple yet effective design. The core of R1 lies in its\nrule-based reward formulation, which leverages tasks with deterministic\nground-truth answers to enable precise and stable reward computation. In the\nvisual domain, we similarly observe that a wide range of visual understanding\ntasks are inherently equipped with well-defined ground-truth annotations. This\nproperty makes them naturally compatible with rule-based reward mechanisms.\nMotivated by this observation, we investigate the extension of R1-style\nreinforcement learning to Vision-Language Models (VLMs), aiming to enhance\ntheir visual reasoning capabilities. To this end, we develop VLM-R1, a\ndedicated framework designed to harness RL for improving VLMs' performance on\ngeneral vision-language tasks. Using this framework, we further explore the\nfeasibility of applying RL to visual domain. Experimental results indicate that\nthe RL-based model not only delivers competitive performance on visual\nunderstanding tasks but also surpasses Supervised Fine-Tuning (SFT) in\ngeneralization ability. Furthermore, we conduct comprehensive ablation studies\nthat uncover a series of noteworthy insights, including the presence of reward\nhacking in object detection, the emergence of the \"OD aha moment\", the impact\nof training data quality, and the scaling behavior of RL across different model\nsizes. Through these analyses, we aim to deepen the understanding of how\nreinforcement learning enhances the capabilities of vision-language models, and\nwe hope our findings and open-source contributions will support continued\nprogress in the vision-language RL community. Our code and model are available\nat https://github.com/om-ai-lab/VLM-R1"
                },
                "authors": [
                    {
                        "name": "Haozhan Shen"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Jingcheng Li"
                    },
                    {
                        "name": "Chunxin Fang"
                    },
                    {
                        "name": "Yibo Ma"
                    },
                    {
                        "name": "Jiajia Liao"
                    },
                    {
                        "name": "Qiaoli Shen"
                    },
                    {
                        "name": "Zilun Zhang"
                    },
                    {
                        "name": "Kangjia Zhao"
                    },
                    {
                        "name": "Qianqian Zhang"
                    },
                    {
                        "name": "Ruochen Xu"
                    },
                    {
                        "name": "Tiancheng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tiancheng Zhao"
                },
                "author": "Tiancheng Zhao",
                "arxiv_comment": "11 pages, fix some minor typos in the previous version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07615v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07615v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03338v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03338v2",
                "updated": "2025-04-14T15:12:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    12,
                    17,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-04T10:42:56Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    10,
                    42,
                    56,
                    4,
                    94,
                    0
                ],
                "title": "BabyLM's First Words: Word Segmentation as a Phonological Probing Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BabyLM's First Words: Word Segmentation as a Phonological Probing Task"
                },
                "summary": "Language models provide a key framework for studying linguistic theories\nbased on prediction, but phonological analysis using large language models\n(LLMs) is difficult; there are few phonological benchmarks beyond English and\nthe standard input representation used in LLMs (subwords of graphemes) is not\nsuitable for analyzing the representation of phonemes. In this work, we\ndemonstrate how word segmentation can be used as a phonological probing task,\nallowing us to study the representations learned by phoneme-based language\nmodels trained on child-directed speech across 31 languages. Following\ncomputational models of word segmentation, we present unsupervised methods for\nextracting word boundaries from a trained model using the observation that\nprediction-error peaks at the start of words. We also use linear probes to\nidentify that these models implicitly track word boundaries, even when they do\nnot appear in training. This cross-lingual work corroborates statistical\nlearning theories of acquisition and empirically motivates new methods for\ntraining subword tokenizers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models provide a key framework for studying linguistic theories\nbased on prediction, but phonological analysis using large language models\n(LLMs) is difficult; there are few phonological benchmarks beyond English and\nthe standard input representation used in LLMs (subwords of graphemes) is not\nsuitable for analyzing the representation of phonemes. In this work, we\ndemonstrate how word segmentation can be used as a phonological probing task,\nallowing us to study the representations learned by phoneme-based language\nmodels trained on child-directed speech across 31 languages. Following\ncomputational models of word segmentation, we present unsupervised methods for\nextracting word boundaries from a trained model using the observation that\nprediction-error peaks at the start of words. We also use linear probes to\nidentify that these models implicitly track word boundaries, even when they do\nnot appear in training. This cross-lingual work corroborates statistical\nlearning theories of acquisition and empirically motivates new methods for\ntraining subword tokenizers."
                },
                "authors": [
                    {
                        "name": "Zbulon Goriely"
                    },
                    {
                        "name": "Paula Buttery"
                    }
                ],
                "author_detail": {
                    "name": "Paula Buttery"
                },
                "author": "Paula Buttery",
                "arxiv_comment": "17 pages, 10 figures, submitted to CoNLL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03338v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03338v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13843v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13843v2",
                "updated": "2025-04-14T15:10:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    10,
                    31,
                    0,
                    104,
                    0
                ],
                "published": "2024-03-17T17:45:04Z",
                "published_parsed": [
                    2024,
                    3,
                    17,
                    17,
                    45,
                    4,
                    6,
                    77,
                    0
                ],
                "title": "Machine Learning and Transformers for Thyroid Carcinoma Diagnosis: A\n  Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Learning and Transformers for Thyroid Carcinoma Diagnosis: A\n  Review"
                },
                "summary": "The growing interest in developing smart diagnostic systems to help medical\nexperts process extensive data for treating incurable diseases has been\nnotable. In particular, the challenge of identifying thyroid cancer (TC) has\nseen progress with the use of machine learning (ML) and big data analysis,\nincorporating Transformers to evaluate TC prognosis and determine the risk of\nmalignancy in individuals. This review article presents a summary of various\nstudies on AI-based approaches, especially those employing Transformers, for\ndiagnosing TC. It introduces a new categorization system for these methods\nbased on artificial intelligence (AI) algorithms, the goals of the framework,\nand the computing environments used. Additionally, it scrutinizes and contrasts\nthe available TC datasets by their features. The paper highlights the\nimportance of AI instruments in aiding the diagnosis and treatment of TC\nthrough supervised, unsupervised, or mixed approaches, with a special focus on\nthe ongoing importance of Transformers and large language models (LLMs) in\nmedical diagnostics and disease management. It further discusses the progress\nmade and the continuing obstacles in this area. Lastly, it explores future\ndirections and focuses within this research field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing interest in developing smart diagnostic systems to help medical\nexperts process extensive data for treating incurable diseases has been\nnotable. In particular, the challenge of identifying thyroid cancer (TC) has\nseen progress with the use of machine learning (ML) and big data analysis,\nincorporating Transformers to evaluate TC prognosis and determine the risk of\nmalignancy in individuals. This review article presents a summary of various\nstudies on AI-based approaches, especially those employing Transformers, for\ndiagnosing TC. It introduces a new categorization system for these methods\nbased on artificial intelligence (AI) algorithms, the goals of the framework,\nand the computing environments used. Additionally, it scrutinizes and contrasts\nthe available TC datasets by their features. The paper highlights the\nimportance of AI instruments in aiding the diagnosis and treatment of TC\nthrough supervised, unsupervised, or mixed approaches, with a special focus on\nthe ongoing importance of Transformers and large language models (LLMs) in\nmedical diagnostics and disease management. It further discusses the progress\nmade and the continuing obstacles in this area. Lastly, it explores future\ndirections and focuses within this research field."
                },
                "authors": [
                    {
                        "name": "Yassine Habchi"
                    },
                    {
                        "name": "Hamza Kheddar"
                    },
                    {
                        "name": "Yassine Himeur"
                    },
                    {
                        "name": "Mohamed Chahine Ghanem"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Chahine Ghanem"
                },
                "author": "Mohamed Chahine Ghanem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13843v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13843v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05168v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05168v4",
                "updated": "2025-04-14T14:58:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    58,
                    1,
                    0,
                    104,
                    0
                ],
                "published": "2024-10-07T16:25:39Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    16,
                    25,
                    39,
                    0,
                    281,
                    0
                ],
                "title": "ReasoningRank: Teaching Student Models to Rank through Reasoning-Based\n  Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReasoningRank: Teaching Student Models to Rank through Reasoning-Based\n  Knowledge Distillation"
                },
                "summary": "Reranking documents based on their relevance to a given query is a critical\ntask in information retrieval. Traditional reranking methods often lack\ntransparency and rely on proprietary models, hindering reproducibility and\ninterpretability. We propose Reason-to-Rank (R2R), a novel open-source\nreranking approach that enhances transparency by generating two types of\nreasoning: direct relevance reasoning, which explains how a document addresses\nthe query, and comparison reasoning, which justifies the relevance of one\ndocument over another. We leverage large language models (LLMs) as teacher\nmodels to generate these explanations and distill this knowledge into smaller,\nopenly available student models. Our student models are trained to generate\nmeaningful reasoning and rerank documents, achieving competitive performance\nacross multiple datasets, including MSMARCO and BRIGHT. Experiments demonstrate\nthat R2R not only improves reranking accuracy but also provides valuable\ninsights into the decision-making process. By offering a structured and\ninterpretable solution with openly accessible resources, R2R aims to bridge the\ngap between effectiveness and transparency in information retrieval, fostering\nreproducibility and further research in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reranking documents based on their relevance to a given query is a critical\ntask in information retrieval. Traditional reranking methods often lack\ntransparency and rely on proprietary models, hindering reproducibility and\ninterpretability. We propose Reason-to-Rank (R2R), a novel open-source\nreranking approach that enhances transparency by generating two types of\nreasoning: direct relevance reasoning, which explains how a document addresses\nthe query, and comparison reasoning, which justifies the relevance of one\ndocument over another. We leverage large language models (LLMs) as teacher\nmodels to generate these explanations and distill this knowledge into smaller,\nopenly available student models. Our student models are trained to generate\nmeaningful reasoning and rerank documents, achieving competitive performance\nacross multiple datasets, including MSMARCO and BRIGHT. Experiments demonstrate\nthat R2R not only improves reranking accuracy but also provides valuable\ninsights into the decision-making process. By offering a structured and\ninterpretable solution with openly accessible resources, R2R aims to bridge the\ngap between effectiveness and transparency in information retrieval, fostering\nreproducibility and further research in the field."
                },
                "authors": [
                    {
                        "name": "Yuelyu Ji"
                    },
                    {
                        "name": "Zhuochun Li"
                    },
                    {
                        "name": "Rui Meng"
                    },
                    {
                        "name": "Daqing He"
                    }
                ],
                "author_detail": {
                    "name": "Daqing He"
                },
                "author": "Daqing He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05168v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05168v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10286v1",
                "updated": "2025-04-14T14:53:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    53,
                    31,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T14:53:31Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    53,
                    31,
                    0,
                    104,
                    0
                ],
                "title": "Characterizing LLM-driven Social Network: The Chirper.ai Case",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing LLM-driven Social Network: The Chirper.ai Case"
                },
                "summary": "Large language models (LLMs) demonstrate the ability to simulate human\ndecision-making processes, enabling their use as agents in modeling\nsophisticated social networks, both offline and online. Recent research has\nexplored collective behavioral patterns and structural characteristics of LLM\nagents within simulated networks. However, empirical comparisons between\nLLM-driven and human-driven online social networks remain scarce, limiting our\nunderstanding of how LLM agents differ from human users. This paper presents a\nlarge-scale analysis of Chirper.ai, an X/Twitter-like social network entirely\npopulated by LLM agents, comprising over 65,000 agents and 7.7 million\nAI-generated posts. For comparison, we collect a parallel dataset from\nMastodon, a human-driven decentralized social network, with over 117,000 users\nand 16 million posts. We examine key differences between LLM agents and humans\nin posting behaviors, abusive content, and social network structures. Our\nfindings provide critical insights into the evolving landscape of online social\nnetwork analysis in the AI era, offering a comprehensive profile of LLM agents\nin social simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate the ability to simulate human\ndecision-making processes, enabling their use as agents in modeling\nsophisticated social networks, both offline and online. Recent research has\nexplored collective behavioral patterns and structural characteristics of LLM\nagents within simulated networks. However, empirical comparisons between\nLLM-driven and human-driven online social networks remain scarce, limiting our\nunderstanding of how LLM agents differ from human users. This paper presents a\nlarge-scale analysis of Chirper.ai, an X/Twitter-like social network entirely\npopulated by LLM agents, comprising over 65,000 agents and 7.7 million\nAI-generated posts. For comparison, we collect a parallel dataset from\nMastodon, a human-driven decentralized social network, with over 117,000 users\nand 16 million posts. We examine key differences between LLM agents and humans\nin posting behaviors, abusive content, and social network structures. Our\nfindings provide critical insights into the evolving landscape of online social\nnetwork analysis in the AI era, offering a comprehensive profile of LLM agents\nin social simulations."
                },
                "authors": [
                    {
                        "name": "Yiming Zhu"
                    },
                    {
                        "name": "Yupeng He"
                    },
                    {
                        "name": "Ehsan-Ul Haq"
                    },
                    {
                        "name": "Gareth Tyson"
                    },
                    {
                        "name": "Pan Hui"
                    }
                ],
                "author_detail": {
                    "name": "Pan Hui"
                },
                "author": "Pan Hui",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18865v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18865v3",
                "updated": "2025-04-14T14:52:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    52,
                    38,
                    0,
                    104,
                    0
                ],
                "published": "2025-03-24T16:41:17Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    41,
                    17,
                    0,
                    83,
                    0
                ],
                "title": "Structuring Scientific Innovation: A Framework for Modeling and\n  Discovering Impactful Knowledge Combinations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structuring Scientific Innovation: A Framework for Modeling and\n  Discovering Impactful Knowledge Combinations"
                },
                "summary": "The emergence of large language models offers new possibilities for\nstructured exploration of scientific knowledge. Rather than viewing scientific\ndiscovery as isolated ideas or content, we propose a structured approach that\nemphasizes the role of method combinations in shaping disruptive insights.\nSpecifically, we investigate how knowledge unit--especially those tied to\nmethodological design--can be modeled and recombined to yield research\nbreakthroughs. Our proposed framework addresses two key challenges. First, we\nintroduce a contrastive learning-based mechanism to identify distinguishing\nfeatures of historically disruptive method combinations within problem-driven\ncontexts. Second, we propose a reasoning-guided Monte Carlo search algorithm\nthat leverages the chain-of-thought capability of LLMs to identify promising\nknowledge recombinations for new problem statements.Empirical studies across\nmultiple domains show that the framework is capable of modeling the structural\ndynamics of innovation and successfully highlights combinations with high\ndisruptive potential. This research provides a new path for computationally\nguided scientific ideation grounded in structured reasoning and historical data\nmodeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models offers new possibilities for\nstructured exploration of scientific knowledge. Rather than viewing scientific\ndiscovery as isolated ideas or content, we propose a structured approach that\nemphasizes the role of method combinations in shaping disruptive insights.\nSpecifically, we investigate how knowledge unit--especially those tied to\nmethodological design--can be modeled and recombined to yield research\nbreakthroughs. Our proposed framework addresses two key challenges. First, we\nintroduce a contrastive learning-based mechanism to identify distinguishing\nfeatures of historically disruptive method combinations within problem-driven\ncontexts. Second, we propose a reasoning-guided Monte Carlo search algorithm\nthat leverages the chain-of-thought capability of LLMs to identify promising\nknowledge recombinations for new problem statements.Empirical studies across\nmultiple domains show that the framework is capable of modeling the structural\ndynamics of innovation and successfully highlights combinations with high\ndisruptive potential. This research provides a new path for computationally\nguided scientific ideation grounded in structured reasoning and historical data\nmodeling."
                },
                "authors": [
                    {
                        "name": "Junlan Chen"
                    },
                    {
                        "name": "Kexin Zhang"
                    },
                    {
                        "name": "Daifeng Li"
                    },
                    {
                        "name": "Yangyang Feng"
                    },
                    {
                        "name": "Yuxuan Zhang"
                    },
                    {
                        "name": "Bowen Deng"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Deng"
                },
                "author": "Bowen Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18865v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18865v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10284v1",
                "updated": "2025-04-14T14:52:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    52,
                    28,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T14:52:28Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    52,
                    28,
                    0,
                    104,
                    0
                ],
                "title": "Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the\n  Evaluation Protocol",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the\n  Evaluation Protocol"
                },
                "summary": "Literature review tables are essential for summarizing and comparing\ncollections of scientific papers. We explore the task of generating tables that\nbest fulfill a user's informational needs given a collection of scientific\npapers. Building on recent work (Newman et al., 2024), we extend prior\napproaches to address real-world complexities through a combination of\nLLM-based methods and human annotations. Our contributions focus on three key\nchallenges encountered in real-world use: (i) User prompts are often\nunder-specified; (ii) Retrieved candidate papers frequently contain irrelevant\ncontent; and (iii) Task evaluation should move beyond shallow text similarity\ntechniques and instead assess the utility of inferred tables for\ninformation-seeking tasks (e.g., comparing papers). To support reproducible\nevaluation, we introduce ARXIV2TABLE, a more realistic and challenging\nbenchmark for this task, along with a novel approach to improve literature\nreview table generation in real-world scenarios. Our extensive experiments on\nthis benchmark show that both open-weight and proprietary LLMs struggle with\nthe task, highlighting its difficulty and the need for further advancements.\nOur dataset and code are available at https://github.com/JHU-CLSP/arXiv2Table.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Literature review tables are essential for summarizing and comparing\ncollections of scientific papers. We explore the task of generating tables that\nbest fulfill a user's informational needs given a collection of scientific\npapers. Building on recent work (Newman et al., 2024), we extend prior\napproaches to address real-world complexities through a combination of\nLLM-based methods and human annotations. Our contributions focus on three key\nchallenges encountered in real-world use: (i) User prompts are often\nunder-specified; (ii) Retrieved candidate papers frequently contain irrelevant\ncontent; and (iii) Task evaluation should move beyond shallow text similarity\ntechniques and instead assess the utility of inferred tables for\ninformation-seeking tasks (e.g., comparing papers). To support reproducible\nevaluation, we introduce ARXIV2TABLE, a more realistic and challenging\nbenchmark for this task, along with a novel approach to improve literature\nreview table generation in real-world scenarios. Our extensive experiments on\nthis benchmark show that both open-weight and proprietary LLMs struggle with\nthe task, highlighting its difficulty and the need for further advancements.\nOur dataset and code are available at https://github.com/JHU-CLSP/arXiv2Table."
                },
                "authors": [
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Jiefu Ou"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    },
                    {
                        "name": "Daniel Khashabi"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Khashabi"
                },
                "author": "Daniel Khashabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07029v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07029v2",
                "updated": "2025-04-14T14:47:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    47,
                    35,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-09T16:44:19Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    16,
                    44,
                    19,
                    2,
                    99,
                    0
                ],
                "title": "Distilling Textual Priors from LLM to Efficient Image Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling Textual Priors from LLM to Efficient Image Fusion"
                },
                "summary": "Multi-modality image fusion aims to synthesize a single, comprehensive image\nfrom multiple source inputs. Traditional approaches, such as CNNs and GANs,\noffer efficiency but struggle to handle low-quality or complex inputs. Recent\nadvances in text-guided methods leverage large model priors to overcome these\nlimitations, but at the cost of significant computational overhead, both in\nmemory and inference time. To address this challenge, we propose a novel\nframework for distilling large model priors, eliminating the need for text\nguidance during inference while dramatically reducing model size. Our framework\nutilizes a teacher-student architecture, where the teacher network incorporates\nlarge model priors and transfers this knowledge to a smaller student network\nvia a tailored distillation process. Additionally, we introduce spatial-channel\ncross-fusion module to enhance the model's ability to leverage textual priors\nacross both spatial and channel dimensions. Our method achieves a favorable\ntrade-off between computational efficiency and fusion quality. The distilled\nnetwork, requiring only 10% of the parameters and inference time of the teacher\nnetwork, retains 90% of its performance and outperforms existing SOTA methods.\nExtensive experiments demonstrate the effectiveness of our approach. The\nimplementation will be made publicly available as an open-source resource.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modality image fusion aims to synthesize a single, comprehensive image\nfrom multiple source inputs. Traditional approaches, such as CNNs and GANs,\noffer efficiency but struggle to handle low-quality or complex inputs. Recent\nadvances in text-guided methods leverage large model priors to overcome these\nlimitations, but at the cost of significant computational overhead, both in\nmemory and inference time. To address this challenge, we propose a novel\nframework for distilling large model priors, eliminating the need for text\nguidance during inference while dramatically reducing model size. Our framework\nutilizes a teacher-student architecture, where the teacher network incorporates\nlarge model priors and transfers this knowledge to a smaller student network\nvia a tailored distillation process. Additionally, we introduce spatial-channel\ncross-fusion module to enhance the model's ability to leverage textual priors\nacross both spatial and channel dimensions. Our method achieves a favorable\ntrade-off between computational efficiency and fusion quality. The distilled\nnetwork, requiring only 10% of the parameters and inference time of the teacher\nnetwork, retains 90% of its performance and outperforms existing SOTA methods.\nExtensive experiments demonstrate the effectiveness of our approach. The\nimplementation will be made publicly available as an open-source resource."
                },
                "authors": [
                    {
                        "name": "Ran Zhang"
                    },
                    {
                        "name": "Xuanhua He"
                    },
                    {
                        "name": "Ke Cao"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Man Zhou"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07029v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07029v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10277v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10277v1",
                "updated": "2025-04-14T14:44:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    44,
                    41,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T14:44:41Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    44,
                    41,
                    0,
                    104,
                    0
                ],
                "title": "RealHarm: A Collection of Real-World Language Model Application Failures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RealHarm: A Collection of Real-World Language Model Application Failures"
                },
                "summary": "Language model deployments in consumer-facing applications introduce numerous\nrisks. While existing research on harms and hazards of such applications\nfollows top-down approaches derived from regulatory frameworks and theoretical\nanalyses, empirical evidence of real-world failure modes remains underexplored.\nIn this work, we introduce RealHarm, a dataset of annotated problematic\ninteractions with AI agents built from a systematic review of publicly reported\nincidents. Analyzing harms, causes, and hazards specifically from the\ndeployer's perspective, we find that reputational damage constitutes the\npredominant organizational harm, while misinformation emerges as the most\ncommon hazard category. We empirically evaluate state-of-the-art guardrails and\ncontent moderation systems to probe whether such systems would have prevented\nthe incidents, revealing a significant gap in the protection of AI\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language model deployments in consumer-facing applications introduce numerous\nrisks. While existing research on harms and hazards of such applications\nfollows top-down approaches derived from regulatory frameworks and theoretical\nanalyses, empirical evidence of real-world failure modes remains underexplored.\nIn this work, we introduce RealHarm, a dataset of annotated problematic\ninteractions with AI agents built from a systematic review of publicly reported\nincidents. Analyzing harms, causes, and hazards specifically from the\ndeployer's perspective, we find that reputational damage constitutes the\npredominant organizational harm, while misinformation emerges as the most\ncommon hazard category. We empirically evaluate state-of-the-art guardrails and\ncontent moderation systems to probe whether such systems would have prevented\nthe incidents, revealing a significant gap in the protection of AI\napplications."
                },
                "authors": [
                    {
                        "name": "Pierre Le Jeune"
                    },
                    {
                        "name": "Jiaen Liu"
                    },
                    {
                        "name": "Luca Rossi"
                    },
                    {
                        "name": "Matteo Dora"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Dora"
                },
                "author": "Matteo Dora",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10277v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23611v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23611v2",
                "updated": "2025-04-14T14:23:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    23,
                    0,
                    0,
                    104,
                    0
                ],
                "published": "2025-03-30T22:25:18Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    22,
                    25,
                    18,
                    6,
                    89,
                    0
                ],
                "title": "My CXL Pool Obviates Your PCIe Switch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "My CXL Pool Obviates Your PCIe Switch"
                },
                "summary": "Pooling PCIe devices across multiple hosts offers a promising solution to\nmitigate stranded I/O resources, enhance device utilization, address device\nfailures, and reduce total cost of ownership. The only viable option today are\nPCIe switches, which decouple PCIe devices from hosts by connecting them\nthrough a hardware switch. However, the high cost and limited flexibility of\nPCIe switches hinder their widespread adoption beyond specialized datacenter\nuse cases.\n  This paper argues that PCIe device pooling can be effectively implemented in\nsoftware using CXL memory pools. CXL memory pools improve memory utilization\nand already have positive return on investment. We find that, once CXL pools\nare in place, they can serve as a building block for pooling any kind of PCIe\ndevice. We demonstrate that PCIe devices can directly use CXL memory as I/O\nbuffers without device modifications, which enables routing PCIe traffic\nthrough CXL pool memory. This software-based approach is deployable on today's\nhardware and is more flexible than hardware PCIe switches. In particular, we\nexplore how disaggregating devices such as NICs can transform datacenter\ninfrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pooling PCIe devices across multiple hosts offers a promising solution to\nmitigate stranded I/O resources, enhance device utilization, address device\nfailures, and reduce total cost of ownership. The only viable option today are\nPCIe switches, which decouple PCIe devices from hosts by connecting them\nthrough a hardware switch. However, the high cost and limited flexibility of\nPCIe switches hinder their widespread adoption beyond specialized datacenter\nuse cases.\n  This paper argues that PCIe device pooling can be effectively implemented in\nsoftware using CXL memory pools. CXL memory pools improve memory utilization\nand already have positive return on investment. We find that, once CXL pools\nare in place, they can serve as a building block for pooling any kind of PCIe\ndevice. We demonstrate that PCIe devices can directly use CXL memory as I/O\nbuffers without device modifications, which enables routing PCIe traffic\nthrough CXL pool memory. This software-based approach is deployable on today's\nhardware and is more flexible than hardware PCIe switches. In particular, we\nexplore how disaggregating devices such as NICs can transform datacenter\ninfrastructure."
                },
                "authors": [
                    {
                        "name": "Yuhong Zhong"
                    },
                    {
                        "name": "Daniel S. Berger"
                    },
                    {
                        "name": "Pantea Zardoshti"
                    },
                    {
                        "name": "Enrique Saurez"
                    },
                    {
                        "name": "Jacob Nelson"
                    },
                    {
                        "name": "Antonis Psistakis"
                    },
                    {
                        "name": "Joshua Fried"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "arxiv_doi": "10.1145/3713082.3730393",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3713082.3730393",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.23611v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23611v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23633v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23633v5",
                "updated": "2025-04-14T14:21:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    21,
                    34,
                    0,
                    104,
                    0
                ],
                "published": "2025-03-31T00:12:48Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    0,
                    12,
                    48,
                    0,
                    90,
                    0
                ],
                "title": "GIScience in the Era of Artificial Intelligence: A Research Agenda\n  Towards Autonomous GIS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GIScience in the Era of Artificial Intelligence: A Research Agenda\n  Towards Autonomous GIS"
                },
                "summary": "The advent of generative AI exemplified by large language models (LLMs) opens\nnew ways to represent and compute geographic information and transcends the\nprocess of geographic knowledge production, driving geographic information\nsystems (GIS) towards autonomous GIS. Leveraging LLMs as the decision core,\nautonomous GIS can independently generate and execute geoprocessing workflows\nto perform spatial analysis. In this vision paper, we further elaborate on the\nconcept of autonomous GIS and present a conceptual framework that defines its\nfive autonomous goals, five autonomous levels, five core functions, and three\noperational scales. We demonstrate how autonomous GIS could perform geospatial\ndata retrieval, spatial analysis, and map making with four proof-of-concept GIS\nagents. We conclude by identifying critical challenges and future research\ndirections, including fine-tuning and self-growing decision-cores, autonomous\nmodeling, and examining the societal and practical implications of autonomous\nGIS. By establishing the groundwork for a paradigm shift in GIScience, this\npaper envisions a future where GIS moves beyond traditional workflows to\nautonomously reason, derive, innovate, and advance geospatial solutions to\npressing global challenges. Meanwhile, as we design and deploy increasingly\nintelligent geospatial systems, we carry a responsibility to ensure they are\ndeveloped in socially responsible ways, serve the public good, and support the\ncontinued value of human geographic insight in an AI-augmented future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of generative AI exemplified by large language models (LLMs) opens\nnew ways to represent and compute geographic information and transcends the\nprocess of geographic knowledge production, driving geographic information\nsystems (GIS) towards autonomous GIS. Leveraging LLMs as the decision core,\nautonomous GIS can independently generate and execute geoprocessing workflows\nto perform spatial analysis. In this vision paper, we further elaborate on the\nconcept of autonomous GIS and present a conceptual framework that defines its\nfive autonomous goals, five autonomous levels, five core functions, and three\noperational scales. We demonstrate how autonomous GIS could perform geospatial\ndata retrieval, spatial analysis, and map making with four proof-of-concept GIS\nagents. We conclude by identifying critical challenges and future research\ndirections, including fine-tuning and self-growing decision-cores, autonomous\nmodeling, and examining the societal and practical implications of autonomous\nGIS. By establishing the groundwork for a paradigm shift in GIScience, this\npaper envisions a future where GIS moves beyond traditional workflows to\nautonomously reason, derive, innovate, and advance geospatial solutions to\npressing global challenges. Meanwhile, as we design and deploy increasingly\nintelligent geospatial systems, we carry a responsibility to ensure they are\ndeveloped in socially responsible ways, serve the public good, and support the\ncontinued value of human geographic insight in an AI-augmented future."
                },
                "authors": [
                    {
                        "name": "Zhenlong Li"
                    },
                    {
                        "name": "Huan Ning"
                    },
                    {
                        "name": "Song Gao"
                    },
                    {
                        "name": "Krzysztof Janowicz"
                    },
                    {
                        "name": "Wenwen Li"
                    },
                    {
                        "name": "Samantha T. Arundel"
                    },
                    {
                        "name": "Chaowei Yang"
                    },
                    {
                        "name": "Budhendra Bhaduri"
                    },
                    {
                        "name": "Shaowen Wang"
                    },
                    {
                        "name": "A-Xing Zhu"
                    },
                    {
                        "name": "Mark Gahegan"
                    },
                    {
                        "name": "Shashi Shekhar"
                    },
                    {
                        "name": "Xinyue Ye"
                    },
                    {
                        "name": "Grant McKenzie"
                    },
                    {
                        "name": "Guido Cervone"
                    },
                    {
                        "name": "Michael E. Hodgson"
                    }
                ],
                "author_detail": {
                    "name": "Michael E. Hodgson"
                },
                "author": "Michael E. Hodgson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23633v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23633v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10258v1",
                "updated": "2025-04-14T14:19:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    19,
                    57,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T14:19:57Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    19,
                    57,
                    0,
                    104,
                    0
                ],
                "title": "XY-Cut++: Advanced Layout Ordering via Hierarchical Mask Mechanism on a\n  Novel Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XY-Cut++: Advanced Layout Ordering via Hierarchical Mask Mechanism on a\n  Novel Benchmark"
                },
                "summary": "Document Reading Order Recovery is a fundamental task in document image\nunderstanding, playing a pivotal role in enhancing Retrieval-Augmented\nGeneration (RAG) and serving as a critical preprocessing step for large\nlanguage models (LLMs). Existing methods often struggle with complex\nlayouts(e.g., multi-column newspapers), high-overhead interactions between\ncross-modal elements (visual regions and textual semantics), and a lack of\nrobust evaluation benchmarks. We introduce XY-Cut++, an advanced layout\nordering method that integrates pre-mask processing, multi-granularity\nsegmentation, and cross-modal matching to address these challenges. Our method\nsignificantly enhances layout ordering accuracy compared to traditional XY-Cut\ntechniques. Specifically, XY-Cut++ achieves state-of-the-art performance (98.8\nBLEU overall) while maintaining simplicity and efficiency. It outperforms\nexisting baselines by up to 24\\% and demonstrates consistent accuracy across\nsimple and complex layouts on the newly introduced DocBench-100 dataset. This\nadvancement establishes a reliable foundation for document structure recovery,\nsetting a new standard for layout ordering tasks and facilitating more\neffective RAG and LLM preprocessing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Document Reading Order Recovery is a fundamental task in document image\nunderstanding, playing a pivotal role in enhancing Retrieval-Augmented\nGeneration (RAG) and serving as a critical preprocessing step for large\nlanguage models (LLMs). Existing methods often struggle with complex\nlayouts(e.g., multi-column newspapers), high-overhead interactions between\ncross-modal elements (visual regions and textual semantics), and a lack of\nrobust evaluation benchmarks. We introduce XY-Cut++, an advanced layout\nordering method that integrates pre-mask processing, multi-granularity\nsegmentation, and cross-modal matching to address these challenges. Our method\nsignificantly enhances layout ordering accuracy compared to traditional XY-Cut\ntechniques. Specifically, XY-Cut++ achieves state-of-the-art performance (98.8\nBLEU overall) while maintaining simplicity and efficiency. It outperforms\nexisting baselines by up to 24\\% and demonstrates consistent accuracy across\nsimple and complex layouts on the newly introduced DocBench-100 dataset. This\nadvancement establishes a reliable foundation for document structure recovery,\nsetting a new standard for layout ordering tasks and facilitating more\neffective RAG and LLM preprocessing."
                },
                "authors": [
                    {
                        "name": "Shuai Liu"
                    },
                    {
                        "name": "Youmeng Li"
                    },
                    {
                        "name": "Jizeng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Jizeng Wei"
                },
                "author": "Jizeng Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10248v1",
                "updated": "2025-04-14T14:11:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    11,
                    0,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T14:11:00Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    11,
                    0,
                    0,
                    104,
                    0
                ],
                "title": "Adaptive Sensor Steering Strategy Using Deep Reinforcement Learning for\n  Dynamic Data Acquisition in Digital Twins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Sensor Steering Strategy Using Deep Reinforcement Learning for\n  Dynamic Data Acquisition in Digital Twins"
                },
                "summary": "This paper introduces a sensor steering methodology based on deep\nreinforcement learning to enhance the predictive accuracy and decision support\ncapabilities of digital twins by optimising the data acquisition process.\nTraditional sensor placement techniques are often constrained by one-off\noptimisation strategies, which limit their applicability for online\napplications requiring continuous informative data assimilation. The proposed\napproach addresses this limitation by offering an adaptive framework for sensor\nplacement within the digital twin paradigm. The sensor placement problem is\nformulated as a Markov decision process, enabling the training and deployment\nof an agent capable of dynamically repositioning sensors in response to the\nevolving conditions of the physical structure as represented by the digital\ntwin. This ensures that the digital twin maintains a highly representative and\nreliable connection to its physical counterpart. The proposed framework is\nvalidated through a series of comprehensive case studies involving a cantilever\nplate structure subjected to diverse conditions, including healthy and damaged\nconditions. The results demonstrate the capability of the deep reinforcement\nlearning agent to adaptively reposition sensors improving the quality of data\nacquisition and hence enhancing the overall accuracy of digital twins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a sensor steering methodology based on deep\nreinforcement learning to enhance the predictive accuracy and decision support\ncapabilities of digital twins by optimising the data acquisition process.\nTraditional sensor placement techniques are often constrained by one-off\noptimisation strategies, which limit their applicability for online\napplications requiring continuous informative data assimilation. The proposed\napproach addresses this limitation by offering an adaptive framework for sensor\nplacement within the digital twin paradigm. The sensor placement problem is\nformulated as a Markov decision process, enabling the training and deployment\nof an agent capable of dynamically repositioning sensors in response to the\nevolving conditions of the physical structure as represented by the digital\ntwin. This ensures that the digital twin maintains a highly representative and\nreliable connection to its physical counterpart. The proposed framework is\nvalidated through a series of comprehensive case studies involving a cantilever\nplate structure subjected to diverse conditions, including healthy and damaged\nconditions. The results demonstrate the capability of the deep reinforcement\nlearning agent to adaptively reposition sensors improving the quality of data\nacquisition and hence enhancing the overall accuracy of digital twins."
                },
                "authors": [
                    {
                        "name": "Collins O. Ogbodo"
                    },
                    {
                        "name": "Timothy J. Rogers"
                    },
                    {
                        "name": "Mattia Dal Borgo"
                    },
                    {
                        "name": "David J. Wagg"
                    }
                ],
                "author_detail": {
                    "name": "David J. Wagg"
                },
                "author": "David J. Wagg",
                "arxiv_comment": "18 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10240v1",
                "updated": "2025-04-14T14:02:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    2,
                    9,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T14:02:09Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    14,
                    2,
                    9,
                    0,
                    104,
                    0
                ],
                "title": "GNN-ACLP: Graph Neural Networks based Analog Circuit Link Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GNN-ACLP: Graph Neural Networks based Analog Circuit Link Prediction"
                },
                "summary": "Circuit link prediction identifying missing component connections from\nincomplete netlists is crucial in automating analog circuit design. However,\nexisting methods face three main challenges: 1) Insufficient use of topological\npatterns in circuit graphs reduces prediction accuracy; 2) Data scarcity due to\nthe complexity of annotations hinders model generalization; 3) Limited\nadaptability to various netlist formats. We propose GNN-ACLP, a Graph Neural\nNetworks (GNNs) based framework featuring three innovations to tackle these\nchallenges. First, we introduce the SEAL (Subgraphs, Embeddings, and Attributes\nfor Link Prediction) framework and achieve port-level accuracy in circuit link\nprediction. Second, we propose Netlist Babel Fish, a netlist format conversion\ntool leveraging retrieval-augmented generation (RAG) with large language model\n(LLM) to enhance the compatibility of netlist formats. Finally, we construct\nSpiceNetlist, a comprehensive dataset that contains 775 annotated circuits\nacross 10 different classes of components. The experimental results demonstrate\nan improvement of 15.05% on the SpiceNetlist dataset and 12.01% on the\nImage2Net dataset over the existing approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Circuit link prediction identifying missing component connections from\nincomplete netlists is crucial in automating analog circuit design. However,\nexisting methods face three main challenges: 1) Insufficient use of topological\npatterns in circuit graphs reduces prediction accuracy; 2) Data scarcity due to\nthe complexity of annotations hinders model generalization; 3) Limited\nadaptability to various netlist formats. We propose GNN-ACLP, a Graph Neural\nNetworks (GNNs) based framework featuring three innovations to tackle these\nchallenges. First, we introduce the SEAL (Subgraphs, Embeddings, and Attributes\nfor Link Prediction) framework and achieve port-level accuracy in circuit link\nprediction. Second, we propose Netlist Babel Fish, a netlist format conversion\ntool leveraging retrieval-augmented generation (RAG) with large language model\n(LLM) to enhance the compatibility of netlist formats. Finally, we construct\nSpiceNetlist, a comprehensive dataset that contains 775 annotated circuits\nacross 10 different classes of components. The experimental results demonstrate\nan improvement of 15.05% on the SpiceNetlist dataset and 12.01% on the\nImage2Net dataset over the existing approach."
                },
                "authors": [
                    {
                        "name": "Guanyuan Pan"
                    },
                    {
                        "name": "Tiansheng Zhou"
                    },
                    {
                        "name": "Bingtao Ma"
                    },
                    {
                        "name": "Yaqi Wang"
                    },
                    {
                        "name": "Jianxiang Zhao"
                    },
                    {
                        "name": "Shuai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shuai Wang"
                },
                "author": "Shuai Wang",
                "arxiv_comment": "Data will be made available on request",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12899v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12899v2",
                "updated": "2025-04-14T13:57:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    13,
                    57,
                    28,
                    0,
                    104,
                    0
                ],
                "published": "2025-03-17T07:59:42Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    7,
                    59,
                    42,
                    0,
                    76,
                    0
                ],
                "title": "A Semantic-based Optimization Approach for Repairing LLMs: Case Study on\n  Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Semantic-based Optimization Approach for Repairing LLMs: Case Study on\n  Code Generation"
                },
                "summary": "Language Models (LMs) are widely used in software engineering for code\ngeneration, but they may produce code with errors. Rather than repairing the\ngenerated code, an alternative way is to address the underlying failures of\nmodels. LM repair offers a lightweight solution to this challenge: it requires\nminimal data, reduces computational costs, and reduces the side effects. Unlike\nretraining, LM repair focuses on applying tailored updates to targeted neurons,\nmaking it ideal for scenarios with limited resources, high-performance demands,\nor strict safety requirements. In this paper, we propose \\ul{S}emantic\n\\ul{T}argeting for \\ul{A}nalytical \\ul{R}epair (\\textsc{STAR}), a pioneering\nand novel semantic-based optimization approach for repairing LLMs.\n\\textsc{STAR} realizes main operations in LM repair methods in an optimization\nprocess, including locating ``buggy neurons'', solving ``neuron patches'', and\npatching ``buggy neurons''. Correspondingly, it computes the deltas of weight\nmatrix as the prior information to guide optimization; and attributes the\ntargeted layers and neurons leveraging statistical insights. The neuron patches\nare computed with a solid semantic-based analytical formula, which directly\nbridges the changes to logits with the deltas of neurons, by steering latent\nrepresentations. Compared to the prior work of LM repair (\\textsc{MINT}) and\noptimization methods (\\textsc{SGD}), \\textsc{STAR} integrates their strengths\nwhile mitigating their limitations. \\textsc{STAR} supports solving multiple\nfailures together, significantly improving the usefulness. Evaluated on three\ncode generation tasks using popular code LMs, \\textsc{STAR} demonstrates\nsuperior effectiveness. Additionally, \\textsc{STAR} exhibits better efficiency.\nIn terms of side effects, namely the balance between generalization and\nspecificity, \\textsc{STAR} outperforms prior work by a significant margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models (LMs) are widely used in software engineering for code\ngeneration, but they may produce code with errors. Rather than repairing the\ngenerated code, an alternative way is to address the underlying failures of\nmodels. LM repair offers a lightweight solution to this challenge: it requires\nminimal data, reduces computational costs, and reduces the side effects. Unlike\nretraining, LM repair focuses on applying tailored updates to targeted neurons,\nmaking it ideal for scenarios with limited resources, high-performance demands,\nor strict safety requirements. In this paper, we propose \\ul{S}emantic\n\\ul{T}argeting for \\ul{A}nalytical \\ul{R}epair (\\textsc{STAR}), a pioneering\nand novel semantic-based optimization approach for repairing LLMs.\n\\textsc{STAR} realizes main operations in LM repair methods in an optimization\nprocess, including locating ``buggy neurons'', solving ``neuron patches'', and\npatching ``buggy neurons''. Correspondingly, it computes the deltas of weight\nmatrix as the prior information to guide optimization; and attributes the\ntargeted layers and neurons leveraging statistical insights. The neuron patches\nare computed with a solid semantic-based analytical formula, which directly\nbridges the changes to logits with the deltas of neurons, by steering latent\nrepresentations. Compared to the prior work of LM repair (\\textsc{MINT}) and\noptimization methods (\\textsc{SGD}), \\textsc{STAR} integrates their strengths\nwhile mitigating their limitations. \\textsc{STAR} supports solving multiple\nfailures together, significantly improving the usefulness. Evaluated on three\ncode generation tasks using popular code LMs, \\textsc{STAR} demonstrates\nsuperior effectiveness. Additionally, \\textsc{STAR} exhibits better efficiency.\nIn terms of side effects, namely the balance between generalization and\nspecificity, \\textsc{STAR} outperforms prior work by a significant margin."
                },
                "authors": [
                    {
                        "name": "Jian Gu"
                    },
                    {
                        "name": "Aldeida Aleti"
                    },
                    {
                        "name": "Chunyang Chen"
                    },
                    {
                        "name": "Hongyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hongyu Zhang"
                },
                "author": "Hongyu Zhang",
                "arxiv_comment": "12 pages, 6 figure, 6 tables, under peer-review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12899v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12899v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04285v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04285v2",
                "updated": "2025-04-14T13:55:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    13,
                    55,
                    42,
                    0,
                    104,
                    0
                ],
                "published": "2025-01-08T05:17:09Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    5,
                    17,
                    9,
                    2,
                    8,
                    0
                ],
                "title": "Separate Source Channel Coding Is Still What You Need: An LLM-based\n  Rethinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Separate Source Channel Coding Is Still What You Need: An LLM-based\n  Rethinking"
                },
                "summary": "Along with the proliferating research interest in Semantic Communication\n(SemCom), Joint Source Channel Coding (JSCC) has dominated the attention due to\nthe widely assumed existence in efficiently delivering information semantics.\n%has emerged as a pivotal area of research, aiming to enhance the efficiency\nand reliability of information transmission through deep learning-based\nmethods. Nevertheless, this paper challenges the conventional JSCC paradigm,\nand advocates for adoption of Separate Source Channel Coding (SSCC) to enjoy\nthe underlying more degree of freedom for optimization. We demonstrate that\nSSCC, after leveraging the strengths of Large Language Model (LLM) for source\ncoding and Error Correction Code Transformer (ECCT) complemented for channel\ndecoding, offers superior performance over JSCC. Our proposed framework also\neffectively highlights the compatibility challenges between SemCom approaches\nand digital communication systems, particularly concerning the resource costs\nassociated with the transmission of high precision floating point numbers.\nThrough comprehensive evaluations, we establish that empowered by LLM-based\ncompression and ECCT-enhanced error correction, SSCC remains a viable and\neffective solution for modern communication systems. In other words, separate\nsource and channel coding is still what we need!",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Along with the proliferating research interest in Semantic Communication\n(SemCom), Joint Source Channel Coding (JSCC) has dominated the attention due to\nthe widely assumed existence in efficiently delivering information semantics.\n%has emerged as a pivotal area of research, aiming to enhance the efficiency\nand reliability of information transmission through deep learning-based\nmethods. Nevertheless, this paper challenges the conventional JSCC paradigm,\nand advocates for adoption of Separate Source Channel Coding (SSCC) to enjoy\nthe underlying more degree of freedom for optimization. We demonstrate that\nSSCC, after leveraging the strengths of Large Language Model (LLM) for source\ncoding and Error Correction Code Transformer (ECCT) complemented for channel\ndecoding, offers superior performance over JSCC. Our proposed framework also\neffectively highlights the compatibility challenges between SemCom approaches\nand digital communication systems, particularly concerning the resource costs\nassociated with the transmission of high precision floating point numbers.\nThrough comprehensive evaluations, we establish that empowered by LLM-based\ncompression and ECCT-enhanced error correction, SSCC remains a viable and\neffective solution for modern communication systems. In other words, separate\nsource and channel coding is still what we need!"
                },
                "authors": [
                    {
                        "name": "Tianqi Ren"
                    },
                    {
                        "name": "Rongpeng Li"
                    },
                    {
                        "name": "Ming-min Zhao"
                    },
                    {
                        "name": "Xianfu Chen"
                    },
                    {
                        "name": "Guangyi Liu"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Zhifeng Zhao"
                    },
                    {
                        "name": "Honggang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Honggang Zhang"
                },
                "author": "Honggang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04285v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04285v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10227v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10227v1",
                "updated": "2025-04-14T13:46:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    13,
                    46,
                    35,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T13:46:35Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    13,
                    46,
                    35,
                    0,
                    104,
                    0
                ],
                "title": "Probing then Editing Response Personality of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing then Editing Response Personality of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated promising capabilities to\ngenerate responses that exhibit consistent personality traits. Despite the\nmajor attempts to analyze personality expression through output-based\nevaluations, little is known about how such traits are internally encoded\nwithin LLM parameters. In this paper, we introduce a layer-wise probing\nframework to systematically investigate the layer-wise capability of LLMs in\nencoding personality for responding. We conduct probing experiments on 11\nopen-source LLMs over the PersonalityEdit benchmark and find that LLMs\npredominantly encode personality for responding in their middle and upper\nlayers, with instruction-tuned models demonstrating a slightly clearer\nseparation of personality traits. Furthermore, by interpreting the trained\nprobing hyperplane as a layer-wise boundary for each personality category, we\npropose a layer-wise perturbation method to edit the personality expressed by\nLLMs during inference. Our results show that even when the prompt explicitly\nspecifies a particular personality, our method can still successfully alter the\nresponse personality of LLMs. Interestingly, the difficulty of converting\nbetween certain personality traits varies substantially, which aligns with the\nrepresentational distances in our probing experiments. Finally, we conduct a\ncomprehensive MMLU benchmark evaluation and time overhead analysis,\ndemonstrating that our proposed personality editing method incurs only minimal\ndegradation in general capabilities while maintaining low training costs and\nacceptable inference latency. Our code is publicly available at\nhttps://github.com/universe-sky/probing-then-editing-personality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated promising capabilities to\ngenerate responses that exhibit consistent personality traits. Despite the\nmajor attempts to analyze personality expression through output-based\nevaluations, little is known about how such traits are internally encoded\nwithin LLM parameters. In this paper, we introduce a layer-wise probing\nframework to systematically investigate the layer-wise capability of LLMs in\nencoding personality for responding. We conduct probing experiments on 11\nopen-source LLMs over the PersonalityEdit benchmark and find that LLMs\npredominantly encode personality for responding in their middle and upper\nlayers, with instruction-tuned models demonstrating a slightly clearer\nseparation of personality traits. Furthermore, by interpreting the trained\nprobing hyperplane as a layer-wise boundary for each personality category, we\npropose a layer-wise perturbation method to edit the personality expressed by\nLLMs during inference. Our results show that even when the prompt explicitly\nspecifies a particular personality, our method can still successfully alter the\nresponse personality of LLMs. Interestingly, the difficulty of converting\nbetween certain personality traits varies substantially, which aligns with the\nrepresentational distances in our probing experiments. Finally, we conduct a\ncomprehensive MMLU benchmark evaluation and time overhead analysis,\ndemonstrating that our proposed personality editing method incurs only minimal\ndegradation in general capabilities while maintaining low training costs and\nacceptable inference latency. Our code is publicly available at\nhttps://github.com/universe-sky/probing-then-editing-personality."
                },
                "authors": [
                    {
                        "name": "Tianjie Ju"
                    },
                    {
                        "name": "Zhenyu Shao"
                    },
                    {
                        "name": "Bowen Wang"
                    },
                    {
                        "name": "Yujia Chen"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Mong-Li Lee"
                    },
                    {
                        "name": "Wynne Hsu"
                    },
                    {
                        "name": "Sufeng Duan"
                    },
                    {
                        "name": "Gongshen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Gongshen Liu"
                },
                "author": "Gongshen Liu",
                "arxiv_comment": "Working in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10227v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10227v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08619v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08619v2",
                "updated": "2025-04-14T13:45:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    13,
                    45,
                    49,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-11T15:24:23Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    24,
                    23,
                    4,
                    101,
                    0
                ],
                "title": "Analyzing 16,193 LLM Papers for Fun and Profits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing 16,193 LLM Papers for Fun and Profits"
                },
                "summary": "Large Language Models (LLMs) are reshaping the landscape of computer science\nresearch, driving significant shifts in research priorities across diverse\nconferences and fields. This study provides a comprehensive analysis of the\npublication trend of LLM-related papers in 77 top-tier computer science\nconferences over the past six years (2019-2024). We approach this analysis from\nfour distinct perspectives: (1) We investigate how LLM research is driving\ntopic shifts within major conferences. (2) We adopt a topic modeling approach\nto identify various areas of LLM-related topic growth and reveal the topics of\nconcern at different conferences. (3) We explore distinct contribution patterns\nof academic and industrial institutions. (4) We study the influence of national\norigins on LLM development trajectories. Synthesizing the findings from these\ndiverse analytical angles, we derive ten key insights that illuminate the\ndynamics and evolution of the LLM research ecosystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are reshaping the landscape of computer science\nresearch, driving significant shifts in research priorities across diverse\nconferences and fields. This study provides a comprehensive analysis of the\npublication trend of LLM-related papers in 77 top-tier computer science\nconferences over the past six years (2019-2024). We approach this analysis from\nfour distinct perspectives: (1) We investigate how LLM research is driving\ntopic shifts within major conferences. (2) We adopt a topic modeling approach\nto identify various areas of LLM-related topic growth and reveal the topics of\nconcern at different conferences. (3) We explore distinct contribution patterns\nof academic and industrial institutions. (4) We study the influence of national\norigins on LLM development trajectories. Synthesizing the findings from these\ndiverse analytical angles, we derive ten key insights that illuminate the\ndynamics and evolution of the LLM research ecosystem."
                },
                "authors": [
                    {
                        "name": "Zhiqiu Xia"
                    },
                    {
                        "name": "Lang Zhu"
                    },
                    {
                        "name": "Bingzhe Li"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Qiannan Li"
                    },
                    {
                        "name": "Hang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Liu"
                },
                "author": "Hang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08619v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08619v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24157v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24157v2",
                "updated": "2025-04-14T13:31:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    13,
                    31,
                    18,
                    0,
                    104,
                    0
                ],
                "published": "2025-03-31T14:40:31Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    14,
                    40,
                    31,
                    0,
                    90,
                    0
                ],
                "title": "LLM4FS: Leveraging Large Language Models for Feature Selection and How\n  to Improve It",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4FS: Leveraging Large Language Models for Feature Selection and How\n  to Improve It"
                },
                "summary": "Recent advances in large language models (LLMs) have provided new\nopportunities for decision-making, particularly in the task of automated\nfeature selection. In this paper, we first comprehensively evaluate LLM-based\nfeature selection methods, covering the state-of-the-art DeepSeek-R1,\nGPT-o3-mini, and GPT-4.5. Then, we propose a novel hybrid strategy called\nLLM4FS that integrates LLMs with traditional data-driven methods. Specifically,\ninput data samples into LLMs, and directly call traditional data-driven\ntechniques such as random forest and forward sequential selection. Notably, our\nanalysis reveals that the hybrid strategy leverages the contextual\nunderstanding of LLMs and the high statistical reliability of traditional\ndata-driven methods to achieve excellent feature selection performance, even\nsurpassing LLMs and traditional data-driven methods. Finally, we point out the\nlimitations of its application in decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have provided new\nopportunities for decision-making, particularly in the task of automated\nfeature selection. In this paper, we first comprehensively evaluate LLM-based\nfeature selection methods, covering the state-of-the-art DeepSeek-R1,\nGPT-o3-mini, and GPT-4.5. Then, we propose a novel hybrid strategy called\nLLM4FS that integrates LLMs with traditional data-driven methods. Specifically,\ninput data samples into LLMs, and directly call traditional data-driven\ntechniques such as random forest and forward sequential selection. Notably, our\nanalysis reveals that the hybrid strategy leverages the contextual\nunderstanding of LLMs and the high statistical reliability of traditional\ndata-driven methods to achieve excellent feature selection performance, even\nsurpassing LLMs and traditional data-driven methods. Finally, we point out the\nlimitations of its application in decision-making."
                },
                "authors": [
                    {
                        "name": "Jianhao Li"
                    },
                    {
                        "name": "Xianchao Xiu"
                    }
                ],
                "author_detail": {
                    "name": "Xianchao Xiu"
                },
                "author": "Xianchao Xiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24157v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24157v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10210v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10210v1",
                "updated": "2025-04-14T13:25:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    13,
                    25,
                    50,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T13:25:50Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    13,
                    25,
                    50,
                    0,
                    104,
                    0
                ],
                "title": "Can Competition Enhance the Proficiency of Agents Powered by Large\n  Language Models in the Realm of News-driven Time Series Forecasting?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Competition Enhance the Proficiency of Agents Powered by Large\n  Language Models in the Realm of News-driven Time Series Forecasting?"
                },
                "summary": "Multi-agents-based news-driven time series forecasting is considered as a\npotential paradigm shift in the era of large language models (LLMs). The\nchallenge of this task lies in measuring the influences of different news\nevents towards the fluctuations of time series. This requires agents to possess\nstronger abilities of innovative thinking and the identifying misleading logic.\nHowever, the existing multi-agent discussion framework has limited enhancement\non time series prediction in terms of optimizing these two capabilities.\nInspired by the role of competition in fostering innovation, this study embeds\na competition mechanism within the multi-agent discussion to enhance agents'\ncapability of generating innovative thoughts. Furthermore, to bolster the\nmodel's proficiency in identifying misleading information, we incorporate a\nfine-tuned small-scale LLM model within the reflective stage, offering\nauxiliary decision-making support. Experimental results confirm that the\ncompetition can boost agents' capacity for innovative thinking, which can\nsignificantly improve the performances of time series prediction. Similar to\nthe findings of social science, the intensity of competition within this\nframework can influence the performances of agents, providing a new perspective\nfor studying LLMs-based multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agents-based news-driven time series forecasting is considered as a\npotential paradigm shift in the era of large language models (LLMs). The\nchallenge of this task lies in measuring the influences of different news\nevents towards the fluctuations of time series. This requires agents to possess\nstronger abilities of innovative thinking and the identifying misleading logic.\nHowever, the existing multi-agent discussion framework has limited enhancement\non time series prediction in terms of optimizing these two capabilities.\nInspired by the role of competition in fostering innovation, this study embeds\na competition mechanism within the multi-agent discussion to enhance agents'\ncapability of generating innovative thoughts. Furthermore, to bolster the\nmodel's proficiency in identifying misleading information, we incorporate a\nfine-tuned small-scale LLM model within the reflective stage, offering\nauxiliary decision-making support. Experimental results confirm that the\ncompetition can boost agents' capacity for innovative thinking, which can\nsignificantly improve the performances of time series prediction. Similar to\nthe findings of social science, the intensity of competition within this\nframework can influence the performances of agents, providing a new perspective\nfor studying LLMs-based multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhang"
                    },
                    {
                        "name": "Yangyang Feng"
                    },
                    {
                        "name": "Daifeng Li"
                    },
                    {
                        "name": "Kexin Zhang"
                    },
                    {
                        "name": "Junlan Chen"
                    },
                    {
                        "name": "Bowen Deng"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Deng"
                },
                "author": "Bowen Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10210v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10208v1",
                "updated": "2025-04-14T13:21:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    13,
                    21,
                    29,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T13:21:29Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    13,
                    21,
                    29,
                    0,
                    104,
                    0
                ],
                "title": "From Prompting to Alignment: A Generative Framework for Query\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Prompting to Alignment: A Generative Framework for Query\n  Recommendation"
                },
                "summary": "In modern search systems, search engines often suggest relevant queries to\nusers through various panels or components, helping refine their information\nneeds. Traditionally, these recommendations heavily rely on historical search\nlogs to build models, which suffer from cold-start or long-tail issues.\nFurthermore, tasks such as query suggestion, completion or clarification are\nstudied separately by specific design, which lacks generalizability and hinders\nadaptation to novel applications. Despite recent attempts to explore the use of\nLLMs for query recommendation, these methods mainly rely on the inherent\nknowledge of LLMs or external sources like few-shot examples, retrieved\ndocuments, or knowledge bases, neglecting the importance of the calibration and\nalignment with user feedback, thus limiting their practical utility. To address\nthese challenges, we first propose a general Generative Query Recommendation\n(GQR) framework that aligns LLM-based query generation with user preference.\nSpecifically, we unify diverse query recommendation tasks by a universal prompt\nframework, leveraging the instruct-following capability of LLMs for effective\ngeneration. Secondly, we align LLMs with user feedback via presenting a\nCTR-alignment framework, which involves training a query-wise CTR predictor as\na process reward model and employing list-wise preference alignment to maximize\nthe click probability of the generated query list. Furthermore, recognizing the\ninconsistency between LLM knowledge and proactive search intents arising from\nthe separation of user-initiated queries from models, we align LLMs with user\ninitiative via retrieving co-occurrence queries as side information when\nhistorical logs are available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern search systems, search engines often suggest relevant queries to\nusers through various panels or components, helping refine their information\nneeds. Traditionally, these recommendations heavily rely on historical search\nlogs to build models, which suffer from cold-start or long-tail issues.\nFurthermore, tasks such as query suggestion, completion or clarification are\nstudied separately by specific design, which lacks generalizability and hinders\nadaptation to novel applications. Despite recent attempts to explore the use of\nLLMs for query recommendation, these methods mainly rely on the inherent\nknowledge of LLMs or external sources like few-shot examples, retrieved\ndocuments, or knowledge bases, neglecting the importance of the calibration and\nalignment with user feedback, thus limiting their practical utility. To address\nthese challenges, we first propose a general Generative Query Recommendation\n(GQR) framework that aligns LLM-based query generation with user preference.\nSpecifically, we unify diverse query recommendation tasks by a universal prompt\nframework, leveraging the instruct-following capability of LLMs for effective\ngeneration. Secondly, we align LLMs with user feedback via presenting a\nCTR-alignment framework, which involves training a query-wise CTR predictor as\na process reward model and employing list-wise preference alignment to maximize\nthe click probability of the generated query list. Furthermore, recognizing the\ninconsistency between LLM knowledge and proactive search intents arising from\nthe separation of user-initiated queries from models, we align LLMs with user\ninitiative via retrieving co-occurrence queries as side information when\nhistorical logs are available."
                },
                "authors": [
                    {
                        "name": "Erxue Min"
                    },
                    {
                        "name": "Hsiu-Yuan Huang"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Xihong Yang"
                    },
                    {
                        "name": "Xin Jia"
                    },
                    {
                        "name": "Yunfang Wu"
                    },
                    {
                        "name": "Hengyi Cai"
                    },
                    {
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "name": "Dawei Yin"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Yin"
                },
                "author": "Dawei Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10198v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10198v1",
                "updated": "2025-04-14T13:02:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    13,
                    2,
                    53,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T13:02:53Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    13,
                    2,
                    53,
                    0,
                    104,
                    0
                ],
                "title": "DioR: Adaptive Cognitive Detection and Contextual Retrieval Optimization\n  for Dynamic Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DioR: Adaptive Cognitive Detection and Contextual Retrieval Optimization\n  for Dynamic Retrieval-Augmented Generation"
                },
                "summary": "Dynamic Retrieval-augmented Generation (RAG) has shown great success in\nmitigating hallucinations in large language models (LLMs) during generation.\nHowever, existing dynamic RAG methods face significant limitations in two key\naspects: 1) Lack of an effective mechanism to control retrieval triggers, and\n2) Lack of effective scrutiny of retrieval content. To address these\nlimitations, we propose an innovative dynamic RAG method, DioR (Adaptive\nCognitive Detection and Contextual Retrieval Optimization), which consists of\ntwo main components: adaptive cognitive detection and contextual retrieval\noptimization, specifically designed to determine when retrieval is needed and\nwhat to retrieve for LLMs is useful. Experimental results demonstrate that DioR\nachieves superior performance on all tasks, demonstrating the effectiveness of\nour work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Retrieval-augmented Generation (RAG) has shown great success in\nmitigating hallucinations in large language models (LLMs) during generation.\nHowever, existing dynamic RAG methods face significant limitations in two key\naspects: 1) Lack of an effective mechanism to control retrieval triggers, and\n2) Lack of effective scrutiny of retrieval content. To address these\nlimitations, we propose an innovative dynamic RAG method, DioR (Adaptive\nCognitive Detection and Contextual Retrieval Optimization), which consists of\ntwo main components: adaptive cognitive detection and contextual retrieval\noptimization, specifically designed to determine when retrieval is needed and\nwhat to retrieve for LLMs is useful. Experimental results demonstrate that DioR\nachieves superior performance on all tasks, demonstrating the effectiveness of\nour work."
                },
                "authors": [
                    {
                        "name": "Hanghui Guo"
                    },
                    {
                        "name": "Jia Zhu"
                    },
                    {
                        "name": "Shimin Di"
                    },
                    {
                        "name": "Weijie Shi"
                    },
                    {
                        "name": "Zhangze Chen"
                    },
                    {
                        "name": "Jiajie Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jiajie Xu"
                },
                "author": "Jiajie Xu",
                "arxiv_comment": "24 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10198v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10191v1",
                "updated": "2025-04-14T12:53:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    53,
                    58,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T12:53:58Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    53,
                    58,
                    0,
                    104,
                    0
                ],
                "title": "Localized Cultural Knowledge is Conserved and Controllable in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localized Cultural Knowledge is Conserved and Controllable in Large\n  Language Models"
                },
                "summary": "Just as humans display language patterns influenced by their native tongue\nwhen speaking new languages, LLMs often default to English-centric responses\neven when generating in other languages. Nevertheless, we observe that local\ncultural information persists within the models and can be readily activated\nfor cultural customization. We first demonstrate that explicitly providing\ncultural context in prompts significantly improves the models' ability to\ngenerate culturally localized responses. We term the disparity in model\nperformance with versus without explicit cultural context the explicit-implicit\nlocalization gap, indicating that while cultural knowledge exists within LLMs,\nit may not naturally surface in multilingual interactions if cultural context\nis not explicitly provided. Despite the explicit prompting benefit, however,\nthe answers reduce in diversity and tend toward stereotypes. Second, we\nidentify an explicit cultural customization vector, conserved across all\nnon-English languages we explore, which enables LLMs to be steered from the\nsynthetic English cultural world-model toward each non-English cultural world.\nSteered responses retain the diversity of implicit prompting and reduce\nstereotypes to dramatically improve the potential for customization. We discuss\nthe implications of explicit cultural customization for understanding the\nconservation of alternative cultural world models within LLMs, and their\ncontrollable utility for translation, cultural customization, and the\npossibility of making the explicit implicit through soft control for expanded\nLLM function and appeal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Just as humans display language patterns influenced by their native tongue\nwhen speaking new languages, LLMs often default to English-centric responses\neven when generating in other languages. Nevertheless, we observe that local\ncultural information persists within the models and can be readily activated\nfor cultural customization. We first demonstrate that explicitly providing\ncultural context in prompts significantly improves the models' ability to\ngenerate culturally localized responses. We term the disparity in model\nperformance with versus without explicit cultural context the explicit-implicit\nlocalization gap, indicating that while cultural knowledge exists within LLMs,\nit may not naturally surface in multilingual interactions if cultural context\nis not explicitly provided. Despite the explicit prompting benefit, however,\nthe answers reduce in diversity and tend toward stereotypes. Second, we\nidentify an explicit cultural customization vector, conserved across all\nnon-English languages we explore, which enables LLMs to be steered from the\nsynthetic English cultural world-model toward each non-English cultural world.\nSteered responses retain the diversity of implicit prompting and reduce\nstereotypes to dramatically improve the potential for customization. We discuss\nthe implications of explicit cultural customization for understanding the\nconservation of alternative cultural world models within LLMs, and their\ncontrollable utility for translation, cultural customization, and the\npossibility of making the explicit implicit through soft control for expanded\nLLM function and appeal."
                },
                "authors": [
                    {
                        "name": "Veniamin Veselovsky"
                    },
                    {
                        "name": "Berke Argin"
                    },
                    {
                        "name": "Benedikt Stroebl"
                    },
                    {
                        "name": "Chris Wendler"
                    },
                    {
                        "name": "Robert West"
                    },
                    {
                        "name": "James Evans"
                    },
                    {
                        "name": "Thomas L. Griffiths"
                    },
                    {
                        "name": "Arvind Narayanan"
                    }
                ],
                "author_detail": {
                    "name": "Arvind Narayanan"
                },
                "author": "Arvind Narayanan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10423v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10423v2",
                "updated": "2025-04-14T12:52:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    52,
                    24,
                    0,
                    104,
                    0
                ],
                "published": "2024-12-10T12:42:33Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    12,
                    42,
                    33,
                    1,
                    345,
                    0
                ],
                "title": "Look Before You Leap: Enhancing Attention and Vigilance Regarding\n  Harmful Content with GuidelineLLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Look Before You Leap: Enhancing Attention and Vigilance Regarding\n  Harmful Content with GuidelineLLM"
                },
                "summary": "Despite being empowered with alignment mechanisms, large language models\n(LLMs) are increasingly vulnerable to emerging jailbreak attacks that can\ncompromise their alignment mechanisms. This vulnerability poses significant\nrisks to real-world applications. Existing work faces challenges in both\ntraining efficiency and generalization capabilities (i.e., Reinforcement\nLearning from Human Feedback and Red-Teaming). Developing effective strategies\nto enable LLMs to resist continuously evolving jailbreak attempts represents a\nsignificant challenge. To address this challenge, we propose a novel defensive\nparadigm called GuidelineLLM, which assists LLMs in recognizing queries that\nmay have harmful content. Before LLMs respond to a query, GuidelineLLM first\nidentifies potential risks associated with the query, summarizes these risks\ninto guideline suggestions, and then feeds these guidelines to the responding\nLLMs. Importantly, our approach eliminates the necessity for additional safety\nfine-tuning of the LLMs themselves; only the GuidelineLLM requires fine-tuning.\nThis characteristic enhances the general applicability of GuidelineLLM across\nvarious LLMs. Experimental results demonstrate that GuidelineLLM can\nsignificantly reduce the attack success rate (ASR) against LLM (an average\nreduction of 34.17\\% ASR) while maintaining the usefulness of LLM in handling\nbenign queries. The code is available at\nhttps://github.com/sqzhang-lazy/GuidelineLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite being empowered with alignment mechanisms, large language models\n(LLMs) are increasingly vulnerable to emerging jailbreak attacks that can\ncompromise their alignment mechanisms. This vulnerability poses significant\nrisks to real-world applications. Existing work faces challenges in both\ntraining efficiency and generalization capabilities (i.e., Reinforcement\nLearning from Human Feedback and Red-Teaming). Developing effective strategies\nto enable LLMs to resist continuously evolving jailbreak attempts represents a\nsignificant challenge. To address this challenge, we propose a novel defensive\nparadigm called GuidelineLLM, which assists LLMs in recognizing queries that\nmay have harmful content. Before LLMs respond to a query, GuidelineLLM first\nidentifies potential risks associated with the query, summarizes these risks\ninto guideline suggestions, and then feeds these guidelines to the responding\nLLMs. Importantly, our approach eliminates the necessity for additional safety\nfine-tuning of the LLMs themselves; only the GuidelineLLM requires fine-tuning.\nThis characteristic enhances the general applicability of GuidelineLLM across\nvarious LLMs. Experimental results demonstrate that GuidelineLLM can\nsignificantly reduce the attack success rate (ASR) against LLM (an average\nreduction of 34.17\\% ASR) while maintaining the usefulness of LLM in handling\nbenign queries. The code is available at\nhttps://github.com/sqzhang-lazy/GuidelineLLM."
                },
                "authors": [
                    {
                        "name": "Shaoqing Zhang"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Rongxiang Weng"
                    },
                    {
                        "name": "Muyun Yang"
                    },
                    {
                        "name": "Tiejun Zhao"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10423v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10423v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17274v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17274v5",
                "updated": "2025-04-14T12:50:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    50,
                    55,
                    0,
                    104,
                    0
                ],
                "published": "2024-11-26T09:51:55Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    9,
                    51,
                    55,
                    1,
                    331,
                    0
                ],
                "title": "CleanVul: Automatic Function-Level Vulnerability Detection in Code\n  Commits Using LLM Heuristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CleanVul: Automatic Function-Level Vulnerability Detection in Code\n  Commits Using LLM Heuristics"
                },
                "summary": "Accurate identification of software vulnerabilities is crucial for system\nintegrity. Vulnerability datasets, often derived from the National\nVulnerability Database (NVD) or directly from GitHub, are essential for\ntraining machine learning models to detect these security flaws. However, these\ndatasets frequently suffer from significant noise, typically 40% to 75%, due\nprimarily to the automatic and indiscriminate labeling of all changes in\nvulnerability-fixing commits (VFCs) as vulnerability-related. This\nmisclassification occurs because not all changes in a commit aimed at fixing\nvulnerabilities pertain to security threats; many are routine updates like bug\nfixes or test improvements.\n  This paper introduces the first methodology that uses the Large Language\nModel (LLM) with a heuristic enhancement to automatically identify\nvulnerability-fixing changes from VFCs, achieving an F1-score of 0.82.\nVulSifter was applied to a large-scale study, where we conducted a crawl of\n127,063 repositories on GitHub, resulting in the acquisition of 5,352,105\ncommits. VulSifter involves utilizing an LLM to comprehend code semantics and\ncontextual information, while applying heuristics to filter out unrelated\nchanges. We then developed CleanVul, a high-quality dataset comprising 8,203\nfunctions using our LLM heuristic enhancement approach, demonstrating\nCorrectness (90.6%) comparable to established datasets such as SVEN and\nPrimeVul.\n  To evaluate the CleanVul dataset, we conducted experiments focusing on\nfine-tuning various LLMs on CleanVul and other high-quality datasets.\nEvaluation results reveal that LLMs fine-tuned on CleanVul not only exhibit\nenhanced accuracy but also superior generalization capabilities compared to\nthose trained on uncleaned datasets. Specifically, models trained on CleanVul\nand tested on PrimeVul achieve accuracy higher than those trained and tested\nexclusively on PrimeVul.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate identification of software vulnerabilities is crucial for system\nintegrity. Vulnerability datasets, often derived from the National\nVulnerability Database (NVD) or directly from GitHub, are essential for\ntraining machine learning models to detect these security flaws. However, these\ndatasets frequently suffer from significant noise, typically 40% to 75%, due\nprimarily to the automatic and indiscriminate labeling of all changes in\nvulnerability-fixing commits (VFCs) as vulnerability-related. This\nmisclassification occurs because not all changes in a commit aimed at fixing\nvulnerabilities pertain to security threats; many are routine updates like bug\nfixes or test improvements.\n  This paper introduces the first methodology that uses the Large Language\nModel (LLM) with a heuristic enhancement to automatically identify\nvulnerability-fixing changes from VFCs, achieving an F1-score of 0.82.\nVulSifter was applied to a large-scale study, where we conducted a crawl of\n127,063 repositories on GitHub, resulting in the acquisition of 5,352,105\ncommits. VulSifter involves utilizing an LLM to comprehend code semantics and\ncontextual information, while applying heuristics to filter out unrelated\nchanges. We then developed CleanVul, a high-quality dataset comprising 8,203\nfunctions using our LLM heuristic enhancement approach, demonstrating\nCorrectness (90.6%) comparable to established datasets such as SVEN and\nPrimeVul.\n  To evaluate the CleanVul dataset, we conducted experiments focusing on\nfine-tuning various LLMs on CleanVul and other high-quality datasets.\nEvaluation results reveal that LLMs fine-tuned on CleanVul not only exhibit\nenhanced accuracy but also superior generalization capabilities compared to\nthose trained on uncleaned datasets. Specifically, models trained on CleanVul\nand tested on PrimeVul achieve accuracy higher than those trained and tested\nexclusively on PrimeVul."
                },
                "authors": [
                    {
                        "name": "Yikun Li"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "Ratnadira Widyasari"
                    },
                    {
                        "name": "Yan Naing Tun"
                    },
                    {
                        "name": "Huu Hung Nguyen"
                    },
                    {
                        "name": "Tan Bui"
                    },
                    {
                        "name": "Ivana Clairine Irsan"
                    },
                    {
                        "name": "Yiran Cheng"
                    },
                    {
                        "name": "Xiang Lan"
                    },
                    {
                        "name": "Han Wei Ang"
                    },
                    {
                        "name": "Frank Liauw"
                    },
                    {
                        "name": "Martin Weyssow"
                    },
                    {
                        "name": "Hong Jin Kang"
                    },
                    {
                        "name": "Eng Lieh Ouh"
                    },
                    {
                        "name": "Lwin Khin Shar"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17274v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17274v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10190v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10190v1",
                "updated": "2025-04-14T12:50:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    50,
                    37,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T12:50:37Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    50,
                    37,
                    0,
                    104,
                    0
                ],
                "title": "Differentially Private 2D Human Pose Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially Private 2D Human Pose Estimation"
                },
                "summary": "Human pose estimation (HPE) has become essential in numerous applications\nincluding healthcare, activity recognition, and human-computer interaction.\nHowever, the privacy implications of processing sensitive visual data present\nsignificant deployment barriers in critical domains. While traditional\nanonymization techniques offer limited protection and often compromise data\nutility for broader motion analysis, Differential Privacy (DP) provides formal\nprivacy guarantees but typically degrades model performance when applied\nnaively. In this work, we present the first differentially private 2D human\npose estimation (2D-HPE) by applying Differentially Private Stochastic Gradient\nDescent (DP-SGD) to this task. To effectively balance privacy with performance,\nwe adopt Projected DP-SGD (PDP-SGD), which projects the noisy gradients to a\nlow-dimensional subspace. Additionally, we adapt TinyViT, a compact and\nefficient vision transformer for coordinate classification in HPE, providing a\nlightweight yet powerful backbone that enhances privacy-preserving deployment\nfeasibility on resource-limited devices. Our approach is particularly valuable\nfor multimedia interpretation tasks, enabling privacy-safe analysis and\nunderstanding of human motion across diverse visual media while preserving the\nsemantic meaning required for downstream applications. Comprehensive\nexperiments on the MPII Human Pose Dataset demonstrate significant performance\nenhancement with PDP-SGD achieving 78.48% PCKh@0.5 at a strict privacy budget\n($\\epsilon=0.2$), compared to 63.85% for standard DP-SGD. This work lays\nfoundation for privacy-preserving human pose estimation in real-world,\nsensitive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human pose estimation (HPE) has become essential in numerous applications\nincluding healthcare, activity recognition, and human-computer interaction.\nHowever, the privacy implications of processing sensitive visual data present\nsignificant deployment barriers in critical domains. While traditional\nanonymization techniques offer limited protection and often compromise data\nutility for broader motion analysis, Differential Privacy (DP) provides formal\nprivacy guarantees but typically degrades model performance when applied\nnaively. In this work, we present the first differentially private 2D human\npose estimation (2D-HPE) by applying Differentially Private Stochastic Gradient\nDescent (DP-SGD) to this task. To effectively balance privacy with performance,\nwe adopt Projected DP-SGD (PDP-SGD), which projects the noisy gradients to a\nlow-dimensional subspace. Additionally, we adapt TinyViT, a compact and\nefficient vision transformer for coordinate classification in HPE, providing a\nlightweight yet powerful backbone that enhances privacy-preserving deployment\nfeasibility on resource-limited devices. Our approach is particularly valuable\nfor multimedia interpretation tasks, enabling privacy-safe analysis and\nunderstanding of human motion across diverse visual media while preserving the\nsemantic meaning required for downstream applications. Comprehensive\nexperiments on the MPII Human Pose Dataset demonstrate significant performance\nenhancement with PDP-SGD achieving 78.48% PCKh@0.5 at a strict privacy budget\n($\\epsilon=0.2$), compared to 63.85% for standard DP-SGD. This work lays\nfoundation for privacy-preserving human pose estimation in real-world,\nsensitive applications."
                },
                "authors": [
                    {
                        "name": "Kaushik Bhargav Sivangi"
                    },
                    {
                        "name": "Idris Zakariyya"
                    },
                    {
                        "name": "Paul Henderson"
                    },
                    {
                        "name": "Fani Deligianni"
                    }
                ],
                "author_detail": {
                    "name": "Fani Deligianni"
                },
                "author": "Fani Deligianni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10190v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10190v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10187v1",
                "updated": "2025-04-14T12:40:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    40,
                    39,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T12:40:39Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    40,
                    39,
                    0,
                    104,
                    0
                ],
                "title": "Deep Reasoning Translation via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Reasoning Translation via Reinforcement Learning"
                },
                "summary": "Recently, deep reasoning LLMs (e.g., OpenAI o1/o3 and DeepSeek-R1) have shown\npromising performance in various complex tasks. Free translation is an\nimportant and interesting task in the multilingual world, which requires going\nbeyond word-for-word translation and taking cultural differences into account.\nThis task is still under-explored in deep reasoning LLMs. In this paper, we\nintroduce DeepTrans, a deep reasoning translation model that learns free\ntranslation via reinforcement learning. Specifically, we carefully build a\nreward model with pre-defined scoring criteria on both the translation results\nand the thought process. Given the source sentences, the reward model teaches\nthe deep translation model how to think and free-translate them during\nreinforcement learning. In this way, training DeepTrans does not need any\nlabeled translations, avoiding the human-intensive annotation or\nresource-intensive data synthesis. Experimental results show the effectiveness\nof DeepTrans. Using Qwen2.5-7B as the backbone, DeepTrans improves performance\nby 16.3% in literature translation, and outperforms strong deep reasoning\nbaselines as well as baselines that are fine-tuned with synthesized data.\nMoreover, we summarize the failures and interesting findings during our RL\nexploration. We hope this work could inspire other researchers in free\ntranslation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, deep reasoning LLMs (e.g., OpenAI o1/o3 and DeepSeek-R1) have shown\npromising performance in various complex tasks. Free translation is an\nimportant and interesting task in the multilingual world, which requires going\nbeyond word-for-word translation and taking cultural differences into account.\nThis task is still under-explored in deep reasoning LLMs. In this paper, we\nintroduce DeepTrans, a deep reasoning translation model that learns free\ntranslation via reinforcement learning. Specifically, we carefully build a\nreward model with pre-defined scoring criteria on both the translation results\nand the thought process. Given the source sentences, the reward model teaches\nthe deep translation model how to think and free-translate them during\nreinforcement learning. In this way, training DeepTrans does not need any\nlabeled translations, avoiding the human-intensive annotation or\nresource-intensive data synthesis. Experimental results show the effectiveness\nof DeepTrans. Using Qwen2.5-7B as the backbone, DeepTrans improves performance\nby 16.3% in literature translation, and outperforms strong deep reasoning\nbaselines as well as baselines that are fine-tuned with synthesized data.\nMoreover, we summarize the failures and interesting findings during our RL\nexploration. We hope this work could inspire other researchers in free\ntranslation."
                },
                "authors": [
                    {
                        "name": "Jiaan Wang"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhou"
                },
                "author": "Jie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10186v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10186v1",
                "updated": "2025-04-14T12:39:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    39,
                    25,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T12:39:25Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    39,
                    25,
                    0,
                    104,
                    0
                ],
                "title": "Entanglement-Enabled Connectivity Bounds for Quantum Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entanglement-Enabled Connectivity Bounds for Quantum Networks"
                },
                "summary": "In the Quantum Internet, multipartite entanglement enables a new form of\nnetwork connectivity, referred to as artificial connectivity namely and able to\naugment the physical connectivity with artificial links between pairs of nodes,\nwithout any additional physical link deployment. In this paper, by engineering\nsuch an artificial connectivity, we theoretically determine upper and lower\nbounds for the number of EPR pairs and GHZ states that can be extracted among\nnodes that are not adjacent in the artificial network topology. The\naforementioned analysis is crucial, since the extraction of EPR pairs and GHZ\nstates among remote nodes constitutes the resource primitives for on-demand and\nend-to-end communications. Indeed, within the paper, we not only determine\nwhether a certain number of remote EPR pairs and GHZ states can be extracted,\nbut we also provide the locations, namely the identities, of the nodes\ninterconnected by such entangled resources. Thus, our analysis is far from\nbeing purely theoretical, rather it is constructive, since we provide the\nsequence of operations required for performing such extractions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the Quantum Internet, multipartite entanglement enables a new form of\nnetwork connectivity, referred to as artificial connectivity namely and able to\naugment the physical connectivity with artificial links between pairs of nodes,\nwithout any additional physical link deployment. In this paper, by engineering\nsuch an artificial connectivity, we theoretically determine upper and lower\nbounds for the number of EPR pairs and GHZ states that can be extracted among\nnodes that are not adjacent in the artificial network topology. The\naforementioned analysis is crucial, since the extraction of EPR pairs and GHZ\nstates among remote nodes constitutes the resource primitives for on-demand and\nend-to-end communications. Indeed, within the paper, we not only determine\nwhether a certain number of remote EPR pairs and GHZ states can be extracted,\nbut we also provide the locations, namely the identities, of the nodes\ninterconnected by such entangled resources. Thus, our analysis is far from\nbeing purely theoretical, rather it is constructive, since we provide the\nsequence of operations required for performing such extractions."
                },
                "authors": [
                    {
                        "name": "Si-Yi Chen"
                    },
                    {
                        "name": "Angela Sara Cacciapuoti"
                    },
                    {
                        "name": "Marcello Caleffi"
                    }
                ],
                "author_detail": {
                    "name": "Marcello Caleffi"
                },
                "author": "Marcello Caleffi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10186v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10186v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10185v1",
                "updated": "2025-04-14T12:38:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    38,
                    37,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T12:38:37Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    38,
                    37,
                    0,
                    104,
                    0
                ],
                "title": "LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in\n  Current Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in\n  Current Benchmarks"
                },
                "summary": "Large language model unlearning has become a critical challenge in ensuring\nsafety and controlled model behavior by removing undesired data-model\ninfluences from the pretrained model while preserving general utility.\nSignificant recent efforts have been dedicated to developing LLM unlearning\nbenchmarks such as WMDP (Weapons of Mass Destruction Proxy) and MUSE (Machine\nUnlearning Six-way Evaluation), facilitating standardized unlearning\nperformance assessment and method comparison. Despite their usefulness, we\nuncover for the first time a novel coreset effect within these benchmarks.\nSpecifically, we find that LLM unlearning achieved with the original (full)\nforget set can be effectively maintained using a significantly smaller subset\n(functioning as a \"coreset\"), e.g., as little as 5% of the forget set, even\nwhen selected at random. This suggests that LLM unlearning in these benchmarks\ncan be performed surprisingly easily, even in an extremely low-data regime. We\ndemonstrate that this coreset effect remains strong, regardless of the LLM\nunlearning method used, such as NPO (Negative Preference Optimization) and RMU\n(Representation Misdirection Unlearning), the popular ones in these benchmarks.\nThe surprisingly strong coreset effect is also robust across various data\nselection methods, ranging from random selection to more sophisticated\nheuristic approaches. We explain the coreset effect in LLM unlearning through a\nkeyword-based perspective, showing that keywords extracted from the forget set\nalone contribute significantly to unlearning effectiveness and indicating that\ncurrent unlearning is driven by a compact set of high-impact tokens rather than\nthe entire dataset. We further justify the faithfulness of coreset-unlearned\nmodels along additional dimensions, such as mode connectivity and robustness to\njailbreaking attacks. Codes are available at\nhttps://github.com/OPTML-Group/MU-Coreset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model unlearning has become a critical challenge in ensuring\nsafety and controlled model behavior by removing undesired data-model\ninfluences from the pretrained model while preserving general utility.\nSignificant recent efforts have been dedicated to developing LLM unlearning\nbenchmarks such as WMDP (Weapons of Mass Destruction Proxy) and MUSE (Machine\nUnlearning Six-way Evaluation), facilitating standardized unlearning\nperformance assessment and method comparison. Despite their usefulness, we\nuncover for the first time a novel coreset effect within these benchmarks.\nSpecifically, we find that LLM unlearning achieved with the original (full)\nforget set can be effectively maintained using a significantly smaller subset\n(functioning as a \"coreset\"), e.g., as little as 5% of the forget set, even\nwhen selected at random. This suggests that LLM unlearning in these benchmarks\ncan be performed surprisingly easily, even in an extremely low-data regime. We\ndemonstrate that this coreset effect remains strong, regardless of the LLM\nunlearning method used, such as NPO (Negative Preference Optimization) and RMU\n(Representation Misdirection Unlearning), the popular ones in these benchmarks.\nThe surprisingly strong coreset effect is also robust across various data\nselection methods, ranging from random selection to more sophisticated\nheuristic approaches. We explain the coreset effect in LLM unlearning through a\nkeyword-based perspective, showing that keywords extracted from the forget set\nalone contribute significantly to unlearning effectiveness and indicating that\ncurrent unlearning is driven by a compact set of high-impact tokens rather than\nthe entire dataset. We further justify the faithfulness of coreset-unlearned\nmodels along additional dimensions, such as mode connectivity and robustness to\njailbreaking attacks. Codes are available at\nhttps://github.com/OPTML-Group/MU-Coreset."
                },
                "authors": [
                    {
                        "name": "Soumyadeep Pal"
                    },
                    {
                        "name": "Changsheng Wang"
                    },
                    {
                        "name": "James Diffenderfer"
                    },
                    {
                        "name": "Bhavya Kailkhura"
                    },
                    {
                        "name": "Sijia Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sijia Liu"
                },
                "author": "Sijia Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05045v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05045v3",
                "updated": "2025-04-14T12:33:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    33,
                    32,
                    0,
                    104,
                    0
                ],
                "published": "2024-09-08T10:06:54Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    10,
                    6,
                    54,
                    6,
                    252,
                    0
                ],
                "title": "Using Large Language Models for Template Detection from Security Event\n  Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Large Language Models for Template Detection from Security Event\n  Logs"
                },
                "summary": "In modern IT systems and computer networks, real-time and offline event log\nanalysis is a crucial part of cyber security monitoring. In particular, event\nlog analysis techniques are essential for the timely detection of cyber attacks\nand for assisting security experts with the analysis of past security\nincidents. The detection of line patterns or templates from unstructured\ntextual event logs has been identified as an important task of event log\nanalysis since detected templates represent event types in the event log and\nprepare the logs for downstream online or offline security monitoring tasks.\nDuring the last two decades, a number of template mining algorithms have been\nproposed. However, many proposed algorithms rely on traditional data mining\ntechniques, and the usage of Large Language Models (LLMs) has received less\nattention so far. Also, most approaches that harness LLMs are supervised, and\nunsupervised LLM-based template mining remains an understudied area. The\ncurrent paper addresses this research gap and investigates the application of\nLLMs for unsupervised detection of templates from unstructured security event\nlogs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern IT systems and computer networks, real-time and offline event log\nanalysis is a crucial part of cyber security monitoring. In particular, event\nlog analysis techniques are essential for the timely detection of cyber attacks\nand for assisting security experts with the analysis of past security\nincidents. The detection of line patterns or templates from unstructured\ntextual event logs has been identified as an important task of event log\nanalysis since detected templates represent event types in the event log and\nprepare the logs for downstream online or offline security monitoring tasks.\nDuring the last two decades, a number of template mining algorithms have been\nproposed. However, many proposed algorithms rely on traditional data mining\ntechniques, and the usage of Large Language Models (LLMs) has received less\nattention so far. Also, most approaches that harness LLMs are supervised, and\nunsupervised LLM-based template mining remains an understudied area. The\ncurrent paper addresses this research gap and investigates the application of\nLLMs for unsupervised detection of templates from unstructured security event\nlogs."
                },
                "authors": [
                    {
                        "name": "Risto Vaarandi"
                    },
                    {
                        "name": "Hayretdin Bahsi"
                    }
                ],
                "author_detail": {
                    "name": "Hayretdin Bahsi"
                },
                "author": "Hayretdin Bahsi",
                "arxiv_doi": "10.1007/s10207-025-01018-y",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10207-025-01018-y",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.05045v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05045v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication in International Journal of Information\n  Security",
                "arxiv_journal_ref": "Int. J. Inf. Secur. 24, 104 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10179v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10179v1",
                "updated": "2025-04-14T12:31:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    31,
                    39,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T12:31:39Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    31,
                    39,
                    0,
                    104,
                    0
                ],
                "title": "The Future of MLLM Prompting is Adaptive: A Comprehensive Experimental\n  Evaluation of Prompt Engineering Methods for Robust Multimodal Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Future of MLLM Prompting is Adaptive: A Comprehensive Experimental\n  Evaluation of Prompt Engineering Methods for Robust Multimodal Performance"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are set to transform how machines\nprocess and generate human-like responses by integrating diverse modalities\nsuch as text, images, and code. Yet, effectively harnessing their capabilities\nhinges on optimal prompt engineering. We present a comprehensive experimental\nevaluation of seven prompt engineering methods applied to 13 open-source MLLMs\nover 24 tasks spanning Reasoning and Compositionality, Multimodal Understanding\nand Alignment, Complex Code Generation and Execution, and Knowledge Retrieval\nand Integration. Our approach stratifies models by parameter count into Small\n(<4B), Medium (4B-10B), and Large (>10B) categories and compares prompting\ntechniques including Zero-Shot, One-Shot, Few-Shot, Chain-of-Thought,\nAnalogical, Generated Knowledge, and Tree-of-Thought. While Large MLLMs excel\nin structured tasks such as code generation, achieving accuracies up to 96.88%\nunder Few-Shot prompting, all models struggle with complex reasoning and\nabstract understanding, often yielding accuracies below 60% and high\nhallucination rates. Structured reasoning prompts frequently increased\nhallucination up to 75% in small models and led to longer response times (over\n20 seconds in Large MLLMs), while simpler prompting methods provided more\nconcise and efficient outputs. No single prompting method uniformly optimises\nall task types. Instead, adaptive strategies combining example-based guidance\nwith selective structured reasoning are essential to enhance robustness,\nefficiency, and factual accuracy. Our findings offer practical recommendations\nfor prompt engineering and support more reliable deployment of MLLMs across\napplications including AI-assisted coding, knowledge retrieval, and multimodal\ncontent understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are set to transform how machines\nprocess and generate human-like responses by integrating diverse modalities\nsuch as text, images, and code. Yet, effectively harnessing their capabilities\nhinges on optimal prompt engineering. We present a comprehensive experimental\nevaluation of seven prompt engineering methods applied to 13 open-source MLLMs\nover 24 tasks spanning Reasoning and Compositionality, Multimodal Understanding\nand Alignment, Complex Code Generation and Execution, and Knowledge Retrieval\nand Integration. Our approach stratifies models by parameter count into Small\n(<4B), Medium (4B-10B), and Large (>10B) categories and compares prompting\ntechniques including Zero-Shot, One-Shot, Few-Shot, Chain-of-Thought,\nAnalogical, Generated Knowledge, and Tree-of-Thought. While Large MLLMs excel\nin structured tasks such as code generation, achieving accuracies up to 96.88%\nunder Few-Shot prompting, all models struggle with complex reasoning and\nabstract understanding, often yielding accuracies below 60% and high\nhallucination rates. Structured reasoning prompts frequently increased\nhallucination up to 75% in small models and led to longer response times (over\n20 seconds in Large MLLMs), while simpler prompting methods provided more\nconcise and efficient outputs. No single prompting method uniformly optimises\nall task types. Instead, adaptive strategies combining example-based guidance\nwith selective structured reasoning are essential to enhance robustness,\nefficiency, and factual accuracy. Our findings offer practical recommendations\nfor prompt engineering and support more reliable deployment of MLLMs across\napplications including AI-assisted coding, knowledge retrieval, and multimodal\ncontent understanding."
                },
                "authors": [
                    {
                        "name": "Anwesha Mohanty"
                    },
                    {
                        "name": "Venkatesh Balavadhani Parthasarathy"
                    },
                    {
                        "name": "Arsalan Shahid"
                    }
                ],
                "author_detail": {
                    "name": "Arsalan Shahid"
                },
                "author": "Arsalan Shahid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10179v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05946v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05946v2",
                "updated": "2025-04-14T12:28:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    28,
                    2,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-08T11:59:00Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    11,
                    59,
                    0,
                    1,
                    98,
                    0
                ],
                "title": "InstructMPC: A Human-LLM-in-the-Loop Framework for Context-Aware Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstructMPC: A Human-LLM-in-the-Loop Framework for Context-Aware Control"
                },
                "summary": "Model Predictive Control (MPC) is a powerful control strategy widely utilized\nin domains like energy management, building control, and autonomous systems.\nHowever, its effectiveness in real-world settings is challenged by the need to\nincorporate context-specific predictions and expert instructions, which\ntraditional MPC often neglects. We propose InstructMPC, a novel framework that\naddresses this gap by integrating real-time human instructions through a Large\nLanguage Model (LLM) to produce context-aware predictions for MPC. Our method\nemploys a Language-to-Distribution (L2D) module to translate contextual\ninformation into predictive disturbance trajectories, which are then\nincorporated into the MPC optimization. Unlike existing context-aware and\nlanguage-based MPC models, InstructMPC enables dynamic human-LLM interaction\nand fine-tunes the L2D module in a closed loop with theoretical performance\nguarantees, achieving a regret bound of $O(\\sqrt{T\\log T})$ for linear dynamics\nwhen optimized via advanced fine-tuning methods such as Direct Preference\nOptimization (DPO) using a tailored loss function.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Predictive Control (MPC) is a powerful control strategy widely utilized\nin domains like energy management, building control, and autonomous systems.\nHowever, its effectiveness in real-world settings is challenged by the need to\nincorporate context-specific predictions and expert instructions, which\ntraditional MPC often neglects. We propose InstructMPC, a novel framework that\naddresses this gap by integrating real-time human instructions through a Large\nLanguage Model (LLM) to produce context-aware predictions for MPC. Our method\nemploys a Language-to-Distribution (L2D) module to translate contextual\ninformation into predictive disturbance trajectories, which are then\nincorporated into the MPC optimization. Unlike existing context-aware and\nlanguage-based MPC models, InstructMPC enables dynamic human-LLM interaction\nand fine-tunes the L2D module in a closed loop with theoretical performance\nguarantees, achieving a regret bound of $O(\\sqrt{T\\log T})$ for linear dynamics\nwhen optimized via advanced fine-tuning methods such as Direct Preference\nOptimization (DPO) using a tailored loss function."
                },
                "authors": [
                    {
                        "name": "Ruixiang Wu"
                    },
                    {
                        "name": "Jiahao Ai"
                    },
                    {
                        "name": "Tongxin Li"
                    }
                ],
                "author_detail": {
                    "name": "Tongxin Li"
                },
                "author": "Tongxin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05946v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05946v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13929v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13929v6",
                "updated": "2025-04-14T12:23:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    23,
                    27,
                    0,
                    104,
                    0
                ],
                "published": "2024-05-22T18:58:58Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    18,
                    58,
                    58,
                    2,
                    143,
                    0
                ],
                "title": "Vikhr: The Family of Open-Source Instruction-Tuned Large Language Models\n  for Russian",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vikhr: The Family of Open-Source Instruction-Tuned Large Language Models\n  for Russian"
                },
                "summary": "There has been a surge in the development of various Large Language Models\n(LLMs). However, text generation for languages other than English often faces\nsignificant challenges, including poor generation quality and reduced\ncomputational performance due to the disproportionate representation of tokens\nin the model's vocabulary. In this work, we address these issues by developing\na pipeline for the adaptation of English-oriented pre-trained models to other\nlanguages and constructing efficient bilingual LLMs. Using this pipeline, we\nconstruct Vikhr, a series of bilingual open-source instruction-following LLMs\ndesigned specifically for the Russian language. ``Vikhr'' refers to the name of\nthe Mistral LLM series and means a ``strong gust of wind.'' Unlike previous\nRussian-language models that typically rely on LoRA adapters on top of\nEnglish-oriented models, sacrificing performance for lower training costs,\nVikhr features an adapted tokenizer vocabulary and undergoes the continued\npre-training and instruction tuning of all weights. This not only enhances the\nmodel's performance but also significantly improves its computational and\ncontextual efficiency. We also expanded the instruction datasets and corpora\nfor continued pre-training. The model weights, instruction sets, and code are\npublicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There has been a surge in the development of various Large Language Models\n(LLMs). However, text generation for languages other than English often faces\nsignificant challenges, including poor generation quality and reduced\ncomputational performance due to the disproportionate representation of tokens\nin the model's vocabulary. In this work, we address these issues by developing\na pipeline for the adaptation of English-oriented pre-trained models to other\nlanguages and constructing efficient bilingual LLMs. Using this pipeline, we\nconstruct Vikhr, a series of bilingual open-source instruction-following LLMs\ndesigned specifically for the Russian language. ``Vikhr'' refers to the name of\nthe Mistral LLM series and means a ``strong gust of wind.'' Unlike previous\nRussian-language models that typically rely on LoRA adapters on top of\nEnglish-oriented models, sacrificing performance for lower training costs,\nVikhr features an adapted tokenizer vocabulary and undergoes the continued\npre-training and instruction tuning of all weights. This not only enhances the\nmodel's performance but also significantly improves its computational and\ncontextual efficiency. We also expanded the instruction datasets and corpora\nfor continued pre-training. The model weights, instruction sets, and code are\npublicly available."
                },
                "authors": [
                    {
                        "name": "Aleksandr Nikolich"
                    },
                    {
                        "name": "Konstantin Korolev"
                    },
                    {
                        "name": "Sergei Bratchikov"
                    },
                    {
                        "name": "Igor Kiselev"
                    },
                    {
                        "name": "Artem Shelmanov"
                    }
                ],
                "author_detail": {
                    "name": "Artem Shelmanov"
                },
                "author": "Artem Shelmanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13929v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13929v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10168v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10168v1",
                "updated": "2025-04-14T12:22:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    22,
                    30,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T12:22:30Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    22,
                    30,
                    0,
                    104,
                    0
                ],
                "title": "HalluSearch at SemEval-2025 Task 3: A Search-Enhanced RAG Pipeline for\n  Hallucination Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HalluSearch at SemEval-2025 Task 3: A Search-Enhanced RAG Pipeline for\n  Hallucination Detection"
                },
                "summary": "In this paper, we present HalluSearch, a multilingual pipeline designed to\ndetect fabricated text spans in Large Language Model (LLM) outputs. Developed\nas part of Mu-SHROOM, the Multilingual Shared-task on Hallucinations and\nRelated Observable Overgeneration Mistakes, HalluSearch couples\nretrieval-augmented verification with fine-grained factual splitting to\nidentify and localize hallucinations in fourteen different languages. Empirical\nevaluations show that HalluSearch performs competitively, placing fourth in\nboth English (within the top ten percent) and Czech. While the system's\nretrieval-based strategy generally proves robust, it faces challenges in\nlanguages with limited online coverage, underscoring the need for further\nresearch to ensure consistent hallucination detection across diverse linguistic\ncontexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present HalluSearch, a multilingual pipeline designed to\ndetect fabricated text spans in Large Language Model (LLM) outputs. Developed\nas part of Mu-SHROOM, the Multilingual Shared-task on Hallucinations and\nRelated Observable Overgeneration Mistakes, HalluSearch couples\nretrieval-augmented verification with fine-grained factual splitting to\nidentify and localize hallucinations in fourteen different languages. Empirical\nevaluations show that HalluSearch performs competitively, placing fourth in\nboth English (within the top ten percent) and Czech. While the system's\nretrieval-based strategy generally proves robust, it faces challenges in\nlanguages with limited online coverage, underscoring the need for further\nresearch to ensure consistent hallucination detection across diverse linguistic\ncontexts."
                },
                "authors": [
                    {
                        "name": "Mohamed A. Abdallah"
                    },
                    {
                        "name": "Samhaa R. El-Beltagy"
                    }
                ],
                "author_detail": {
                    "name": "Samhaa R. El-Beltagy"
                },
                "author": "Samhaa R. El-Beltagy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10168v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10167v1",
                "updated": "2025-04-14T12:21:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    21,
                    55,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T12:21:55Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    21,
                    55,
                    0,
                    104,
                    0
                ],
                "title": "C-FAITH: A Chinese Fine-Grained Benchmark for Automated Hallucination\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C-FAITH: A Chinese Fine-Grained Benchmark for Automated Hallucination\n  Evaluation"
                },
                "summary": "Despite the rapid advancement of large language models, they remain highly\nsusceptible to generating hallucinations, which significantly hinders their\nwidespread application. Hallucination research requires dynamic and\nfine-grained evaluation. However, most existing hallucination benchmarks\n(especially in Chinese language) rely on human annotations, making automatical\nand cost-effective hallucination evaluation challenging. To address this, we\nintroduce HaluAgent, an agentic framework that automatically constructs\nfine-grained QA dataset based on some knowledge documents. Our experiments\ndemonstrate that the manually designed rules and prompt optimization can\nimprove the quality of generated data. Using HaluAgent, we construct C-FAITH, a\nChinese QA hallucination benchmark created from 1,399 knowledge documents\nobtained from web scraping, totaling 60,702 entries. We comprehensively\nevaluate 16 mainstream LLMs with our proposed C-FAITH, providing detailed\nexperimental results and analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the rapid advancement of large language models, they remain highly\nsusceptible to generating hallucinations, which significantly hinders their\nwidespread application. Hallucination research requires dynamic and\nfine-grained evaluation. However, most existing hallucination benchmarks\n(especially in Chinese language) rely on human annotations, making automatical\nand cost-effective hallucination evaluation challenging. To address this, we\nintroduce HaluAgent, an agentic framework that automatically constructs\nfine-grained QA dataset based on some knowledge documents. Our experiments\ndemonstrate that the manually designed rules and prompt optimization can\nimprove the quality of generated data. Using HaluAgent, we construct C-FAITH, a\nChinese QA hallucination benchmark created from 1,399 knowledge documents\nobtained from web scraping, totaling 60,702 entries. We comprehensively\nevaluate 16 mainstream LLMs with our proposed C-FAITH, providing detailed\nexperimental results and analysis."
                },
                "authors": [
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Zhifei Liu"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Huixuan Zhang"
                    },
                    {
                        "name": "Fan Xu"
                    },
                    {
                        "name": "Junzhe Zhang"
                    },
                    {
                        "name": "Xiaojun Wan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Wan"
                },
                "author": "Xiaojun Wan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10166v1",
                "updated": "2025-04-14T12:21:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    21,
                    27,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T12:21:27Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    21,
                    27,
                    0,
                    104,
                    0
                ],
                "title": "Fact-Checking with Contextual Narratives: Leveraging Retrieval-Augmented\n  LLMs for Social Media Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fact-Checking with Contextual Narratives: Leveraging Retrieval-Augmented\n  LLMs for Social Media Analysis"
                },
                "summary": "We propose CRAVE (Cluster-based Retrieval Augmented Verification with\nExplanation); a novel framework that integrates retrieval-augmented Large\nLanguage Models (LLMs) with clustering techniques to address fact-checking\nchallenges on social media. CRAVE automatically retrieves multimodal evidence\nfrom diverse, often contradictory, sources. Evidence is clustered into coherent\nnarratives, and evaluated via an LLM-based judge to deliver fact-checking\nverdicts explained by evidence summaries. By synthesizing evidence from both\ntext and image modalities and incorporating agent-based refinement, CRAVE\nensures consistency and diversity in evidence representation. Comprehensive\nexperiments demonstrate CRAVE's efficacy in retrieval precision, clustering\nquality, and judgment accuracy, showcasing its potential as a robust\ndecision-support tool for fact-checkers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose CRAVE (Cluster-based Retrieval Augmented Verification with\nExplanation); a novel framework that integrates retrieval-augmented Large\nLanguage Models (LLMs) with clustering techniques to address fact-checking\nchallenges on social media. CRAVE automatically retrieves multimodal evidence\nfrom diverse, often contradictory, sources. Evidence is clustered into coherent\nnarratives, and evaluated via an LLM-based judge to deliver fact-checking\nverdicts explained by evidence summaries. By synthesizing evidence from both\ntext and image modalities and incorporating agent-based refinement, CRAVE\nensures consistency and diversity in evidence representation. Comprehensive\nexperiments demonstrate CRAVE's efficacy in retrieval precision, clustering\nquality, and judgment accuracy, showcasing its potential as a robust\ndecision-support tool for fact-checkers."
                },
                "authors": [
                    {
                        "name": "Arka Ujjal Dey"
                    },
                    {
                        "name": "Muhammad Junaid Awan"
                    },
                    {
                        "name": "Georgia Channing"
                    },
                    {
                        "name": "Christian Schroeder de Witt"
                    },
                    {
                        "name": "John Collomosse"
                    }
                ],
                "author_detail": {
                    "name": "John Collomosse"
                },
                "author": "John Collomosse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10160v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10160v1",
                "updated": "2025-04-14T12:14:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    14,
                    18,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T12:14:18Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    14,
                    18,
                    0,
                    104,
                    0
                ],
                "title": "MT-R1-Zero: Advancing LLM-based Machine Translation via R1-Zero-like\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MT-R1-Zero: Advancing LLM-based Machine Translation via R1-Zero-like\n  Reinforcement Learning"
                },
                "summary": "Large-scale reinforcement learning (RL) methods have proven highly effective\nin enhancing the reasoning abilities of large language models (LLMs),\nparticularly for tasks with verifiable solutions such as mathematics and\ncoding. However, applying this idea to machine translation (MT), where outputs\nare flexibly formatted and difficult to automatically evaluate with explicit\nrules, remains underexplored. In this work, we introduce MT-R1-Zero, the first\nopen-source adaptation of the R1-Zero RL framework for MT without supervised\nfine-tuning or cold-start. We propose a rule-metric mixed reward mechanism to\nguide LLMs towards improved translation quality via emergent reasoning. On the\nWMT 24 English-Chinese benchmark, our MT-R1-Zero-3B-Mix achieves competitive\nperformance, surpassing TowerInstruct-7B-v0.2 by an average of 1.26 points.\nMeanwhile, our MT-R1-Zero-7B-Mix attains a high average score of 62.25 across\nall metrics, placing it on par with advanced proprietary models such as GPT-4o\nand Claude-3.5-Sonnet, while the MT-R1-Zero-7B-Sem variant achieves\nstate-of-the-art scores on semantic metrics. Moreover, our work exhibits strong\ngeneralization capabilities on out-of-distribution MT tasks, robustly\nsupporting multilingual and low-resource settings. Extensive analysis of model\nbehavior across different initializations and reward metrics offers pioneering\ninsight into the critical role of reward design, LLM adaptability, training\ndynamics, and emergent reasoning patterns within the R1-Zero paradigm for MT.\nOur code is available at https://github.com/fzp0424/MT-R1-Zero.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale reinforcement learning (RL) methods have proven highly effective\nin enhancing the reasoning abilities of large language models (LLMs),\nparticularly for tasks with verifiable solutions such as mathematics and\ncoding. However, applying this idea to machine translation (MT), where outputs\nare flexibly formatted and difficult to automatically evaluate with explicit\nrules, remains underexplored. In this work, we introduce MT-R1-Zero, the first\nopen-source adaptation of the R1-Zero RL framework for MT without supervised\nfine-tuning or cold-start. We propose a rule-metric mixed reward mechanism to\nguide LLMs towards improved translation quality via emergent reasoning. On the\nWMT 24 English-Chinese benchmark, our MT-R1-Zero-3B-Mix achieves competitive\nperformance, surpassing TowerInstruct-7B-v0.2 by an average of 1.26 points.\nMeanwhile, our MT-R1-Zero-7B-Mix attains a high average score of 62.25 across\nall metrics, placing it on par with advanced proprietary models such as GPT-4o\nand Claude-3.5-Sonnet, while the MT-R1-Zero-7B-Sem variant achieves\nstate-of-the-art scores on semantic metrics. Moreover, our work exhibits strong\ngeneralization capabilities on out-of-distribution MT tasks, robustly\nsupporting multilingual and low-resource settings. Extensive analysis of model\nbehavior across different initializations and reward metrics offers pioneering\ninsight into the critical role of reward design, LLM adaptability, training\ndynamics, and emergent reasoning patterns within the R1-Zero paradigm for MT.\nOur code is available at https://github.com/fzp0424/MT-R1-Zero."
                },
                "authors": [
                    {
                        "name": "Zhaopeng Feng"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Jiahan Ren"
                    },
                    {
                        "name": "Jiayuan Su"
                    },
                    {
                        "name": "Ruizhe Chen"
                    },
                    {
                        "name": "Yan Zhang"
                    },
                    {
                        "name": "Zhe Xu"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Jian Wu"
                    },
                    {
                        "name": "Zuozhu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zuozhu Liu"
                },
                "author": "Zuozhu Liu",
                "arxiv_comment": "Work in progress. Our code is available at\n  https://github.com/fzp0424/MT-R1-Zero",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10160v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10160v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10157v1",
                "updated": "2025-04-14T12:12:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    12,
                    52,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T12:12:52Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    12,
                    52,
                    0,
                    104,
                    0
                ],
                "title": "SocioVerse: A World Model for Social Simulation Powered by LLM Agents\n  and A Pool of 10 Million Real-World Users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SocioVerse: A World Model for Social Simulation Powered by LLM Agents\n  and A Pool of 10 Million Real-World Users"
                },
                "summary": "Social simulation is transforming traditional social science research by\nmodeling human behavior through interactions between virtual individuals and\ntheir environments. With recent advances in large language models (LLMs), this\napproach has shown growing potential in capturing individual differences and\npredicting group behaviors. However, existing methods face alignment challenges\nrelated to the environment, target users, interaction mechanisms, and\nbehavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven\nworld model for social simulation. Our framework features four powerful\nalignment components and a user pool of 10 million real individuals. To\nvalidate its effectiveness, we conducted large-scale simulation experiments\nacross three distinct domains: politics, news, and economics. Results\ndemonstrate that SocioVerse can reflect large-scale population dynamics while\nensuring diversity, credibility, and representativeness through standardized\nprocedures and minimal manual adjustments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social simulation is transforming traditional social science research by\nmodeling human behavior through interactions between virtual individuals and\ntheir environments. With recent advances in large language models (LLMs), this\napproach has shown growing potential in capturing individual differences and\npredicting group behaviors. However, existing methods face alignment challenges\nrelated to the environment, target users, interaction mechanisms, and\nbehavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven\nworld model for social simulation. Our framework features four powerful\nalignment components and a user pool of 10 million real individuals. To\nvalidate its effectiveness, we conducted large-scale simulation experiments\nacross three distinct domains: politics, news, and economics. Results\ndemonstrate that SocioVerse can reflect large-scale population dynamics while\nensuring diversity, credibility, and representativeness through standardized\nprocedures and minimal manual adjustments."
                },
                "authors": [
                    {
                        "name": "Xinnong Zhang"
                    },
                    {
                        "name": "Jiayu Lin"
                    },
                    {
                        "name": "Xinyi Mou"
                    },
                    {
                        "name": "Shiyue Yang"
                    },
                    {
                        "name": "Xiawei Liu"
                    },
                    {
                        "name": "Libo Sun"
                    },
                    {
                        "name": "Hanjia Lyu"
                    },
                    {
                        "name": "Yihang Yang"
                    },
                    {
                        "name": "Weihong Qi"
                    },
                    {
                        "name": "Yue Chen"
                    },
                    {
                        "name": "Guanying Li"
                    },
                    {
                        "name": "Ling Yan"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Siming Chen"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Jingxuan Huang"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Shiping Tang"
                    },
                    {
                        "name": "Libo Wu"
                    },
                    {
                        "name": "Baohua Zhou"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10150v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10150v1",
                "updated": "2025-04-14T12:01:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    1,
                    11,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T12:01:11Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    1,
                    11,
                    0,
                    104,
                    0
                ],
                "title": "HistLLM: A Unified Framework for LLM-Based Multimodal Recommendation\n  with User History Encoding and Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HistLLM: A Unified Framework for LLM-Based Multimodal Recommendation\n  with User History Encoding and Compression"
                },
                "summary": "While large language models (LLMs) have proven effective in leveraging\ntextual data for recommendations, their application to multimodal\nrecommendation tasks remains relatively underexplored. Although LLMs can\nprocess multimodal information through projection functions that map visual\nfeatures into their semantic space, recommendation tasks often require\nrepresenting users' history interactions through lengthy prompts combining text\nand visual elements, which not only hampers training and inference efficiency\nbut also makes it difficult for the model to accurately capture user\npreferences from complex and extended prompts, leading to reduced\nrecommendation performance. To address this challenge, we introduce HistLLM, an\ninnovative multimodal recommendation framework that integrates textual and\nvisual features through a User History Encoding Module (UHEM), compressing\nmultimodal user history interactions into a single token representation,\neffectively facilitating LLMs in processing user preferences. Extensive\nexperiments demonstrate the effectiveness and efficiency of our proposed\nmechanism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have proven effective in leveraging\ntextual data for recommendations, their application to multimodal\nrecommendation tasks remains relatively underexplored. Although LLMs can\nprocess multimodal information through projection functions that map visual\nfeatures into their semantic space, recommendation tasks often require\nrepresenting users' history interactions through lengthy prompts combining text\nand visual elements, which not only hampers training and inference efficiency\nbut also makes it difficult for the model to accurately capture user\npreferences from complex and extended prompts, leading to reduced\nrecommendation performance. To address this challenge, we introduce HistLLM, an\ninnovative multimodal recommendation framework that integrates textual and\nvisual features through a User History Encoding Module (UHEM), compressing\nmultimodal user history interactions into a single token representation,\neffectively facilitating LLMs in processing user preferences. Extensive\nexperiments demonstrate the effectiveness and efficiency of our proposed\nmechanism."
                },
                "authors": [
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Bo Hu"
                    },
                    {
                        "name": "Weidong Chen"
                    },
                    {
                        "name": "Zhendong Mao"
                    }
                ],
                "author_detail": {
                    "name": "Zhendong Mao"
                },
                "author": "Zhendong Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10150v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10150v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10149v1",
                "updated": "2025-04-14T12:00:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    0,
                    0,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T12:00:00Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    0,
                    0,
                    0,
                    104,
                    0
                ],
                "title": "BoTTA: Benchmarking on-device Test Time Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BoTTA: Benchmarking on-device Test Time Adaptation"
                },
                "summary": "The performance of deep learning models depends heavily on test samples at\nruntime, and shifts from the training data distribution can significantly\nreduce accuracy. Test-time adaptation (TTA) addresses this by adapting models\nduring inference without requiring labeled test data or access to the original\ntraining set. While research has explored TTA from various perspectives like\nalgorithmic complexity, data and class distribution shifts, model\narchitectures, and offline versus continuous learning, constraints specific to\nmobile and edge devices remain underexplored. We propose BoTTA, a benchmark\ndesigned to evaluate TTA methods under practical constraints on mobile and edge\ndevices. Our evaluation targets four key challenges caused by limited resources\nand usage conditions: (i) limited test samples, (ii) limited exposure to\ncategories, (iii) diverse distribution shifts, and (iv) overlapping shifts\nwithin a sample. We assess state-of-the-art TTA methods under these scenarios\nusing benchmark datasets and report system-level metrics on a real testbed.\nFurthermore, unlike prior work, we align with on-device requirements by\nadvocating periodic adaptation instead of continuous inference-time adaptation.\nExperiments reveal key insights: many recent TTA algorithms struggle with small\ndatasets, fail to generalize to unseen categories, and depend on the diversity\nand complexity of distribution shifts. BoTTA also reports device-specific\nresource use. For example, while SHOT improves accuracy by $2.25\\times$ with\n$512$ adaptation samples, it uses $1.08\\times$ peak memory on Raspberry Pi\nversus the base model. BoTTA offers actionable guidance for TTA in real-world,\nresource-constrained deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of deep learning models depends heavily on test samples at\nruntime, and shifts from the training data distribution can significantly\nreduce accuracy. Test-time adaptation (TTA) addresses this by adapting models\nduring inference without requiring labeled test data or access to the original\ntraining set. While research has explored TTA from various perspectives like\nalgorithmic complexity, data and class distribution shifts, model\narchitectures, and offline versus continuous learning, constraints specific to\nmobile and edge devices remain underexplored. We propose BoTTA, a benchmark\ndesigned to evaluate TTA methods under practical constraints on mobile and edge\ndevices. Our evaluation targets four key challenges caused by limited resources\nand usage conditions: (i) limited test samples, (ii) limited exposure to\ncategories, (iii) diverse distribution shifts, and (iv) overlapping shifts\nwithin a sample. We assess state-of-the-art TTA methods under these scenarios\nusing benchmark datasets and report system-level metrics on a real testbed.\nFurthermore, unlike prior work, we align with on-device requirements by\nadvocating periodic adaptation instead of continuous inference-time adaptation.\nExperiments reveal key insights: many recent TTA algorithms struggle with small\ndatasets, fail to generalize to unseen categories, and depend on the diversity\nand complexity of distribution shifts. BoTTA also reports device-specific\nresource use. For example, while SHOT improves accuracy by $2.25\\times$ with\n$512$ adaptation samples, it uses $1.08\\times$ peak memory on Raspberry Pi\nversus the base model. BoTTA offers actionable guidance for TTA in real-world,\nresource-constrained deployments."
                },
                "authors": [
                    {
                        "name": "Michal Danilowski"
                    },
                    {
                        "name": "Soumyajit Chatterjee"
                    },
                    {
                        "name": "Abhirup Ghosh"
                    }
                ],
                "author_detail": {
                    "name": "Abhirup Ghosh"
                },
                "author": "Abhirup Ghosh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10147v1",
                "updated": "2025-04-14T11:57:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    57,
                    52,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T11:57:52Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    57,
                    52,
                    0,
                    104,
                    0
                ],
                "title": "A Survey of Personalization: From RAG to Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Personalization: From RAG to Agent"
                },
                "summary": "Personalization has become an essential capability in modern AI systems,\nenabling customized interactions that align with individual user preferences,\ncontexts, and goals. Recent research has increasingly concentrated on\nRetrieval-Augmented Generation (RAG) frameworks and their evolution into more\nadvanced agent-based architectures within personalized settings to enhance user\nsatisfaction. Building on this foundation, this survey systematically examines\npersonalization across the three core stages of RAG: pre-retrieval, retrieval,\nand generation. Beyond RAG, we further extend its capabilities into the realm\nof Personalized LLM-based Agents, which enhance traditional RAG systems with\nagentic functionalities, including user understanding, personalized planning\nand execution, and dynamic generation. For both personalization in RAG and\nagent-based personalization, we provide formal definitions, conduct a\ncomprehensive review of recent literature, and summarize key datasets and\nevaluation metrics. Additionally, we discuss fundamental challenges,\nlimitations, and promising research directions in this evolving field. Relevant\npapers and resources are continuously updated at\nhttps://github.com/Applied-Machine-Learning-Lab/Awesome-Personalized-RAG-Agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalization has become an essential capability in modern AI systems,\nenabling customized interactions that align with individual user preferences,\ncontexts, and goals. Recent research has increasingly concentrated on\nRetrieval-Augmented Generation (RAG) frameworks and their evolution into more\nadvanced agent-based architectures within personalized settings to enhance user\nsatisfaction. Building on this foundation, this survey systematically examines\npersonalization across the three core stages of RAG: pre-retrieval, retrieval,\nand generation. Beyond RAG, we further extend its capabilities into the realm\nof Personalized LLM-based Agents, which enhance traditional RAG systems with\nagentic functionalities, including user understanding, personalized planning\nand execution, and dynamic generation. For both personalization in RAG and\nagent-based personalization, we provide formal definitions, conduct a\ncomprehensive review of recent literature, and summarize key datasets and\nevaluation metrics. Additionally, we discuss fundamental challenges,\nlimitations, and promising research directions in this evolving field. Relevant\npapers and resources are continuously updated at\nhttps://github.com/Applied-Machine-Learning-Lab/Awesome-Personalized-RAG-Agent."
                },
                "authors": [
                    {
                        "name": "Xiaopeng Li"
                    },
                    {
                        "name": "Pengyue Jia"
                    },
                    {
                        "name": "Derong Xu"
                    },
                    {
                        "name": "Yi Wen"
                    },
                    {
                        "name": "Yingyi Zhang"
                    },
                    {
                        "name": "Wenlin Zhang"
                    },
                    {
                        "name": "Wanyu Wang"
                    },
                    {
                        "name": "Yichao Wang"
                    },
                    {
                        "name": "Zhaocheng Du"
                    },
                    {
                        "name": "Xiangyang Li"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Huifeng Guo"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhao"
                },
                "author": "Xiangyu Zhao",
                "arxiv_comment": "18 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20384v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20384v2",
                "updated": "2025-04-14T11:39:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    39,
                    39,
                    0,
                    104,
                    0
                ],
                "published": "2025-03-26T10:05:38Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    10,
                    5,
                    38,
                    2,
                    85,
                    0
                ],
                "title": "MoLe-VLA: Dynamic Layer-skipping Vision Language Action Model via\n  Mixture-of-Layers for Efficient Robot Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoLe-VLA: Dynamic Layer-skipping Vision Language Action Model via\n  Mixture-of-Layers for Efficient Robot Manipulation"
                },
                "summary": "Multimodal Large Language Models (MLLMs) excel in understanding complex\nlanguage and visual data, enabling generalist robotic systems to interpret\ninstructions and perform embodied tasks. Nevertheless, their real-world\ndeployment is hindered by substantial computational and storage demands. Recent\ninsights into the homogeneous patterns in the LLM layer have inspired\nsparsification techniques to address these challenges, such as early exit and\ntoken pruning. However, these methods often neglect the critical role of the\nfinal layers that encode the semantic information most relevant to downstream\nrobotic tasks. Aligning with the recent breakthrough of the Shallow Brain\nHypothesis (SBH) in neuroscience and the mixture of experts in model\nsparsification, we conceptualize each LLM layer as an expert and propose a\nMixture-of-Layers Vision-Language-Action model (MoLe-VLA, or simply MoLe)\narchitecture for dynamic LLM layer activation. We introduce a Spatial-Temporal\nAware Router (STAR) for MoLe to selectively activate only parts of the layers\nbased on the robot's current state, mimicking the brain's distinct signal\npathways specialized for cognition and causal reasoning. Additionally, to\ncompensate for the cognitive ability of LLMs lost in MoLe, we devise a\nCognition Self-Knowledge Distillation (CogKD) framework. CogKD enhances the\nunderstanding of task demands and improves the generation of task-relevant\naction sequences by leveraging cognitive features. Extensive experiments\nconducted in both RLBench simulation and real-world environments demonstrate\nthe superiority of MoLe-VLA in both efficiency and performance. Specifically,\nMoLe-VLA achieves an 8% improvement in the mean success rate across ten tasks\nwhile reducing computational costs by up to x5.6 compared to standard LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) excel in understanding complex\nlanguage and visual data, enabling generalist robotic systems to interpret\ninstructions and perform embodied tasks. Nevertheless, their real-world\ndeployment is hindered by substantial computational and storage demands. Recent\ninsights into the homogeneous patterns in the LLM layer have inspired\nsparsification techniques to address these challenges, such as early exit and\ntoken pruning. However, these methods often neglect the critical role of the\nfinal layers that encode the semantic information most relevant to downstream\nrobotic tasks. Aligning with the recent breakthrough of the Shallow Brain\nHypothesis (SBH) in neuroscience and the mixture of experts in model\nsparsification, we conceptualize each LLM layer as an expert and propose a\nMixture-of-Layers Vision-Language-Action model (MoLe-VLA, or simply MoLe)\narchitecture for dynamic LLM layer activation. We introduce a Spatial-Temporal\nAware Router (STAR) for MoLe to selectively activate only parts of the layers\nbased on the robot's current state, mimicking the brain's distinct signal\npathways specialized for cognition and causal reasoning. Additionally, to\ncompensate for the cognitive ability of LLMs lost in MoLe, we devise a\nCognition Self-Knowledge Distillation (CogKD) framework. CogKD enhances the\nunderstanding of task demands and improves the generation of task-relevant\naction sequences by leveraging cognitive features. Extensive experiments\nconducted in both RLBench simulation and real-world environments demonstrate\nthe superiority of MoLe-VLA in both efficiency and performance. Specifically,\nMoLe-VLA achieves an 8% improvement in the mean success rate across ten tasks\nwhile reducing computational costs by up to x5.6 compared to standard LLMs."
                },
                "authors": [
                    {
                        "name": "Rongyu Zhang"
                    },
                    {
                        "name": "Menghang Dong"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Liang Heng"
                    },
                    {
                        "name": "Xiaowei Chi"
                    },
                    {
                        "name": "Gaole Dai"
                    },
                    {
                        "name": "Li Du"
                    },
                    {
                        "name": "Yuan Du"
                    },
                    {
                        "name": "Shanghang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shanghang Zhang"
                },
                "author": "Shanghang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20384v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20384v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13116v2",
                "updated": "2025-04-14T11:28:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    28,
                    43,
                    0,
                    104,
                    0
                ],
                "published": "2025-03-17T12:38:03Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    38,
                    3,
                    0,
                    76,
                    0
                ],
                "title": "VeriLeaky: Navigating IP Protection vs Utility in Fine-Tuning for\n  LLM-Driven Verilog Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeriLeaky: Navigating IP Protection vs Utility in Fine-Tuning for\n  LLM-Driven Verilog Coding"
                },
                "summary": "Large language models (LLMs) offer significant potential for coding, yet\nfine-tuning (FT) with curated data is essential for niche languages like\nVerilog. Using proprietary intellectual property (IP) for FT presents a serious\nrisk, as FT data can be leaked through LLM inference. This leads to a critical\ndilemma for design houses: seeking to build externally accessible LLMs offering\ncompetitive Verilog coding, how can they leverage in-house IP to enhance FT\nutility while ensuring IP protection?\n  For the first time in the literature, we study this dilemma. Using LLaMA\n3.1-8B, we conduct in-house FT on a baseline Verilog dataset (RTLCoder)\nsupplemented with our own in-house IP, which is validated through multiple\ntape-outs. To rigorously assess IP leakage, we quantify structural similarity\n(AST/Dolos) and functional equivalence (Synopsys Formality) between generated\ncodes and our in-house IP. We show that our IP can indeed be leaked, confirming\nthe threat. As defense, we evaluate logic locking of Verilog codes (ASSURE).\nThis offers some level of protection, yet reduces the IP's utility for FT and\ndegrades the LLM's performance. Our study shows the need for novel strategies\nthat are both effective and minimally disruptive to FT, an essential effort for\nenabling design houses to fully utilize their proprietary IP toward LLM-driven\nVerilog coding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) offer significant potential for coding, yet\nfine-tuning (FT) with curated data is essential for niche languages like\nVerilog. Using proprietary intellectual property (IP) for FT presents a serious\nrisk, as FT data can be leaked through LLM inference. This leads to a critical\ndilemma for design houses: seeking to build externally accessible LLMs offering\ncompetitive Verilog coding, how can they leverage in-house IP to enhance FT\nutility while ensuring IP protection?\n  For the first time in the literature, we study this dilemma. Using LLaMA\n3.1-8B, we conduct in-house FT on a baseline Verilog dataset (RTLCoder)\nsupplemented with our own in-house IP, which is validated through multiple\ntape-outs. To rigorously assess IP leakage, we quantify structural similarity\n(AST/Dolos) and functional equivalence (Synopsys Formality) between generated\ncodes and our in-house IP. We show that our IP can indeed be leaked, confirming\nthe threat. As defense, we evaluate logic locking of Verilog codes (ASSURE).\nThis offers some level of protection, yet reduces the IP's utility for FT and\ndegrades the LLM's performance. Our study shows the need for novel strategies\nthat are both effective and minimally disruptive to FT, an essential effort for\nenabling design houses to fully utilize their proprietary IP toward LLM-driven\nVerilog coding."
                },
                "authors": [
                    {
                        "name": "Zeng Wang"
                    },
                    {
                        "name": "Minghao Shao"
                    },
                    {
                        "name": "Mohammed Nabeel"
                    },
                    {
                        "name": "Prithwish Basu Roy"
                    },
                    {
                        "name": "Likhitha Mankali"
                    },
                    {
                        "name": "Jitendra Bhandari"
                    },
                    {
                        "name": "Ramesh Karri"
                    },
                    {
                        "name": "Ozgur Sinanoglu"
                    },
                    {
                        "name": "Muhammad Shafique"
                    },
                    {
                        "name": "Johann Knechtel"
                    }
                ],
                "author_detail": {
                    "name": "Johann Knechtel"
                },
                "author": "Johann Knechtel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10112v1",
                "updated": "2025-04-14T11:21:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    21,
                    33,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T11:21:33Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    21,
                    33,
                    0,
                    104,
                    0
                ],
                "title": "Benchmarking Practices in LLM-driven Offensive Security: Testbeds,\n  Metrics, and Experiment Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Practices in LLM-driven Offensive Security: Testbeds,\n  Metrics, and Experiment Design"
                },
                "summary": "Large Language Models (LLMs) have emerged as a powerful approach for driving\noffensive penetration-testing tooling. This paper analyzes the methodology and\nbenchmarking practices used for evaluating Large Language Model (LLM)-driven\nattacks, focusing on offensive uses of LLMs in cybersecurity. We review 16\nresearch papers detailing 15 prototypes and their respective testbeds.\n  We detail our findings and provide actionable recommendations for future\nresearch, emphasizing the importance of extending existing testbeds, creating\nbaselines, and including comprehensive metrics and qualitative analysis. We\nalso note the distinction between security research and practice, suggesting\nthat CTF-based challenges may not fully represent real-world penetration\ntesting scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as a powerful approach for driving\noffensive penetration-testing tooling. This paper analyzes the methodology and\nbenchmarking practices used for evaluating Large Language Model (LLM)-driven\nattacks, focusing on offensive uses of LLMs in cybersecurity. We review 16\nresearch papers detailing 15 prototypes and their respective testbeds.\n  We detail our findings and provide actionable recommendations for future\nresearch, emphasizing the importance of extending existing testbeds, creating\nbaselines, and including comprehensive metrics and qualitative analysis. We\nalso note the distinction between security research and practice, suggesting\nthat CTF-based challenges may not fully represent real-world penetration\ntesting scenarios."
                },
                "authors": [
                    {
                        "name": "Andreas Happe"
                    },
                    {
                        "name": "Jrgen Cito"
                    }
                ],
                "author_detail": {
                    "name": "Jrgen Cito"
                },
                "author": "Jrgen Cito",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10107v1",
                "updated": "2025-04-14T11:15:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    15,
                    30,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T11:15:30Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    15,
                    30,
                    0,
                    104,
                    0
                ],
                "title": "Enhancing LLM-based Recommendation through Semantic-Aligned\n  Collaborative Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM-based Recommendation through Semantic-Aligned\n  Collaborative Knowledge"
                },
                "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities in\nleveraging comprehensive world knowledge and sophisticated reasoning mechanisms\nfor recommendation tasks. However, a notable limitation lies in their inability\nto effectively model sparse identifiers (e.g., user and item IDs), unlike\nconventional collaborative filtering models (Collabs.), thus hindering LLM to\nlearn distinctive user-item representations and creating a performance\nbottleneck. Prior studies indicate that integrating collaborative knowledge\nfrom Collabs. into LLMs can mitigate the above limitations and enhance their\nrecommendation performance. Nevertheless, the significant discrepancy in\nknowledge distribution and semantic space between LLMs and Collab. presents\nsubstantial challenges for effective knowledge transfer. To tackle these\nchallenges, we propose a novel framework, SeLLa-Rec, which focuses on achieving\nalignment between the semantic spaces of Collabs. and LLMs. This alignment\nfosters effective knowledge fusion, mitigating the influence of discriminative\nnoise and facilitating the deep integration of knowledge from diverse models.\nSpecifically, three special tokens with collaborative knowledge are embedded\ninto the LLM's semantic space through a hybrid projection layer and integrated\ninto task-specific prompts to guide the recommendation process. Experiments\nconducted on two public benchmark datasets (MovieLens-1M and Amazon Book)\ndemonstrate that SeLLa-Rec achieves state-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate remarkable capabilities in\nleveraging comprehensive world knowledge and sophisticated reasoning mechanisms\nfor recommendation tasks. However, a notable limitation lies in their inability\nto effectively model sparse identifiers (e.g., user and item IDs), unlike\nconventional collaborative filtering models (Collabs.), thus hindering LLM to\nlearn distinctive user-item representations and creating a performance\nbottleneck. Prior studies indicate that integrating collaborative knowledge\nfrom Collabs. into LLMs can mitigate the above limitations and enhance their\nrecommendation performance. Nevertheless, the significant discrepancy in\nknowledge distribution and semantic space between LLMs and Collab. presents\nsubstantial challenges for effective knowledge transfer. To tackle these\nchallenges, we propose a novel framework, SeLLa-Rec, which focuses on achieving\nalignment between the semantic spaces of Collabs. and LLMs. This alignment\nfosters effective knowledge fusion, mitigating the influence of discriminative\nnoise and facilitating the deep integration of knowledge from diverse models.\nSpecifically, three special tokens with collaborative knowledge are embedded\ninto the LLM's semantic space through a hybrid projection layer and integrated\ninto task-specific prompts to guide the recommendation process. Experiments\nconducted on two public benchmark datasets (MovieLens-1M and Amazon Book)\ndemonstrate that SeLLa-Rec achieves state-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Jinghao Lin"
                    },
                    {
                        "name": "Xiaocui Yang"
                    },
                    {
                        "name": "Yongkang Liu"
                    },
                    {
                        "name": "Shi Feng"
                    },
                    {
                        "name": "Daling Wang"
                    },
                    {
                        "name": "Yifei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yifei Zhang"
                },
                "author": "Yifei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10101v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10101v1",
                "updated": "2025-04-14T11:07:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    7,
                    11,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T11:07:11Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    7,
                    11,
                    0,
                    104,
                    0
                ],
                "title": "The Human Visual System Can Inspire New Interaction Paradigms for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Human Visual System Can Inspire New Interaction Paradigms for LLMs"
                },
                "summary": "The dominant metaphor of LLMs-as-minds leads to misleading conceptions of\nmachine agency and is limited in its ability to help both users and developers\nbuild the right degree of trust and understanding for outputs from LLMs. It\nmakes it harder to disentangle hallucinations from useful model interactions.\nThis position paper argues that there are fundamental similarities between\nvisual perception and the way LLMs process and present language. These\nsimilarities inspire a metaphor for LLMs which could open new avenues for\nresearch into interaction paradigms and shared representations. Our visual\nsystem metaphor introduces possibilities for addressing these challenges by\nunderstanding the information landscape assimilated by LLMs. In this paper we\nmotivate our proposal, introduce the interrelating theories from the fields\nthat inspired this view and discuss research directions that stem from this\nabstraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dominant metaphor of LLMs-as-minds leads to misleading conceptions of\nmachine agency and is limited in its ability to help both users and developers\nbuild the right degree of trust and understanding for outputs from LLMs. It\nmakes it harder to disentangle hallucinations from useful model interactions.\nThis position paper argues that there are fundamental similarities between\nvisual perception and the way LLMs process and present language. These\nsimilarities inspire a metaphor for LLMs which could open new avenues for\nresearch into interaction paradigms and shared representations. Our visual\nsystem metaphor introduces possibilities for addressing these challenges by\nunderstanding the information landscape assimilated by LLMs. In this paper we\nmotivate our proposal, introduce the interrelating theories from the fields\nthat inspired this view and discuss research directions that stem from this\nabstraction."
                },
                "authors": [
                    {
                        "name": "Diana Robinson"
                    },
                    {
                        "name": "Neil Lawrence"
                    }
                ],
                "author_detail": {
                    "name": "Neil Lawrence"
                },
                "author": "Neil Lawrence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10101v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10101v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08980v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08980v2",
                "updated": "2025-04-14T11:00:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    0,
                    31,
                    0,
                    104,
                    0
                ],
                "published": "2025-03-12T01:21:17Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    1,
                    21,
                    17,
                    2,
                    71,
                    0
                ],
                "title": "I Predict Therefore I Am: Is Next Token Prediction Enough to Learn\n  Human-Interpretable Concepts from Data?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Predict Therefore I Am: Is Next Token Prediction Enough to Learn\n  Human-Interpretable Concepts from Data?"
                },
                "summary": "The remarkable achievements of large language models (LLMs) have led many to\nconclude that they exhibit a form of intelligence. This is as opposed to\nexplanations of their capabilities based on their ability to perform relatively\nsimple manipulations of vast volumes of data. To illuminate the distinction\nbetween these explanations, we introduce a novel generative model that\ngenerates tokens on the basis of human interpretable concepts represented as\nlatent discrete variables. Under mild conditions, even when the mapping from\nthe latent space to the observed space is non-invertible, we establish an\nidentifiability result: the representations learned by LLMs through next-token\nprediction can be approximately modeled as the logarithm of the posterior\nprobabilities of these latent discrete concepts, up to an invertible linear\ntransformation. This theoretical finding not only provides evidence that LLMs\ncapture underlying generative factors, but also strongly reinforces the linear\nrepresentation hypothesis, which posits that LLMs learn linear representations\nof human-interpretable concepts. Empirically, we validate our theoretical\nresults through evaluations on both simulation data and the Pythia, Llama, and\nDeepSeek model families.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable achievements of large language models (LLMs) have led many to\nconclude that they exhibit a form of intelligence. This is as opposed to\nexplanations of their capabilities based on their ability to perform relatively\nsimple manipulations of vast volumes of data. To illuminate the distinction\nbetween these explanations, we introduce a novel generative model that\ngenerates tokens on the basis of human interpretable concepts represented as\nlatent discrete variables. Under mild conditions, even when the mapping from\nthe latent space to the observed space is non-invertible, we establish an\nidentifiability result: the representations learned by LLMs through next-token\nprediction can be approximately modeled as the logarithm of the posterior\nprobabilities of these latent discrete concepts, up to an invertible linear\ntransformation. This theoretical finding not only provides evidence that LLMs\ncapture underlying generative factors, but also strongly reinforces the linear\nrepresentation hypothesis, which posits that LLMs learn linear representations\nof human-interpretable concepts. Empirically, we validate our theoretical\nresults through evaluations on both simulation data and the Pythia, Llama, and\nDeepSeek model families."
                },
                "authors": [
                    {
                        "name": "Yuhang Liu"
                    },
                    {
                        "name": "Dong Gong"
                    },
                    {
                        "name": "Erdun Gao"
                    },
                    {
                        "name": "Zhen Zhang"
                    },
                    {
                        "name": "Biwei Huang"
                    },
                    {
                        "name": "Mingming Gong"
                    },
                    {
                        "name": "Anton van den Hengel"
                    },
                    {
                        "name": "Javen Qinfeng Shi"
                    }
                ],
                "author_detail": {
                    "name": "Javen Qinfeng Shi"
                },
                "author": "Javen Qinfeng Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08980v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08980v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10090v1",
                "updated": "2025-04-14T10:53:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    10,
                    53,
                    44,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T10:53:44Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    10,
                    53,
                    44,
                    0,
                    104,
                    0
                ],
                "title": "CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography"
                },
                "summary": "Large language models (LLMs) and multimodal large language models (MLLMs)\nhave significantly advanced artificial intelligence. However, visual reasoning,\nreasoning involving both visual and textual inputs, remains underexplored.\nRecent advancements, including the reasoning models like OpenAI o1 and Gemini\n2.0 Flash Thinking, which incorporate image inputs, have opened this\ncapability. In this ongoing work, we focus specifically on photography-related\ntasks because a photo is a visual snapshot of the physical world where the\nunderlying physics (i.e., illumination, blur extent, etc.) interplay with the\ncamera parameters. Successfully reasoning from the visual information of a\nphoto to identify these numerical camera settings requires the MLLMs to have a\ndeeper understanding of the underlying physics for precise visual\ncomprehension, representing a challenging and intelligent capability essential\nfor practical applications like photography assistant agents. We aim to\nevaluate MLLMs on their ability to distinguish visual differences related to\nnumerical camera settings, extending a methodology previously proposed for\nvision-language models (VLMs). Our preliminary results demonstrate the\nimportance of visual reasoning in photography-related tasks. Moreover, these\nresults show that no single MLLM consistently dominates across all evaluation\ntasks, demonstrating ongoing challenges and opportunities in developing MLLMs\nwith better visual reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) and multimodal large language models (MLLMs)\nhave significantly advanced artificial intelligence. However, visual reasoning,\nreasoning involving both visual and textual inputs, remains underexplored.\nRecent advancements, including the reasoning models like OpenAI o1 and Gemini\n2.0 Flash Thinking, which incorporate image inputs, have opened this\ncapability. In this ongoing work, we focus specifically on photography-related\ntasks because a photo is a visual snapshot of the physical world where the\nunderlying physics (i.e., illumination, blur extent, etc.) interplay with the\ncamera parameters. Successfully reasoning from the visual information of a\nphoto to identify these numerical camera settings requires the MLLMs to have a\ndeeper understanding of the underlying physics for precise visual\ncomprehension, representing a challenging and intelligent capability essential\nfor practical applications like photography assistant agents. We aim to\nevaluate MLLMs on their ability to distinguish visual differences related to\nnumerical camera settings, extending a methodology previously proposed for\nvision-language models (VLMs). Our preliminary results demonstrate the\nimportance of visual reasoning in photography-related tasks. Moreover, these\nresults show that no single MLLM consistently dominates across all evaluation\ntasks, demonstrating ongoing challenges and opportunities in developing MLLMs\nwith better visual reasoning."
                },
                "authors": [
                    {
                        "name": "I-Sheng Fang"
                    },
                    {
                        "name": "Jun-Cheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jun-Cheng Chen"
                },
                "author": "Jun-Cheng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05336v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05336v3",
                "updated": "2025-04-14T10:51:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    10,
                    51,
                    41,
                    0,
                    104,
                    0
                ],
                "published": "2024-05-08T18:10:59Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    18,
                    10,
                    59,
                    2,
                    129,
                    0
                ],
                "title": "Joint semi-supervised and contrastive learning enables domain\n  generalization and multi-domain segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint semi-supervised and contrastive learning enables domain\n  generalization and multi-domain segmentation"
                },
                "summary": "Despite their effectiveness, current deep learning models face challenges\nwith images coming from different domains with varying appearance and content.\nWe introduce SegCLR, a versatile framework designed to segment images across\ndifferent domains, employing supervised and contrastive learning simultaneously\nto effectively learn from both labeled and unlabeled data. We demonstrate the\nsuperior performance of SegCLR through a comprehensive evaluation involving\nthree diverse clinical datasets of 3D retinal Optical Coherence Tomography\n(OCT) images, for the slice-wise segmentation of fluids with various network\nconfigurations and verification across 10 different network initializations. In\nan unsupervised domain adaptation context, SegCLR achieves results on par with\na supervised upper-bound model trained on the intended target domain. Notably,\nwe discover that the segmentation performance of SegCLR framework is marginally\nimpacted by the abundance of unlabeled data from the target domain, thereby we\nalso propose an effective domain generalization extension of SegCLR, known also\nas zero-shot domain adaptation, which eliminates the need for any target domain\ninformation. This shows that our proposed addition of contrastive loss in\nstandard supervised training for segmentation leads to superior models,\ninherently more generalizable to both in- and out-of-domain test data. We\nadditionally propose a pragmatic solution for SegCLR deployment in realistic\nscenarios with multiple domains containing labeled data. Accordingly, our\nframework pushes the boundaries of deep-learning based segmentation in\nmulti-domain applications, regardless of data availability - labeled,\nunlabeled, or nonexistent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their effectiveness, current deep learning models face challenges\nwith images coming from different domains with varying appearance and content.\nWe introduce SegCLR, a versatile framework designed to segment images across\ndifferent domains, employing supervised and contrastive learning simultaneously\nto effectively learn from both labeled and unlabeled data. We demonstrate the\nsuperior performance of SegCLR through a comprehensive evaluation involving\nthree diverse clinical datasets of 3D retinal Optical Coherence Tomography\n(OCT) images, for the slice-wise segmentation of fluids with various network\nconfigurations and verification across 10 different network initializations. In\nan unsupervised domain adaptation context, SegCLR achieves results on par with\na supervised upper-bound model trained on the intended target domain. Notably,\nwe discover that the segmentation performance of SegCLR framework is marginally\nimpacted by the abundance of unlabeled data from the target domain, thereby we\nalso propose an effective domain generalization extension of SegCLR, known also\nas zero-shot domain adaptation, which eliminates the need for any target domain\ninformation. This shows that our proposed addition of contrastive loss in\nstandard supervised training for segmentation leads to superior models,\ninherently more generalizable to both in- and out-of-domain test data. We\nadditionally propose a pragmatic solution for SegCLR deployment in realistic\nscenarios with multiple domains containing labeled data. Accordingly, our\nframework pushes the boundaries of deep-learning based segmentation in\nmulti-domain applications, regardless of data availability - labeled,\nunlabeled, or nonexistent."
                },
                "authors": [
                    {
                        "name": "Alvaro Gomariz"
                    },
                    {
                        "name": "Yusuke Kikuchi"
                    },
                    {
                        "name": "Yun Yvonna Li"
                    },
                    {
                        "name": "Thomas Albrecht"
                    },
                    {
                        "name": "Andreas Maunz"
                    },
                    {
                        "name": "Daniela Ferrara"
                    },
                    {
                        "name": "Huanxiang Lu"
                    },
                    {
                        "name": "Orcun Goksel"
                    }
                ],
                "author_detail": {
                    "name": "Orcun Goksel"
                },
                "author": "Orcun Goksel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.05336v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05336v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00757v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00757v2",
                "updated": "2025-04-14T10:39:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    10,
                    39,
                    33,
                    0,
                    104,
                    0
                ],
                "published": "2025-02-02T11:40:07Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    11,
                    40,
                    7,
                    6,
                    33,
                    0
                ],
                "title": "AgentBreeder: Mitigating the AI Safety Impact of Multi-Agent Scaffolds\n  via Self-Improvement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentBreeder: Mitigating the AI Safety Impact of Multi-Agent Scaffolds\n  via Self-Improvement"
                },
                "summary": "Scaffolding Large Language Models (LLMs) into multi-agent systems often\nimproves performance on complex tasks, but the safety impact of such scaffolds\nhas not been thoroughly explored. We introduce AgentBreeder, a framework for\nmulti-objective self-improving evolutionary search over scaffolds. We evaluate\ndiscovered scaffolds on widely recognized reasoning, mathematics, and safety\nbenchmarks and compare them with popular baselines. In 'blue' mode, we see a\n79.4% average uplift in safety benchmark performance while maintaining or\nimproving capability scores. In 'red' mode, we find adversarially weak\nscaffolds emerging concurrently with capability optimization. Our work\ndemonstrates the risks of multi-agent scaffolding and provides a framework for\nmitigating them. Code is available at\nhttps://github.com/J-Rosser-UK/AgentBreeder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaffolding Large Language Models (LLMs) into multi-agent systems often\nimproves performance on complex tasks, but the safety impact of such scaffolds\nhas not been thoroughly explored. We introduce AgentBreeder, a framework for\nmulti-objective self-improving evolutionary search over scaffolds. We evaluate\ndiscovered scaffolds on widely recognized reasoning, mathematics, and safety\nbenchmarks and compare them with popular baselines. In 'blue' mode, we see a\n79.4% average uplift in safety benchmark performance while maintaining or\nimproving capability scores. In 'red' mode, we find adversarially weak\nscaffolds emerging concurrently with capability optimization. Our work\ndemonstrates the risks of multi-agent scaffolding and provides a framework for\nmitigating them. Code is available at\nhttps://github.com/J-Rosser-UK/AgentBreeder."
                },
                "authors": [
                    {
                        "name": "J Rosser"
                    },
                    {
                        "name": "Jakob Nicolaus Foerster"
                    }
                ],
                "author_detail": {
                    "name": "Jakob Nicolaus Foerster"
                },
                "author": "Jakob Nicolaus Foerster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00757v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00757v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T42, 68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15004v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15004v3",
                "updated": "2025-04-14T10:36:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    10,
                    36,
                    33,
                    0,
                    104,
                    0
                ],
                "published": "2024-12-19T16:20:22Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    20,
                    22,
                    3,
                    354,
                    0
                ],
                "title": "From Vulnerabilities to Remediation: A Systematic Literature Review of\n  LLMs in Code Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Vulnerabilities to Remediation: A Systematic Literature Review of\n  LLMs in Code Security"
                },
                "summary": "Large Language Models (LLMs) have emerged as powerful tools for automating\nvarious programming tasks, including security-related ones, such as detecting\nand fixing vulnerabilities. Despite their promising capabilities, when required\nto produce or modify pre-existing code, LLMs could introduce vulnerabilities\nunbeknown to the programmer. When analyzing code, they could miss clear\nvulnerabilities or signal nonexistent ones. In this Systematic Literature\nReview (SLR), we aim to investigate both the security benefits and potential\ndrawbacks of using LLMs for a variety of code-related tasks. In particular,\nfirst we focus on the types of vulnerabilities that could be introduced by\nLLMs, when used for producing code. Second, we analyze the capabilities of LLMs\nto detect and fix vulnerabilities, in any given code, and how the prompting\nstrategy of choice impacts their performance in these two tasks. Last, we\nprovide an in-depth analysis on how data poisoning attacks on LLMs can impact\nperformance in the aforementioned tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as powerful tools for automating\nvarious programming tasks, including security-related ones, such as detecting\nand fixing vulnerabilities. Despite their promising capabilities, when required\nto produce or modify pre-existing code, LLMs could introduce vulnerabilities\nunbeknown to the programmer. When analyzing code, they could miss clear\nvulnerabilities or signal nonexistent ones. In this Systematic Literature\nReview (SLR), we aim to investigate both the security benefits and potential\ndrawbacks of using LLMs for a variety of code-related tasks. In particular,\nfirst we focus on the types of vulnerabilities that could be introduced by\nLLMs, when used for producing code. Second, we analyze the capabilities of LLMs\nto detect and fix vulnerabilities, in any given code, and how the prompting\nstrategy of choice impacts their performance in these two tasks. Last, we\nprovide an in-depth analysis on how data poisoning attacks on LLMs can impact\nperformance in the aforementioned tasks."
                },
                "authors": [
                    {
                        "name": "Enna Basic"
                    },
                    {
                        "name": "Alberto Giaretta"
                    }
                ],
                "author_detail": {
                    "name": "Alberto Giaretta"
                },
                "author": "Alberto Giaretta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15004v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15004v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01894v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01894v2",
                "updated": "2025-04-14T10:25:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    10,
                    25,
                    47,
                    0,
                    104,
                    0
                ],
                "published": "2024-11-04T08:50:52Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    8,
                    50,
                    52,
                    0,
                    309,
                    0
                ],
                "title": "Efficient Active Imitation Learning with Random Network Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Active Imitation Learning with Random Network Distillation"
                },
                "summary": "Developing agents for complex and underspecified tasks, where no clear\nobjective exists, remains challenging but offers many opportunities. This is\nespecially true in video games, where simulated players (bots) need to play\nrealistically, and there is no clear reward to evaluate them. While imitation\nlearning has shown promise in such domains, these methods often fail when\nagents encounter out-of-distribution scenarios during deployment. Expanding the\ntraining dataset is a common solution, but it becomes impractical or costly\nwhen relying on human demonstrations. This article addresses active imitation\nlearning, aiming to trigger expert intervention only when necessary, reducing\nthe need for constant expert input along training. We introduce Random Network\nDistillation DAgger (RND-DAgger), a new active imitation learning method that\nlimits expert querying by using a learned state-based out-of-distribution\nmeasure to trigger interventions. This approach avoids frequent expert-agent\naction comparisons, thus making the expert intervene only when it is useful. We\nevaluate RND-DAgger against traditional imitation learning and other active\napproaches in 3D video games (racing and third-person navigation) and in a\nrobotic locomotion task and show that RND-DAgger surpasses previous methods by\nreducing expert queries. https://sites.google.com/view/rnd-dagger",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing agents for complex and underspecified tasks, where no clear\nobjective exists, remains challenging but offers many opportunities. This is\nespecially true in video games, where simulated players (bots) need to play\nrealistically, and there is no clear reward to evaluate them. While imitation\nlearning has shown promise in such domains, these methods often fail when\nagents encounter out-of-distribution scenarios during deployment. Expanding the\ntraining dataset is a common solution, but it becomes impractical or costly\nwhen relying on human demonstrations. This article addresses active imitation\nlearning, aiming to trigger expert intervention only when necessary, reducing\nthe need for constant expert input along training. We introduce Random Network\nDistillation DAgger (RND-DAgger), a new active imitation learning method that\nlimits expert querying by using a learned state-based out-of-distribution\nmeasure to trigger interventions. This approach avoids frequent expert-agent\naction comparisons, thus making the expert intervene only when it is useful. We\nevaluate RND-DAgger against traditional imitation learning and other active\napproaches in 3D video games (racing and third-person navigation) and in a\nrobotic locomotion task and show that RND-DAgger surpasses previous methods by\nreducing expert queries. https://sites.google.com/view/rnd-dagger"
                },
                "authors": [
                    {
                        "name": "Emilien Bir"
                    },
                    {
                        "name": "Anthony Kobanda"
                    },
                    {
                        "name": "Ludovic Denoyer"
                    },
                    {
                        "name": "Rmy Portelas"
                    }
                ],
                "author_detail": {
                    "name": "Rmy Portelas"
                },
                "author": "Rmy Portelas",
                "arxiv_comment": "Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01894v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01894v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10077v1",
                "updated": "2025-04-14T10:21:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    10,
                    21,
                    59,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T10:21:59Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    10,
                    21,
                    59,
                    0,
                    104,
                    0
                ],
                "title": "Towards Quantifying Commonsense Reasoning with Mechanistic Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Quantifying Commonsense Reasoning with Mechanistic Insights"
                },
                "summary": "Commonsense reasoning deals with the implicit knowledge that is well\nunderstood by humans and typically acquired via interactions with the world. In\nrecent times, commonsense reasoning and understanding of various LLMs have been\nevaluated using text-based tasks. In this work, we argue that a proxy of this\nunderstanding can be maintained as a graphical structure that can further help\nto perform a rigorous evaluation of commonsense reasoning abilities about\nvarious real-world activities. We create an annotation scheme for capturing\nthis implicit knowledge in the form of a graphical structure for 37 daily human\nactivities. We find that the created resource can be used to frame an enormous\nnumber of commonsense queries (~ 10^{17}), facilitating rigorous evaluation of\ncommonsense reasoning in LLMs. Moreover, recently, the remarkable performance\nof LLMs has raised questions about whether these models are truly capable of\nreasoning in the wild and, in general, how reasoning occurs inside these\nmodels. In this resource paper, we bridge this gap by proposing design\nmechanisms that facilitate research in a similar direction. Our findings\nsuggest that the reasoning components are localized in LLMs that play a\nprominent role in decision-making when prompted with a commonsense query.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commonsense reasoning deals with the implicit knowledge that is well\nunderstood by humans and typically acquired via interactions with the world. In\nrecent times, commonsense reasoning and understanding of various LLMs have been\nevaluated using text-based tasks. In this work, we argue that a proxy of this\nunderstanding can be maintained as a graphical structure that can further help\nto perform a rigorous evaluation of commonsense reasoning abilities about\nvarious real-world activities. We create an annotation scheme for capturing\nthis implicit knowledge in the form of a graphical structure for 37 daily human\nactivities. We find that the created resource can be used to frame an enormous\nnumber of commonsense queries (~ 10^{17}), facilitating rigorous evaluation of\ncommonsense reasoning in LLMs. Moreover, recently, the remarkable performance\nof LLMs has raised questions about whether these models are truly capable of\nreasoning in the wild and, in general, how reasoning occurs inside these\nmodels. In this resource paper, we bridge this gap by proposing design\nmechanisms that facilitate research in a similar direction. Our findings\nsuggest that the reasoning components are localized in LLMs that play a\nprominent role in decision-making when prompted with a commonsense query."
                },
                "authors": [
                    {
                        "name": "Abhinav Joshi"
                    },
                    {
                        "name": "Areeb Ahmad"
                    },
                    {
                        "name": "Divyaksh Shukla"
                    },
                    {
                        "name": "Ashutosh Modi"
                    }
                ],
                "author_detail": {
                    "name": "Ashutosh Modi"
                },
                "author": "Ashutosh Modi",
                "arxiv_comment": "Accepted at NAACL 2025; 28 pages (9 pages + 7 pages references + 12\n  pages appendix)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10074v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10074v2",
                "updated": "2025-04-15T06:19:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    6,
                    19,
                    0,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-14T10:19:47Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    10,
                    19,
                    47,
                    0,
                    104,
                    0
                ],
                "title": "MMKB-RAG: A Multi-Modal Knowledge-Based Retrieval-Augmented Generation\n  Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMKB-RAG: A Multi-Modal Knowledge-Based Retrieval-Augmented Generation\n  Framework"
                },
                "summary": "Recent advancements in large language models (LLMs) and multi-modal LLMs have\nbeen remarkable. However, these models still rely solely on their parametric\nknowledge, which limits their ability to generate up-to-date information and\nincreases the risk of producing erroneous content. Retrieval-Augmented\nGeneration (RAG) partially mitigates these challenges by incorporating external\ndata sources, yet the reliance on databases and retrieval systems can introduce\nirrelevant or inaccurate documents, ultimately undermining both performance and\nreasoning quality. In this paper, we propose Multi-Modal Knowledge-Based\nRetrieval-Augmented Generation (MMKB-RAG), a novel multi-modal RAG framework\nthat leverages the inherent knowledge boundaries of models to dynamically\ngenerate semantic tags for the retrieval process. This strategy enables the\njoint filtering of retrieved documents, retaining only the most relevant and\naccurate references. Extensive experiments on knowledge-based visual\nquestion-answering tasks demonstrate the efficacy of our approach: on the E-VQA\ndataset, our method improves performance by +4.2% on the Single-Hop subset and\n+0.4% on the full dataset, while on the InfoSeek dataset, it achieves gains of\n+7.8% on the Unseen-Q subset, +8.2% on the Unseen-E subset, and +8.1% on the\nfull dataset. These results highlight significant enhancements in both accuracy\nand robustness over the current state-of-the-art MLLM and RAG frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) and multi-modal LLMs have\nbeen remarkable. However, these models still rely solely on their parametric\nknowledge, which limits their ability to generate up-to-date information and\nincreases the risk of producing erroneous content. Retrieval-Augmented\nGeneration (RAG) partially mitigates these challenges by incorporating external\ndata sources, yet the reliance on databases and retrieval systems can introduce\nirrelevant or inaccurate documents, ultimately undermining both performance and\nreasoning quality. In this paper, we propose Multi-Modal Knowledge-Based\nRetrieval-Augmented Generation (MMKB-RAG), a novel multi-modal RAG framework\nthat leverages the inherent knowledge boundaries of models to dynamically\ngenerate semantic tags for the retrieval process. This strategy enables the\njoint filtering of retrieved documents, retaining only the most relevant and\naccurate references. Extensive experiments on knowledge-based visual\nquestion-answering tasks demonstrate the efficacy of our approach: on the E-VQA\ndataset, our method improves performance by +4.2% on the Single-Hop subset and\n+0.4% on the full dataset, while on the InfoSeek dataset, it achieves gains of\n+7.8% on the Unseen-Q subset, +8.2% on the Unseen-E subset, and +8.1% on the\nfull dataset. These results highlight significant enhancements in both accuracy\nand robustness over the current state-of-the-art MLLM and RAG frameworks."
                },
                "authors": [
                    {
                        "name": "Zihan Ling"
                    },
                    {
                        "name": "Zhiyao Guo"
                    },
                    {
                        "name": "Yixuan Huang"
                    },
                    {
                        "name": "Yi An"
                    },
                    {
                        "name": "Shuai Xiao"
                    },
                    {
                        "name": "Jinsong Lan"
                    },
                    {
                        "name": "Xiaoyong Zhu"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10074v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10074v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.07510v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.07510v4",
                "updated": "2025-04-14T10:17:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    10,
                    17,
                    38,
                    0,
                    104,
                    0
                ],
                "published": "2024-02-12T09:31:21Z",
                "published_parsed": [
                    2024,
                    2,
                    12,
                    9,
                    31,
                    21,
                    0,
                    43,
                    0
                ],
                "title": "Secret Collusion among Generative AI Agents: Multi-Agent Deception via\n  Steganography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secret Collusion among Generative AI Agents: Multi-Agent Deception via\n  Steganography"
                },
                "summary": "Recent capability increases in large language models (LLMs) open up\napplications in which groups of communicating generative AI agents solve joint\ntasks. This poses privacy and security challenges concerning the unauthorised\nsharing of information, or other unwanted forms of agent coordination. Modern\nsteganographic techniques could render such dynamics hard to detect. In this\npaper, we comprehensively formalise the problem of secret collusion in systems\nof generative AI agents by drawing on relevant concepts from both AI and\nsecurity literature. We study incentives for the use of steganography, and\npropose a variety of mitigation measures. Our investigations result in a model\nevaluation framework that systematically tests capabilities required for\nvarious forms of secret collusion. We provide extensive empirical results\nacross a range of contemporary LLMs. While the steganographic capabilities of\ncurrent models remain limited, GPT-4 displays a capability jump suggesting the\nneed for continuous monitoring of steganographic frontier model capabilities.\nWe conclude by laying out a comprehensive research program to mitigate future\nrisks of collusion between generative AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent capability increases in large language models (LLMs) open up\napplications in which groups of communicating generative AI agents solve joint\ntasks. This poses privacy and security challenges concerning the unauthorised\nsharing of information, or other unwanted forms of agent coordination. Modern\nsteganographic techniques could render such dynamics hard to detect. In this\npaper, we comprehensively formalise the problem of secret collusion in systems\nof generative AI agents by drawing on relevant concepts from both AI and\nsecurity literature. We study incentives for the use of steganography, and\npropose a variety of mitigation measures. Our investigations result in a model\nevaluation framework that systematically tests capabilities required for\nvarious forms of secret collusion. We provide extensive empirical results\nacross a range of contemporary LLMs. While the steganographic capabilities of\ncurrent models remain limited, GPT-4 displays a capability jump suggesting the\nneed for continuous monitoring of steganographic frontier model capabilities.\nWe conclude by laying out a comprehensive research program to mitigate future\nrisks of collusion between generative AI models."
                },
                "authors": [
                    {
                        "name": "Sumeet Ramesh Motwani"
                    },
                    {
                        "name": "Mikhail Baranchuk"
                    },
                    {
                        "name": "Martin Strohmeier"
                    },
                    {
                        "name": "Vijay Bolina"
                    },
                    {
                        "name": "Philip H. S. Torr"
                    },
                    {
                        "name": "Lewis Hammond"
                    },
                    {
                        "name": "Christian Schroeder de Witt"
                    }
                ],
                "author_detail": {
                    "name": "Christian Schroeder de Witt"
                },
                "author": "Christian Schroeder de Witt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.07510v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.07510v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.00448v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.00448v3",
                "updated": "2025-04-14T10:11:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    10,
                    11,
                    13,
                    0,
                    104,
                    0
                ],
                "published": "2023-12-31T10:53:58Z",
                "published_parsed": [
                    2023,
                    12,
                    31,
                    10,
                    53,
                    58,
                    6,
                    365,
                    0
                ],
                "title": "Beyond Chinchilla-Optimal: Accounting for Inference in Language Model\n  Scaling Laws",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Chinchilla-Optimal: Accounting for Inference in Language Model\n  Scaling Laws"
                },
                "summary": "Large language model (LLM) scaling laws are empirical formulas that estimate\nchanges in model quality as a result of increasing parameter count and training\ndata. However, these formulas, including the popular Deepmind Chinchilla\nscaling laws, neglect to include the cost of inference. We modify the\nChinchilla scaling laws to calculate the optimal LLM parameter count and\npre-training data size to train and deploy a model of a given quality and\ninference demand. We conduct our analysis both in terms of a compute budget and\nreal-world costs and find that LLM researchers expecting reasonably large\ninference demand (~1B requests) should train models smaller and longer than\nChinchilla-optimal. Furthermore, we train 47 models of varying sizes and\nparameter counts to validate our formula and find that model quality continues\nto improve as we scale tokens per parameter to extreme ranges (up to 10,000).\nFinally, we ablate the procedure used to fit the Chinchilla scaling law\ncoefficients and find that developing scaling laws only from data collected at\ntypical token/parameter ratios overestimates the impact of additional tokens at\nthese extreme ranges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) scaling laws are empirical formulas that estimate\nchanges in model quality as a result of increasing parameter count and training\ndata. However, these formulas, including the popular Deepmind Chinchilla\nscaling laws, neglect to include the cost of inference. We modify the\nChinchilla scaling laws to calculate the optimal LLM parameter count and\npre-training data size to train and deploy a model of a given quality and\ninference demand. We conduct our analysis both in terms of a compute budget and\nreal-world costs and find that LLM researchers expecting reasonably large\ninference demand (~1B requests) should train models smaller and longer than\nChinchilla-optimal. Furthermore, we train 47 models of varying sizes and\nparameter counts to validate our formula and find that model quality continues\nto improve as we scale tokens per parameter to extreme ranges (up to 10,000).\nFinally, we ablate the procedure used to fit the Chinchilla scaling law\ncoefficients and find that developing scaling laws only from data collected at\ntypical token/parameter ratios overestimates the impact of additional tokens at\nthese extreme ranges."
                },
                "authors": [
                    {
                        "name": "Nikhil Sardana"
                    },
                    {
                        "name": "Jacob Portes"
                    },
                    {
                        "name": "Sasha Doubov"
                    },
                    {
                        "name": "Jonathan Frankle"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Frankle"
                },
                "author": "Jonathan Frankle",
                "arxiv_comment": "16 pages, 7 figures, In the 41st International Conference on Machine\n  Learning, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.00448v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.00448v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10063v1",
                "updated": "2025-04-14T10:06:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    10,
                    6,
                    27,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T10:06:27Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    10,
                    6,
                    27,
                    0,
                    104,
                    0
                ],
                "title": "Hallucination Detection in LLMs via Topological Divergence on Attention\n  Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination Detection in LLMs via Topological Divergence on Attention\n  Graphs"
                },
                "summary": "Hallucination, i.e., generating factually incorrect content, remains a\ncritical challenge for large language models (LLMs). We introduce TOHA, a\nTOpology-based HAllucination detector in the RAG setting, which leverages a\ntopological divergence metric to quantify the structural properties of graphs\ninduced by attention matrices. Examining the topological divergence between\nprompt and response subgraphs reveals consistent patterns: higher divergence\nvalues in specific attention heads correlate with hallucinated outputs,\nindependent of the dataset. Extensive experiments, including evaluation on\nquestion answering and data-to-text tasks, show that our approach achieves\nstate-of-the-art or competitive results on several benchmarks, two of which\nwere annotated by us and are being publicly released to facilitate further\nresearch. Beyond its strong in-domain performance, TOHA maintains remarkable\ndomain transferability across multiple open-source LLMs. Our findings suggest\nthat analyzing the topological structure of attention matrices can serve as an\nefficient and robust indicator of factual reliability in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination, i.e., generating factually incorrect content, remains a\ncritical challenge for large language models (LLMs). We introduce TOHA, a\nTOpology-based HAllucination detector in the RAG setting, which leverages a\ntopological divergence metric to quantify the structural properties of graphs\ninduced by attention matrices. Examining the topological divergence between\nprompt and response subgraphs reveals consistent patterns: higher divergence\nvalues in specific attention heads correlate with hallucinated outputs,\nindependent of the dataset. Extensive experiments, including evaluation on\nquestion answering and data-to-text tasks, show that our approach achieves\nstate-of-the-art or competitive results on several benchmarks, two of which\nwere annotated by us and are being publicly released to facilitate further\nresearch. Beyond its strong in-domain performance, TOHA maintains remarkable\ndomain transferability across multiple open-source LLMs. Our findings suggest\nthat analyzing the topological structure of attention matrices can serve as an\nefficient and robust indicator of factual reliability in LLMs."
                },
                "authors": [
                    {
                        "name": "Alexandra Bazarova"
                    },
                    {
                        "name": "Aleksandr Yugay"
                    },
                    {
                        "name": "Andrey Shulga"
                    },
                    {
                        "name": "Alina Ermilova"
                    },
                    {
                        "name": "Andrei Volodichev"
                    },
                    {
                        "name": "Konstantin Polev"
                    },
                    {
                        "name": "Julia Belikova"
                    },
                    {
                        "name": "Rauf Parchiev"
                    },
                    {
                        "name": "Dmitry Simakov"
                    },
                    {
                        "name": "Maxim Savchenko"
                    },
                    {
                        "name": "Andrey Savchenko"
                    },
                    {
                        "name": "Serguei Barannikov"
                    },
                    {
                        "name": "Alexey Zaytsev"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Zaytsev"
                },
                "author": "Alexey Zaytsev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13572v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13572v2",
                "updated": "2025-04-14T10:02:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    10,
                    2,
                    7,
                    0,
                    104,
                    0
                ],
                "published": "2025-03-17T12:26:49Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    26,
                    49,
                    0,
                    76,
                    0
                ],
                "title": "VeriContaminated: Assessing LLM-Driven Verilog Coding for Data\n  Contamination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeriContaminated: Assessing LLM-Driven Verilog Coding for Data\n  Contamination"
                },
                "summary": "Large Language Models (LLMs) have revolutionized code generation, achieving\nexceptional results on various established benchmarking frameworks. However,\nconcerns about data contamination - where benchmark data inadvertently leaks\ninto pre-training or fine-tuning datasets - raise questions about the validity\nof these evaluations. While this issue is known, limiting the industrial\nadoption of LLM-driven software engineering, hardware coding has received\nlittle to no attention regarding these risks. For the first time, we analyze\nstate-of-the-art (SOTA) evaluation frameworks for Verilog code generation\n(VerilogEval and RTLLM), using established methods for contamination detection\n(CCD and Min-K% Prob). We cover SOTA commercial and open-source LLMs\n(CodeGen2.5, Minitron 4b, Mistral 7b, phi-4 mini, LLaMA-{1,2,3.1},\nGPT-{2,3.5,4o}, Deepseek-Coder, and CodeQwen 1.5), in baseline and fine-tuned\nmodels (RTLCoder and Verigen). Our study confirms that data contamination is a\ncritical concern. We explore mitigations and the resulting trade-offs for code\nquality vs fairness (i.e., reducing contamination toward unbiased\nbenchmarking).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized code generation, achieving\nexceptional results on various established benchmarking frameworks. However,\nconcerns about data contamination - where benchmark data inadvertently leaks\ninto pre-training or fine-tuning datasets - raise questions about the validity\nof these evaluations. While this issue is known, limiting the industrial\nadoption of LLM-driven software engineering, hardware coding has received\nlittle to no attention regarding these risks. For the first time, we analyze\nstate-of-the-art (SOTA) evaluation frameworks for Verilog code generation\n(VerilogEval and RTLLM), using established methods for contamination detection\n(CCD and Min-K% Prob). We cover SOTA commercial and open-source LLMs\n(CodeGen2.5, Minitron 4b, Mistral 7b, phi-4 mini, LLaMA-{1,2,3.1},\nGPT-{2,3.5,4o}, Deepseek-Coder, and CodeQwen 1.5), in baseline and fine-tuned\nmodels (RTLCoder and Verigen). Our study confirms that data contamination is a\ncritical concern. We explore mitigations and the resulting trade-offs for code\nquality vs fairness (i.e., reducing contamination toward unbiased\nbenchmarking)."
                },
                "authors": [
                    {
                        "name": "Zeng Wang"
                    },
                    {
                        "name": "Minghao Shao"
                    },
                    {
                        "name": "Jitendra Bhandari"
                    },
                    {
                        "name": "Likhitha Mankali"
                    },
                    {
                        "name": "Ramesh Karri"
                    },
                    {
                        "name": "Ozgur Sinanoglu"
                    },
                    {
                        "name": "Muhammad Shafique"
                    },
                    {
                        "name": "Johann Knechtel"
                    }
                ],
                "author_detail": {
                    "name": "Johann Knechtel"
                },
                "author": "Johann Knechtel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13572v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13572v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10054v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10054v1",
                "updated": "2025-04-14T09:57:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    9,
                    57,
                    35,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T09:57:35Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    9,
                    57,
                    35,
                    0,
                    104,
                    0
                ],
                "title": "Implementation and Performance Evaluation of TCP over QUIC Tunnels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implementation and Performance Evaluation of TCP over QUIC Tunnels"
                },
                "summary": "QUIC, a UDP-based transport protocol, addresses several limitations of TCP by\noffering built-in encryption, stream multiplexing, and improved loss recovery.\nTo extend these benefits to legacy TCP-based applications, this paper explores\nthe implementation and evaluation of a TCP over QUIC tunneling approach. A\nlightweight, stream-based tunnel is constructed using the Rust-based Quinn\nlibrary, enabling TCP traffic to traverse QUIC connections transparently.\nPerformance is evaluated under varying network conditions, including packet\nloss, high latency, and out-of-order delivery. Results indicate that TCP over\nQUIC maintains significantly higher throughput than native TCP in lossy or\nunstable environments, with up to a high improvement under 20\\% packet loss.\nHowever, under ideal network conditions, tunneling introduces modest overhead\ndue to encryption and user-space processing. These findings provide insights\ninto the trade-offs of TCP over QUIC tunneling and its suitability for\ndeployment in dynamic or impaired networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QUIC, a UDP-based transport protocol, addresses several limitations of TCP by\noffering built-in encryption, stream multiplexing, and improved loss recovery.\nTo extend these benefits to legacy TCP-based applications, this paper explores\nthe implementation and evaluation of a TCP over QUIC tunneling approach. A\nlightweight, stream-based tunnel is constructed using the Rust-based Quinn\nlibrary, enabling TCP traffic to traverse QUIC connections transparently.\nPerformance is evaluated under varying network conditions, including packet\nloss, high latency, and out-of-order delivery. Results indicate that TCP over\nQUIC maintains significantly higher throughput than native TCP in lossy or\nunstable environments, with up to a high improvement under 20\\% packet loss.\nHowever, under ideal network conditions, tunneling introduces modest overhead\ndue to encryption and user-space processing. These findings provide insights\ninto the trade-offs of TCP over QUIC tunneling and its suitability for\ndeployment in dynamic or impaired networks."
                },
                "authors": [
                    {
                        "name": "Xuanhong Guo"
                    },
                    {
                        "name": "Zekun Bao"
                    },
                    {
                        "name": "Ying Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ying Chen"
                },
                "author": "Ying Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10054v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10050v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10050v2",
                "updated": "2025-04-15T08:42:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    8,
                    42,
                    15,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-14T09:55:47Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    9,
                    55,
                    47,
                    0,
                    104,
                    0
                ],
                "title": "Emotional Strain and Frustration in LLM Interactions in Software\n  Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotional Strain and Frustration in LLM Interactions in Software\n  Engineering"
                },
                "summary": "Large Language Models (LLMs) are increasingly integrated into various daily\ntasks in Software Engineering such as coding and requirement elicitation.\nDespite their various capabilities and constant use, some interactions can lead\nto unexpected challenges (e.g. hallucinations or verbose answers) and, in turn,\ncause emotions that develop into frustration. Frustration can negatively impact\nengineers' productivity and well-being if they escalate into stress and\nburnout. In this paper, we assess the impact of LLM interactions on software\nengineers' emotional responses, specifically strains, and identify common\ncauses of frustration when interacting with LLMs at work. Based on 62 survey\nresponses from software engineers in industry and academia across various\ncompanies and universities, we found that a majority of our respondents\nexperience frustrations or other related emotions regardless of the nature of\ntheir work. Additionally, our results showed that frustration mainly stemmed\nfrom issues with correctness and less critical issues such as adaptability to\ncontext or specific format. While such issues may not cause frustration in\ngeneral, artefacts that do not follow certain preferences, standards, or best\npractices can make the output unusable without extensive modification, causing\nfrustration over time. In addition to the frustration triggers, our study\noffers guidelines to improve the software engineers' experience, aiming to\nminimise long-term consequences on mental health.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly integrated into various daily\ntasks in Software Engineering such as coding and requirement elicitation.\nDespite their various capabilities and constant use, some interactions can lead\nto unexpected challenges (e.g. hallucinations or verbose answers) and, in turn,\ncause emotions that develop into frustration. Frustration can negatively impact\nengineers' productivity and well-being if they escalate into stress and\nburnout. In this paper, we assess the impact of LLM interactions on software\nengineers' emotional responses, specifically strains, and identify common\ncauses of frustration when interacting with LLMs at work. Based on 62 survey\nresponses from software engineers in industry and academia across various\ncompanies and universities, we found that a majority of our respondents\nexperience frustrations or other related emotions regardless of the nature of\ntheir work. Additionally, our results showed that frustration mainly stemmed\nfrom issues with correctness and less critical issues such as adaptability to\ncontext or specific format. While such issues may not cause frustration in\ngeneral, artefacts that do not follow certain preferences, standards, or best\npractices can make the output unusable without extensive modification, causing\nfrustration over time. In addition to the frustration triggers, our study\noffers guidelines to improve the software engineers' experience, aiming to\nminimise long-term consequences on mental health."
                },
                "authors": [
                    {
                        "name": "Cristina Martinez Montes"
                    },
                    {
                        "name": "Ranim Khojah"
                    }
                ],
                "author_detail": {
                    "name": "Ranim Khojah"
                },
                "author": "Ranim Khojah",
                "arxiv_comment": "Accepted in EASE'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10050v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10050v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10046v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10046v1",
                "updated": "2025-04-14T09:51:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    9,
                    51,
                    23,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T09:51:23Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    9,
                    51,
                    23,
                    0,
                    104,
                    0
                ],
                "title": "CodeRAG: Supportive Code Retrieval on Bigraph for Real-World Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeRAG: Supportive Code Retrieval on Bigraph for Real-World Code\n  Generation"
                },
                "summary": "Large language models (LLMs) have shown promising performance in automated\ncode generation, especially excelling in simple tasks such as generating\nstandalone codes. Different from simple tasks, real-world code generation\nusually depends on specific programming environment (e.g., code repositories).\nIt contains complex dependencies and domain knowledge, which is needed for LLMs\nwhen generating target code snippets. In this paper, we propose CodeRAG, a\nretrieval-augmented code generation (RAG) framework to comprehensively retrieve\nsupportive codes for real-world code generation. Beginning with the\nrequirement, CodeRAG first constructs a requirement graph for the current\nrepository, and retrieves sub- and similar- requirement nodes of the target\nrequirement on the graph. Meanwhile, it models the repository into a DS-code\ngraph. CodeRAG then maps these relevant requirement nodes into their\ncorresponding code nodes, and treats these code nodes as archors for LLM\nreasoning on DS-code graph. Finally, CodeRAG introduces a code-oriented agentic\nreasoning process, seamlessly allowing LLMs to reason and comprehensively\nretrieve for supportive codes which LLMs' need for generating correct programs.\nExperiments show that CodeRAG achieves significant improvements (i.e.,\nincreasing 40.90 and 37.79 Pass@1 on GPT-4o and Gemini-Pro on DevEval) compared\nto no RAG scenarios. Further tests on reasoning LLMs (i.e., QwQ-32B) confirm\nCodeRAG's adaptability and efficacy across various types of LLMs. In addition,\nCodeRAG outperforms commercial programming products such as Copilit and Cursor.\nWe further investigate the performance of our framework on different dependency\ntypes, and observe that CodeRAG is superior in generating examples where target\ncodes invoke predefined cross-file code snippets. These results demonstrate\nCodeRAG's potential in solving real-world repo-level coding challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown promising performance in automated\ncode generation, especially excelling in simple tasks such as generating\nstandalone codes. Different from simple tasks, real-world code generation\nusually depends on specific programming environment (e.g., code repositories).\nIt contains complex dependencies and domain knowledge, which is needed for LLMs\nwhen generating target code snippets. In this paper, we propose CodeRAG, a\nretrieval-augmented code generation (RAG) framework to comprehensively retrieve\nsupportive codes for real-world code generation. Beginning with the\nrequirement, CodeRAG first constructs a requirement graph for the current\nrepository, and retrieves sub- and similar- requirement nodes of the target\nrequirement on the graph. Meanwhile, it models the repository into a DS-code\ngraph. CodeRAG then maps these relevant requirement nodes into their\ncorresponding code nodes, and treats these code nodes as archors for LLM\nreasoning on DS-code graph. Finally, CodeRAG introduces a code-oriented agentic\nreasoning process, seamlessly allowing LLMs to reason and comprehensively\nretrieve for supportive codes which LLMs' need for generating correct programs.\nExperiments show that CodeRAG achieves significant improvements (i.e.,\nincreasing 40.90 and 37.79 Pass@1 on GPT-4o and Gemini-Pro on DevEval) compared\nto no RAG scenarios. Further tests on reasoning LLMs (i.e., QwQ-32B) confirm\nCodeRAG's adaptability and efficacy across various types of LLMs. In addition,\nCodeRAG outperforms commercial programming products such as Copilit and Cursor.\nWe further investigate the performance of our framework on different dependency\ntypes, and observe that CodeRAG is superior in generating examples where target\ncodes invoke predefined cross-file code snippets. These results demonstrate\nCodeRAG's potential in solving real-world repo-level coding challenges."
                },
                "authors": [
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Xianjie Shi"
                    },
                    {
                        "name": "Kechi Zhang"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Ge Li"
                    },
                    {
                        "name": "Zhengwei Tao"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Chongyang Tao"
                    },
                    {
                        "name": "Zhi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Jin"
                },
                "author": "Zhi Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10046v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10043v1",
                "updated": "2025-04-14T09:44:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    9,
                    44,
                    13,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T09:44:13Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    9,
                    44,
                    13,
                    0,
                    104,
                    0
                ],
                "title": "Instrumenting a Lake as a Wide-Field Gamma-ray Detector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instrumenting a Lake as a Wide-Field Gamma-ray Detector"
                },
                "summary": "Ground-level particle detection has recently emerged as an extremely powerful\napproach to TeV-PeV gamma-ray astronomy. The most successful observatories of\nthis type, HAWC and LHAASO, utilise water-Cherenkov based detector units,\nhoused in tanks or buildings. Here we explore the possibility of deploying\nwater-Cherenkov detector units directly in to a natural or artificial lake.\nPossible advantages include reduced cost and improved performance due to better\nshielding. The lake concept has been developed as an option for the future\nSouthern Wide-view Gamma-ray Observatory, and is now under consideration for a\npossible future extension of the observatory, beyond its recently selected land\nsite. We present results from prototypes operated in a custom built facility,\nand concepts for full-scale array deployment and long-term operation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ground-level particle detection has recently emerged as an extremely powerful\napproach to TeV-PeV gamma-ray astronomy. The most successful observatories of\nthis type, HAWC and LHAASO, utilise water-Cherenkov based detector units,\nhoused in tanks or buildings. Here we explore the possibility of deploying\nwater-Cherenkov detector units directly in to a natural or artificial lake.\nPossible advantages include reduced cost and improved performance due to better\nshielding. The lake concept has been developed as an option for the future\nSouthern Wide-view Gamma-ray Observatory, and is now under consideration for a\npossible future extension of the observatory, beyond its recently selected land\nsite. We present results from prototypes operated in a custom built facility,\nand concepts for full-scale array deployment and long-term operation."
                },
                "authors": [
                    {
                        "name": "Hazal Goksu"
                    },
                    {
                        "name": "Werner Hofmann"
                    },
                    {
                        "name": "Felix Werner"
                    },
                    {
                        "name": "Fabian Haist"
                    },
                    {
                        "name": "Jim Hinton"
                    }
                ],
                "author_detail": {
                    "name": "Jim Hinton"
                },
                "author": "Jim Hinton",
                "arxiv_doi": "10.1016/j.nima.2025.170450.",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.nima.2025.170450.",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.10043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 12 figures",
                "arxiv_journal_ref": "Nuclear Instruments and Methods in Physics Research Section A:\n  Accelerators, Spectrometers, Detectors and Associated Equipment, Volume 1076,\n  2025, 170450, ISSN 0168-9002",
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10040v1",
                "updated": "2025-04-14T09:42:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    9,
                    42,
                    16,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T09:42:16Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    9,
                    42,
                    16,
                    0,
                    104,
                    0
                ],
                "title": "The Security of Quantum Computing in 6G: from Technical Perspectives to\n  Ethical Implications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Security of Quantum Computing in 6G: from Technical Perspectives to\n  Ethical Implications"
                },
                "summary": "Quantum technologies hold promise as essential components for the upcoming\ndeployment of the future 6G network. In this future network, the security and\ntrustworthiness requirements are not considered fulfilled with the current\nstate of the quantum computers, as the malicious behaviour on the part of the\nservice provider towards the user may still be present. Therefore, this article\nprovides an initial interdisciplinary work of regulations and solutions in the\nscope of trustworthy quantum computing for future 6G that can be viewed as\ncomplimentary regulations to the existing strategies shared by different actors\nof states and organizations. More precisely, we describe the importance of a\nreliable quantum service provider and its implication on the ethical aspects\nconcerning digital sovereignty. By exploring the critical relationship between\ntrustworthiness and digital sovereignty in the context of future 6G networks,\nwe analyse a trade-off between accessibility to this new technology and\npreservation of digital sovereignty engaging in parallel the United Nation's\n(UN's) sustainable development goals. Furthermore, we propose a partnership\nmodel based on cooperation, coordination, and collaboration giving rise to a\ntrusted, ethical, and inclusive quantum ecosystem, whose implications can spill\nover to the entire global scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum technologies hold promise as essential components for the upcoming\ndeployment of the future 6G network. In this future network, the security and\ntrustworthiness requirements are not considered fulfilled with the current\nstate of the quantum computers, as the malicious behaviour on the part of the\nservice provider towards the user may still be present. Therefore, this article\nprovides an initial interdisciplinary work of regulations and solutions in the\nscope of trustworthy quantum computing for future 6G that can be viewed as\ncomplimentary regulations to the existing strategies shared by different actors\nof states and organizations. More precisely, we describe the importance of a\nreliable quantum service provider and its implication on the ethical aspects\nconcerning digital sovereignty. By exploring the critical relationship between\ntrustworthiness and digital sovereignty in the context of future 6G networks,\nwe analyse a trade-off between accessibility to this new technology and\npreservation of digital sovereignty engaging in parallel the United Nation's\n(UN's) sustainable development goals. Furthermore, we propose a partnership\nmodel based on cooperation, coordination, and collaboration giving rise to a\ntrusted, ethical, and inclusive quantum ecosystem, whose implications can spill\nover to the entire global scenario."
                },
                "authors": [
                    {
                        "name": "Luca Barbieri"
                    },
                    {
                        "name": "Abdelkrim Menina"
                    },
                    {
                        "name": "Riccardo Bassoli"
                    },
                    {
                        "name": "Frank H. P. Fitzek"
                    }
                ],
                "author_detail": {
                    "name": "Frank H. P. Fitzek"
                },
                "author": "Frank H. P. Fitzek",
                "arxiv_comment": "This paper contains 9 pages and 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10036v1",
                "updated": "2025-04-14T09:38:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    9,
                    38,
                    23,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T09:38:23Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    9,
                    38,
                    23,
                    0,
                    104,
                    0
                ],
                "title": "DataMosaic: Explainable and Verifiable Multi-Modal Data Analytics\n  through Extract-Reason-Verify",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DataMosaic: Explainable and Verifiable Multi-Modal Data Analytics\n  through Extract-Reason-Verify"
                },
                "summary": "Large Language Models (LLMs) are transforming data analytics, but their\nwidespread adoption is hindered by two critical limitations: they are not\nexplainable (opaque reasoning processes) and not verifiable (prone to\nhallucinations and unchecked errors). While retrieval-augmented generation\n(RAG) improves accuracy by grounding LLMs in external data, it fails to address\nthe core challenges of trustworthy analytics - especially when processing\nnoisy, inconsistent, or multi-modal data (for example, text, tables, images).\nWe propose DataMosaic, a framework designed to make LLM-powered analytics both\nexplainable and verifiable. By dynamically extracting task-specific structures\n(for example, tables, graphs, trees) from raw data, DataMosaic provides\ntransparent, step-by-step reasoning traces and enables validation of\nintermediate results. Built on a multi-agent framework, DataMosaic orchestrates\nself-adaptive agents that align with downstream task requirements, enhancing\nconsistency, completeness, and privacy. Through this approach, DataMosaic not\nonly tackles the limitations of current LLM-powered analytics systems but also\nlays the groundwork for a new paradigm of grounded, accurate, and explainable\nmulti-modal data analytics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are transforming data analytics, but their\nwidespread adoption is hindered by two critical limitations: they are not\nexplainable (opaque reasoning processes) and not verifiable (prone to\nhallucinations and unchecked errors). While retrieval-augmented generation\n(RAG) improves accuracy by grounding LLMs in external data, it fails to address\nthe core challenges of trustworthy analytics - especially when processing\nnoisy, inconsistent, or multi-modal data (for example, text, tables, images).\nWe propose DataMosaic, a framework designed to make LLM-powered analytics both\nexplainable and verifiable. By dynamically extracting task-specific structures\n(for example, tables, graphs, trees) from raw data, DataMosaic provides\ntransparent, step-by-step reasoning traces and enables validation of\nintermediate results. Built on a multi-agent framework, DataMosaic orchestrates\nself-adaptive agents that align with downstream task requirements, enhancing\nconsistency, completeness, and privacy. Through this approach, DataMosaic not\nonly tackles the limitations of current LLM-powered analytics systems but also\nlays the groundwork for a new paradigm of grounded, accurate, and explainable\nmulti-modal data analytics."
                },
                "authors": [
                    {
                        "name": "Zhengxuan Zhang"
                    },
                    {
                        "name": "Zhuowen Liang"
                    },
                    {
                        "name": "Yin Wu"
                    },
                    {
                        "name": "Teng Lin"
                    },
                    {
                        "name": "Yuyu Luo"
                    },
                    {
                        "name": "Nan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Nan Tang"
                },
                "author": "Nan Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08525v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08525v2",
                "updated": "2025-04-14T09:38:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    9,
                    38,
                    19,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-11T13:38:36Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    13,
                    38,
                    36,
                    4,
                    101,
                    0
                ],
                "title": "Task Memory Engine (TME): A Structured Memory Framework with Graph-Aware\n  Extensions for Multi-Step LLM Agent Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task Memory Engine (TME): A Structured Memory Framework with Graph-Aware\n  Extensions for Multi-Step LLM Agent Tasks"
                },
                "summary": "Large Language Models (LLMs) are increasingly used as autonomous agents for\nmulti-step tasks. However, most existing frameworks fail to maintain a\nstructured understanding of the task state, often relying on linear prompt\nconcatenation or shallow memory buffers. This leads to brittle performance,\nfrequent hallucinations, and poor long-range coherence. In this work, we\npropose the Task Memory Engine (TME), a lightweight and structured memory\nmodule that tracks task execution using a hierarchical Task Memory Tree (TMT).\nEach node in the tree corresponds to a task step, storing relevant input,\noutput, status, and sub-task relationships. We introduce a prompt synthesis\nmethod that dynamically generates LLM prompts based on the active node path,\nsignificantly improving execution consistency and contextual grounding. Through\ncase studies and comparative experiments on multi-step agent tasks, we\ndemonstrate that TME leads to better task completion accuracy and more\ninterpretable behavior with minimal implementation overhead. A reference\nimplementation of the core TME components is available at\nhttps://github.com/biubiutomato/TME-Agent, including basic examples and\nstructured memory integration. While the current implementation uses a\ntree-based structure, TME is designed to be graph-aware, supporting reusable\nsubsteps, converging task paths, and shared dependencies. This lays the\ngroundwork for future DAG-based memory architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used as autonomous agents for\nmulti-step tasks. However, most existing frameworks fail to maintain a\nstructured understanding of the task state, often relying on linear prompt\nconcatenation or shallow memory buffers. This leads to brittle performance,\nfrequent hallucinations, and poor long-range coherence. In this work, we\npropose the Task Memory Engine (TME), a lightweight and structured memory\nmodule that tracks task execution using a hierarchical Task Memory Tree (TMT).\nEach node in the tree corresponds to a task step, storing relevant input,\noutput, status, and sub-task relationships. We introduce a prompt synthesis\nmethod that dynamically generates LLM prompts based on the active node path,\nsignificantly improving execution consistency and contextual grounding. Through\ncase studies and comparative experiments on multi-step agent tasks, we\ndemonstrate that TME leads to better task completion accuracy and more\ninterpretable behavior with minimal implementation overhead. A reference\nimplementation of the core TME components is available at\nhttps://github.com/biubiutomato/TME-Agent, including basic examples and\nstructured memory integration. While the current implementation uses a\ntree-based structure, TME is designed to be graph-aware, supporting reusable\nsubsteps, converging task paths, and shared dependencies. This lays the\ngroundwork for future DAG-based memory architectures."
                },
                "authors": [
                    {
                        "name": "Ye Ye"
                    }
                ],
                "author_detail": {
                    "name": "Ye Ye"
                },
                "author": "Ye Ye",
                "arxiv_comment": "14 pages, 5 figures. Preprint prepared for future submission.\n  Includes implementation and token-efficiency analysis. Code at\n  https://github.com/biubiutomato/TME-Agent",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08525v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08525v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.8; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]