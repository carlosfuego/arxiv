[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.18434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18434v1",
                "updated": "2025-04-25T15:45:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    45,
                    36,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:45:36Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    45,
                    36,
                    4,
                    115,
                    0
                ],
                "title": "Constructing Hamiltonian Decompositions of Complete $k$-Uniform\n  Hypergraphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing Hamiltonian Decompositions of Complete $k$-Uniform\n  Hypergraphs"
                },
                "summary": "Motivated by the wide-ranging applications of Hamiltonian decompositions in\ndistributed computing, coded caching, routing, resource allocation, load\nbalancing, and fault tolerance, our work presents a comprehensive design for\nHamiltonian decompositions of complete $k$-uniform hypergraphs $K_n^k$.\nBuilding upon the resolution of the long-standing conjecture of the existence\nof Hamiltonian decompositions of complete hypergraphs, a problem that was\nresolved using existence-based methods, our contribution goes beyond the\nprevious explicit designs, which were confined to the specific cases of $k=2$\nand $k=3$, by providing explicit designs for all $k$ and $n$ prime, allowing\nfor a broad applicability of Hamiltonian decompositions in various settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by the wide-ranging applications of Hamiltonian decompositions in\ndistributed computing, coded caching, routing, resource allocation, load\nbalancing, and fault tolerance, our work presents a comprehensive design for\nHamiltonian decompositions of complete $k$-uniform hypergraphs $K_n^k$.\nBuilding upon the resolution of the long-standing conjecture of the existence\nof Hamiltonian decompositions of complete hypergraphs, a problem that was\nresolved using existence-based methods, our contribution goes beyond the\nprevious explicit designs, which were confined to the specific cases of $k=2$\nand $k=3$, by providing explicit designs for all $k$ and $n$ prime, allowing\nfor a broad applicability of Hamiltonian decompositions in various settings."
                },
                "authors": [
                    {
                        "name": "Javad Maheri"
                    },
                    {
                        "name": "Petros Elia"
                    }
                ],
                "author_detail": {
                    "name": "Petros Elia"
                },
                "author": "Petros Elia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18432v1",
                "updated": "2025-04-25T15:44:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    44,
                    38,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:44:38Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    44,
                    38,
                    4,
                    115,
                    0
                ],
                "title": "FlexiNS: A SmartNIC-Centric, Line-Rate and Flexible Network Stack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexiNS: A SmartNIC-Centric, Line-Rate and Flexible Network Stack"
                },
                "summary": "As the gap between network and CPU speeds rapidly increases, the CPU-centric\nnetwork stack proves inadequate due to excessive CPU and memory overhead. While\nhardware-offloaded network stacks alleviate these issues, they suffer from\nlimited flexibility in both control and data planes. Offloading network stack\nto off-path SmartNIC seems promising to provide high flexibility; however,\nthroughput remains constrained by inherent SmartNIC architectural limitations.\n  To this end, we design FlexiNS, a SmartNIC-centric network stack with\nsoftware transport programmability and line-rate packet processing\ncapabilities. To grapple with the limitation of SmartNIC-induced challenges,\nFlexiNS introduces: (a) a header-only offloading TX path; (b) an\nunlimited-working-set in-cache processing RX path; (c) a high-performance\nDMA-only notification pipe; and (d) a programmable offloading engine. We\nprototype FlexiNS using Nvidia BlueField-3 SmartNIC and provide out-of-the-box\nRDMA IBV verbs compatibility to users. FlexiNS achieves 2.2$\\times$ higher\nthroughput than the microkernel-based baseline in block storage disaggregation\nand 1.3$\\times$ higher throughput than the hardware-offloaded baseline in\nKVCache transfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the gap between network and CPU speeds rapidly increases, the CPU-centric\nnetwork stack proves inadequate due to excessive CPU and memory overhead. While\nhardware-offloaded network stacks alleviate these issues, they suffer from\nlimited flexibility in both control and data planes. Offloading network stack\nto off-path SmartNIC seems promising to provide high flexibility; however,\nthroughput remains constrained by inherent SmartNIC architectural limitations.\n  To this end, we design FlexiNS, a SmartNIC-centric network stack with\nsoftware transport programmability and line-rate packet processing\ncapabilities. To grapple with the limitation of SmartNIC-induced challenges,\nFlexiNS introduces: (a) a header-only offloading TX path; (b) an\nunlimited-working-set in-cache processing RX path; (c) a high-performance\nDMA-only notification pipe; and (d) a programmable offloading engine. We\nprototype FlexiNS using Nvidia BlueField-3 SmartNIC and provide out-of-the-box\nRDMA IBV verbs compatibility to users. FlexiNS achieves 2.2$\\times$ higher\nthroughput than the microkernel-based baseline in block storage disaggregation\nand 1.3$\\times$ higher throughput than the hardware-offloaded baseline in\nKVCache transfer."
                },
                "authors": [
                    {
                        "name": "Xuzheng Chen"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Baolin Zhu"
                    },
                    {
                        "name": "Xueying Zhu"
                    },
                    {
                        "name": "Zhongqing Chen"
                    },
                    {
                        "name": "Shu Ma"
                    },
                    {
                        "name": "Lingjun Zhu"
                    },
                    {
                        "name": "Chao Shi"
                    },
                    {
                        "name": "Yin Zhang"
                    },
                    {
                        "name": "Zeke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zeke Wang"
                },
                "author": "Zeke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18242v1",
                "updated": "2025-04-25T10:43:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    43,
                    23,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T10:43:23Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    43,
                    23,
                    4,
                    115,
                    0
                ],
                "title": "Demand Private Coded Caching: Small Cache Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demand Private Coded Caching: Small Cache Size"
                },
                "summary": "We investigate the demand private coded caching problem, which is an $(N,K)$\ncoded caching problem with $N$ files, $K$ users, each equipped with a cache of\nsize $M$, and an additional privacy constraint on user demands, i.e., each user\ncan not gain any information about the demands of other users. We focus on\nscenarios where the size of users' caches is small, aiming to further\ncharacterize the fundamental limits of this problem. We first present a new\nvirtual-user-based achievable scheme for arbitrary number of users and files,\nand two MDS-code-based achievable schemes for the case $N \\le K$. With a newly\nderived converse bound for the case $N \\le K$, these proposed schemes lead to\nthe optimal memory-rate tradeoff of the demand private coded caching problem\nfor $M \\in \\big[0, \\frac{N}{(K+1)(N-1)} \\big] $ where $N \\le K \\le 2N-2$, and\nthe optimal memory-rate tradeoff for $M \\in \\big[0, \\frac{1}{K+1} \\big] $ where\n$ K > 2N-2$. Moreover, for the case of 2 files and arbitrary number of users,\nby deriving another new converse bound, the optimal memory-rate tradeoff is\ncharacterized for $M\\in \\big[0,\\frac{2}{K}\\big] \\cup\n\\big[\\frac{2(K-1)}{K+1},2\\big]$. Finally, we provide the optimal memory-rate\ntradeoff of the demand private coded caching problem for 2 files and 3 users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the demand private coded caching problem, which is an $(N,K)$\ncoded caching problem with $N$ files, $K$ users, each equipped with a cache of\nsize $M$, and an additional privacy constraint on user demands, i.e., each user\ncan not gain any information about the demands of other users. We focus on\nscenarios where the size of users' caches is small, aiming to further\ncharacterize the fundamental limits of this problem. We first present a new\nvirtual-user-based achievable scheme for arbitrary number of users and files,\nand two MDS-code-based achievable schemes for the case $N \\le K$. With a newly\nderived converse bound for the case $N \\le K$, these proposed schemes lead to\nthe optimal memory-rate tradeoff of the demand private coded caching problem\nfor $M \\in \\big[0, \\frac{N}{(K+1)(N-1)} \\big] $ where $N \\le K \\le 2N-2$, and\nthe optimal memory-rate tradeoff for $M \\in \\big[0, \\frac{1}{K+1} \\big] $ where\n$ K > 2N-2$. Moreover, for the case of 2 files and arbitrary number of users,\nby deriving another new converse bound, the optimal memory-rate tradeoff is\ncharacterized for $M\\in \\big[0,\\frac{2}{K}\\big] \\cup\n\\big[\\frac{2(K-1)}{K+1},2\\big]$. Finally, we provide the optimal memory-rate\ntradeoff of the demand private coded caching problem for 2 files and 3 users."
                },
                "authors": [
                    {
                        "name": "Qinyi Lu"
                    },
                    {
                        "name": "Nan Liu"
                    },
                    {
                        "name": "Wei Kang"
                    },
                    {
                        "name": "Chunguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Chunguo Li"
                },
                "author": "Chunguo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18082v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18082v1",
                "updated": "2025-04-25T05:16:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    16,
                    53,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T05:16:53Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    16,
                    53,
                    4,
                    115,
                    0
                ],
                "title": "Efficient GNN Training Through Structure-Aware Randomized Mini-Batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient GNN Training Through Structure-Aware Randomized Mini-Batching"
                },
                "summary": "Graph Neural Networks (GNNs) enable learning on realworld graphs and\nmini-batch training has emerged as the de facto standard for training GNNs\nbecause it can scale to very large graphs and improve convergence. Current\nmini-batch construction policies largely ignore efficiency considerations of\nGNN training. Specifically, existing mini-batching techniques employ\nrandomization schemes to improve accuracy and convergence. However, these\nrandomization schemes are often agnostic to the structural properties of the\ngraph (for eg. community structure), resulting in highly irregular memory\naccess patterns during GNN training that make suboptimal use of on-chip GPU\ncaches. On the other hand, while deterministic mini-batching based solely on\ngraph structure delivers fast runtime performance, the lack of randomness\ncompromises both the final model accuracy and training convergence speed. In\nthis paper, we present Community-structure-aware Randomized Mini-batching\n(COMM-RAND), a novel methodology that bridges the gap between the above\nextremes. COMM-RAND allows practitioners to explore the space between pure\nrandomness and pure graph structural awareness during mini-batch construction,\nleading to significantly more efficient GNN training with similar accuracy. We\nevaluated COMM-RAND across four popular graph learning benchmarks. COMM-RAND\ncuts down GNN training time by up to 2.76x (1.8x on average) while achieving an\naccuracy that is within 1.79% points (0.42% on average) compared to popular\nrandom mini-batching approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) enable learning on realworld graphs and\nmini-batch training has emerged as the de facto standard for training GNNs\nbecause it can scale to very large graphs and improve convergence. Current\nmini-batch construction policies largely ignore efficiency considerations of\nGNN training. Specifically, existing mini-batching techniques employ\nrandomization schemes to improve accuracy and convergence. However, these\nrandomization schemes are often agnostic to the structural properties of the\ngraph (for eg. community structure), resulting in highly irregular memory\naccess patterns during GNN training that make suboptimal use of on-chip GPU\ncaches. On the other hand, while deterministic mini-batching based solely on\ngraph structure delivers fast runtime performance, the lack of randomness\ncompromises both the final model accuracy and training convergence speed. In\nthis paper, we present Community-structure-aware Randomized Mini-batching\n(COMM-RAND), a novel methodology that bridges the gap between the above\nextremes. COMM-RAND allows practitioners to explore the space between pure\nrandomness and pure graph structural awareness during mini-batch construction,\nleading to significantly more efficient GNN training with similar accuracy. We\nevaluated COMM-RAND across four popular graph learning benchmarks. COMM-RAND\ncuts down GNN training time by up to 2.76x (1.8x on average) while achieving an\naccuracy that is within 1.79% points (0.42% on average) compared to popular\nrandom mini-batching approaches."
                },
                "authors": [
                    {
                        "name": "Vignesh Balaji"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    },
                    {
                        "name": "Gal Chechik"
                    },
                    {
                        "name": "Haggai Maron"
                    }
                ],
                "author_detail": {
                    "name": "Haggai Maron"
                },
                "author": "Haggai Maron",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18082v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14335v2",
                "updated": "2025-04-25T05:08:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    8,
                    45,
                    4,
                    115,
                    0
                ],
                "published": "2024-12-18T21:09:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines"
                },
                "summary": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). That is, C3 on average achieves only 21% of ideal\nspeedup. This is so, due to known challenges of compute and memory interference\nbetween concurrent GPU kernels (that is, sharing of GPU's compute units, caches\nand HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build concurrent communication collectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). That is, C3 on average achieves only 21% of ideal\nspeedup. This is so, due to known challenges of compute and memory interference\nbetween concurrent GPU kernels (that is, sharing of GPU's compute units, caches\nand HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build concurrent communication collectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs."
                },
                "authors": [
                    {
                        "name": "Anirudha Agrawal"
                    },
                    {
                        "name": "Shaizeen Aga"
                    },
                    {
                        "name": "Suchita Pati"
                    },
                    {
                        "name": "Mahzabeen Islam"
                    }
                ],
                "author_detail": {
                    "name": "Mahzabeen Islam"
                },
                "author": "Mahzabeen Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16620v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16620v2",
                "updated": "2025-04-25T05:05:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    5,
                    49,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-23T11:18:34Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    18,
                    34,
                    2,
                    113,
                    0
                ],
                "title": "Fluctuated lattice-driven charge density wave far above the condensation\n  temperature in kagome superconductor KV$_3$Sb$_5$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fluctuated lattice-driven charge density wave far above the condensation\n  temperature in kagome superconductor KV$_3$Sb$_5$"
                },
                "summary": "The kagome material AV$_3$Sb$_5$ exhibits multiple exotic orders, including\nan unconventional charge density wave (CDW). Elucidating the underlying\nmechanism behind the CDW transition is crucial for unraveling the complex\ninteractions among these phases. However, the driving force of the CDW remains\na topic of debate due to the intertwined interactions among the system's\nvarious excitations. Here we investigated the CDW transition in KV$_3$Sb$_5$ by\nisolating the ultrafast electronic phase transition using time- and\nangleresolved photoemission spectroscopy. An ultrafast electronic phase\ntransition was observed at a critical photoexcitation fluence, F$_c$, without\nreduction in CDW lattice-distortion-induced band folding. This folded band\npersisted up to 150 K under equilibrium heating, well above the CDW\ncondensation temperature of T$_c$ = 78 K. Notably, the pump-induced band shifts\nat F$_c$ were comparable to those caused by thermal effects at T$_c$. These\nfindings suggest that in KV$_3$Sb$_5$, a fluctuating lattice-driven in-plane\nCDW emerges above 150 K, with out-of-plane electronic correlations leading to\nthe $2\\times2 \\times 2$ CDW near T$_c$, offering key insights into the\ninterplay between the electronic and structural dynamics in AV$_3$Sb$_5$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The kagome material AV$_3$Sb$_5$ exhibits multiple exotic orders, including\nan unconventional charge density wave (CDW). Elucidating the underlying\nmechanism behind the CDW transition is crucial for unraveling the complex\ninteractions among these phases. However, the driving force of the CDW remains\na topic of debate due to the intertwined interactions among the system's\nvarious excitations. Here we investigated the CDW transition in KV$_3$Sb$_5$ by\nisolating the ultrafast electronic phase transition using time- and\nangleresolved photoemission spectroscopy. An ultrafast electronic phase\ntransition was observed at a critical photoexcitation fluence, F$_c$, without\nreduction in CDW lattice-distortion-induced band folding. This folded band\npersisted up to 150 K under equilibrium heating, well above the CDW\ncondensation temperature of T$_c$ = 78 K. Notably, the pump-induced band shifts\nat F$_c$ were comparable to those caused by thermal effects at T$_c$. These\nfindings suggest that in KV$_3$Sb$_5$, a fluctuating lattice-driven in-plane\nCDW emerges above 150 K, with out-of-plane electronic correlations leading to\nthe $2\\times2 \\times 2$ CDW near T$_c$, offering key insights into the\ninterplay between the electronic and structural dynamics in AV$_3$Sb$_5$."
                },
                "authors": [
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Shaofeng Duan"
                    },
                    {
                        "name": "Xiangqi Liu"
                    },
                    {
                        "name": "Zhihua Liu"
                    },
                    {
                        "name": "Shichong Wang"
                    },
                    {
                        "name": "Lingxiao Gu"
                    },
                    {
                        "name": "Jiongyu Huang"
                    },
                    {
                        "name": "Wenxuan Yang"
                    },
                    {
                        "name": "Jianzhe Liu"
                    },
                    {
                        "name": "Dong Qian"
                    },
                    {
                        "name": "Yanfeng Guo"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "arxiv_doi": "10.1016/j.scib.2025.02.018",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.scib.2025.02.018",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.16620v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16620v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages, 4 figures",
                "arxiv_journal_ref": "Science Bulletin 70, 1211-1214 (2025)",
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18001v1",
                "updated": "2025-04-25T01:10:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    1,
                    10,
                    49,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T01:10:49Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    1,
                    10,
                    49,
                    4,
                    115,
                    0
                ],
                "title": "From Cluster to Desktop: A Cache-Accelerated INR framework for\n  Interactive Visualization of Tera-Scale Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Cluster to Desktop: A Cache-Accelerated INR framework for\n  Interactive Visualization of Tera-Scale Data"
                },
                "summary": "Machine learning has enabled the use of implicit neural representations\n(INRs) to efficiently compress and reconstruct massive scientific datasets.\nHowever, despite advances in fast INR rendering algorithms, INR-based rendering\nremains computationally expensive, as computing data values from an INR is\nsignificantly slower than reading them from GPU memory. This bottleneck\ncurrently restricts interactive INR visualization to professional workstations.\nTo address this challenge, we introduce an INR rendering framework accelerated\nby a scalable, multi-resolution GPU cache capable of efficiently representing\ntera-scale datasets. By minimizing redundant data queries and prioritizing\nnovel volume regions, our method reduces the number of INR computations per\nframe, achieving an average 5x speedup over the state-of-the-art INR rendering\nmethod while still maintaining high visualization quality. Coupled with\nexisting hardware-accelerated INR compressors, our framework enables scientists\nto generate and compress massive datasets in situ on high-performance computing\nplatforms and then interactively explore them on consumer-grade hardware post\nhoc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning has enabled the use of implicit neural representations\n(INRs) to efficiently compress and reconstruct massive scientific datasets.\nHowever, despite advances in fast INR rendering algorithms, INR-based rendering\nremains computationally expensive, as computing data values from an INR is\nsignificantly slower than reading them from GPU memory. This bottleneck\ncurrently restricts interactive INR visualization to professional workstations.\nTo address this challenge, we introduce an INR rendering framework accelerated\nby a scalable, multi-resolution GPU cache capable of efficiently representing\ntera-scale datasets. By minimizing redundant data queries and prioritizing\nnovel volume regions, our method reduces the number of INR computations per\nframe, achieving an average 5x speedup over the state-of-the-art INR rendering\nmethod while still maintaining high visualization quality. Coupled with\nexisting hardware-accelerated INR compressors, our framework enables scientists\nto generate and compress massive datasets in situ on high-performance computing\nplatforms and then interactively explore them on consumer-grade hardware post\nhoc."
                },
                "authors": [
                    {
                        "name": "Daniel Zavorotny"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "David Bauer"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "arxiv_comment": "11 pages, 11 figures, EGPGV25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17995v1",
                "updated": "2025-04-25T00:41:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    41,
                    43,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T00:41:43Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    41,
                    43,
                    4,
                    115,
                    0
                ],
                "title": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study"
                },
                "summary": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal electronic correlations. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal electronic correlations. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials."
                },
                "authors": [
                    {
                        "name": "Indukuru Ramesh Reddy"
                    },
                    {
                        "name": "Sayandeep Ghosh"
                    },
                    {
                        "name": "Bongjae Kim"
                    },
                    {
                        "name": "Chang-Jong Kang"
                    }
                ],
                "author_detail": {
                    "name": "Chang-Jong Kang"
                },
                "author": "Chang-Jong Kang",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17866v1",
                "updated": "2025-04-24T18:09:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    18,
                    9,
                    25,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T18:09:25Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    18,
                    9,
                    25,
                    3,
                    114,
                    0
                ],
                "title": "Updated parameters of the LArQL model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Updated parameters of the LArQL model"
                },
                "summary": "The need for a microscopic description of scintillation light generation in\nliquid argon becomes increasingly desirable with the upcoming operation of\nlarge scale LArTPCs in the next decade. While a detailed mathematical account\nof the process is still to be achieved, a phenomenological model for\nsimultaneously treating ionization and scintillation, LArQL, has been\nsuccessfully employed to describe the range of electric fields from 0 to 0.75\nkV/cm and dE/dx from 2 to 40 MeV/cm providing the anti-correlation between the\nfree ionization charge and scintillation light. A reanalysis of the original\nmodel parameter values has been performed within a global fit procedure and is\npresented.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The need for a microscopic description of scintillation light generation in\nliquid argon becomes increasingly desirable with the upcoming operation of\nlarge scale LArTPCs in the next decade. While a detailed mathematical account\nof the process is still to be achieved, a phenomenological model for\nsimultaneously treating ionization and scintillation, LArQL, has been\nsuccessfully employed to describe the range of electric fields from 0 to 0.75\nkV/cm and dE/dx from 2 to 40 MeV/cm providing the anti-correlation between the\nfree ionization charge and scintillation light. A reanalysis of the original\nmodel parameter values has been performed within a global fit procedure and is\npresented."
                },
                "authors": [
                    {
                        "name": "L. Paulucci"
                    },
                    {
                        "name": "F. Cavanna"
                    },
                    {
                        "name": "V. Vale"
                    },
                    {
                        "name": "F. Marinho"
                    }
                ],
                "author_detail": {
                    "name": "F. Marinho"
                },
                "author": "F. Marinho",
                "arxiv_comment": "Part of the proceedings of LIDINE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17584v1",
                "updated": "2025-04-24T14:14:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    14,
                    7,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T14:14:07Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    14,
                    7,
                    3,
                    114,
                    0
                ],
                "title": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes."
                },
                "authors": [
                    {
                        "name": "Qingyuan Liu"
                    },
                    {
                        "name": "Liyan Chen"
                    },
                    {
                        "name": "Yanning Yang"
                    },
                    {
                        "name": "Haocheng Wang"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Zhigang Mao"
                    },
                    {
                        "name": "Naifeng Jing"
                    },
                    {
                        "name": "Yubin Xia"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "16 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17554v1",
                "updated": "2025-04-24T13:47:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    47,
                    35,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T13:47:35Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    47,
                    35,
                    3,
                    114,
                    0
                ],
                "title": "Rethinking PM Crash Consistency in the CXL Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking PM Crash Consistency in the CXL Era"
                },
                "summary": "Persistent Memory (PM) introduces new opportunities for designing\ncrash-consistent applications without the traditional storage overheads.\nHowever, ensuring crash consistency in PM demands intricate knowledge of CPU,\ncache, and memory interactions. Hardware and software mechanisms have been\nproposed to ease this burden, but neither proved sufficient, prompting a\nvariety of bug detection tools.\n  With the sunset of Intel Optane comes the rise of Compute Express Link (CXL)\nfor PM. In this position paper, we discuss the impact of CXL's disaggregated\nand heterogeneous nature in the development of crash-consistent PM\napplications, and outline three research directions: hardware primitives,\npersistency frameworks, and bug detection tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Memory (PM) introduces new opportunities for designing\ncrash-consistent applications without the traditional storage overheads.\nHowever, ensuring crash consistency in PM demands intricate knowledge of CPU,\ncache, and memory interactions. Hardware and software mechanisms have been\nproposed to ease this burden, but neither proved sufficient, prompting a\nvariety of bug detection tools.\n  With the sunset of Intel Optane comes the rise of Compute Express Link (CXL)\nfor PM. In this position paper, we discuss the impact of CXL's disaggregated\nand heterogeneous nature in the development of crash-consistent PM\napplications, and outline three research directions: hardware primitives,\npersistency frameworks, and bug detection tools."
                },
                "authors": [
                    {
                        "name": "João Oliveira"
                    },
                    {
                        "name": "João Gonçalves"
                    },
                    {
                        "name": "Miguel Matos"
                    }
                ],
                "author_detail": {
                    "name": "Miguel Matos"
                },
                "author": "Miguel Matos",
                "arxiv_comment": "5 pages (2 extra pages for references), 1 figure, 2 algorithms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v3",
                "updated": "2025-04-24T08:39:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    39,
                    13,
                    3,
                    114,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should use Direct Memory Access (DMA)\nto offload data transfer, descriptor rings for buffering and queuing, and\ninterrupts for asynchrony between cores and device.\n  In this paper we question this wisdom in the light of two trends: modern and\nemerging cache-coherent interconnects like CXL3.0, and workloads, particularly\nmicroservices and serverless computing. Like some others before us, we argue\nthat the assumptions of the DMA-based model are obsolete, and in many use-cases\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, delivers a more efficient system.\n  However, we push this idea much further. We show, in a real hardware\nimplementation, the gains in latency for fine-grained communication achievable\nusing an open cache-coherence protocol which exposes cache transitions to a\nsmart device, and that throughput is competitive with DMA over modern\ninterconnects. We also demonstrate three use-cases: fine-grained RPC-style\ninvocation of functions on an accelerator, offloading of operators in a\nstreaming dataflow engine, and a network interface targeting serverless\nfunctions, comparing our use of coherence with both traditional DMA-style\ninteraction and a highly-optimized implementation using memory-mapped\nprogrammed I/O over PCIe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should use Direct Memory Access (DMA)\nto offload data transfer, descriptor rings for buffering and queuing, and\ninterrupts for asynchrony between cores and device.\n  In this paper we question this wisdom in the light of two trends: modern and\nemerging cache-coherent interconnects like CXL3.0, and workloads, particularly\nmicroservices and serverless computing. Like some others before us, we argue\nthat the assumptions of the DMA-based model are obsolete, and in many use-cases\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, delivers a more efficient system.\n  However, we push this idea much further. We show, in a real hardware\nimplementation, the gains in latency for fine-grained communication achievable\nusing an open cache-coherence protocol which exposes cache transitions to a\nsmart device, and that throughput is competitive with DMA over modern\ninterconnects. We also demonstrate three use-cases: fine-grained RPC-style\ninvocation of functions on an accelerator, offloading of operators in a\nstreaming dataflow engine, and a network interface targeting serverless\nfunctions, comparing our use of coherence with both traditional DMA-style\ninteraction and a highly-optimized implementation using memory-mapped\nprogrammed I/O over PCIe."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15192v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15192v2",
                "updated": "2025-04-24T04:36:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    4,
                    36,
                    20,
                    3,
                    114,
                    0
                ],
                "published": "2025-02-21T04:07:00Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    4,
                    7,
                    0,
                    4,
                    52,
                    0
                ],
                "title": "SPAARC: Spatial Proximity and Association based prefetching for\n  Augmented Reality in edge Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPAARC: Spatial Proximity and Association based prefetching for\n  Augmented Reality in edge Cache"
                },
                "summary": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SPAARC, a Spatial Proximity and\nAssociation-based Prefetching policy specifically designed for MAR Caches.\nSPAARC intelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SPAARC significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3% to 40% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SPAARC parameters\nto achieve optimal performance. Our findings demonstrate the potential of\nSPAARC to substantially enhance the user experience in MAR applications by\nensuring the timely availability of virtual objects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SPAARC, a Spatial Proximity and\nAssociation-based Prefetching policy specifically designed for MAR Caches.\nSPAARC intelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SPAARC significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3% to 40% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SPAARC parameters\nto achieve optimal performance. Our findings demonstrate the potential of\nSPAARC to substantially enhance the user experience in MAR applications by\nensuring the timely availability of virtual objects."
                },
                "authors": [
                    {
                        "name": "Nikhil Sreekumar"
                    },
                    {
                        "name": "Abhishek Chandra"
                    },
                    {
                        "name": "Jon Weissman"
                    }
                ],
                "author_detail": {
                    "name": "Jon Weissman"
                },
                "author": "Jon Weissman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15192v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15192v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14992v2",
                "updated": "2025-04-24T04:13:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    4,
                    13,
                    49,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-21T09:41:26Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    9,
                    41,
                    26,
                    0,
                    111,
                    0
                ],
                "title": "Efficient Pretraining Length Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Pretraining Length Scaling"
                },
                "summary": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks."
                },
                "authors": [
                    {
                        "name": "Bohong Wu"
                    },
                    {
                        "name": "Shen Yan"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Ya Wang"
                    },
                    {
                        "name": "Xun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xun Zhou"
                },
                "author": "Xun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02441v2",
                "updated": "2025-04-24T01:47:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    1,
                    47,
                    25,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-03T09:58:19Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "title": "Cognitive Memory in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Memory in Large Language Models"
                },
                "summary": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions."
                },
                "authors": [
                    {
                        "name": "Lianlei Shan"
                    },
                    {
                        "name": "Shixian Luo"
                    },
                    {
                        "name": "Zezhou Zhu"
                    },
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Yong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Wu"
                },
                "author": "Yong Wu",
                "arxiv_comment": "37 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15364v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15364v2",
                "updated": "2025-04-23T18:02:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    18,
                    2,
                    55,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-21T18:12:46Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    18,
                    12,
                    46,
                    0,
                    111,
                    0
                ],
                "title": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments"
                },
                "summary": "In this work, we demonstrate that distinctive keys during LLM inference tend\nto have high attention scores. We explore this phenomenon and propose KeyDiff,\na training-free KV cache eviction method based on key similarity. This method\nfacilitates the deployment of LLM-based application requiring long input\nprompts in resource-constrained environments with limited memory and compute\nbudgets. Unlike other KV cache eviction methods, KeyDiff can process\narbitrarily long prompts within strict resource constraints and efficiently\ngenerate responses. We demonstrate that KeyDiff computes the optimal solution\nto a KV cache selection problem that maximizes key diversity, providing a\ntheoretical understanding of KeyDiff. Notably,KeyDiff does not rely on\nattention scores, allowing the use of optimized attention mechanisms like\nFlashAttention. We demonstrate the effectiveness of KeyDiff across diverse\ntasks and models, illustrating a performance gap of less than 0.04\\% with 8K\ncache budget ($\\sim$ 23\\% KV cache reduction) from the non-evicting baseline on\nthe LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we demonstrate that distinctive keys during LLM inference tend\nto have high attention scores. We explore this phenomenon and propose KeyDiff,\na training-free KV cache eviction method based on key similarity. This method\nfacilitates the deployment of LLM-based application requiring long input\nprompts in resource-constrained environments with limited memory and compute\nbudgets. Unlike other KV cache eviction methods, KeyDiff can process\narbitrarily long prompts within strict resource constraints and efficiently\ngenerate responses. We demonstrate that KeyDiff computes the optimal solution\nto a KV cache selection problem that maximizes key diversity, providing a\ntheoretical understanding of KeyDiff. Notably,KeyDiff does not rely on\nattention scores, allowing the use of optimized attention mechanisms like\nFlashAttention. We demonstrate the effectiveness of KeyDiff across diverse\ntasks and models, illustrating a performance gap of less than 0.04\\% with 8K\ncache budget ($\\sim$ 23\\% KV cache reduction) from the non-evicting baseline on\nthe LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B."
                },
                "authors": [
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matt J Morse"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "8 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15364v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15364v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15437v2",
                "updated": "2025-04-23T15:02:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    2,
                    16,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-21T21:01:57Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    21,
                    1,
                    57,
                    0,
                    111,
                    0
                ],
                "title": "Iris: A Next Generation Digital Pathology Rendering Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iris: A Next Generation Digital Pathology Rendering Engine"
                },
                "summary": "Digital pathology is a tool of rapidly evolving importance within the\ndiscipline of pathology. Whole slide imaging promises numerous advantages;\nhowever, adoption is limited by challenges in ease of use and speed of\nhigh-quality image rendering relative to the simplicity and visual quality of\nglass slides. We introduce Iris, a new high-performance digital pathology\nrendering system. Specifically, we outline and detail the performance metrics\nof Iris Core, the core rendering engine technology. Iris Core comprises machine\ncode modules written from the ground up in C++ and using Vulkan, a low-level\nand low-overhead cross-platform graphical processing unit application program\ninterface, and our novel rapid tile buffering algorithms. We provide a detailed\nexplanation of Iris Core's system architecture, including the stateless\nisolation of core processes, interprocess communication paradigms, and explicit\nsynchronization paradigms that provide powerful control over the graphical\nprocessing unit. Iris Core achieves slide rendering at the sustained maximum\nframe rate on all tested platforms and buffers an entire new slide field of,\nview without overlapping pixels, in 10 ms with enhanced detail in 30 ms. It is\nable to buffer and compute high-fidelity reduction-enhancements for viewing\nlow-power cytology with increased visual quality at a rate of 100-160 us per\nslide tile, and with a cumulative median buffering rate of 1.36 GB of\ndecompressed image data per second. This buffering rate allows for an entirely\nnew field of view to be fully buffered and rendered in less than a single\nmonitor refresh on a standard display, and high detail features within 2-3\nmonitor refresh frames. These metrics far exceed previously published\nspecifications, beyond an order of magnitude in some contexts. The system shows\nno slowing with high use loads, but rather increases performance due to cache\nmechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital pathology is a tool of rapidly evolving importance within the\ndiscipline of pathology. Whole slide imaging promises numerous advantages;\nhowever, adoption is limited by challenges in ease of use and speed of\nhigh-quality image rendering relative to the simplicity and visual quality of\nglass slides. We introduce Iris, a new high-performance digital pathology\nrendering system. Specifically, we outline and detail the performance metrics\nof Iris Core, the core rendering engine technology. Iris Core comprises machine\ncode modules written from the ground up in C++ and using Vulkan, a low-level\nand low-overhead cross-platform graphical processing unit application program\ninterface, and our novel rapid tile buffering algorithms. We provide a detailed\nexplanation of Iris Core's system architecture, including the stateless\nisolation of core processes, interprocess communication paradigms, and explicit\nsynchronization paradigms that provide powerful control over the graphical\nprocessing unit. Iris Core achieves slide rendering at the sustained maximum\nframe rate on all tested platforms and buffers an entire new slide field of,\nview without overlapping pixels, in 10 ms with enhanced detail in 30 ms. It is\nable to buffer and compute high-fidelity reduction-enhancements for viewing\nlow-power cytology with increased visual quality at a rate of 100-160 us per\nslide tile, and with a cumulative median buffering rate of 1.36 GB of\ndecompressed image data per second. This buffering rate allows for an entirely\nnew field of view to be fully buffered and rendered in less than a single\nmonitor refresh on a standard display, and high detail features within 2-3\nmonitor refresh frames. These metrics far exceed previously published\nspecifications, beyond an order of magnitude in some contexts. The system shows\nno slowing with high use loads, but rather increases performance due to cache\nmechanisms."
                },
                "authors": [
                    {
                        "name": "Ryan Erik Landvater"
                    },
                    {
                        "name": "Ulysses Balis"
                    }
                ],
                "author_detail": {
                    "name": "Ulysses Balis"
                },
                "author": "Ulysses Balis",
                "arxiv_doi": "10.1016/j.jpi.2024.100414",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jpi.2024.100414",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.15437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 8 figures",
                "arxiv_journal_ref": "Journal of Pathology Informatics, 16, 100414 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10138v2",
                "updated": "2025-04-23T10:48:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    48,
                    52,
                    2,
                    113,
                    0
                ],
                "published": "2025-01-17T12:01:28Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    1,
                    28,
                    4,
                    17,
                    0
                ],
                "title": "The NIC should be part of the OS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The NIC should be part of the OS"
                },
                "summary": "The network interface adapter (NIC) is a critical component of a cloud server\noccupying a unique position. Not only is network performance vital to efficient\noperation of the machine, but unlike compute accelerators like GPUs, the\nnetwork subsystem must react to unpredictable events like the arrival of a\nnetwork packet and communicate with the appropriate application end point with\nminimal latency.\n  Current approaches to server stacks navigate a trade-off between flexibility,\nefficiency, and performance: the fastest kernel-bypass approaches dedicate\ncores to applications, busy-wait on receive queues, etc. while more flexible\napproaches appropriate to more dynamic workload mixes incur much greater\nsoftware overhead on the data path.\n  However, we reject this trade-off, which we ascribe to an arbitrary (and\nsub-optimal) split in system state between the OS and the NIC. Instead, by\nexploiting the properties of cache-coherent interconnects and integrating the\nNIC closely with the OS kernel, we can achieve something surprising:\nperformance for RPC workloads better than the fastest kernelbypass approaches\nwithout sacrificing the robustness and dynamic adaptation of kernel-based\nnetwork subsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The network interface adapter (NIC) is a critical component of a cloud server\noccupying a unique position. Not only is network performance vital to efficient\noperation of the machine, but unlike compute accelerators like GPUs, the\nnetwork subsystem must react to unpredictable events like the arrival of a\nnetwork packet and communicate with the appropriate application end point with\nminimal latency.\n  Current approaches to server stacks navigate a trade-off between flexibility,\nefficiency, and performance: the fastest kernel-bypass approaches dedicate\ncores to applications, busy-wait on receive queues, etc. while more flexible\napproaches appropriate to more dynamic workload mixes incur much greater\nsoftware overhead on the data path.\n  However, we reject this trade-off, which we ascribe to an arbitrary (and\nsub-optimal) split in system state between the OS and the NIC. Instead, by\nexploiting the properties of cache-coherent interconnects and integrating the\nNIC closely with the OS kernel, we can achieve something surprising:\nperformance for RPC workloads better than the fastest kernelbypass approaches\nwithout sacrificing the robustness and dynamic adaptation of kernel-based\nnetwork subsystems."
                },
                "authors": [
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "arxiv_doi": "10.1145/3713082.3730388",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3713082.3730388",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.10138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Camera ready for HotOS'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v2",
                "updated": "2025-04-23T05:04:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    5,
                    4,
                    58,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Caching through Attention Output Error based Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Caching through Attention Output Error based Token Eviction"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "14 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06015v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06015v2",
                "updated": "2025-04-23T04:21:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    4,
                    21,
                    49,
                    2,
                    113,
                    0
                ],
                "published": "2025-03-08T02:35:16Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    2,
                    35,
                    16,
                    5,
                    67,
                    0
                ],
                "title": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems"
                },
                "summary": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e they do not adapt to changing cache access\npatterns. Newer developments such as the High-Luminosity - Large Hadron\nCollider (HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move\ntoward streaming readout based Data Acquisition systems (DAQs) will increase\nthe data production exponentially and hence burden the storage, compute &\nnetwork infrastructures. Moreover, existing caching frameworks are optimized to\nreduce latency, but not optimized for storage. This, in combination with\nlimited cache capacities relative to total data, makes it difficult to achieve\ndata locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, we first present a Long Short-Term Memory-based (LSTM) hourly and\nmulti-step cache usage prediction. Second, we present an hourly file-level\naccess prediction model based on CatboostRegressor. To date, most ML-based\ncache prediction strategies in HEP have focused on daily cache usage and\nlimited works tackled hourly cache usage and even fewer strategies addressed\nhourly file-level access prediction. File-level access prediction allows for\nthe design of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending the\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e they do not adapt to changing cache access\npatterns. Newer developments such as the High-Luminosity - Large Hadron\nCollider (HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move\ntoward streaming readout based Data Acquisition systems (DAQs) will increase\nthe data production exponentially and hence burden the storage, compute &\nnetwork infrastructures. Moreover, existing caching frameworks are optimized to\nreduce latency, but not optimized for storage. This, in combination with\nlimited cache capacities relative to total data, makes it difficult to achieve\ndata locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, we first present a Long Short-Term Memory-based (LSTM) hourly and\nmulti-step cache usage prediction. Second, we present an hourly file-level\naccess prediction model based on CatboostRegressor. To date, most ML-based\ncache prediction strategies in HEP have focused on daily cache usage and\nlimited works tackled hourly cache usage and even fewer strategies addressed\nhourly file-level access prediction. File-level access prediction allows for\nthe design of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending the\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations."
                },
                "authors": [
                    {
                        "name": "Venkat Sai Suman Lamba Karanam"
                    },
                    {
                        "name": "Sarat Sasank Barla"
                    },
                    {
                        "name": "Byrav Ramamurthy"
                    },
                    {
                        "name": "Derek Weitzel"
                    }
                ],
                "author_detail": {
                    "name": "Derek Weitzel"
                },
                "author": "Derek Weitzel",
                "arxiv_comment": "Submitted as a contribution to the CHEP 2024 proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06015v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06015v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16324v1",
                "updated": "2025-04-22T23:52:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    23,
                    52,
                    13,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T23:52:13Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    23,
                    52,
                    13,
                    1,
                    112,
                    0
                ],
                "title": "The Dawn of Disaggregation and the Coherence Conundrum: A Call for\n  Federated Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dawn of Disaggregation and the Coherence Conundrum: A Call for\n  Federated Coherence"
                },
                "summary": "Disaggregated memory is an upcoming data center technology that will allow\nnodes (servers) to share data efficiently. Sharing data creates a debate on the\nlevel of cache coherence the system should provide. While current proposals aim\nto provide coherence for all or parts of the disaggregated memory, we argue\nthat this approach is problematic, because of scalability limitations and\nhardware complexity. Instead, we propose and formally define federated\ncoherence, a model that provides coherence only within nodes, not across nodes.\nFederated coherence can use current intra-node coherence provided by processors\nwithout requiring expensive mechanisms for inter-node coherence. Developers can\nuse federated coherence with a few simple programming paradigms and a\nsynchronization library. We sketch some potential applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated memory is an upcoming data center technology that will allow\nnodes (servers) to share data efficiently. Sharing data creates a debate on the\nlevel of cache coherence the system should provide. While current proposals aim\nto provide coherence for all or parts of the disaggregated memory, we argue\nthat this approach is problematic, because of scalability limitations and\nhardware complexity. Instead, we propose and formally define federated\ncoherence, a model that provides coherence only within nodes, not across nodes.\nFederated coherence can use current intra-node coherence provided by processors\nwithout requiring expensive mechanisms for inter-node coherence. Developers can\nuse federated coherence with a few simple programming paradigms and a\nsynchronization library. We sketch some potential applications."
                },
                "authors": [
                    {
                        "name": "Jaewan Hong"
                    },
                    {
                        "name": "Marcos K. Aguilera"
                    },
                    {
                        "name": "Emmanuel Amaro"
                    },
                    {
                        "name": "Vincent Liu"
                    },
                    {
                        "name": "Aurojit Panda"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11816v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11816v3",
                "updated": "2025-04-22T17:34:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    34,
                    34,
                    1,
                    112,
                    0
                ],
                "published": "2025-03-14T19:02:16Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations."
                },
                "authors": [
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "arxiv_comment": "Presented at IEEE Custom Integrated Circuits Conference (CICC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11816v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11816v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v2",
                "updated": "2025-04-22T17:23:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    23,
                    28,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators"
                },
                "summary": "As AI workloads drive soaring memory requirements, there is a need for\nhigher-density on-chip memory for domain-specific accelerators that goes beyond\nwhat current SRAM technology can provide. We motivate that algorithms and\napplication behavior should guide the composition of heterogeneous on-chip\nmemories. However, there has been little work in factoring dynamic application\nprofiles into such design decisions. We present GainSight, a profiling\nframework that analyzes fine-grained memory access patterns and computes data\nlifetimes in domain-specific accelerators. By combining instrumentation and\nsimulation across retargetable hardware backends, GainSight aligns\nheterogeneous memory designs with workload-specific traffic and lifetime\nmetrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA\nH100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2\nGPU cache accesses, and 79% of systolic array scratchpad accesses across\nprofiled workloads are short-lived and suitable for silicon-based gain cell RAM\n(Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3)\nUp to 90% of GPU cache fetches are never reused, highlighting inefficiencies in\nterms of cache pollution. These insights that GainSight provides can be used to\nbetter understand the design spaces of both emerging on-chip memories and\nsoftware algorithmic optimizations for the next generation of AI accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive soaring memory requirements, there is a need for\nhigher-density on-chip memory for domain-specific accelerators that goes beyond\nwhat current SRAM technology can provide. We motivate that algorithms and\napplication behavior should guide the composition of heterogeneous on-chip\nmemories. However, there has been little work in factoring dynamic application\nprofiles into such design decisions. We present GainSight, a profiling\nframework that analyzes fine-grained memory access patterns and computes data\nlifetimes in domain-specific accelerators. By combining instrumentation and\nsimulation across retargetable hardware backends, GainSight aligns\nheterogeneous memory designs with workload-specific traffic and lifetime\nmetrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA\nH100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2\nGPU cache accesses, and 79% of systolic array scratchpad accesses across\nprofiled workloads are short-lived and suitable for silicon-based gain cell RAM\n(Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3)\nUp to 90% of GPU cache fetches are never reused, highlighting inefficiencies in\nterms of cache pollution. These insights that GainSight provides can be used to\nbetter understand the design spaces of both emerging on-chip memories and\nsoftware algorithmic optimizations for the next generation of AI accelerators."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin Hoßfeld"
                    },
                    {
                        "name": "Jake Cheng Jiajun"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "15 pages, 10 figures. Updated references and author name presentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14489v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14489v2",
                "updated": "2025-04-22T15:19:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    19,
                    48,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-20T04:46:34Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    4,
                    46,
                    34,
                    6,
                    110,
                    0
                ],
                "title": "Optimizing SLO-oriented LLM Serving with PD-Multiplexing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing SLO-oriented LLM Serving with PD-Multiplexing"
                },
                "summary": "Modern LLM services demand high throughput and stringent SLO guarantees\nacross two distinct inference phases-prefill and decode-and complex multi-turn\nworkflows. However, current systems face a fundamental tradeoff: out-of-place\ncompute partition enables per-phase SLO attainment, while in-place memory\nsharing maximizes throughput via KV cache reuse. Moreover, existing in-place\ncompute partition also encounters low utilization and high overhead due to\nphase-coupling design. We present Drift, a new LLM serving framework that\nresolves this tension via PD multiplexing, enabling in-place and\nphase-decoupled compute partition. Drift leverages low-level GPU partitioning\ntechniques to multiplex prefill and decode phases spatially and adaptively on\nshared GPUs, while preserving in-place memory sharing. To fully leverage the\nmultiplexing capability, Drift introduces an adaptive gang scheduling\nmechanism, a contention-free modeling method, and a SLO-aware dispatching\npolicy. Evaluation shows that Drift achieves an average $5.1\\times$ throughput\nimprovement (up to $17.5\\times$) over state-of-the-art baselines, while\nconsistently meeting SLO targets under complex LLM workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM services demand high throughput and stringent SLO guarantees\nacross two distinct inference phases-prefill and decode-and complex multi-turn\nworkflows. However, current systems face a fundamental tradeoff: out-of-place\ncompute partition enables per-phase SLO attainment, while in-place memory\nsharing maximizes throughput via KV cache reuse. Moreover, existing in-place\ncompute partition also encounters low utilization and high overhead due to\nphase-coupling design. We present Drift, a new LLM serving framework that\nresolves this tension via PD multiplexing, enabling in-place and\nphase-decoupled compute partition. Drift leverages low-level GPU partitioning\ntechniques to multiplex prefill and decode phases spatially and adaptively on\nshared GPUs, while preserving in-place memory sharing. To fully leverage the\nmultiplexing capability, Drift introduces an adaptive gang scheduling\nmechanism, a contention-free modeling method, and a SLO-aware dispatching\npolicy. Evaluation shows that Drift achieves an average $5.1\\times$ throughput\nimprovement (up to $17.5\\times$) over state-of-the-art baselines, while\nconsistently meeting SLO targets under complex LLM workloads."
                },
                "authors": [
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Ziyi Xu"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14489v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14489v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15720v1",
                "updated": "2025-04-22T09:08:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    8,
                    46,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T09:08:46Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    8,
                    46,
                    1,
                    112,
                    0
                ],
                "title": "SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large\n  Language Model Inference"
                },
                "summary": "Large language models (LLMs) with different architectures and sizes have been\ndeveloped. Serving each LLM with dedicated GPUs leads to resource waste and\nservice inefficiency due to the varying demand of LLM requests. A common\npractice is to share multiple LLMs. However, existing sharing systems either do\nnot consider the autoregressive pattern of LLM services, or only focus on\nimproving the throughput, which impairs the sharing performance, especially the\nserving latency. We present SeaLLM, which enables service-aware and\nlatency-optimized LLM sharing. SeaLLM improves the overall sharing performance\nby (1) a latency-optimized scheduling algorithm utilizing the characteristics\nof LLM services, (2) a placement algorithm to determine the placement plan and\nan adaptive replacement algorithm to decide the replacement interval, and (3) a\nunified key-value cache to share GPU memory among LLM services efficiently. Our\nevaluation under real-world traces and LLM services demonstrates that SeaLLM\nimproves the normalized latency by up to $13.60\\times$, the tail latency by up\nto $18.69\\times$, and the SLO attainment by up to $3.64\\times$ compared to\nexisting solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with different architectures and sizes have been\ndeveloped. Serving each LLM with dedicated GPUs leads to resource waste and\nservice inefficiency due to the varying demand of LLM requests. A common\npractice is to share multiple LLMs. However, existing sharing systems either do\nnot consider the autoregressive pattern of LLM services, or only focus on\nimproving the throughput, which impairs the sharing performance, especially the\nserving latency. We present SeaLLM, which enables service-aware and\nlatency-optimized LLM sharing. SeaLLM improves the overall sharing performance\nby (1) a latency-optimized scheduling algorithm utilizing the characteristics\nof LLM services, (2) a placement algorithm to determine the placement plan and\nan adaptive replacement algorithm to decide the replacement interval, and (3) a\nunified key-value cache to share GPU memory among LLM services efficiently. Our\nevaluation under real-world traces and LLM services demonstrates that SeaLLM\nimproves the normalized latency by up to $13.60\\times$, the tail latency by up\nto $18.69\\times$, and the SLO attainment by up to $3.64\\times$ compared to\nexisting solutions."
                },
                "authors": [
                    {
                        "name": "Yihao Zhao"
                    },
                    {
                        "name": "Jiadun Chen"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18869v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18869v3",
                "updated": "2025-04-21T22:13:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    22,
                    13,
                    7,
                    0,
                    111,
                    0
                ],
                "published": "2025-03-24T16:44:32Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    44,
                    32,
                    0,
                    83,
                    0
                ],
                "title": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design"
                },
                "summary": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18869v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18869v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01005v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01005v2",
                "updated": "2025-04-21T20:10:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    20,
                    10,
                    11,
                    0,
                    111,
                    0
                ],
                "published": "2025-01-02T02:02:20Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving"
                },
                "summary": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation."
                },
                "authors": [
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Lequn Chen"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Wuwei Lin"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Stephanie Wang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Vinod Grover"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Luis Ceze"
                    }
                ],
                "author_detail": {
                    "name": "Luis Ceze"
                },
                "author": "Luis Ceze",
                "arxiv_comment": "Accepted by MLSys 2025, code available at\n  http://github.com/flashinfer-ai/flashinfer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01005v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01005v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15260v1",
                "updated": "2025-04-21T17:39:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    39,
                    59,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:39:59Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    39,
                    59,
                    0,
                    111,
                    0
                ],
                "title": "Joint Knowledge and Power Management for Secure Semantic Communication\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Knowledge and Power Management for Secure Semantic Communication\n  Networks"
                },
                "summary": "Recently, semantic communication (SemCom) has shown its great superiorities\nin resource savings and information exchanges. However, while its unique\nbackground knowledge guarantees accurate semantic reasoning and recovery,\nsemantic information security-related concerns are introduced at the same time.\nSince the potential eavesdroppers may have the same background knowledge to\naccurately decrypt the private semantic information transmitted between legal\nSemCom users, this makes the knowledge management in SemCom networks rather\nchallenging in joint consideration with the power control. To this end, this\npaper focuses on jointly addressing three core issues of power allocation,\nknowledge base caching (KBC), and device-to-device (D2D) user pairing (DUP) in\nsecure SemCom networks. We first develop a novel performance metric, namely\nsemantic secrecy throughput (SST), to quantify the information security level\nthat can be achieved at each pair of D2D SemCom users. Next, an SST\nmaximization problem is formulated subject to secure SemCom-related delay and\nreliability constraints. Afterward, we propose a security-aware resource\nmanagement solution using the Lagrange primal-dual method and a two-stage\nmethod. Simulation results demonstrate our proposed solution nearly doubles the\nSST performance and realizes less than half of the queuing delay performance\ncompared to different benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, semantic communication (SemCom) has shown its great superiorities\nin resource savings and information exchanges. However, while its unique\nbackground knowledge guarantees accurate semantic reasoning and recovery,\nsemantic information security-related concerns are introduced at the same time.\nSince the potential eavesdroppers may have the same background knowledge to\naccurately decrypt the private semantic information transmitted between legal\nSemCom users, this makes the knowledge management in SemCom networks rather\nchallenging in joint consideration with the power control. To this end, this\npaper focuses on jointly addressing three core issues of power allocation,\nknowledge base caching (KBC), and device-to-device (D2D) user pairing (DUP) in\nsecure SemCom networks. We first develop a novel performance metric, namely\nsemantic secrecy throughput (SST), to quantify the information security level\nthat can be achieved at each pair of D2D SemCom users. Next, an SST\nmaximization problem is formulated subject to secure SemCom-related delay and\nreliability constraints. Afterward, we propose a security-aware resource\nmanagement solution using the Lagrange primal-dual method and a two-stage\nmethod. Simulation results demonstrate our proposed solution nearly doubles the\nSST performance and realizes less than half of the queuing delay performance\ncompared to different benchmarks."
                },
                "authors": [
                    {
                        "name": "Xuesong Liu"
                    },
                    {
                        "name": "Yansong Liu"
                    },
                    {
                        "name": "Haoyu Tang"
                    },
                    {
                        "name": "Fangzhou Zhao"
                    },
                    {
                        "name": "Le Xia"
                    },
                    {
                        "name": "Yao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yao Sun"
                },
                "author": "Yao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15247v1",
                "updated": "2025-04-21T17:22:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    22,
                    18,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:22:18Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    22,
                    18,
                    0,
                    111,
                    0
                ],
                "title": "Lance: Efficient Random Access in Columnar Storage through Adaptive\n  Structural Encodings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lance: Efficient Random Access in Columnar Storage through Adaptive\n  Structural Encodings"
                },
                "summary": "The growing interest in artificial intelligence has created workloads that\nrequire both sequential and random access. At the same time, NVMe-backed\nstorage solutions have emerged, providing caching capability for large columnar\ndatasets in cloud storage. Current columnar storage libraries fall short of\neffectively utilizing an NVMe device's capabilities, especially when it comes\nto random access. Historically, this has been assumed an implicit weakness in\ncolumnar storage formats, but this has not been sufficiently explored. In this\npaper, we examine the effectiveness of popular columnar formats such as Apache\nArrow, Apache Parquet, and Lance in both random access and full scan tasks\nagainst NVMe storage.\n  We argue that effective encoding of a column's structure, such as the\nrepetition and validity information, is the key to unlocking the disk's\nperformance. We show that Parquet, when configured correctly, can achieve over\n60x better random access performance than default settings. We also show that\nthis high random access performance requires making minor trade-offs in scan\nperformance and RAM utilization. We then describe the Lance structural encoding\nscheme, which alternates between two different structural encodings based on\ndata width, and achieves better random access performance without making\ntrade-offs in scan performance or RAM utilization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing interest in artificial intelligence has created workloads that\nrequire both sequential and random access. At the same time, NVMe-backed\nstorage solutions have emerged, providing caching capability for large columnar\ndatasets in cloud storage. Current columnar storage libraries fall short of\neffectively utilizing an NVMe device's capabilities, especially when it comes\nto random access. Historically, this has been assumed an implicit weakness in\ncolumnar storage formats, but this has not been sufficiently explored. In this\npaper, we examine the effectiveness of popular columnar formats such as Apache\nArrow, Apache Parquet, and Lance in both random access and full scan tasks\nagainst NVMe storage.\n  We argue that effective encoding of a column's structure, such as the\nrepetition and validity information, is the key to unlocking the disk's\nperformance. We show that Parquet, when configured correctly, can achieve over\n60x better random access performance than default settings. We also show that\nthis high random access performance requires making minor trade-offs in scan\nperformance and RAM utilization. We then describe the Lance structural encoding\nscheme, which alternates between two different structural encodings based on\ndata width, and achieves better random access performance without making\ntrade-offs in scan performance or RAM utilization."
                },
                "authors": [
                    {
                        "name": "Weston Pace"
                    },
                    {
                        "name": "Chang She"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Will Jones"
                    },
                    {
                        "name": "Albert Lockett"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Raunak Shah"
                    }
                ],
                "author_detail": {
                    "name": "Raunak Shah"
                },
                "author": "Raunak Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16588v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16588v3",
                "updated": "2025-04-21T15:36:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    36,
                    53,
                    0,
                    111,
                    0
                ],
                "published": "2025-03-20T17:37:15Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    37,
                    15,
                    3,
                    79,
                    0
                ],
                "title": "A Unified Framework for Quantitative Cache Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Quantitative Cache Analysis"
                },
                "summary": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU."
                },
                "authors": [
                    {
                        "name": "Sophie Kahlen"
                    },
                    {
                        "name": "Jan Reineke"
                    }
                ],
                "author_detail": {
                    "name": "Jan Reineke"
                },
                "author": "Jan Reineke",
                "arxiv_comment": "Extended version of RTAS 2025 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16588v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16588v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14866v2",
                "updated": "2025-04-21T15:13:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    13,
                    44,
                    0,
                    111,
                    0
                ],
                "published": "2025-02-20T18:59:52Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention"
                },
                "summary": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences and complex reasoning tasks, yet efficiently serving these\nmodels remains challenging due to the quadratic computational complexity of\nattention in the prefilling stage and the large memory footprint of the KV\ncache in the decoding stage. To address these issues, we introduce LServe, an\nefficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity\npatterns for both prefilling and decoding attention into a single framework,\nwhere computations on less important tokens are skipped block-wise. LServe\ndemonstrates the compatibility of static and dynamic sparsity in long-context\nLLM attention. This design enables multiplicative speedups by combining these\noptimizations. Specifically, we convert half of the attention heads to nearly\nfree streaming heads in both the prefilling and decoding stages. Additionally,\nwe find that only a constant number of KV pages is required to preserve\nlong-context and reasoning capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV\npages based on query-centric similarity. On average, LServe accelerates LLM\nprefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining\nlong-context accuracy. Code is released at\nhttps://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences and complex reasoning tasks, yet efficiently serving these\nmodels remains challenging due to the quadratic computational complexity of\nattention in the prefilling stage and the large memory footprint of the KV\ncache in the decoding stage. To address these issues, we introduce LServe, an\nefficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity\npatterns for both prefilling and decoding attention into a single framework,\nwhere computations on less important tokens are skipped block-wise. LServe\ndemonstrates the compatibility of static and dynamic sparsity in long-context\nLLM attention. This design enables multiplicative speedups by combining these\noptimizations. Specifically, we convert half of the attention heads to nearly\nfree streaming heads in both the prefilling and decoding stages. Additionally,\nwe find that only a constant number of KV pages is required to preserve\nlong-context and reasoning capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV\npages based on query-centric similarity. On average, LServe accelerates LLM\nprefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining\nlong-context accuracy. Code is released at\nhttps://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Accepted by MLSys 2025. Code available at:\n  https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15021v1",
                "updated": "2025-04-21T11:09:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    9,
                    43,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T11:09:43Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    9,
                    43,
                    0,
                    111,
                    0
                ],
                "title": "Is Intelligence the Right Direction in New OS Scheduling for Multiple\n  Resources in Cloud Environments?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Intelligence the Right Direction in New OS Scheduling for Multiple\n  Resources in Cloud Environments?"
                },
                "summary": "Making it intelligent is a promising way in System/OS design. This paper\nproposes OSML+, a new ML-based resource scheduling mechanism for co-located\ncloud services. OSML+ intelligently schedules the cache and main memory\nbandwidth resources at the memory hierarchy and the computing core resources\nsimultaneously. OSML+ uses a multi-model collaborative learning approach during\nits scheduling and thus can handle complicated cases, e.g., avoiding resource\ncliffs, sharing resources among applications, enabling different scheduling\npolicies for applications with different priorities, etc. OSML+ can converge\nfaster using ML models than previous studies. Moreover, OSML+ can automatically\nlearn on the fly and handle dynamically changing workloads accordingly. Using\ntransfer learning technologies, we show our design can work well across various\ncloud servers, including the latest off-the-shelf large-scale servers. Our\nexperimental results show that OSML+ supports higher loads and meets QoS\ntargets with lower overheads than previous studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making it intelligent is a promising way in System/OS design. This paper\nproposes OSML+, a new ML-based resource scheduling mechanism for co-located\ncloud services. OSML+ intelligently schedules the cache and main memory\nbandwidth resources at the memory hierarchy and the computing core resources\nsimultaneously. OSML+ uses a multi-model collaborative learning approach during\nits scheduling and thus can handle complicated cases, e.g., avoiding resource\ncliffs, sharing resources among applications, enabling different scheduling\npolicies for applications with different priorities, etc. OSML+ can converge\nfaster using ML models than previous studies. Moreover, OSML+ can automatically\nlearn on the fly and handle dynamically changing workloads accordingly. Using\ntransfer learning technologies, we show our design can work well across various\ncloud servers, including the latest off-the-shelf large-scale servers. Our\nexperimental results show that OSML+ supports higher loads and meets QoS\ntargets with lower overheads than previous studies."
                },
                "authors": [
                    {
                        "name": "Xinglei Dou"
                    },
                    {
                        "name": "Lei Liu"
                    },
                    {
                        "name": "Limin Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Limin Xiao"
                },
                "author": "Limin Xiao",
                "arxiv_comment": "25 pages, 14 figures, to be published in ACM Transactions on Storage",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01783v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01783v3",
                "updated": "2025-04-21T03:40:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    3,
                    40,
                    10,
                    0,
                    111,
                    0
                ],
                "published": "2024-11-04T04:15:36Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "title": "Context Parallelism for Scalable Million-Token Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Parallelism for Scalable Million-Token Inference"
                },
                "summary": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth."
                },
                "authors": [
                    {
                        "name": "Amy Yang"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Aya Ibrahim"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Bangsheng Tang"
                    },
                    {
                        "name": "Grigory Sizov"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Jongsoo Park"
                    },
                    {
                        "name": "Jianyu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyu Huang"
                },
                "author": "Jianyu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01783v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01783v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14775v1",
                "updated": "2025-04-21T00:07:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    7,
                    49,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T00:07:49Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    7,
                    49,
                    0,
                    111,
                    0
                ],
                "title": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling"
                },
                "summary": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency."
                },
                "authors": [
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Xianwei Zhang"
                    },
                    {
                        "name": "Jiangsu Du"
                    },
                    {
                        "name": "Zhiguang Chen"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Yutong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yutong Lu"
                },
                "author": "Yutong Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17116v2",
                "updated": "2025-04-20T21:50:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    21,
                    50,
                    3,
                    6,
                    110,
                    0
                ],
                "published": "2024-11-26T05:10:04Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "title": "Star Attention: Efficient LLM Inference over Long Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star Attention: Efficient LLM Inference over Long Sequences"
                },
                "summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy."
                },
                "authors": [
                    {
                        "name": "Shantanu Acharya"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Code: https://github.com/NVIDIA/Star-Attention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09775v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09775v3",
                "updated": "2025-04-20T19:57:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    19,
                    57,
                    16,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-14T00:29:49Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    0,
                    29,
                    49,
                    0,
                    104,
                    0
                ],
                "title": "Understanding and Optimizing Multi-Stage AI Inference Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Optimizing Multi-Stage AI Inference Pipelines"
                },
                "summary": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads."
                },
                "authors": [
                    {
                        "name": "Abhimanyu Rajeshkumar Bambhaniya"
                    },
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Suvinay Subramanian"
                    },
                    {
                        "name": "Sudarshan Srinivasan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Amir Yazdanbakhsh"
                    },
                    {
                        "name": "Midhilesh Elavazhagan"
                    },
                    {
                        "name": "Madhu Kumar"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "arxiv_comment": "Inference System Design for Multi-Stage AI Inference Pipelines. 13\n  Pages, 15 Figues, 3 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09775v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09775v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11208v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11208v2",
                "updated": "2025-04-20T07:53:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    7,
                    53,
                    9,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-15T14:11:38Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    11,
                    38,
                    1,
                    105,
                    0
                ],
                "title": "Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink\n  of an Eye",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink\n  of an Eye"
                },
                "summary": "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively."
                },
                "authors": [
                    {
                        "name": "Bradley Morgan"
                    },
                    {
                        "name": "Gal Horowitz"
                    },
                    {
                        "name": "Sioli O'Connell"
                    },
                    {
                        "name": "Stephan van Schaik"
                    },
                    {
                        "name": "Chitchanok Chuengsatiansup"
                    },
                    {
                        "name": "Daniel Genkin"
                    },
                    {
                        "name": "Olaf Maennel"
                    },
                    {
                        "name": "Paul Montague"
                    },
                    {
                        "name": "Eyal Ronen"
                    },
                    {
                        "name": "Yuval Yarom"
                    }
                ],
                "author_detail": {
                    "name": "Yuval Yarom"
                },
                "author": "Yuval Yarom",
                "arxiv_comment": "Added reference to the ID3 decision tree induction algorithm by J. R.\n  Quinlan in Section 5.4",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11208v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14435v1",
                "updated": "2025-04-20T00:49:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    0,
                    49,
                    27,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-20T00:49:27Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    0,
                    49,
                    27,
                    6,
                    110,
                    0
                ],
                "title": "Deuteronomy 2.0: Record Caching and Latch Freedom",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deuteronomy 2.0: Record Caching and Latch Freedom"
                },
                "summary": "The Deuteronomy transactional key-value store is unique architecturally in\nproviding separation between transaction functionality -- its Transactional\nComponent (TC) and data management -- its Data Component (DC). It is unique in\ntechnology by (1) supporting record caching, a smaller unit than the\ntraditional page; and (2) protecting resources during concurrent execution\nusing a latch-free approach. Both technologies are enabled by delta updating.\nThis paper explains how record caching improves cache cost/performance. It also\nshows how a new latch-free approach makes implementation easier and improves\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Deuteronomy transactional key-value store is unique architecturally in\nproviding separation between transaction functionality -- its Transactional\nComponent (TC) and data management -- its Data Component (DC). It is unique in\ntechnology by (1) supporting record caching, a smaller unit than the\ntraditional page; and (2) protecting resources during concurrent execution\nusing a latch-free approach. Both technologies are enabled by delta updating.\nThis paper explains how record caching improves cache cost/performance. It also\nshows how a new latch-free approach makes implementation easier and improves\nperformance."
                },
                "authors": [
                    {
                        "name": "David Lomet"
                    }
                ],
                "author_detail": {
                    "name": "David Lomet"
                },
                "author": "David Lomet",
                "arxiv_comment": "6 pages, 5 figures, potential CIDR submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14374v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14374v1",
                "updated": "2025-04-19T18:25:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "published": "2025-04-19T18:25:20Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "title": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation"
                },
                "summary": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model."
                },
                "authors": [
                    {
                        "name": "Max Lübke"
                    },
                    {
                        "name": "Marco De Lucia"
                    },
                    {
                        "name": "Stefan Petri"
                    },
                    {
                        "name": "Bettina Schnor"
                    }
                ],
                "author_detail": {
                    "name": "Bettina Schnor"
                },
                "author": "Bettina Schnor",
                "arxiv_comment": "Long version, 15 pages, 6 figures; Short version (8 pages) submitted\n  to \"25th International Conference on Computational Science\" (ICCS25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14374v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14196v1",
                "updated": "2025-04-19T06:18:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    6,
                    18,
                    56,
                    5,
                    109,
                    0
                ],
                "published": "2025-04-19T06:18:56Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    6,
                    18,
                    56,
                    5,
                    109,
                    0
                ],
                "title": "Room-temperature high-average-power strong-field terahertz source based\n  on industrial high-repetition-rate femtosecond laser",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Room-temperature high-average-power strong-field terahertz source based\n  on industrial high-repetition-rate femtosecond laser"
                },
                "summary": "Free-space strong-field terahertz (THz) pulses, generated via optical\nrectification of femtosecond lasers in nonlinear crystals, are pivotal in\nvarious applications. However, conventional Ti:sapphire lasers struggle to\nproduce high-average-power THz due to their limited output power. While\nkilowatt ytterbium lasers are increasingly adopted, their application in THz\ngeneration faces challenges: low optical-to-THz conversion efficiency\n(attributed to long pulse durations and low energy) and crystal damage under\nhigh pumping power. Here, we report a high-average-power strong-field THz\nsource using a lithium niobate crystal pumped by a 1030-nm, 570-fs, 1-mJ,\n50-kHz ytterbium femtosecond laser with tilted pulse front pumping (TPFP). By\nsystematically optimizing TPFP implementations and comparing grating- and\nechelon-type configurations, we achieve a THz source with 64.5 mW average power\nat 42-W, 50-kHz pumping, and a focused peak electric field of 525 kV/cm at\n0.83-mJ, 1-kHz operation. Additionally, we observe Zeeman torque signals in\ncobalt-iron ferromagnetic nanofilms. This high-repetition-rate,\nhigh-average-power THz system, combined with its potential capabilities in high\nsignal-to-noise spectroscopy and imaging, promises transformative impacts in\nquantum matter manipulation, non-destructive testing, and biomedicine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free-space strong-field terahertz (THz) pulses, generated via optical\nrectification of femtosecond lasers in nonlinear crystals, are pivotal in\nvarious applications. However, conventional Ti:sapphire lasers struggle to\nproduce high-average-power THz due to their limited output power. While\nkilowatt ytterbium lasers are increasingly adopted, their application in THz\ngeneration faces challenges: low optical-to-THz conversion efficiency\n(attributed to long pulse durations and low energy) and crystal damage under\nhigh pumping power. Here, we report a high-average-power strong-field THz\nsource using a lithium niobate crystal pumped by a 1030-nm, 570-fs, 1-mJ,\n50-kHz ytterbium femtosecond laser with tilted pulse front pumping (TPFP). By\nsystematically optimizing TPFP implementations and comparing grating- and\nechelon-type configurations, we achieve a THz source with 64.5 mW average power\nat 42-W, 50-kHz pumping, and a focused peak electric field of 525 kV/cm at\n0.83-mJ, 1-kHz operation. Additionally, we observe Zeeman torque signals in\ncobalt-iron ferromagnetic nanofilms. This high-repetition-rate,\nhigh-average-power THz system, combined with its potential capabilities in high\nsignal-to-noise spectroscopy and imaging, promises transformative impacts in\nquantum matter manipulation, non-destructive testing, and biomedicine."
                },
                "authors": [
                    {
                        "name": "Deyin Kong"
                    },
                    {
                        "name": "Yichen Su"
                    },
                    {
                        "name": "Cheng Song"
                    },
                    {
                        "name": "Xiaojun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Wu"
                },
                "author": "Xiaojun Wu",
                "arxiv_comment": "15 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v5",
                "updated": "2025-04-19T05:57:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    5,
                    57,
                    44,
                    5,
                    109,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14089v1",
                "updated": "2025-04-18T22:10:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    22,
                    10,
                    2,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T22:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    22,
                    10,
                    2,
                    4,
                    108,
                    0
                ],
                "title": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models"
                },
                "summary": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average."
                },
                "authors": [
                    {
                        "name": "Kang He"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13989v1",
                "updated": "2025-04-18T13:46:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    46,
                    58,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T13:46:58Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    46,
                    58,
                    4,
                    108,
                    0
                ],
                "title": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs"
                },
                "summary": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40\\% increase\nin accuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40\\% increase\nin accuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization."
                },
                "authors": [
                    {
                        "name": "Lucas Maisonnave"
                    },
                    {
                        "name": "Cyril Moineau"
                    },
                    {
                        "name": "Olivier Bichler"
                    },
                    {
                        "name": "Fabrice Rastello"
                    }
                ],
                "author_detail": {
                    "name": "Fabrice Rastello"
                },
                "author": "Fabrice Rastello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13981v1",
                "updated": "2025-04-18T06:34:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    6,
                    34,
                    57,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T06:34:57Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    6,
                    34,
                    57,
                    4,
                    108,
                    0
                ],
                "title": "CacheFormer: High Attention-Based Segment Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFormer: High Attention-Based Segment Caching"
                },
                "summary": "Efficiently handling long contexts in transformer-based language models with\nlow perplexity is an active area of research. Numerous recent approaches like\nLinformer, Longformer, Performer, and Structured state space models (SSMs).,\nhave not fully resolved this problem. All these models strive to reduce the\nquadratic time complexity of the attention mechanism while minimizing the loss\nin quality due to the effective compression of the long context. Inspired by\nthe cache and virtual memory principle in computers, where in case of a cache\nmiss, not only the needed data is retrieved from the memory, but the adjacent\ndata is also obtained, we apply this concept to handling long contexts by\ndividing it into small segments. In our design, we retrieve the nearby segments\nin an uncompressed form when high segment-level attention occurs at the\ncompressed level. Our en-hancements for handling long context include\naggregating four attention mechanisms consisting of short sliding window\nattention, long compressed segmented attention, dynamically retrieving top k\nhigh attention uncompressed segments, and overlapping segments in long segment\nattention to avoid segment fragmentation. These enhancements result in an\narchitecture that outperforms ex-isting SOTA architectures with an average\nperplexity improvement of 8.5% over similar model sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently handling long contexts in transformer-based language models with\nlow perplexity is an active area of research. Numerous recent approaches like\nLinformer, Longformer, Performer, and Structured state space models (SSMs).,\nhave not fully resolved this problem. All these models strive to reduce the\nquadratic time complexity of the attention mechanism while minimizing the loss\nin quality due to the effective compression of the long context. Inspired by\nthe cache and virtual memory principle in computers, where in case of a cache\nmiss, not only the needed data is retrieved from the memory, but the adjacent\ndata is also obtained, we apply this concept to handling long contexts by\ndividing it into small segments. In our design, we retrieve the nearby segments\nin an uncompressed form when high segment-level attention occurs at the\ncompressed level. Our en-hancements for handling long context include\naggregating four attention mechanisms consisting of short sliding window\nattention, long compressed segmented attention, dynamically retrieving top k\nhigh attention uncompressed segments, and overlapping segments in long segment\nattention to avoid segment fragmentation. These enhancements result in an\narchitecture that outperforms ex-isting SOTA architectures with an average\nperplexity improvement of 8.5% over similar model sizes."
                },
                "authors": [
                    {
                        "name": "Sushant Singh"
                    },
                    {
                        "name": "Ausif Mahmood"
                    }
                ],
                "author_detail": {
                    "name": "Ausif Mahmood"
                },
                "author": "Ausif Mahmood",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09146v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09146v2",
                "updated": "2025-04-18T05:13:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    13,
                    52,
                    4,
                    108,
                    0
                ],
                "published": "2025-01-15T20:55:13Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    55,
                    13,
                    2,
                    15,
                    0
                ],
                "title": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs"
                },
                "summary": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Bhuyan"
                    },
                    {
                        "name": "Hrishikesh Dutta"
                    },
                    {
                        "name": "Subir Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Subir Biswas"
                },
                "author": "Subir Biswas",
                "arxiv_comment": "32 pages, 11 figures, 1 table, 4 algorithms, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09146v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09146v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16112v1",
                "updated": "2025-04-18T03:31:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    3,
                    31,
                    8,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T03:31:08Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    3,
                    31,
                    8,
                    4,
                    108,
                    0
                ],
                "title": "HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM\n  Inference via GPU Co-processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM\n  Inference via GPU Co-processing"
                },
                "summary": "The attention layer, a core component of Transformer-based LLMs, brings out\ninefficiencies in current GPU systems due to its low operational intensity and\nthe substantial memory requirements of KV caches. We propose a High-bandwidth\nProcessing Unit (HPU), a memoryintensive co-processor that enhances GPU\nresource utilization during large-batched LLM inference. By offloading\nmemory-bound operations, the HPU allows the GPU to focus on compute-intensive\ntasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales\nout to accommodate surging memory demands driven by large batch sizes and\nextended sequence lengths. In this paper, we show the HPU prototype implemented\nwith PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU\nheterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy\nefficiency improvements over a GPUonly system, providing scalability without\nincreasing the number of GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The attention layer, a core component of Transformer-based LLMs, brings out\ninefficiencies in current GPU systems due to its low operational intensity and\nthe substantial memory requirements of KV caches. We propose a High-bandwidth\nProcessing Unit (HPU), a memoryintensive co-processor that enhances GPU\nresource utilization during large-batched LLM inference. By offloading\nmemory-bound operations, the HPU allows the GPU to focus on compute-intensive\ntasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales\nout to accommodate surging memory demands driven by large batch sizes and\nextended sequence lengths. In this paper, we show the HPU prototype implemented\nwith PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU\nheterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy\nefficiency improvements over a GPUonly system, providing scalability without\nincreasing the number of GPUs."
                },
                "authors": [
                    {
                        "name": "Myunghyun Rhee"
                    },
                    {
                        "name": "Joonseop Sim"
                    },
                    {
                        "name": "Taeyoung Ahn"
                    },
                    {
                        "name": "Seungyong Lee"
                    },
                    {
                        "name": "Daegun Yoon"
                    },
                    {
                        "name": "Euiseok Kim"
                    },
                    {
                        "name": "Kyoung Park"
                    },
                    {
                        "name": "Youngpyo Joo"
                    },
                    {
                        "name": "Hosik Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hosik Kim"
                },
                "author": "Hosik Kim",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13385v1",
                "updated": "2025-04-18T00:21:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    0,
                    21,
                    0,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T00:21:00Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    0,
                    21,
                    0,
                    4,
                    108,
                    0
                ],
                "title": "EXAM: Exploiting Exclusive System-Level Cache in Apple M-Series SoCs for\n  Enhanced Cache Occupancy Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EXAM: Exploiting Exclusive System-Level Cache in Apple M-Series SoCs for\n  Enhanced Cache Occupancy Attacks"
                },
                "summary": "Cache occupancy attacks exploit the shared nature of cache hierarchies to\ninfer a victim's activities by monitoring overall cache usage, unlike\naccess-driven cache attacks that focus on specific cache lines or sets. There\nexists some prior work that target the last-level cache (LLC) of Intel\nprocessors, which is inclusive of higher-level caches, and L2 caches of ARM\nsystems. In this paper, we target the System-Level Cache (SLC) of Apple\nM-series SoCs, which is exclusive to higher-level CPU caches. We address the\nchallenges of the exclusiveness and propose a suite of SLC-cache occupancy\nattacks, the first of its kind, where an adversary can monitor GPU and other\nCPU cluster activities from their own CPU cluster. We first discover the\nstructure of SLC in Apple M1 SOC and various policies pertaining to access and\nsharing through reverse engineering. We propose two attacks against websites.\nOne is a coarse-grained fingerprinting attack, recognizing which website is\naccessed based on their different GPU memory access patterns monitored through\nthe SLC occupancy channel. The other attack is a fine-grained pixel stealing\nattack, which precisely monitors the GPU memory usage for rendering different\npixels, through the SLC occupancy channel. Third, we introduce a novel screen\ncapturing attack which works beyond webpages, with the monitoring granularity\nof 57 rows of pixels (there are 1600 rows for the screen). This significantly\nexpands the attack surface, allowing the adversary to retrieve any screen\ndisplay, posing a substantial new threat to system security. Our findings\nreveal critical vulnerabilities in Apple's M-series SoCs and emphasize the\nurgent need for effective countermeasures against cache occupancy attacks in\nheterogeneous computing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache occupancy attacks exploit the shared nature of cache hierarchies to\ninfer a victim's activities by monitoring overall cache usage, unlike\naccess-driven cache attacks that focus on specific cache lines or sets. There\nexists some prior work that target the last-level cache (LLC) of Intel\nprocessors, which is inclusive of higher-level caches, and L2 caches of ARM\nsystems. In this paper, we target the System-Level Cache (SLC) of Apple\nM-series SoCs, which is exclusive to higher-level CPU caches. We address the\nchallenges of the exclusiveness and propose a suite of SLC-cache occupancy\nattacks, the first of its kind, where an adversary can monitor GPU and other\nCPU cluster activities from their own CPU cluster. We first discover the\nstructure of SLC in Apple M1 SOC and various policies pertaining to access and\nsharing through reverse engineering. We propose two attacks against websites.\nOne is a coarse-grained fingerprinting attack, recognizing which website is\naccessed based on their different GPU memory access patterns monitored through\nthe SLC occupancy channel. The other attack is a fine-grained pixel stealing\nattack, which precisely monitors the GPU memory usage for rendering different\npixels, through the SLC occupancy channel. Third, we introduce a novel screen\ncapturing attack which works beyond webpages, with the monitoring granularity\nof 57 rows of pixels (there are 1600 rows for the screen). This significantly\nexpands the attack surface, allowing the adversary to retrieve any screen\ndisplay, posing a substantial new threat to system security. Our findings\nreveal critical vulnerabilities in Apple's M-series SoCs and emphasize the\nurgent need for effective countermeasures against cache occupancy attacks in\nheterogeneous computing environments."
                },
                "authors": [
                    {
                        "name": "Tianhong Xu"
                    },
                    {
                        "name": "Aidong Adam Ding"
                    },
                    {
                        "name": "Yunsi Fei"
                    }
                ],
                "author_detail": {
                    "name": "Yunsi Fei"
                },
                "author": "Yunsi Fei",
                "arxiv_comment": "Accepted to ACM ASIA CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01291v2",
                "updated": "2025-04-17T23:45:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    23,
                    45,
                    51,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-02T01:49:58Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    49,
                    58,
                    2,
                    92,
                    0
                ],
                "title": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures"
                },
                "summary": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs."
                },
                "authors": [
                    {
                        "name": "Seungheon Shin"
                    },
                    {
                        "name": "Kyle Liddy"
                    },
                    {
                        "name": "Yinxuan Zhu"
                    },
                    {
                        "name": "Chandan Joishi"
                    },
                    {
                        "name": "Brianna A. Klein"
                    },
                    {
                        "name": "Andrew Armstrong"
                    },
                    {
                        "name": "Andrew A. Allerman"
                    },
                    {
                        "name": "Siddharth Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Siddharth Rajan"
                },
                "author": "Siddharth Rajan",
                "arxiv_comment": "12 pages, 7 figures, and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21465v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21465v3",
                "updated": "2025-04-25T19:40:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    19,
                    40,
                    54,
                    4,
                    115,
                    0
                ],
                "published": "2024-10-28T19:08:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference"
                },
                "summary": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Wenlei Bao"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Ningxin Zheng"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Harry Dong"
                    },
                    {
                        "name": "Yuejie Chi"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21465v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21465v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19325v2",
                "updated": "2025-04-17T15:26:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    26,
                    4,
                    3,
                    107,
                    0
                ],
                "published": "2025-03-25T03:38:06Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    3,
                    38,
                    6,
                    1,
                    84,
                    0
                ],
                "title": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction"
                },
                "summary": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context video modeling\nfaces challenges due to visual redundancy. Training on long videos is\ncomputationally expensive, as vision tokens grow much faster than language\ntokens. To tackle this issue, we propose balancing locality and long-range\ndependency through long short-term context modeling. A high-resolution\nshort-term context window ensures fine-grained temporal consistency, while an\nunlimited long-term context window encodes long-range information using fewer\ntokens. With this approach, we can train on long video sequences with a\nmanageable token context length, thereby significantly reducing training time\nand memory usage. Furthermore, we propose a multi-level KV cache designed to\nsupport the long short-term context modeling, which accelerating inference on\nlong video sequences. We demonstrate that FAR achieves state-of-the-art\nperformance in both short- and long-video generation, providing a simple yet\neffective baseline for video autoregressive modeling. The code is released at\nhttps://github.com/showlab/FAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context video modeling\nfaces challenges due to visual redundancy. Training on long videos is\ncomputationally expensive, as vision tokens grow much faster than language\ntokens. To tackle this issue, we propose balancing locality and long-range\ndependency through long short-term context modeling. A high-resolution\nshort-term context window ensures fine-grained temporal consistency, while an\nunlimited long-term context window encodes long-range information using fewer\ntokens. With this approach, we can train on long video sequences with a\nmanageable token context length, thereby significantly reducing training time\nand memory usage. Furthermore, we propose a multi-level KV cache designed to\nsupport the long short-term context modeling, which accelerating inference on\nlong video sequences. We demonstrate that FAR achieves state-of-the-art\nperformance in both short- and long-video generation, providing a simple yet\neffective baseline for video autoregressive modeling. The code is released at\nhttps://github.com/showlab/FAR."
                },
                "authors": [
                    {
                        "name": "Yuchao Gu"
                    },
                    {
                        "name": "Weijia Mao"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "arxiv_comment": "Project page at https://farlongctx.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12876v3",
                "updated": "2025-04-17T03:51:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    3,
                    51,
                    6,
                    3,
                    107,
                    0
                ],
                "published": "2024-10-15T05:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    1,
                    19,
                    1,
                    289,
                    0
                ],
                "title": "In-context KV-Cache Eviction for LLMs via Attention-Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context KV-Cache Eviction for LLMs via Attention-Gate"
                },
                "summary": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system. This paper enables a novel dynamic\nKV-Cache eviction policy by injecting a lightweight module called\nAttention-Gate to the model. It accepts the global context as input and yields\neviction flags for each token. The self-attention modules in the model proceed\naccording to the flags and cache only a subset of the KV states for next token\nprediction. The Attention-Gates can yield various flags for different heads and\nlayers and be easily tuned on top of a pre-trained LLM via continual\npre-training or supervised fine-tuning. The computational and memory overhead\nintroduced by Attention-Gates can be minimal. We empirically evaluate the\nproposed approach across multiple scenarios, showing that effective eviction of\nredundant tokens can not only improve efficiency but also enhance performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system. This paper enables a novel dynamic\nKV-Cache eviction policy by injecting a lightweight module called\nAttention-Gate to the model. It accepts the global context as input and yields\neviction flags for each token. The self-attention modules in the model proceed\naccording to the flags and cache only a subset of the KV states for next token\nprediction. The Attention-Gates can yield various flags for different heads and\nlayers and be easily tuned on top of a pre-trained LLM via continual\npre-training or supervised fine-tuning. The computational and memory overhead\nintroduced by Attention-Gates can be minimal. We empirically evaluate the\nproposed approach across multiple scenarios, showing that effective eviction of\nredundant tokens can not only improve efficiency but also enhance performance."
                },
                "authors": [
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10074v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10074v2",
                "updated": "2025-04-17T00:38:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    0,
                    38,
                    24,
                    3,
                    107,
                    0
                ],
                "published": "2025-03-13T05:43:14Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    43,
                    14,
                    3,
                    72,
                    0
                ],
                "title": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension"
                },
                "summary": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions."
                },
                "authors": [
                    {
                        "name": "Taehun Kim"
                    },
                    {
                        "name": "Hyerean Jang"
                    },
                    {
                        "name": "Youngjoo Shin"
                    }
                ],
                "author_detail": {
                    "name": "Youngjoo Shin"
                },
                "author": "Youngjoo Shin",
                "arxiv_comment": "The modified version of this preprint has been submitted to ESORICS\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10074v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10074v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12526v1",
                "updated": "2025-04-16T23:15:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    23,
                    15,
                    9,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T23:15:09Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    23,
                    15,
                    9,
                    2,
                    106,
                    0
                ],
                "title": "MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context\n  Language Models"
                },
                "summary": "Long-context language models exhibit impressive performance but remain\nchallenging to deploy due to high GPU memory demands during inference. We\npropose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that\npartitions critical layers into smaller \"mini-sequences\" and integrates\nseamlessly with KV cache offloading. Experiments on various Llama, Qwen, and\nMistral models demonstrate that MOM reduces peak memory usage by over 50\\% on\naverage. On Meta-Llama-3.2-8B, MOM extends the maximum context length from 155k\nto 455k tokens on a single A100 80GB GPU, while keeping outputs identical and\nnot compromising accuracy. MOM also maintains highly competitive throughput due\nto minimal computational overhead and efficient last-layer processing. Compared\nto traditional chunked prefill methods, MOM achieves a 35\\% greater context\nlength extension. More importantly, our method drastically reduces prefill\nmemory consumption, eliminating it as the longstanding dominant memory\nbottleneck during inference. This breakthrough fundamentally changes research\npriorities, redirecting future efforts from prefill-stage optimizations to\nimproving decode-stage residual KV cache efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context language models exhibit impressive performance but remain\nchallenging to deploy due to high GPU memory demands during inference. We\npropose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that\npartitions critical layers into smaller \"mini-sequences\" and integrates\nseamlessly with KV cache offloading. Experiments on various Llama, Qwen, and\nMistral models demonstrate that MOM reduces peak memory usage by over 50\\% on\naverage. On Meta-Llama-3.2-8B, MOM extends the maximum context length from 155k\nto 455k tokens on a single A100 80GB GPU, while keeping outputs identical and\nnot compromising accuracy. MOM also maintains highly competitive throughput due\nto minimal computational overhead and efficient last-layer processing. Compared\nto traditional chunked prefill methods, MOM achieves a 35\\% greater context\nlength extension. More importantly, our method drastically reduces prefill\nmemory consumption, eliminating it as the longstanding dominant memory\nbottleneck during inference. This breakthrough fundamentally changes research\npriorities, redirecting future efforts from prefill-stage optimizations to\nimproving decode-stage residual KV cache efficiency."
                },
                "authors": [
                    {
                        "name": "Junyang Zhang"
                    },
                    {
                        "name": "Tianyi Zhu"
                    },
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Anima Anandkumar"
                    }
                ],
                "author_detail": {
                    "name": "Anima Anandkumar"
                },
                "author": "Anima Anandkumar",
                "arxiv_comment": "Submitted to COLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v1",
                "updated": "2025-04-16T18:03:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2504.11704",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12240v1",
                "updated": "2025-04-16T16:45:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    45,
                    19,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T16:45:19Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    45,
                    19,
                    2,
                    106,
                    0
                ],
                "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cobra: Efficient Line Art COlorization with BRoAder References"
                },
                "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/."
                },
                "authors": [
                    {
                        "name": "Junhao Zhuang"
                    },
                    {
                        "name": "Lingen Li"
                    },
                    {
                        "name": "Xuan Ju"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Ying Shan"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shan"
                },
                "author": "Ying Shan",
                "arxiv_comment": "Project page with code: https://zhuang2002.github.io/Cobra/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11816v1",
                "updated": "2025-04-16T07:02:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    2,
                    38,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T07:02:38Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    2,
                    38,
                    2,
                    106,
                    0
                ],
                "title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache\n  Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache\n  Offloading"
                },
                "summary": "LLM inference is essential for applications like text summarization,\ntranslation, and data analysis, but the high cost of GPU instances from Cloud\nService Providers (CSPs) like AWS is a major burden. This paper proposes\nInferSave, a cost-efficient VM selection framework for cloud based LLM\ninference. InferSave optimizes KV cache offloading based on Service Level\nObjectives (SLOs) and workload charac teristics, estimating GPU memory needs,\nand recommending cost-effective VM instances. Additionally, the Compute Time\nCalibration Function (CTCF) improves instance selection accuracy by adjusting\nfor discrepancies between theoretical and actual GPU performance. Experiments\non AWS GPU instances show that selecting lower-cost instances without KV cache\noffloading improves cost efficiency by up to 73.7% for online workloads, while\nKV cache offloading saves up to 20.19% for offline workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference is essential for applications like text summarization,\ntranslation, and data analysis, but the high cost of GPU instances from Cloud\nService Providers (CSPs) like AWS is a major burden. This paper proposes\nInferSave, a cost-efficient VM selection framework for cloud based LLM\ninference. InferSave optimizes KV cache offloading based on Service Level\nObjectives (SLOs) and workload charac teristics, estimating GPU memory needs,\nand recommending cost-effective VM instances. Additionally, the Compute Time\nCalibration Function (CTCF) improves instance selection accuracy by adjusting\nfor discrepancies between theoretical and actual GPU performance. Experiments\non AWS GPU instances show that selecting lower-cost instances without KV cache\noffloading improves cost efficiency by up to 73.7% for online workloads, while\nKV cache offloading saves up to 20.19% for offline workloads."
                },
                "authors": [
                    {
                        "name": "Kihyun Kim"
                    },
                    {
                        "name": "Jinwoo Kim"
                    },
                    {
                        "name": "Hyunsun Chung"
                    },
                    {
                        "name": "Myung-Hoon Cha"
                    },
                    {
                        "name": "Hong-Yeon Kim"
                    },
                    {
                        "name": "Youngjae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Kim"
                },
                "author": "Youngjae Kim",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08334v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08334v3",
                "updated": "2025-04-16T05:57:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    5,
                    57,
                    8,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-11T07:59:06Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    59,
                    6,
                    4,
                    101,
                    0
                ],
                "title": "Efficient Architecture for RISC-V Vector Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Architecture for RISC-V Vector Memory Access"
                },
                "summary": "Vector processors frequently suffer from inefficient memory accesses,\nparticularly for strided and segment patterns. While coalescing strided\naccesses is a natural solution, effectively gathering or scattering elements at\nfixed strides remains challenging. Naive approaches rely on high-overhead\ncrossbars that remap any byte between memory and registers, leading to physical\ndesign issues. Segment operations require row-column transpositions, typically\nhandled using either element-level in-place transposition (degrading\nperformance) or large buffer-based bulk transposition (incurring high area\noverhead). In this paper, we present EARTH, a novel vector memory access\narchitecture designed to overcome these challenges through shifting-based\noptimizations. For strided accesses, EARTH integrates specialized shift\nnetworks for gathering and scattering elements. After coalescing multiple\naccesses within the same cache line, data is routed between memory and\nregisters through the shifting network with minimal overhead. For segment\noperations, EARTH employs a shifted register bank enabling direct column-wise\naccess, eliminating dedicated segment buffers while providing high-performance,\nin-place bulk transposition. Implemented on FPGA with Chisel HDL based on an\nopen-source RISC-V vector unit, EARTH enhances performance for strided memory\naccesses, achieving 4x-8x speedups in benchmarks dominated by strided\noperations. Compared to conventional designs, EARTH reduces hardware area by 9%\nand power consumption by 41%, significantly advancing both performance and\nefficiency of vector processors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector processors frequently suffer from inefficient memory accesses,\nparticularly for strided and segment patterns. While coalescing strided\naccesses is a natural solution, effectively gathering or scattering elements at\nfixed strides remains challenging. Naive approaches rely on high-overhead\ncrossbars that remap any byte between memory and registers, leading to physical\ndesign issues. Segment operations require row-column transpositions, typically\nhandled using either element-level in-place transposition (degrading\nperformance) or large buffer-based bulk transposition (incurring high area\noverhead). In this paper, we present EARTH, a novel vector memory access\narchitecture designed to overcome these challenges through shifting-based\noptimizations. For strided accesses, EARTH integrates specialized shift\nnetworks for gathering and scattering elements. After coalescing multiple\naccesses within the same cache line, data is routed between memory and\nregisters through the shifting network with minimal overhead. For segment\noperations, EARTH employs a shifted register bank enabling direct column-wise\naccess, eliminating dedicated segment buffers while providing high-performance,\nin-place bulk transposition. Implemented on FPGA with Chisel HDL based on an\nopen-source RISC-V vector unit, EARTH enhances performance for strided memory\naccesses, achieving 4x-8x speedups in benchmarks dominated by strided\noperations. Compared to conventional designs, EARTH reduces hardware area by 9%\nand power consumption by 41%, significantly advancing both performance and\nefficiency of vector processors."
                },
                "authors": [
                    {
                        "name": "Hongyi Guan"
                    },
                    {
                        "name": "Yichuan Gao"
                    },
                    {
                        "name": "Chenlu Miao"
                    },
                    {
                        "name": "Haoyang Wu"
                    },
                    {
                        "name": "Hang Zhu"
                    },
                    {
                        "name": "Mingfeng Lin"
                    },
                    {
                        "name": "Huayue Liang"
                    }
                ],
                "author_detail": {
                    "name": "Huayue Liang"
                },
                "author": "Huayue Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08334v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08334v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11765v1",
                "updated": "2025-04-16T04:59:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    4,
                    59,
                    18,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T04:59:18Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    4,
                    59,
                    18,
                    2,
                    106,
                    0
                ],
                "title": "Shared Disk KV Cache Management for Efficient Multi-Instance Inference\n  in RAG-Powered LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared Disk KV Cache Management for Efficient Multi-Instance Inference\n  in RAG-Powered LLMs"
                },
                "summary": "Recent large language models (LLMs) face increasing inference latency as\ninput context length and model size continue to grow. In particular, the\nretrieval-augmented generation (RAG) technique, which enhances LLM responses by\nincorporating external knowledge, exacerbates this issue by significantly\nincreasing the number of input tokens. This expansion in token length leads to\na substantial rise in computational overhead, particularly during the prefill\nstage, resulting in prolonged time-to-first-token (TTFT). To address this\nissue, this paper proposes a method to reduce TTFT by leveraging a disk-based\nkey-value (KV) cache to lessen the computational burden during the prefill\nstage. We also introduce a disk-based shared KV cache management system, called\nShared RAG-DCache, for multi-instance LLM RAG service environments. This\nsystem, together with an optimal system configuration, improves both throughput\nand latency under given resource constraints. Shared RAG-DCache exploits the\nlocality of documents related to user queries in RAG, as well as the queueing\ndelay in LLM inference services. It proactively generates and stores disk KV\ncaches for query-related documents and shares them across multiple LLM\ninstances to enhance inference performance. In experiments on a single host\nequipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in\nthroughput and up to a 12~65% reduction in latency, depending on the resource\nconfiguration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) face increasing inference latency as\ninput context length and model size continue to grow. In particular, the\nretrieval-augmented generation (RAG) technique, which enhances LLM responses by\nincorporating external knowledge, exacerbates this issue by significantly\nincreasing the number of input tokens. This expansion in token length leads to\na substantial rise in computational overhead, particularly during the prefill\nstage, resulting in prolonged time-to-first-token (TTFT). To address this\nissue, this paper proposes a method to reduce TTFT by leveraging a disk-based\nkey-value (KV) cache to lessen the computational burden during the prefill\nstage. We also introduce a disk-based shared KV cache management system, called\nShared RAG-DCache, for multi-instance LLM RAG service environments. This\nsystem, together with an optimal system configuration, improves both throughput\nand latency under given resource constraints. Shared RAG-DCache exploits the\nlocality of documents related to user queries in RAG, as well as the queueing\ndelay in LLM inference services. It proactively generates and stores disk KV\ncaches for query-related documents and shares them across multiple LLM\ninstances to enhance inference performance. In experiments on a single host\nequipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in\nthroughput and up to a 12~65% reduction in latency, depending on the resource\nconfiguration."
                },
                "authors": [
                    {
                        "name": "Hyungwoo Lee"
                    },
                    {
                        "name": "Kihyun Kim"
                    },
                    {
                        "name": "Jinwoo Kim"
                    },
                    {
                        "name": "Jungmin So"
                    },
                    {
                        "name": "Myung-Hoon Cha"
                    },
                    {
                        "name": "Hong-Yeon Kim"
                    },
                    {
                        "name": "James J. Kim"
                    },
                    {
                        "name": "Youngjae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Kim"
                },
                "arxiv_affiliation": "Dept. of Computer Science and Engineering, Sogang University, Seoul, Republic of Korea",
                "author": "Youngjae Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11729v1",
                "updated": "2025-04-16T03:07:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    7,
                    7,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T03:07:07Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    7,
                    7,
                    2,
                    106,
                    0
                ],
                "title": "EdgePrompt: A Distributed Key-Value Inference Framework for LLMs in 6G\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgePrompt: A Distributed Key-Value Inference Framework for LLMs in 6G\n  Networks"
                },
                "summary": "As sixth-generation (6G) networks advance, large language models (LLMs) are\nincreasingly integrated into 6G infrastructure to enhance network management\nand intelligence. However, traditional LLMs architecture struggle to meet the\nstringent latency and security requirements of 6G, especially as the increasing\nin sequence length leads to greater task complexity. This paper proposes\nEdge-Prompt, a cloud-edge collaborative framework based on a hierarchical\nattention splicing mechanism. EdgePrompt employs distributed key-value (KV)\npair optimization techniques to accelerate inference and adapt to network\nconditions. Additionally, to reduce the risk of data leakage, EdgePrompt\nincorporates a privacy preserving strategy by isolating sensitive information\nduring processing. Experiments on public dataset show that EdgePrompt\neffectively improves the inference throughput and reduces the latency, which\nprovides a reliable solution for LLMs deployment in 6G environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As sixth-generation (6G) networks advance, large language models (LLMs) are\nincreasingly integrated into 6G infrastructure to enhance network management\nand intelligence. However, traditional LLMs architecture struggle to meet the\nstringent latency and security requirements of 6G, especially as the increasing\nin sequence length leads to greater task complexity. This paper proposes\nEdge-Prompt, a cloud-edge collaborative framework based on a hierarchical\nattention splicing mechanism. EdgePrompt employs distributed key-value (KV)\npair optimization techniques to accelerate inference and adapt to network\nconditions. Additionally, to reduce the risk of data leakage, EdgePrompt\nincorporates a privacy preserving strategy by isolating sensitive information\nduring processing. Experiments on public dataset show that EdgePrompt\neffectively improves the inference throughput and reduces the latency, which\nprovides a reliable solution for LLMs deployment in 6G environments."
                },
                "authors": [
                    {
                        "name": "Jiahong Ning"
                    },
                    {
                        "name": "Pengyan Zhu"
                    },
                    {
                        "name": "Ce Zheng"
                    },
                    {
                        "name": "Gary Lee"
                    },
                    {
                        "name": "Sumei Sun"
                    },
                    {
                        "name": "Tingting Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Yang"
                },
                "author": "Tingting Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11652v1",
                "updated": "2025-04-15T22:38:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    22,
                    38,
                    54,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T22:38:54Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    22,
                    38,
                    54,
                    1,
                    105,
                    0
                ],
                "title": "Engineering MultiQueues: Fast Relaxed Concurrent Priority Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engineering MultiQueues: Fast Relaxed Concurrent Priority Queues"
                },
                "summary": "Priority queues are used in a wide range of applications, including\nprioritized online scheduling, discrete event simulation, and greedy\nalgorithms. In parallel settings, classical priority queues often become a\nsevere bottleneck, resulting in low throughput. Consequently, there has been\nsignificant interest in concurrent priority queues with relaxed semantics. In\nthis article, we present the MultiQueue, a flexible approach to relaxed\npriority queues that uses multiple internal sequential priority queues. The\nscalability of the MultiQueue is enhanced by buffering elements, batching\noperations on the internal queues, and optimizing access patterns for high\ncache locality. We investigate the complementary quality criteria of rank\nerror, which measures how close deleted elements are to the global minimum, and\ndelay, which quantifies how many smaller elements were deleted before a given\nelement. Extensive experimental evaluation shows that the MultiQueue\noutperforms competing approaches across several benchmarks. This includes\nshortest-path and branch-and-bound benchmarks that resemble real applications.\nMoreover, the MultiQueue can be configured easily to balance throughput and\nquality according to the application's requirements. We employ a seemingly\nparadoxical technique of wait-free locking that might be of broader interest\nfor converting sequential data structures into relaxed concurrent data\nstructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Priority queues are used in a wide range of applications, including\nprioritized online scheduling, discrete event simulation, and greedy\nalgorithms. In parallel settings, classical priority queues often become a\nsevere bottleneck, resulting in low throughput. Consequently, there has been\nsignificant interest in concurrent priority queues with relaxed semantics. In\nthis article, we present the MultiQueue, a flexible approach to relaxed\npriority queues that uses multiple internal sequential priority queues. The\nscalability of the MultiQueue is enhanced by buffering elements, batching\noperations on the internal queues, and optimizing access patterns for high\ncache locality. We investigate the complementary quality criteria of rank\nerror, which measures how close deleted elements are to the global minimum, and\ndelay, which quantifies how many smaller elements were deleted before a given\nelement. Extensive experimental evaluation shows that the MultiQueue\noutperforms competing approaches across several benchmarks. This includes\nshortest-path and branch-and-bound benchmarks that resemble real applications.\nMoreover, the MultiQueue can be configured easily to balance throughput and\nquality according to the application's requirements. We employ a seemingly\nparadoxical technique of wait-free locking that might be of broader interest\nfor converting sequential data structures into relaxed concurrent data\nstructures."
                },
                "authors": [
                    {
                        "name": "Marvin Williams"
                    },
                    {
                        "name": "Peter Sanders"
                    }
                ],
                "author_detail": {
                    "name": "Peter Sanders"
                },
                "author": "Peter Sanders",
                "arxiv_comment": "40 pages, extended journal version of arXiv:2107.01350",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11320v1",
                "updated": "2025-04-15T16:00:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    0,
                    21,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T16:00:21Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    16,
                    0,
                    21,
                    1,
                    105,
                    0
                ],
                "title": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory\n  Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory\n  Constraints"
                },
                "summary": "Large Language Models (LLMs) are indispensable in today's applications, but\ntheir inference procedure -- generating responses by processing text in\nsegments and using a memory-heavy Key-Value (KV) cache -- demands significant\ncomputational resources, particularly under memory constraints. This paper\nformulates LLM inference optimization as a multi-stage online scheduling\nproblem where sequential prompt arrivals and KV cache growth render\nconventional scheduling ineffective. We develop a fluid dynamics approximation\nto provide a tractable benchmark that guides algorithm design. Building on\nthis, we propose the Waiting for Accumulated Inference Threshold (WAIT)\nalgorithm, which uses multiple thresholds to schedule incoming prompts\noptimally when output lengths are known, and extend it to Nested WAIT for cases\nwith unknown output lengths. Theoretical analysis shows that both algorithms\nachieve near-optimal performance against the fluid benchmark in heavy traffic\nconditions, balancing throughput, latency, and Time to First Token (TTFT).\nExperiments with the Llama-7B model on an A100 GPU using both synthetic and\nreal-world datasets demonstrate improved throughput and reduced latency\nrelative to established baselines like vLLM and Sarathi. This work bridges\noperations research and machine learning, offering a rigorous framework for the\nefficient deployment of LLMs under memory constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are indispensable in today's applications, but\ntheir inference procedure -- generating responses by processing text in\nsegments and using a memory-heavy Key-Value (KV) cache -- demands significant\ncomputational resources, particularly under memory constraints. This paper\nformulates LLM inference optimization as a multi-stage online scheduling\nproblem where sequential prompt arrivals and KV cache growth render\nconventional scheduling ineffective. We develop a fluid dynamics approximation\nto provide a tractable benchmark that guides algorithm design. Building on\nthis, we propose the Waiting for Accumulated Inference Threshold (WAIT)\nalgorithm, which uses multiple thresholds to schedule incoming prompts\noptimally when output lengths are known, and extend it to Nested WAIT for cases\nwith unknown output lengths. Theoretical analysis shows that both algorithms\nachieve near-optimal performance against the fluid benchmark in heavy traffic\nconditions, balancing throughput, latency, and Time to First Token (TTFT).\nExperiments with the Llama-7B model on an A100 GPU using both synthetic and\nreal-world datasets demonstrate improved throughput and reduced latency\nrelative to established baselines like vLLM and Sarathi. This work bridges\noperations research and machine learning, offering a rigorous framework for the\nefficient deployment of LLMs under memory constraints."
                },
                "authors": [
                    {
                        "name": "Ruicheng Ao"
                    },
                    {
                        "name": "Gan Luo"
                    },
                    {
                        "name": "David Simchi-Levi"
                    },
                    {
                        "name": "Xinshang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinshang Wang"
                },
                "author": "Xinshang Wang",
                "arxiv_comment": "42 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v3",
                "updated": "2025-04-15T15:40:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    40,
                    25,
                    1,
                    105,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.13195v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.13195v5",
                "updated": "2025-04-15T15:37:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    15,
                    37,
                    58,
                    1,
                    105,
                    0
                ],
                "published": "2024-04-19T22:06:14Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    22,
                    6,
                    14,
                    4,
                    110,
                    0
                ],
                "title": "Automatic BLAS Offloading on Unified Memory Architecture: A Study on\n  NVIDIA Grace-Hopper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic BLAS Offloading on Unified Memory Architecture: A Study on\n  NVIDIA Grace-Hopper"
                },
                "summary": "Porting codes to GPU often requires major efforts. While several tools exist\nfor automatically offload numerical libraries such as BLAS and LAPACK, they\noften prove impractical due to the high cost of mandatory data transfer. The\nnew unified memory architecture in NVIDIA Grace-Hopper allows high bandwidth\ncache-coherent memory access of all memory from both CPU and GPU, potentially\neliminating bottleneck faced in conventional architecture. This breakthrough\nopens up new avenues for application development and porting strategies. In\nthis study, we introduce a new tool for automatic BLAS offload, the tool\nleverages the high speed cache coherent NVLink C2C interconnect in\nGrace-Hopper, and enables performant GPU offload for BLAS heavy applications\nwith no code changes or recompilation. The tool was tested on two quantum\nchemistry or physics codes, great performance benefits were observed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Porting codes to GPU often requires major efforts. While several tools exist\nfor automatically offload numerical libraries such as BLAS and LAPACK, they\noften prove impractical due to the high cost of mandatory data transfer. The\nnew unified memory architecture in NVIDIA Grace-Hopper allows high bandwidth\ncache-coherent memory access of all memory from both CPU and GPU, potentially\neliminating bottleneck faced in conventional architecture. This breakthrough\nopens up new avenues for application development and porting strategies. In\nthis study, we introduce a new tool for automatic BLAS offload, the tool\nleverages the high speed cache coherent NVLink C2C interconnect in\nGrace-Hopper, and enables performant GPU offload for BLAS heavy applications\nwith no code changes or recompilation. The tool was tested on two quantum\nchemistry or physics codes, great performance benefits were observed."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Yinzhi Wang"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Hang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Liu"
                },
                "author": "Hang Liu",
                "arxiv_doi": "10.1145/3626203.3670561",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3626203.3670561",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.13195v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.13195v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11067v1",
                "updated": "2025-04-15T11:02:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    2,
                    34,
                    1,
                    105,
                    0
                ],
                "published": "2025-04-15T11:02:34Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    11,
                    2,
                    34,
                    1,
                    105,
                    0
                ],
                "title": "Morphing-based Compression for Data-centric ML Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Morphing-based Compression for Data-centric ML Pipelines"
                },
                "summary": "Data-centric ML pipelines extend traditional machine learning (ML) pipelines\n-- of feature transformations and ML model training -- by outer loops for data\ncleaning, augmentation, and feature engineering to create high-quality input\ndata. Existing lossless matrix compression applies lightweight compression\nschemes to numeric matrices and performs linear algebra operations such as\nmatrix-vector multiplications directly on the compressed representation but\nstruggles to efficiently rediscover structural data redundancy. Compressed\noperations are effective at fitting data in available memory, reducing I/O\nacross the storage-memory-cache hierarchy, and improving instruction\nparallelism. The applied data cleaning, augmentation, and feature\ntransformations provide a rich source of information about data characteristics\nsuch as distinct items, column sparsity, and column correlations. In this\npaper, we introduce BWARE -- an extension of AWARE for workload-aware lossless\nmatrix compression -- that pushes compression through feature transformations\nand engineering to leverage information about structural transformations.\nBesides compressed feature transformations, we introduce a novel technique for\nlightweight morphing of a compressed representation into workload-optimized\ncompressed representations without decompression. BWARE shows substantial\nend-to-end runtime improvements, reducing the execution time for training\ndata-centric ML pipelines from days to hours.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-centric ML pipelines extend traditional machine learning (ML) pipelines\n-- of feature transformations and ML model training -- by outer loops for data\ncleaning, augmentation, and feature engineering to create high-quality input\ndata. Existing lossless matrix compression applies lightweight compression\nschemes to numeric matrices and performs linear algebra operations such as\nmatrix-vector multiplications directly on the compressed representation but\nstruggles to efficiently rediscover structural data redundancy. Compressed\noperations are effective at fitting data in available memory, reducing I/O\nacross the storage-memory-cache hierarchy, and improving instruction\nparallelism. The applied data cleaning, augmentation, and feature\ntransformations provide a rich source of information about data characteristics\nsuch as distinct items, column sparsity, and column correlations. In this\npaper, we introduce BWARE -- an extension of AWARE for workload-aware lossless\nmatrix compression -- that pushes compression through feature transformations\nand engineering to leverage information about structural transformations.\nBesides compressed feature transformations, we introduce a novel technique for\nlightweight morphing of a compressed representation into workload-optimized\ncompressed representations without decompression. BWARE shows substantial\nend-to-end runtime improvements, reducing the execution time for training\ndata-centric ML pipelines from days to hours."
                },
                "authors": [
                    {
                        "name": "Sebastian Baunsgaard"
                    },
                    {
                        "name": "Matthias Boehm"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Boehm"
                },
                "author": "Matthias Boehm",
                "arxiv_comment": "20 pages, 28 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10326v1",
                "updated": "2025-04-14T15:34:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    34,
                    26,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:34:26Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    34,
                    26,
                    0,
                    104,
                    0
                ],
                "title": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference"
                },
                "summary": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks."
                },
                "authors": [
                    {
                        "name": "Yangshen Deng"
                    },
                    {
                        "name": "Zhengxin You"
                    },
                    {
                        "name": "Long Xiang"
                    },
                    {
                        "name": "Qilong Li"
                    },
                    {
                        "name": "Peiqi Yuan"
                    },
                    {
                        "name": "Zhaoyang Hong"
                    },
                    {
                        "name": "Yitao Zheng"
                    },
                    {
                        "name": "Wanting Li"
                    },
                    {
                        "name": "Runzhong Li"
                    },
                    {
                        "name": "Haotian Liu"
                    },
                    {
                        "name": "Kyriakos Mouratidis"
                    },
                    {
                        "name": "Man Lung Yiu"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Qiaomu Shen"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Bo Tang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Tang"
                },
                "author": "Bo Tang",
                "arxiv_comment": "14 pages, 12 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.1; H.3.2; H.3.3; H.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10318v1",
                "updated": "2025-04-14T15:27:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    27,
                    32,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T15:27:32Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    15,
                    27,
                    32,
                    0,
                    104,
                    0
                ],
                "title": "Shield Bash: Abusing Defensive Coherence State Retrieval to Break Timing\n  Obfuscation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shield Bash: Abusing Defensive Coherence State Retrieval to Break Timing\n  Obfuscation"
                },
                "summary": "Microarchitectural attacks are a significant concern, leading to many\nhardware-based defense proposals. However, different defenses target different\nclasses of attacks, and their impact on each other has not been fully\nconsidered. To raise awareness of this problem, we study an interaction between\ntwo state-of-the art defenses in this paper, timing obfuscations of remote\ncache lines (TORC) and delaying speculative changes to remote cache lines\n(DSRC). TORC mitigates cache-hit based attacks and DSRC mitigates speculative\ncoherence state change attacks.\n  We observe that DSRC enables coherence information to be retrieved into the\nprocessor core, where it is out of the reach of timing obfuscations to protect.\nThis creates an unforeseen consequence that redo operations can be triggered\nwithin the core to detect the presence or absence of remote cache lines, which\nconstitutes a security vulnerability. We demonstrate that a new covert channel\nattack is possible using this vulnerability. We propose two ways to mitigate\nthe attack, whose performance varies depending on an application's cache usage.\nOne way is to never send remote exclusive coherence state (E) information to\nthe core even if it is created. The other way is to never create a remote E\nstate, which is responsible for triggering redos.\n  We demonstrate the timing difference caused by this microarchitectural\ndefense assumption violation using GEM5 simulations. Performance evaluation on\nSPECrate 2017 and PARSEC benchmarks of the two fixes show less than 32\\%\naverage overhead across both sets of benchmarks. The repair which prevented the\ncreation of remote E state had less than 2.8% average overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microarchitectural attacks are a significant concern, leading to many\nhardware-based defense proposals. However, different defenses target different\nclasses of attacks, and their impact on each other has not been fully\nconsidered. To raise awareness of this problem, we study an interaction between\ntwo state-of-the art defenses in this paper, timing obfuscations of remote\ncache lines (TORC) and delaying speculative changes to remote cache lines\n(DSRC). TORC mitigates cache-hit based attacks and DSRC mitigates speculative\ncoherence state change attacks.\n  We observe that DSRC enables coherence information to be retrieved into the\nprocessor core, where it is out of the reach of timing obfuscations to protect.\nThis creates an unforeseen consequence that redo operations can be triggered\nwithin the core to detect the presence or absence of remote cache lines, which\nconstitutes a security vulnerability. We demonstrate that a new covert channel\nattack is possible using this vulnerability. We propose two ways to mitigate\nthe attack, whose performance varies depending on an application's cache usage.\nOne way is to never send remote exclusive coherence state (E) information to\nthe core even if it is created. The other way is to never create a remote E\nstate, which is responsible for triggering redos.\n  We demonstrate the timing difference caused by this microarchitectural\ndefense assumption violation using GEM5 simulations. Performance evaluation on\nSPECrate 2017 and PARSEC benchmarks of the two fixes show less than 32\\%\naverage overhead across both sets of benchmarks. The repair which prevented the\ncreation of remote E state had less than 2.8% average overhead."
                },
                "authors": [
                    {
                        "name": "Kartik Ramkrishnan"
                    },
                    {
                        "name": "Antonia Zhai"
                    },
                    {
                        "name": "Stephen McCamant"
                    },
                    {
                        "name": "Pen Chung Yew"
                    }
                ],
                "author_detail": {
                    "name": "Pen Chung Yew"
                },
                "author": "Pen Chung Yew",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10181v1",
                "updated": "2025-04-14T12:34:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    34,
                    20,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T12:34:20Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    34,
                    20,
                    0,
                    104,
                    0
                ],
                "title": "A New Paradigm in IBR Modeling for Power Flow and Short Circuit Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Paradigm in IBR Modeling for Power Flow and Short Circuit Analysis"
                },
                "summary": "The fault characteristics of inverter-based resources (IBRs) are different\nfrom conventional synchronous generators. The fault response of IBRs is\nnon-linear due to saturation states and mainly determined by fault ride through\n(FRT) strategies of the associated voltage source converter (VSC). This results\nin prohibitively large solution times for power flows considering these short\ncircuit characteristics, especially when the power system states change fast\ndue to uncertainty in IBR generations. To overcome this, a phasor-domain steady\nstate (SS) short circuit (SC) solver for IBR dominated power systems is\nproposed in this paper, and subsequently the developed IBR models are\nincorporated with a novel Jacobian-based Power Flow (PF) solver. In this\nmultiphase PF solver, any power system components can be modeled by considering\ntheir original non-linear or linear mathematical representations. Moreover, two\nnovel FRT strategies are proposed to fully utilize the converter capacity and\nto comply with IEEE-2800 2022 std and German grid code. The results are\ncompared with the Electromagnetic Transient (EMT) simulation on the IEEE 34\ntest network and the 120 kV EPRI benchmark system. The developed IBR sequence\ndomain PF model demonstrates more accurate behavior compared to the classical\nIBR generator model. The error in calculating the short circuit current with\nthe proposed SC solver is less than 3%, while achieving significant speed\nimprovements of three order of magnitudes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fault characteristics of inverter-based resources (IBRs) are different\nfrom conventional synchronous generators. The fault response of IBRs is\nnon-linear due to saturation states and mainly determined by fault ride through\n(FRT) strategies of the associated voltage source converter (VSC). This results\nin prohibitively large solution times for power flows considering these short\ncircuit characteristics, especially when the power system states change fast\ndue to uncertainty in IBR generations. To overcome this, a phasor-domain steady\nstate (SS) short circuit (SC) solver for IBR dominated power systems is\nproposed in this paper, and subsequently the developed IBR models are\nincorporated with a novel Jacobian-based Power Flow (PF) solver. In this\nmultiphase PF solver, any power system components can be modeled by considering\ntheir original non-linear or linear mathematical representations. Moreover, two\nnovel FRT strategies are proposed to fully utilize the converter capacity and\nto comply with IEEE-2800 2022 std and German grid code. The results are\ncompared with the Electromagnetic Transient (EMT) simulation on the IEEE 34\ntest network and the 120 kV EPRI benchmark system. The developed IBR sequence\ndomain PF model demonstrates more accurate behavior compared to the classical\nIBR generator model. The error in calculating the short circuit current with\nthe proposed SC solver is less than 3%, while achieving significant speed\nimprovements of three order of magnitudes."
                },
                "authors": [
                    {
                        "name": "Zahid Javid"
                    },
                    {
                        "name": "Firdous Ul Nazir"
                    },
                    {
                        "name": "Wentao Zhu"
                    },
                    {
                        "name": "Diptargha Chakravorty"
                    },
                    {
                        "name": "Ahmed Aboushady"
                    },
                    {
                        "name": "Mohamed Galeela"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Galeela"
                },
                "author": "Mohamed Galeela",
                "arxiv_comment": "12 Pages, First Revision Submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00601v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00601v3",
                "updated": "2025-04-14T11:20:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    11,
                    20,
                    56,
                    0,
                    104,
                    0
                ],
                "published": "2024-11-01T14:03:21Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    3,
                    21,
                    4,
                    306,
                    0
                ],
                "title": "Diversity in Network-Friendly Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity in Network-Friendly Recommendations"
                },
                "summary": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms."
                },
                "authors": [
                    {
                        "name": "Evangelia Tzimpimpaki"
                    },
                    {
                        "name": "Thrasyvoulos Spyropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Thrasyvoulos Spyropoulos"
                },
                "author": "Thrasyvoulos Spyropoulos",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00601v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00601v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09984v1",
                "updated": "2025-04-14T08:51:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    8,
                    51,
                    35,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T08:51:35Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    8,
                    51,
                    35,
                    0,
                    104,
                    0
                ],
                "title": "On Precomputation and Caching in Information Retrieval Experiments with\n  Pipeline Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Precomputation and Caching in Information Retrieval Experiments with\n  Pipeline Architectures"
                },
                "summary": "Modern information retrieval systems often rely on multiple components\nexecuted in a pipeline. In a research setting, this can lead to substantial\nredundant computations (e.g., retrieving the same query multiple times for\nevaluating different downstream rerankers). To overcome this, researchers take\ncached \"result\" files as inputs, which represent the output of another\npipeline. However, these result files can be brittle and can cause a disconnect\nbetween the conceptual design of the pipeline and its logical implementation.\nTo overcome both the redundancy problem (when executing complete pipelines) and\nthe disconnect problem (when relying on intermediate result files), we describe\nour recent efforts to improve the caching capabilities in the open-source\nPyTerrier IR platform. We focus on two main directions: (1) automatic implicit\ncaching of common pipeline prefixes when comparing systems and (2) explicit\ncaching of operations through a new extension package, pyterrier-caching. These\napproaches allow for the best of both worlds: pipelines can be fully expressed\nend-to-end, while also avoiding redundant computations between pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern information retrieval systems often rely on multiple components\nexecuted in a pipeline. In a research setting, this can lead to substantial\nredundant computations (e.g., retrieving the same query multiple times for\nevaluating different downstream rerankers). To overcome this, researchers take\ncached \"result\" files as inputs, which represent the output of another\npipeline. However, these result files can be brittle and can cause a disconnect\nbetween the conceptual design of the pipeline and its logical implementation.\nTo overcome both the redundancy problem (when executing complete pipelines) and\nthe disconnect problem (when relying on intermediate result files), we describe\nour recent efforts to improve the caching capabilities in the open-source\nPyTerrier IR platform. We focus on two main directions: (1) automatic implicit\ncaching of common pipeline prefixes when comparing systems and (2) explicit\ncaching of operations through a new extension package, pyterrier-caching. These\napproaches allow for the best of both worlds: pipelines can be fully expressed\nend-to-end, while also avoiding redundant computations between pipelines."
                },
                "authors": [
                    {
                        "name": "Sean MacAvaney"
                    },
                    {
                        "name": "Craig Macdonald"
                    }
                ],
                "author_detail": {
                    "name": "Craig Macdonald"
                },
                "author": "Craig Macdonald",
                "arxiv_comment": "WOWS @ ECIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09952v1",
                "updated": "2025-04-14T07:30:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    7,
                    30,
                    3,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T07:30:03Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    7,
                    30,
                    3,
                    0,
                    104,
                    0
                ],
                "title": "Secrecy and Privacy in Multi-Access Combinatorial Topology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secrecy and Privacy in Multi-Access Combinatorial Topology"
                },
                "summary": "In this work, we consider the multi-access combinatorial topology with $C$\ncaches where each user accesses a unique set of $r$ caches. For this setup, we\nconsider secrecy, where each user should not know anything about the files it\ndid not request, and demand privacy, where each user's demand must be kept\nprivate from other non-colluding users. We propose a scheme satisfying both\nconditions and derive a lower bound based on cut-set arguments. Also, we prove\nthat our scheme is optimal when $r\\geq C-1$, and it is order-optimal when the\ncache memory size $M$ is greater than or equal to a certain threshold for\n$r<C-1$. When $r=1$, in most of the memory region, our scheme achieves the same\nrate as the one given by the secretive scheme for the dedicated cache setup by\nRavindrakumar et al. ( 'Private Coded Caching,' in \\textit{IEEE Transactions on\nInformation Forensics and Security}, 2018), while satisfying both secrecy and\ndemand privacy conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we consider the multi-access combinatorial topology with $C$\ncaches where each user accesses a unique set of $r$ caches. For this setup, we\nconsider secrecy, where each user should not know anything about the files it\ndid not request, and demand privacy, where each user's demand must be kept\nprivate from other non-colluding users. We propose a scheme satisfying both\nconditions and derive a lower bound based on cut-set arguments. Also, we prove\nthat our scheme is optimal when $r\\geq C-1$, and it is order-optimal when the\ncache memory size $M$ is greater than or equal to a certain threshold for\n$r<C-1$. When $r=1$, in most of the memory region, our scheme achieves the same\nrate as the one given by the secretive scheme for the dedicated cache setup by\nRavindrakumar et al. ( 'Private Coded Caching,' in \\textit{IEEE Transactions on\nInformation Forensics and Security}, 2018), while satisfying both secrecy and\ndemand privacy conditions."
                },
                "authors": [
                    {
                        "name": "Mallikharjuna Chinnapadamala"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "11 pages and 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09936v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09936v1",
                "updated": "2025-04-14T06:58:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    14,
                    6,
                    58,
                    0,
                    0,
                    104,
                    0
                ],
                "published": "2025-04-14T06:58:00Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    6,
                    58,
                    0,
                    0,
                    104,
                    0
                ],
                "title": "KeepKV: Eliminating Output Perturbation in KV Cache Compression for\n  Efficient LLMs Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeepKV: Eliminating Output Perturbation in KV Cache Compression for\n  Efficient LLMs Inference"
                },
                "summary": "Efficient inference of large language models (LLMs) is hindered by an\never-growing key-value (KV) cache, making KV cache compression a critical\nresearch direction. Traditional methods selectively evict less important KV\ncache entries based on attention scores or position heuristics, which leads to\ninformation loss and hallucinations. Recently, merging-based strategies have\nbeen explored to retain more information by merging KV pairs that would be\ndiscarded; however, these existing approaches inevitably introduce\ninconsistencies in attention distributions before and after merging, causing\noutput perturbation and degraded generation quality. To overcome this\nchallenge, we propose KeepKV, a novel adaptive KV cache merging method designed\nto eliminate output perturbation while preserving performance under strict\nmemory constraints. KeepKV introduces the Electoral Votes mechanism that\nrecords merging history and adaptively adjusts attention scores. Moreover, it\nfurther leverages a novel Zero Inference-Perturbation Merging methods, keeping\nattention consistency and compensating for attention loss resulting from cache\nmerging. KeepKV successfully retains essential context information within a\nsignificantly compressed cache. Extensive experiments on various benchmarks and\nLLM architectures demonstrate that KeepKV substantially reduces memory usage,\nenhances inference throughput by more than 2x and keeps superior generation\nquality even with 10% KV cache budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient inference of large language models (LLMs) is hindered by an\never-growing key-value (KV) cache, making KV cache compression a critical\nresearch direction. Traditional methods selectively evict less important KV\ncache entries based on attention scores or position heuristics, which leads to\ninformation loss and hallucinations. Recently, merging-based strategies have\nbeen explored to retain more information by merging KV pairs that would be\ndiscarded; however, these existing approaches inevitably introduce\ninconsistencies in attention distributions before and after merging, causing\noutput perturbation and degraded generation quality. To overcome this\nchallenge, we propose KeepKV, a novel adaptive KV cache merging method designed\nto eliminate output perturbation while preserving performance under strict\nmemory constraints. KeepKV introduces the Electoral Votes mechanism that\nrecords merging history and adaptively adjusts attention scores. Moreover, it\nfurther leverages a novel Zero Inference-Perturbation Merging methods, keeping\nattention consistency and compensating for attention loss resulting from cache\nmerging. KeepKV successfully retains essential context information within a\nsignificantly compressed cache. Extensive experiments on various benchmarks and\nLLM architectures demonstrate that KeepKV substantially reduces memory usage,\nenhances inference throughput by more than 2x and keeps superior generation\nquality even with 10% KV cache budgets."
                },
                "authors": [
                    {
                        "name": "Yuxuan Tian"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Yebo Peng"
                    },
                    {
                        "name": "Aomufei Yuan"
                    },
                    {
                        "name": "Zhiming Wang"
                    },
                    {
                        "name": "Bairen Yi"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Yong Cui"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09936v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12280v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12280v2",
                "updated": "2025-04-13T14:17:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    17,
                    57,
                    6,
                    103,
                    0
                ],
                "published": "2024-02-19T16:47:04Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    16,
                    47,
                    4,
                    0,
                    50,
                    0
                ],
                "title": "Plato: Plan to Efficiently Decode for Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plato: Plan to Efficiently Decode for Large Language Model Inference"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage tasks, but their inference incurs substantial computational and memory\noverhead. To improve efficiency, parallel decoding methods like\nSkeleton-of-Thought (SoT) decompose prompts into sub-problems for concurrent\nprocessing. However, these methods significantly compromise answer quality by\ntreating semantically linked sub-problems as independent. We propose Plato, a\nnovel approach that co-designs algorithms and systems for semantic-aware\nparallel decoding. Plato leverages LLMs to organize sub-problems into a\ndependency graph based on logical and causal relationships, enabling concurrent\ndecoding of non-dependent nodes while preserving answer coherence and quality.\nTo further enhance efficiency, Plato pipelines planning and node decoding\nstages, implements a global context cache, and carefully structures node\ninference prompts to maximize key-value cache reuse and minimize overhead. Our\nevaluations show that Plato improves throughput by 68% over autoregressive\ndecoding while achieving a 40% net win rate in answer quality. Compared to SoT,\nPlato demonstrates a remarkable 90% quality net-win rate. Ablation studies\nreveal that our pipeline design improves speedup by 29%, while our KV cache\nreuse optimization reduces overhead by 75%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage tasks, but their inference incurs substantial computational and memory\noverhead. To improve efficiency, parallel decoding methods like\nSkeleton-of-Thought (SoT) decompose prompts into sub-problems for concurrent\nprocessing. However, these methods significantly compromise answer quality by\ntreating semantically linked sub-problems as independent. We propose Plato, a\nnovel approach that co-designs algorithms and systems for semantic-aware\nparallel decoding. Plato leverages LLMs to organize sub-problems into a\ndependency graph based on logical and causal relationships, enabling concurrent\ndecoding of non-dependent nodes while preserving answer coherence and quality.\nTo further enhance efficiency, Plato pipelines planning and node decoding\nstages, implements a global context cache, and carefully structures node\ninference prompts to maximize key-value cache reuse and minimize overhead. Our\nevaluations show that Plato improves throughput by 68% over autoregressive\ndecoding while achieving a 40% net win rate in answer quality. Compared to SoT,\nPlato demonstrates a remarkable 90% quality net-win rate. Ablation studies\nreveal that our pipeline design improves speedup by 29%, while our KV cache\nreuse optimization reduces overhead by 75%."
                },
                "authors": [
                    {
                        "name": "Shuowei Jin"
                    },
                    {
                        "name": "Xueshen Liu"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Haizhong Zheng"
                    },
                    {
                        "name": "Qingzhao Zhang"
                    },
                    {
                        "name": "Atul Prakash"
                    },
                    {
                        "name": "Matthew Lentz"
                    },
                    {
                        "name": "Danyang Zhuo"
                    },
                    {
                        "name": "Feng Qian"
                    },
                    {
                        "name": "Z. Morley Mao"
                    }
                ],
                "author_detail": {
                    "name": "Z. Morley Mao"
                },
                "author": "Z. Morley Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12280v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12280v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09590v1",
                "updated": "2025-04-13T14:16:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    16,
                    57,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T14:16:57Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    16,
                    57,
                    6,
                    103,
                    0
                ],
                "title": "Efficient LLM Serving on Hybrid Real-time and Best-effort Requests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Serving on Hybrid Real-time and Best-effort Requests"
                },
                "summary": "Recent breakthroughs in large Language Models (LLMs) have enabled various\ngenerative tasks on a single model. Real-world services (e.g., OpenAI's ChatGPT\n[27]) powered by an LLM often concurrently support latency-critical requests\nfor interactive applications (e.g., question-answering systems, referred to as\nreal-time or RT requests) and throughput-oriented requests for back-of-house\nprocessing (e.g., documents batch processing [28], referred to best-effort or\nBE requests), with complex hybrid inference workloads to the underlying model.\nState-of-the-art (SOTA) LLM serving systems dedicate machines to each type of\nrequest, towards either low inference latency or high serving throughput,\nrespectively. This practice simplifies request scheduling and management but\nsuffers from poor resource utilization. We propose BROS, a hybrid LLM serving\nsystem that aims to collocate RT/BE requests, meeting RT requests' latency\nrequirements while maintaining BE requests' throughput. BROS formulates the\nproblem of hybrid RT/BE request scheduling and solves it with a dynamic\npriority-based algorithm. BROS designs a bidirectional KV cache management\nmechanism, allowing RT requests to share KV memory with BE requests to remove\nthe scheduling restrictions caused by insufficient KV memory and improve\nutilization. Extensive experiments validate that BROS achieves a good trade-off\nwhen serving hybrid RT and BE requests. It significantly reduces the latency of\nRT requests (up to 74.20%), improving their fine-grained service level\nobjectives (SLOs) attainments (up to 36.38x), with negligible throughput\nreduction for BE requests, showing significant advantages over SOTA systems\nlike vLLM and TGI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent breakthroughs in large Language Models (LLMs) have enabled various\ngenerative tasks on a single model. Real-world services (e.g., OpenAI's ChatGPT\n[27]) powered by an LLM often concurrently support latency-critical requests\nfor interactive applications (e.g., question-answering systems, referred to as\nreal-time or RT requests) and throughput-oriented requests for back-of-house\nprocessing (e.g., documents batch processing [28], referred to best-effort or\nBE requests), with complex hybrid inference workloads to the underlying model.\nState-of-the-art (SOTA) LLM serving systems dedicate machines to each type of\nrequest, towards either low inference latency or high serving throughput,\nrespectively. This practice simplifies request scheduling and management but\nsuffers from poor resource utilization. We propose BROS, a hybrid LLM serving\nsystem that aims to collocate RT/BE requests, meeting RT requests' latency\nrequirements while maintaining BE requests' throughput. BROS formulates the\nproblem of hybrid RT/BE request scheduling and solves it with a dynamic\npriority-based algorithm. BROS designs a bidirectional KV cache management\nmechanism, allowing RT requests to share KV memory with BE requests to remove\nthe scheduling restrictions caused by insufficient KV memory and improve\nutilization. Extensive experiments validate that BROS achieves a good trade-off\nwhen serving hybrid RT and BE requests. It significantly reduces the latency of\nRT requests (up to 74.20%), improving their fine-grained service level\nobjectives (SLOs) attainments (up to 36.38x), with negligible throughput\nreduction for BE requests, showing significant advantages over SOTA systems\nlike vLLM and TGI."
                },
                "authors": [
                    {
                        "name": "Wan Borui"
                    },
                    {
                        "name": "Zhao Juntao"
                    },
                    {
                        "name": "Jiang Chenyu"
                    },
                    {
                        "name": "Guo Chuanxiong"
                    },
                    {
                        "name": "Wu Chuan"
                    }
                ],
                "author_detail": {
                    "name": "Wu Chuan"
                },
                "author": "Wu Chuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v5",
                "updated": "2025-04-13T14:02:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    14,
                    2,
                    47,
                    6,
                    103,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient Prefilling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient Prefilling"
                },
                "summary": "We introduce Block-attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context in an\nauto-regressive manner. Instead, Block-attention divides retrieved documents\ninto discrete blocks, with each block independently calculating key-value (KV)\nstates except for the final block. In RAG scenarios, by defining each passage\nas a block, Block-attention enables us to reuse the KV states of passages that\nhave been seen before, thereby significantly reducing the latency and the\ncomputation overhead during inference. The implementation of Block-attention\ninvolves block segmentation, position re-encoding, and fine-tuning the LLM to\nadapt to the Block-attention mechanism. Experiments on 11 diverse benchmarks,\nincluding RAG, ICL, and general domains, demonstrate that after block\nfine-tuning, the Block-attention model not only achieves performance comparable\nto that of full-attention models, but can also seamlessly switch between the\nblock and full attention modes without any performance loss. Notably,\nBlock-attention significantly reduces the time to first token (TTFT) and\nfloating point operations (FLOPs) to a very low level. It only takes 45 ms to\noutput the first token for an input sequence with a total length of 32K.\nCompared to the full-attention models, the TTFT and corresponding FLOPs are\nreduced by 98.7% and 99.8%, respectively. Additionally, in Appendix A, we\nelaborate on how Block-attention is applied in Game AI scenario and the\nsubstantial potential benefits it entails. We strongly suggest researchers in\nthe gaming field not to overlook this section.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context in an\nauto-regressive manner. Instead, Block-attention divides retrieved documents\ninto discrete blocks, with each block independently calculating key-value (KV)\nstates except for the final block. In RAG scenarios, by defining each passage\nas a block, Block-attention enables us to reuse the KV states of passages that\nhave been seen before, thereby significantly reducing the latency and the\ncomputation overhead during inference. The implementation of Block-attention\ninvolves block segmentation, position re-encoding, and fine-tuning the LLM to\nadapt to the Block-attention mechanism. Experiments on 11 diverse benchmarks,\nincluding RAG, ICL, and general domains, demonstrate that after block\nfine-tuning, the Block-attention model not only achieves performance comparable\nto that of full-attention models, but can also seamlessly switch between the\nblock and full attention modes without any performance loss. Notably,\nBlock-attention significantly reduces the time to first token (TTFT) and\nfloating point operations (FLOPs) to a very low level. It only takes 45 ms to\noutput the first token for an input sequence with a total length of 32K.\nCompared to the full-attention models, the TTFT and corresponding FLOPs are\nreduced by 98.7% and 99.8%, respectively. Additionally, in Appendix A, we\nelaborate on how Block-attention is applied in Game AI scenario and the\nsubstantial potential benefits it entails. We strongly suggest researchers in\nthe gaming field not to overlook this section."
                },
                "authors": [
                    {
                        "name": "Dongyang Ma"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10540v1",
                "updated": "2025-04-13T08:29:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    8,
                    29,
                    58,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T08:29:58Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    8,
                    29,
                    58,
                    6,
                    103,
                    0
                ],
                "title": "AB-Cache: Training-Free Acceleration of Diffusion Models via\n  Adams-Bashforth Cached Feature Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AB-Cache: Training-Free Acceleration of Diffusion Models via\n  Adams-Bashforth Cached Feature Reuse"
                },
                "summary": "Diffusion models have demonstrated remarkable success in generative tasks,\nyet their iterative denoising process results in slow inference, limiting their\npracticality. While existing acceleration methods exploit the well-known\nU-shaped similarity pattern between adjacent steps through caching mechanisms,\nthey lack theoretical foundation and rely on simplistic computation reuse,\noften leading to performance degradation. In this work, we provide a\ntheoretical understanding by analyzing the denoising process through the\nsecond-order Adams-Bashforth method, revealing a linear relationship between\nthe outputs of consecutive steps. This analysis explains why the outputs of\nadjacent steps exhibit a U-shaped pattern. Furthermore, extending\nAdams-Bashforth method to higher order, we propose a novel caching-based\nacceleration approach for diffusion models, instead of directly reusing cached\nresults, with a truncation error bound of only \\(O(h^k)\\) where $h$ is the step\nsize. Extensive validation across diverse image and video diffusion models\n(including HunyuanVideo and FLUX.1-dev) with various schedulers demonstrates\nour method's effectiveness in achieving nearly $3\\times$ speedup while\nmaintaining original performance levels, offering a practical real-time\nsolution without compromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have demonstrated remarkable success in generative tasks,\nyet their iterative denoising process results in slow inference, limiting their\npracticality. While existing acceleration methods exploit the well-known\nU-shaped similarity pattern between adjacent steps through caching mechanisms,\nthey lack theoretical foundation and rely on simplistic computation reuse,\noften leading to performance degradation. In this work, we provide a\ntheoretical understanding by analyzing the denoising process through the\nsecond-order Adams-Bashforth method, revealing a linear relationship between\nthe outputs of consecutive steps. This analysis explains why the outputs of\nadjacent steps exhibit a U-shaped pattern. Furthermore, extending\nAdams-Bashforth method to higher order, we propose a novel caching-based\nacceleration approach for diffusion models, instead of directly reusing cached\nresults, with a truncation error bound of only \\(O(h^k)\\) where $h$ is the step\nsize. Extensive validation across diverse image and video diffusion models\n(including HunyuanVideo and FLUX.1-dev) with various schedulers demonstrates\nour method's effectiveness in achieving nearly $3\\times$ speedup while\nmaintaining original performance levels, offering a practical real-time\nsolution without compromising generation quality."
                },
                "authors": [
                    {
                        "name": "Zichao Yu"
                    },
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Guojiang Shao"
                    },
                    {
                        "name": "Chengwei Zhang"
                    },
                    {
                        "name": "Shengze Xu"
                    },
                    {
                        "name": "Jie Huang"
                    },
                    {
                        "name": "Feng Zhao"
                    },
                    {
                        "name": "Xiaodong Cun"
                    },
                    {
                        "name": "Wenyi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenyi Zhang"
                },
                "author": "Wenyi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09431v1",
                "updated": "2025-04-13T04:46:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    13,
                    4,
                    46,
                    2,
                    6,
                    103,
                    0
                ],
                "published": "2025-04-13T04:46:02Z",
                "published_parsed": [
                    2025,
                    4,
                    13,
                    4,
                    46,
                    2,
                    6,
                    103,
                    0
                ],
                "title": "Sub-nanosecond in-plane magnetization switching induced by field-like\n  spin-orbit torques from ferromagnets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-nanosecond in-plane magnetization switching induced by field-like\n  spin-orbit torques from ferromagnets"
                },
                "summary": "Spin-orbit torques (SOTs) generated in SOT-material/ferromagnet structures\nare classified as damping-like (DL) and field-like (FL) torques for\ncurrent-driven magnetization switching. It is well known that both DL- and\nFL-SOTs originate from the SOT-material and DL-SOT dominates the current-driven\nswitching process while FL-SOT contributes limitedly, resulting in an\nincubation time (several nanoseconds) during collinear magnetization switching\nwith the spin polarization because of the DL attributes. Here we report a\nFL-SOT originated from the ferromagnet, different from the origin of DL-SOT,\nand demonstrate that it dominates the collinear magnetization switching. We\nshow that the FL-SOT and resultant collinear switching can be modulated, one\norder of magnitude and sign reversal, by controlling the ferromagnet. Because\nof no incubation time and higher charge-to-spin efficiencies in the FL\nswitching, we further show that the switching time can be down to 200 ps with\none order lower critical switching current density compared to DL switching.\nThese results indicate that the FL switching may provide a practical solution\nfor magnetic memory in speed-priority cache applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spin-orbit torques (SOTs) generated in SOT-material/ferromagnet structures\nare classified as damping-like (DL) and field-like (FL) torques for\ncurrent-driven magnetization switching. It is well known that both DL- and\nFL-SOTs originate from the SOT-material and DL-SOT dominates the current-driven\nswitching process while FL-SOT contributes limitedly, resulting in an\nincubation time (several nanoseconds) during collinear magnetization switching\nwith the spin polarization because of the DL attributes. Here we report a\nFL-SOT originated from the ferromagnet, different from the origin of DL-SOT,\nand demonstrate that it dominates the collinear magnetization switching. We\nshow that the FL-SOT and resultant collinear switching can be modulated, one\norder of magnitude and sign reversal, by controlling the ferromagnet. Because\nof no incubation time and higher charge-to-spin efficiencies in the FL\nswitching, we further show that the switching time can be down to 200 ps with\none order lower critical switching current density compared to DL switching.\nThese results indicate that the FL switching may provide a practical solution\nfor magnetic memory in speed-priority cache applications."
                },
                "authors": [
                    {
                        "name": "Hanying Zhang"
                    },
                    {
                        "name": "Ziqian Cui"
                    },
                    {
                        "name": "Baiqing Jiang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "C. Bi"
                    }
                ],
                "author_detail": {
                    "name": "C. Bi"
                },
                "author": "C. Bi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09261v1",
                "updated": "2025-04-12T15:42:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    12,
                    15,
                    42,
                    17,
                    5,
                    102,
                    0
                ],
                "published": "2025-04-12T15:42:17Z",
                "published_parsed": [
                    2025,
                    4,
                    12,
                    15,
                    42,
                    17,
                    5,
                    102,
                    0
                ],
                "title": "Head-Aware KV Cache Compression for Efficient Visual Autoregressive\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Head-Aware KV Cache Compression for Efficient Visual Autoregressive\n  Modeling"
                },
                "summary": "Visual Autoregressive (VAR) models have emerged as a powerful approach for\nmulti-modal content creation, offering high efficiency and quality across\ndiverse multimedia applications. However, they face significant memory\nbottlenecks due to extensive KV cache accumulation during inference. Existing\nKV cache compression techniques for large language models are suboptimal for\nVAR models due to, as we identify in this paper, two distinct categories of\nattention heads in VAR models: Structural Heads, which preserve spatial\ncoherence through diagonal attention patterns, and Contextual Heads, which\nmaintain semantic consistency through vertical attention patterns. These\ndifferences render single-strategy KV compression techniques ineffective for\nVAR models. To address this, we propose HACK, a training-free Head-Aware\nCompression method for KV cache. HACK allocates asymmetric cache budgets and\nemploys pattern-specific compression strategies tailored to the essential\ncharacteristics of each head category. Experiments on Infinity-2B, Infinity-8B,\nand VAR-d30 demonstrate its effectiveness in text-to-image and\nclass-conditional generation tasks. HACK can hack down up to 50\\% and 70\\% of\ncache with minimal performance degradation for VAR-d30 and Infinity-8B,\nrespectively. Even with 70\\% and 90\\% KV cache compression in VAR-d30 and\nInfinity-8B, HACK still maintains high-quality generation while reducing memory\nusage by 44.2\\% and 58.9\\%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) models have emerged as a powerful approach for\nmulti-modal content creation, offering high efficiency and quality across\ndiverse multimedia applications. However, they face significant memory\nbottlenecks due to extensive KV cache accumulation during inference. Existing\nKV cache compression techniques for large language models are suboptimal for\nVAR models due to, as we identify in this paper, two distinct categories of\nattention heads in VAR models: Structural Heads, which preserve spatial\ncoherence through diagonal attention patterns, and Contextual Heads, which\nmaintain semantic consistency through vertical attention patterns. These\ndifferences render single-strategy KV compression techniques ineffective for\nVAR models. To address this, we propose HACK, a training-free Head-Aware\nCompression method for KV cache. HACK allocates asymmetric cache budgets and\nemploys pattern-specific compression strategies tailored to the essential\ncharacteristics of each head category. Experiments on Infinity-2B, Infinity-8B,\nand VAR-d30 demonstrate its effectiveness in text-to-image and\nclass-conditional generation tasks. HACK can hack down up to 50\\% and 70\\% of\ncache with minimal performance degradation for VAR-d30 and Infinity-8B,\nrespectively. Even with 70\\% and 90\\% KV cache compression in VAR-d30 and\nInfinity-8B, HACK still maintains high-quality generation while reducing memory\nusage by 44.2\\% and 58.9\\%, respectively."
                },
                "authors": [
                    {
                        "name": "Ziran Qin"
                    },
                    {
                        "name": "Youru Lv"
                    },
                    {
                        "name": "Mingbao Lin"
                    },
                    {
                        "name": "Zeren Zhang"
                    },
                    {
                        "name": "Danping Zou"
                    },
                    {
                        "name": "Weiyao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Weiyao Lin"
                },
                "author": "Weiyao Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17459v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17459v3",
                "updated": "2025-04-11T12:31:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    12,
                    31,
                    7,
                    4,
                    101,
                    0
                ],
                "published": "2024-11-26T14:23:53Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    23,
                    53,
                    1,
                    331,
                    0
                ],
                "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model"
                },
                "summary": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE."
                },
                "authors": [
                    {
                        "name": "Zongjian Li"
                    },
                    {
                        "name": "Bin Lin"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Liuhan Chen"
                    },
                    {
                        "name": "Xinhua Cheng"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17459v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17459v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08378v1",
                "updated": "2025-04-11T09:26:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T09:26:47Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "title": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash"
                },
                "summary": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods."
                },
                "authors": [
                    {
                        "name": "Fucheng Jia"
                    },
                    {
                        "name": "Zewen Wu"
                    },
                    {
                        "name": "Shiqi Jiang"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Ju Ren"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Ting Cao"
                    }
                ],
                "author_detail": {
                    "name": "Ting Cao"
                },
                "author": "Ting Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08204v1",
                "updated": "2025-04-11T02:10:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    10,
                    2,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T02:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    10,
                    2,
                    4,
                    101,
                    0
                ],
                "title": "II-NVM: Enhancing Map Accuracy and Consistency with Normal\n  Vector-Assisted Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "II-NVM: Enhancing Map Accuracy and Consistency with Normal\n  Vector-Assisted Mapping"
                },
                "summary": "SLAM technology plays a crucial role in indoor mapping and localization. A\ncommon challenge in indoor environments is the \"double-sided mapping issue\",\nwhere closely positioned walls, doors, and other surfaces are mistakenly\nidentified as a single plane, significantly hindering map accuracy and\nconsistency. To address this issue this paper introduces a SLAM approach that\nensures accurate mapping using normal vector consistency. We enhance the voxel\nmap structure to store both point cloud data and normal vector information,\nenabling the system to evaluate consistency during nearest neighbor searches\nand map updates. This process distinguishes between the front and back sides of\nsurfaces, preventing incorrect point-to-plane constraints. Moreover, we\nimplement an adaptive radius KD-tree search method that dynamically adjusts the\nsearch radius based on the local density of the point cloud, thereby enhancing\nthe accuracy of normal vector calculations. To further improve realtime\nperformance and storage efficiency, we incorporate a Least Recently Used (LRU)\ncache strategy, which facilitates efficient incremental updates of the voxel\nmap. The code is released as open-source and validated in both simulated\nenvironments and real indoor scenarios. Experimental results demonstrate that\nthis approach effectively resolves the \"double-sided mapping issue\" and\nsignificantly improves mapping precision. Additionally, we have developed and\nopen-sourced the first simulation and real world dataset specifically tailored\nfor the \"double-sided mapping issue\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLAM technology plays a crucial role in indoor mapping and localization. A\ncommon challenge in indoor environments is the \"double-sided mapping issue\",\nwhere closely positioned walls, doors, and other surfaces are mistakenly\nidentified as a single plane, significantly hindering map accuracy and\nconsistency. To address this issue this paper introduces a SLAM approach that\nensures accurate mapping using normal vector consistency. We enhance the voxel\nmap structure to store both point cloud data and normal vector information,\nenabling the system to evaluate consistency during nearest neighbor searches\nand map updates. This process distinguishes between the front and back sides of\nsurfaces, preventing incorrect point-to-plane constraints. Moreover, we\nimplement an adaptive radius KD-tree search method that dynamically adjusts the\nsearch radius based on the local density of the point cloud, thereby enhancing\nthe accuracy of normal vector calculations. To further improve realtime\nperformance and storage efficiency, we incorporate a Least Recently Used (LRU)\ncache strategy, which facilitates efficient incremental updates of the voxel\nmap. The code is released as open-source and validated in both simulated\nenvironments and real indoor scenarios. Experimental results demonstrate that\nthis approach effectively resolves the \"double-sided mapping issue\" and\nsignificantly improves mapping precision. Additionally, we have developed and\nopen-sourced the first simulation and real world dataset specifically tailored\nfor the \"double-sided mapping issue\"."
                },
                "authors": [
                    {
                        "name": "Chengwei Zhao"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Yina Jian"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Linji Wang"
                    },
                    {
                        "name": "Yongxin Ma"
                    },
                    {
                        "name": "Xinglai Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xinglai Jin"
                },
                "author": "Xinglai Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07596v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07596v2",
                "updated": "2025-04-11T02:05:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    5,
                    1,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-10T09:48:56Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    48,
                    56,
                    3,
                    100,
                    0
                ],
                "title": "Boosting Universal LLM Reward Design through Heuristic Reward\n  Observation Space Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Universal LLM Reward Design through Heuristic Reward\n  Observation Space Evolution"
                },
                "summary": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward."
                },
                "authors": [
                    {
                        "name": "Zen Kit Heng"
                    },
                    {
                        "name": "Zimeng Zhao"
                    },
                    {
                        "name": "Tianhao Wu"
                    },
                    {
                        "name": "Yuanfei Wang"
                    },
                    {
                        "name": "Mingdong Wu"
                    },
                    {
                        "name": "Yangang Wang"
                    },
                    {
                        "name": "Hao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Hao Dong"
                },
                "author": "Hao Dong",
                "arxiv_comment": "7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07596v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07596v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13915v1",
                "updated": "2025-04-10T17:13:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    13,
                    8,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T17:13:08Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    13,
                    8,
                    3,
                    100,
                    0
                ],
                "title": "Memory-efficient Streaming VideoLLMs for Real-time Procedural Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-efficient Streaming VideoLLMs for Real-time Procedural Video\n  Understanding"
                },
                "summary": "We introduce ProVideLLM, an end-to-end framework for real-time procedural\nvideo understanding. ProVideLLM integrates a multimodal cache configured to\nstore two types of tokens - verbalized text tokens, which provide compressed\ntextual summaries of long-term observations, and visual tokens, encoded with\nDETR-QFormer to capture fine-grained details from short-term observations. This\ndesign reduces token count by 22x over existing methods in representing one\nhour of long-term observations while effectively encoding fine-granularity of\nthe present. By interleaving these tokens in our multimodal cache, ProVideLLM\nensures sub-linear scaling of memory and compute with video length, enabling\nper-frame streaming inference at 10 FPS and streaming dialogue at 25 FPS, with\na minimal 2GB GPU memory footprint. ProVideLLM also sets new state-of-the-art\nresults on six procedural tasks across four datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ProVideLLM, an end-to-end framework for real-time procedural\nvideo understanding. ProVideLLM integrates a multimodal cache configured to\nstore two types of tokens - verbalized text tokens, which provide compressed\ntextual summaries of long-term observations, and visual tokens, encoded with\nDETR-QFormer to capture fine-grained details from short-term observations. This\ndesign reduces token count by 22x over existing methods in representing one\nhour of long-term observations while effectively encoding fine-granularity of\nthe present. By interleaving these tokens in our multimodal cache, ProVideLLM\nensures sub-linear scaling of memory and compute with video length, enabling\nper-frame streaming inference at 10 FPS and streaming dialogue at 25 FPS, with\na minimal 2GB GPU memory footprint. ProVideLLM also sets new state-of-the-art\nresults on six procedural tasks across four datasets."
                },
                "authors": [
                    {
                        "name": "Dibyadip Chatterjee"
                    },
                    {
                        "name": "Edoardo Remelli"
                    },
                    {
                        "name": "Yale Song"
                    },
                    {
                        "name": "Bugra Tekin"
                    },
                    {
                        "name": "Abhay Mittal"
                    },
                    {
                        "name": "Bharat Bhatnagar"
                    },
                    {
                        "name": "Necati Cihan Camgöz"
                    },
                    {
                        "name": "Shreyas Hampali"
                    },
                    {
                        "name": "Eric Sauser"
                    },
                    {
                        "name": "Shugao Ma"
                    },
                    {
                        "name": "Angela Yao"
                    },
                    {
                        "name": "Fadime Sener"
                    }
                ],
                "author_detail": {
                    "name": "Fadime Sener"
                },
                "author": "Fadime Sener",
                "arxiv_comment": "13 pages, 5 figures; https://dibschat.github.io/ProVideLLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07815v1",
                "updated": "2025-04-10T14:52:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    52,
                    3,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T14:52:03Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    52,
                    3,
                    3,
                    100,
                    0
                ],
                "title": "Siren Federate: Bridging document, relational, and graph models for\n  exploratory graph analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Siren Federate: Bridging document, relational, and graph models for\n  exploratory graph analysis"
                },
                "summary": "Investigative workflows require interactive exploratory analysis on large\nheterogeneous knowledge graphs. Current databases show limitations in enabling\nsuch task. This paper discusses the architecture of Siren Federate, a system\nthat efficiently supports exploratory graph analysis by bridging\ndocument-oriented, relational and graph models. Technical contributions include\ndistributed join algorithms, adaptive query planning, query plan folding,\nsemantic caching, and semi-join decomposition for path query. Semi-join\ndecomposition addresses the exponential growth of intermediate results in\npath-based queries. Experiments show that Siren Federate exhibits low latency\nand scales well with the amount of data, the number of users, and the number of\ncomputing nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigative workflows require interactive exploratory analysis on large\nheterogeneous knowledge graphs. Current databases show limitations in enabling\nsuch task. This paper discusses the architecture of Siren Federate, a system\nthat efficiently supports exploratory graph analysis by bridging\ndocument-oriented, relational and graph models. Technical contributions include\ndistributed join algorithms, adaptive query planning, query plan folding,\nsemantic caching, and semi-join decomposition for path query. Semi-join\ndecomposition addresses the exponential growth of intermediate results in\npath-based queries. Experiments show that Siren Federate exhibits low latency\nand scales well with the amount of data, the number of users, and the number of\ncomputing nodes."
                },
                "authors": [
                    {
                        "name": "Georgeta Bordea"
                    },
                    {
                        "name": "Stephane Campinas"
                    },
                    {
                        "name": "Matteo Catena"
                    },
                    {
                        "name": "Renaud Delbru"
                    }
                ],
                "author_detail": {
                    "name": "Renaud Delbru"
                },
                "author": "Renaud Delbru",
                "arxiv_comment": "36 pages, 16 figures, submitted to the ComSIS journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.11; E.1; H.2.4; H.3.3; H.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07642v1",
                "updated": "2025-04-10T10:43:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    43,
                    42,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T10:43:42Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    43,
                    42,
                    3,
                    100,
                    0
                ],
                "title": "Cache-a-lot: Pushing the Limits of Unsatisfiable Core Reuse in SMT-Based\n  Program Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-a-lot: Pushing the Limits of Unsatisfiable Core Reuse in SMT-Based\n  Program Analysis"
                },
                "summary": "Satisfiability Modulo Theories (SMT) solvers are integral to program analysis\ntechniques like concolic and symbolic execution, where they help assess the\nsatisfiability of logical formulae to explore execution paths of the program\nunder test. However, frequent solver invocations are still the main performance\nbottleneck of these techniques. One way to mitigate this challenge is through\noptimizations such as caching and reusing solver results. While current methods\ntypically focus on reusing results from fully equivalent or closely related\nformulas, they often miss broader opportunities for reuse. In this paper, we\npropose a novel approach, Cache-a-lot, that extends the reuse of unsatisfiable\n(unsat) results by systematically considering all possible variable\nsubstitutions. This enables more extensive reuse of results, thereby reducing\nthe number of SMT solver invocations and improving the overall efficiency of\nconcolic and symbolic execution. Our evaluation, conducted against the\nstate-of-the-art Utopia solution using two benchmark sets, shows significant\nimprovements, particularly with more complex formulas. Our method achieves up\nto 74% unsat core reuse, compared to Utopia's 41%, and significant increase in\nthe time savings. These results demonstrate that, despite the additional\ncomputational complexity, the broader reuse of unsat results significantly\nenhances performance, offering valuable advancements for formal verification\nand program analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satisfiability Modulo Theories (SMT) solvers are integral to program analysis\ntechniques like concolic and symbolic execution, where they help assess the\nsatisfiability of logical formulae to explore execution paths of the program\nunder test. However, frequent solver invocations are still the main performance\nbottleneck of these techniques. One way to mitigate this challenge is through\noptimizations such as caching and reusing solver results. While current methods\ntypically focus on reusing results from fully equivalent or closely related\nformulas, they often miss broader opportunities for reuse. In this paper, we\npropose a novel approach, Cache-a-lot, that extends the reuse of unsatisfiable\n(unsat) results by systematically considering all possible variable\nsubstitutions. This enables more extensive reuse of results, thereby reducing\nthe number of SMT solver invocations and improving the overall efficiency of\nconcolic and symbolic execution. Our evaluation, conducted against the\nstate-of-the-art Utopia solution using two benchmark sets, shows significant\nimprovements, particularly with more complex formulas. Our method achieves up\nto 74% unsat core reuse, compared to Utopia's 41%, and significant increase in\nthe time savings. These results demonstrate that, despite the additional\ncomputational complexity, the broader reuse of unsat results significantly\nenhances performance, offering valuable advancements for formal verification\nand program analysis."
                },
                "authors": [
                    {
                        "name": "Rustam Sadykov"
                    },
                    {
                        "name": "Azat Abdullin"
                    },
                    {
                        "name": "Marat Akhin"
                    }
                ],
                "author_detail": {
                    "name": "Marat Akhin"
                },
                "author": "Marat Akhin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07494v1",
                "updated": "2025-04-10T06:51:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    51,
                    23,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T06:51:23Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    51,
                    23,
                    3,
                    100,
                    0
                ],
                "title": "Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM\n  Inference Serving"
                },
                "summary": "Large language model (LLM) inference serving systems are essential to various\nLLM-based applications. As demand for LLM services continues to grow, scaling\nthese systems to handle high request rates while meeting latency Service-Level\nObjectives (SLOs), referred to as effective throughput, becomes critical.\nHowever, existing systems often struggle to improve effective throughput,\nprimarily due to a significant decline in Time To First Token (TTFT) SLO\nattainment. We identify two major causes of this bottleneck: (1)\nmemory-intensive KV cache that limits batch size expansion under GPU memory\nconstraints, and (2) rigid batch composition enforced by the default\nFirst-Come-First-Serve scheduling policy. In this paper, we introduce\nApt-Serve, a scalable framework designed to enhance effective throughput in LLM\ninference serving. Apt-Serve features a new hybrid cache scheme that combines\nKV cache with a memory-efficient hidden cache for reusable input hidden state\nvectors, allowing large batch sizes and improving request concurrency. Based on\nthe hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism\nthat dynamically optimizes batch composition. We formally define the adaptive\nscheduling optimization problem and propose an efficient algorithm with\ntheoretical guarantees. Extensive evaluations on three real-world datasets and\nLLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up\nto 8.8x improvement in effective throughput compared to the state-of-the-art\ninference serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference serving systems are essential to various\nLLM-based applications. As demand for LLM services continues to grow, scaling\nthese systems to handle high request rates while meeting latency Service-Level\nObjectives (SLOs), referred to as effective throughput, becomes critical.\nHowever, existing systems often struggle to improve effective throughput,\nprimarily due to a significant decline in Time To First Token (TTFT) SLO\nattainment. We identify two major causes of this bottleneck: (1)\nmemory-intensive KV cache that limits batch size expansion under GPU memory\nconstraints, and (2) rigid batch composition enforced by the default\nFirst-Come-First-Serve scheduling policy. In this paper, we introduce\nApt-Serve, a scalable framework designed to enhance effective throughput in LLM\ninference serving. Apt-Serve features a new hybrid cache scheme that combines\nKV cache with a memory-efficient hidden cache for reusable input hidden state\nvectors, allowing large batch sizes and improving request concurrency. Based on\nthe hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism\nthat dynamically optimizes batch composition. We formally define the adaptive\nscheduling optimization problem and propose an efficient algorithm with\ntheoretical guarantees. Extensive evaluations on three real-world datasets and\nLLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up\nto 8.8x improvement in effective throughput compared to the state-of-the-art\ninference serving systems."
                },
                "authors": [
                    {
                        "name": "Shihong Gao"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Yanyan Shen"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_doi": "10.1145/3725394",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3725394",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.07494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07479v1",
                "updated": "2025-04-10T06:13:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    13,
                    30,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T06:13:30Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    13,
                    30,
                    3,
                    100,
                    0
                ],
                "title": "UniCAIM: A Unified CAM/CIM Architecture with Static-Dynamic KV Cache\n  Pruning for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniCAIM: A Unified CAM/CIM Architecture with Static-Dynamic KV Cache\n  Pruning for Efficient Long-Context LLM Inference"
                },
                "summary": "Transformer-based large language models (LLMs) have achieved impressive\nperformance in various natural language processing (NLP) applications. However,\nthe high memory and computation cost induced by the KV cache limits the\ninference efficiency, especially for long input sequences. Compute-in-memory\n(CIM)-based accelerators have been proposed for LLM acceleration with KV cache\npruning. However, as existing accelerators only support static pruning with a\nfixed pattern or dynamic pruning with primitive implementations, they suffer\nfrom either high accuracy degradation or low efficiency. In this paper, we\npropose a ferroelectric FET (FeFET)-based unified content addressable memory\n(CAM) and CIM architecture, dubbed as UniCAIM. UniCAIM features simultaneous\nsupport for static and dynamic pruning with 3 computation modes: 1) in the CAM\nmode, UniCAIM enables approximate similarity measurement in O(1) time for\ndynamic KV cache pruning with high energy efficiency; 2) in the charge-domain\nCIM mode, static pruning can be supported based on accumulative similarity\nscore, which is much more flexible compared to fixed patterns; 3) in the\ncurrent-domain mode, exact attention computation can be conducted with a subset\nof selected KV cache. We further propose a novel CAM/CIM cell design that\nleverages the multi-level characteristics of FeFETs for signed multibit storage\nof the KV cache and in-place attention computation. With extensive experimental\nresults, we demonstrate UniCAIM can reduce the area-energy-delay product (AEDP)\nby 8.2-831x over the state-ofthe-art CIM-based LLM accelerators at the circuit\nlevel, along with high accuracy comparable with dense attention at the\napplication level, showing its great potential for efficient long-context LLM\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have achieved impressive\nperformance in various natural language processing (NLP) applications. However,\nthe high memory and computation cost induced by the KV cache limits the\ninference efficiency, especially for long input sequences. Compute-in-memory\n(CIM)-based accelerators have been proposed for LLM acceleration with KV cache\npruning. However, as existing accelerators only support static pruning with a\nfixed pattern or dynamic pruning with primitive implementations, they suffer\nfrom either high accuracy degradation or low efficiency. In this paper, we\npropose a ferroelectric FET (FeFET)-based unified content addressable memory\n(CAM) and CIM architecture, dubbed as UniCAIM. UniCAIM features simultaneous\nsupport for static and dynamic pruning with 3 computation modes: 1) in the CAM\nmode, UniCAIM enables approximate similarity measurement in O(1) time for\ndynamic KV cache pruning with high energy efficiency; 2) in the charge-domain\nCIM mode, static pruning can be supported based on accumulative similarity\nscore, which is much more flexible compared to fixed patterns; 3) in the\ncurrent-domain mode, exact attention computation can be conducted with a subset\nof selected KV cache. We further propose a novel CAM/CIM cell design that\nleverages the multi-level characteristics of FeFETs for signed multibit storage\nof the KV cache and in-place attention computation. With extensive experimental\nresults, we demonstrate UniCAIM can reduce the area-energy-delay product (AEDP)\nby 8.2-831x over the state-ofthe-art CIM-based LLM accelerators at the circuit\nlevel, along with high accuracy comparable with dense attention at the\napplication level, showing its great potential for efficient long-context LLM\ninference."
                },
                "authors": [
                    {
                        "name": "Weikai Xu"
                    },
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Qianqian Huang"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Ru Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ru Huang"
                },
                "author": "Ru Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19379v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19379v3",
                "updated": "2025-04-10T05:06:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    6,
                    29,
                    3,
                    100,
                    0
                ],
                "published": "2024-11-28T21:10:20Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marconi: Prefix Caching for the Era of Hybrid LLMs"
                },
                "summary": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuang Wang"
                    },
                    {
                        "name": "Zhen Jia"
                    },
                    {
                        "name": "Can Karakus"
                    },
                    {
                        "name": "Luca Zancato"
                    },
                    {
                        "name": "Tri Dao"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ravi Netravali"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Netravali"
                },
                "author": "Ravi Netravali",
                "arxiv_comment": "MLSys 2025 camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19379v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19379v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07056v2",
                "updated": "2025-04-09T21:47:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    21,
                    47,
                    31,
                    2,
                    99,
                    0
                ],
                "published": "2025-01-13T04:31:04Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "title": "MAGNUS: Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAGNUS: Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs"
                },
                "summary": "Sparse general matrix-matrix multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines, including Intel MKL, for a variety of different matrices on three\nIntel architectures. For matrices from the SuiteSparse collection, MAGNUS is\nfaster than all the baselines in most cases and is often an order of magnitude\nfaster than at least one baseline. For massive random matrices, MAGNUS scales\nto the largest matrix sizes, while the baselines do not. Furthermore, MAGNUS is\nclose to the optimal bound for these matrices, regardless of the matrix size,\nstructure, and density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse general matrix-matrix multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines, including Intel MKL, for a variety of different matrices on three\nIntel architectures. For matrices from the SuiteSparse collection, MAGNUS is\nfaster than all the baselines in most cases and is often an order of magnitude\nfaster than at least one baseline. For massive random matrices, MAGNUS scales\nto the largest matrix sizes, while the baselines do not. Furthermore, MAGNUS is\nclose to the optimal bound for these matrices, regardless of the matrix size,\nstructure, and density."
                },
                "authors": [
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Jan Laukemann"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Petrini"
                },
                "author": "Fabrizio Petrini",
                "arxiv_comment": "Accepted to ICS25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v3",
                "updated": "2025-04-09T20:51:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    20,
                    51,
                    8,
                    2,
                    99,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "31 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06261v2",
                "updated": "2025-04-09T17:56:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    56,
                    8,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-08T17:59:41Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    41,
                    1,
                    98,
                    0
                ],
                "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention"
                },
                "summary": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning."
                },
                "authors": [
                    {
                        "name": "Gleb Rodionov"
                    },
                    {
                        "name": "Roman Garipov"
                    },
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "George Yakushev"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Anton Sinitsin"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04514v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04514v2",
                "updated": "2025-04-09T14:36:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    36,
                    19,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-06T15:15:07Z",
                "published_parsed": [
                    2025,
                    4,
                    6,
                    15,
                    15,
                    7,
                    6,
                    96,
                    0
                ],
                "title": "Saliency-driven Dynamic Token Pruning for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saliency-driven Dynamic Token Pruning for Large Language Models"
                },
                "summary": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression."
                },
                "authors": [
                    {
                        "name": "Yao Tao"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Yun Wang"
                    },
                    {
                        "name": "Mingjian Zhu"
                    },
                    {
                        "name": "Hailin Hu"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04514v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04514v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06813v1",
                "updated": "2025-04-09T12:07:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    7,
                    26,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T12:07:26Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    7,
                    26,
                    2,
                    99,
                    0
                ],
                "title": "Introducing the Arm-membench Throughput Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introducing the Arm-membench Throughput Benchmark"
                },
                "summary": "Application performance of modern day processors is often limited by the\nmemory subsystem rather than actual compute capabilities. Therefore, data\nthroughput specifications play a key role in modeling application performance\nand determining possible bottlenecks. However, while peak instruction\nthroughputs and bandwidths for local caches are often documented, the\nachievable throughput can also depend on the relation between memory access and\ncompute instructions. In this paper, we present an Arm version of the well\nestablished x86-membench throughput benchmark, which we have adapted to support\nall current SIMD extensions of the Armv8 instruction set architecture. We\ndescribe aspects of the Armv8 ISA that need to be considered in the portable\ndesign of this benchmark. We use the benchmark to analyze the memory subsystem\nat a fine spatial granularity and to unveil microarchitectural details of three\nprocessors: Fujitsu A64FX, Ampere Altra and Cavium ThunderX2. Based on the\nresulting performance information, we show that instruction fetch and decoder\nwidths become a potential bottleneck for cache-bandwidth-sensitive workloads\ndue to the load-store concept of the Arm ISA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application performance of modern day processors is often limited by the\nmemory subsystem rather than actual compute capabilities. Therefore, data\nthroughput specifications play a key role in modeling application performance\nand determining possible bottlenecks. However, while peak instruction\nthroughputs and bandwidths for local caches are often documented, the\nachievable throughput can also depend on the relation between memory access and\ncompute instructions. In this paper, we present an Arm version of the well\nestablished x86-membench throughput benchmark, which we have adapted to support\nall current SIMD extensions of the Armv8 instruction set architecture. We\ndescribe aspects of the Armv8 ISA that need to be considered in the portable\ndesign of this benchmark. We use the benchmark to analyze the memory subsystem\nat a fine spatial granularity and to unveil microarchitectural details of three\nprocessors: Fujitsu A64FX, Ampere Altra and Cavium ThunderX2. Based on the\nresulting performance information, we show that instruction fetch and decoder\nwidths become a potential bottleneck for cache-bandwidth-sensitive workloads\ndue to the load-store concept of the Arm ISA."
                },
                "authors": [
                    {
                        "name": "Cyrill Burth"
                    },
                    {
                        "name": "Markus Velten"
                    },
                    {
                        "name": "Robert Schöne"
                    }
                ],
                "author_detail": {
                    "name": "Robert Schöne"
                },
                "author": "Robert Schöne",
                "arxiv_doi": "10.1007/978-3-031-85697-6_7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-85697-6_7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.06813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 6 figures, published in Parallel Processing and Applied\n  Mathematics (PPAM 2024), see https://doi.org/10.1007/978-3-031-85697-6_7",
                "arxiv_journal_ref": "Parallel Processing and Applied Mathematics. PPAM 2024. Lecture\n  Notes in Computer Science, vol 15579. Springer, Cham",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05821v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05821v2",
                "updated": "2025-04-09T10:23:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    10,
                    23,
                    39,
                    2,
                    99,
                    0
                ],
                "published": "2024-03-09T07:01:44Z",
                "published_parsed": [
                    2024,
                    3,
                    9,
                    7,
                    1,
                    44,
                    5,
                    69,
                    0
                ],
                "title": "Optimizing LLM Queries in Relational Data Analytics Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing LLM Queries in Relational Data Analytics Workloads"
                },
                "summary": "Batch data analytics is a growing application for Large Language Models\n(LLMs). LLMs enable users to perform a wide range of natural language tasks,\nsuch as classification, entity extraction, and translation, over large\ndatasets. However, LLM inference is highly costly and slow: for example, an\nNVIDIA L4 GPU running Llama3-8B can only process 6 KB of text per second,\ntaking about a day to handle 15 GB of data; processing a similar amount of data\ncosts around $10K on OpenAI's GPT-4o. In this paper, we propose novel\ntechniques that can significantly reduce the cost of LLM calls for relational\ndata analytics workloads. Our key contribution is developing efficient\nalgorithms for reordering the rows and the fields within each row of an input\ntable to maximize key-value (KV) cache reuse when performing LLM serving. As\nsuch, our approach can be easily applied to existing analytics systems and\nserving platforms. Our evaluation shows that our solution can yield up to 3.4x\nimprovement in job completion time on a benchmark of diverse LLM-based queries\nusing Llama 3 models. Our solution also achieves a 32% cost savings under\nOpenAI and Anthropic pricing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch data analytics is a growing application for Large Language Models\n(LLMs). LLMs enable users to perform a wide range of natural language tasks,\nsuch as classification, entity extraction, and translation, over large\ndatasets. However, LLM inference is highly costly and slow: for example, an\nNVIDIA L4 GPU running Llama3-8B can only process 6 KB of text per second,\ntaking about a day to handle 15 GB of data; processing a similar amount of data\ncosts around $10K on OpenAI's GPT-4o. In this paper, we propose novel\ntechniques that can significantly reduce the cost of LLM calls for relational\ndata analytics workloads. Our key contribution is developing efficient\nalgorithms for reordering the rows and the fields within each row of an input\ntable to maximize key-value (KV) cache reuse when performing LLM serving. As\nsuch, our approach can be easily applied to existing analytics systems and\nserving platforms. Our evaluation shows that our solution can yield up to 3.4x\nimprovement in job completion time on a benchmark of diverse LLM-based queries\nusing Llama 3 models. Our solution also achieves a 32% cost savings under\nOpenAI and Anthropic pricing models."
                },
                "authors": [
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Asim Biswal"
                    },
                    {
                        "name": "Amog Kamsetty"
                    },
                    {
                        "name": "Audrey Cheng"
                    },
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Liana Patel"
                    },
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Matei Zaharia"
                    }
                ],
                "author_detail": {
                    "name": "Matei Zaharia"
                },
                "author": "Matei Zaharia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05821v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05821v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05591v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05591v3",
                "updated": "2025-04-09T09:09:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    9,
                    9,
                    37,
                    2,
                    99,
                    0
                ],
                "published": "2024-09-09T13:20:31Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    13,
                    20,
                    31,
                    0,
                    253,
                    0
                ],
                "title": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced\n  Retrieval Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced\n  Retrieval Augmentation"
                },
                "summary": "Processing long contexts presents a significant challenge for large language\nmodels (LLMs). While recent advancements allow LLMs to handle much longer\ncontexts than before (e.g., 32K or 128K tokens), it is computationally\nexpensive and can still be insufficient for many applications.\nRetrieval-Augmented Generation (RAG) is considered a promising strategy to\naddress this problem. However, conventional RAG methods face inherent\nlimitations because of two underlying requirements: 1) explicitly stated\nqueries, and 2) well-structured knowledge. These conditions, however, do not\nhold in general long-context processing tasks.\n  In this work, we propose MemoRAG, a novel RAG framework empowered by global\nmemory-augmented retrieval. MemoRAG features a dual-system architecture. First,\nit employs a light but long-range system to create a global memory of the long\ncontext. Once a task is presented, it generates draft answers, providing useful\nclues for the retrieval tools to locate relevant information within the long\ncontext. Second, it leverages an expensive but expressive system, which\ngenerates the final answer based on the retrieved information. Building upon\nthis fundamental framework, we realize the memory module in the form of KV\ncompression, and reinforce its memorization and cluing capacity from the\nGeneration quality's Feedback (a.k.a. RLGF). In our experiments, MemoRAG\nachieves superior performances across a variety of long-context evaluation\ntasks, not only complex scenarios where traditional RAG methods struggle, but\nalso simpler ones where RAG is typically applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts presents a significant challenge for large language\nmodels (LLMs). While recent advancements allow LLMs to handle much longer\ncontexts than before (e.g., 32K or 128K tokens), it is computationally\nexpensive and can still be insufficient for many applications.\nRetrieval-Augmented Generation (RAG) is considered a promising strategy to\naddress this problem. However, conventional RAG methods face inherent\nlimitations because of two underlying requirements: 1) explicitly stated\nqueries, and 2) well-structured knowledge. These conditions, however, do not\nhold in general long-context processing tasks.\n  In this work, we propose MemoRAG, a novel RAG framework empowered by global\nmemory-augmented retrieval. MemoRAG features a dual-system architecture. First,\nit employs a light but long-range system to create a global memory of the long\ncontext. Once a task is presented, it generates draft answers, providing useful\nclues for the retrieval tools to locate relevant information within the long\ncontext. Second, it leverages an expensive but expressive system, which\ngenerates the final answer based on the retrieved information. Building upon\nthis fundamental framework, we realize the memory module in the form of KV\ncompression, and reinforce its memorization and cluing capacity from the\nGeneration quality's Feedback (a.k.a. RLGF). In our experiments, MemoRAG\nachieves superior performances across a variety of long-context evaluation\ntasks, not only complex scenarios where traditional RAG methods struggle, but\nalso simpler ones where RAG is typically applied."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Kelong Mao"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Tiejun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Tiejun Huang"
                },
                "author": "Tiejun Huang",
                "arxiv_comment": "theWebConf 2025. Codes and models are in\n  https://github.com/qhjqhj00/MemoRAG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05591v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05591v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v5",
                "updated": "2025-04-09T07:55:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    55,
                    43,
                    2,
                    99,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14396v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14396v5",
                "updated": "2025-04-09T03:49:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    3,
                    49,
                    16,
                    2,
                    99,
                    0
                ],
                "published": "2023-12-22T02:52:59Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    2,
                    52,
                    59,
                    4,
                    356,
                    0
                ],
                "title": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing"
                },
                "summary": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation."
                },
                "authors": [
                    {
                        "name": "Hongfu Li"
                    },
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Feng Yao"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "This paper was accepted by VLDB2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14396v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14396v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06419v1",
                "updated": "2025-04-08T20:39:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    39,
                    20,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T20:39:20Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    39,
                    20,
                    1,
                    98,
                    0
                ],
                "title": "SPIRe: Boosting LLM Inference Throughput with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPIRe: Boosting LLM Inference Throughput with Speculative Decoding"
                },
                "summary": "Speculative decoding (SD) has been shown to reduce the latency of\nautoregressive decoding (AD) by 2-3x for small batch sizes. However, increasing\nthroughput and therefore reducing the cost per token requires decoding with\nlarge batch sizes. Recent work shows that SD can accelerate decoding with large\nbatch sizes too if the context is sufficiently long and the draft model's KV\ncache is sparse. We introduce SPIRe, a draft model that combines static sparse\nattention, pruned initialization, and feedback memory to increase the modeled\nthroughput of speculative decoding by over 100% compared to speculation with a\nmuch smaller draft model and by over 35% compared to the strong baseline of\nsparse self-speculation. Our approach is particularly effective when context\nlengths vary significantly across requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD) has been shown to reduce the latency of\nautoregressive decoding (AD) by 2-3x for small batch sizes. However, increasing\nthroughput and therefore reducing the cost per token requires decoding with\nlarge batch sizes. Recent work shows that SD can accelerate decoding with large\nbatch sizes too if the context is sufficiently long and the draft model's KV\ncache is sparse. We introduce SPIRe, a draft model that combines static sparse\nattention, pruned initialization, and feedback memory to increase the modeled\nthroughput of speculative decoding by over 100% compared to speculation with a\nmuch smaller draft model and by over 35% compared to the strong baseline of\nsparse self-speculation. Our approach is particularly effective when context\nlengths vary significantly across requests."
                },
                "authors": [
                    {
                        "name": "Sanjit Neelam"
                    },
                    {
                        "name": "Daniel Heinlein"
                    },
                    {
                        "name": "Vaclav Cvicek"
                    },
                    {
                        "name": "Akshay Mishra"
                    },
                    {
                        "name": "Reiner Pope"
                    }
                ],
                "author_detail": {
                    "name": "Reiner Pope"
                },
                "author": "Reiner Pope",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06416v1",
                "updated": "2025-04-08T20:32:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    32,
                    10,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T20:32:10Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    32,
                    10,
                    1,
                    98,
                    0
                ],
                "title": "Unifying Autoregressive and Diffusion-Based Sequence Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Autoregressive and Diffusion-Based Sequence Generation"
                },
                "summary": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation."
                },
                "authors": [
                    {
                        "name": "Nima Fathi"
                    },
                    {
                        "name": "Torsten Scholak"
                    },
                    {
                        "name": "Pierre-André Noël"
                    }
                ],
                "author_detail": {
                    "name": "Pierre-André Noël"
                },
                "author": "Pierre-André Noël",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.17692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.17692v2",
                "updated": "2025-04-08T19:26:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    19,
                    26,
                    41,
                    1,
                    98,
                    0
                ],
                "published": "2024-04-26T20:44:36Z",
                "published_parsed": [
                    2024,
                    4,
                    26,
                    20,
                    44,
                    36,
                    4,
                    117,
                    0
                ],
                "title": "Walking on Spheres and Talking to Neighbors: Variance Reduction for\n  Laplace's Equation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Walking on Spheres and Talking to Neighbors: Variance Reduction for\n  Laplace's Equation"
                },
                "summary": "Walk on Spheres algorithms leverage properties of Brownian Motion to create\nMonte Carlo estimates of solutions to a class of elliptic partial differential\nequations. We propose a new caching strategy which leverages the continuity of\npaths of Brownian Motion. In the case of Laplace's equation with Dirichlet\nboundary conditions, our algorithm has improved asymptotic runtime compared to\nprevious approaches. Until recently, estimates were constructed pointwise and\ndid not use the relationship between solutions at nearby points within a\ndomain. Instead, our results are achieved by passing information from a cache\nof fixed size. We also provide bounds on the performance of our algorithm and\ndemonstrate its performance on example problems of increasing complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Walk on Spheres algorithms leverage properties of Brownian Motion to create\nMonte Carlo estimates of solutions to a class of elliptic partial differential\nequations. We propose a new caching strategy which leverages the continuity of\npaths of Brownian Motion. In the case of Laplace's equation with Dirichlet\nboundary conditions, our algorithm has improved asymptotic runtime compared to\nprevious approaches. Until recently, estimates were constructed pointwise and\ndid not use the relationship between solutions at nearby points within a\ndomain. Instead, our results are achieved by passing information from a cache\nof fixed size. We also provide bounds on the performance of our algorithm and\ndemonstrate its performance on example problems of increasing complexity."
                },
                "authors": [
                    {
                        "name": "Michael Czekanski"
                    },
                    {
                        "name": "Benjamin Faber"
                    },
                    {
                        "name": "Margaret Fairborn"
                    },
                    {
                        "name": "Adelle Wright"
                    },
                    {
                        "name": "David Bindel"
                    }
                ],
                "author_detail": {
                    "name": "David Bindel"
                },
                "author": "David Bindel",
                "arxiv_comment": "21 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.17692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.17692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06067v1",
                "updated": "2025-04-08T14:09:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    9,
                    23,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T14:09:23Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    9,
                    23,
                    1,
                    98,
                    0
                ],
                "title": "GPU-accelerated Evolutionary Many-objective Optimization Using\n  Tensorized NSGA-III",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU-accelerated Evolutionary Many-objective Optimization Using\n  Tensorized NSGA-III"
                },
                "summary": "NSGA-III is one of the most widely adopted algorithms for tackling\nmany-objective optimization problems. However, its CPU-based design severely\nlimits scalability and computational efficiency. To address the limitations, we\npropose {TensorNSGA-III}, a fully tensorized implementation of NSGA-III that\nleverages GPU parallelism for large-scale many-objective optimization. Unlike\nconventional GPU-accelerated evolutionary algorithms that rely on heuristic\napproximations to improve efficiency, TensorNSGA-III maintains the exact\nselection and variation mechanisms of NSGA-III while achieving significant\nacceleration. By reformulating the selection process with tensorized data\nstructures and an optimized caching strategy, our approach effectively\neliminates computational bottlenecks inherent in traditional CPU-based and\nna\\\"ive GPU implementations. Experimental results on widely used numerical\nbenchmarks show that TensorNSGA-III achieves speedups of up to $3629\\times$\nover the CPU version of NSGA-III. Additionally, we validate its effectiveness\nin multiobjective robotic control tasks, where it discovers diverse and\nhigh-quality behavioral solutions. Furthermore, we investigate the critical\nrole of large population sizes in many-objective optimization and demonstrate\nthe scalability of TensorNSGA-III in such scenarios. The source code is\navailable at https://github.com/EMI-Group/evomo",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NSGA-III is one of the most widely adopted algorithms for tackling\nmany-objective optimization problems. However, its CPU-based design severely\nlimits scalability and computational efficiency. To address the limitations, we\npropose {TensorNSGA-III}, a fully tensorized implementation of NSGA-III that\nleverages GPU parallelism for large-scale many-objective optimization. Unlike\nconventional GPU-accelerated evolutionary algorithms that rely on heuristic\napproximations to improve efficiency, TensorNSGA-III maintains the exact\nselection and variation mechanisms of NSGA-III while achieving significant\nacceleration. By reformulating the selection process with tensorized data\nstructures and an optimized caching strategy, our approach effectively\neliminates computational bottlenecks inherent in traditional CPU-based and\nna\\\"ive GPU implementations. Experimental results on widely used numerical\nbenchmarks show that TensorNSGA-III achieves speedups of up to $3629\\times$\nover the CPU version of NSGA-III. Additionally, we validate its effectiveness\nin multiobjective robotic control tasks, where it discovers diverse and\nhigh-quality behavioral solutions. Furthermore, we investigate the critical\nrole of large population sizes in many-objective optimization and demonstrate\nthe scalability of TensorNSGA-III in such scenarios. The source code is\navailable at https://github.com/EMI-Group/evomo"
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Zhenyu Liang"
                    },
                    {
                        "name": "Ran Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Ran Cheng"
                },
                "author": "Ran Cheng",
                "arxiv_comment": "Accepted by IEEE CEC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.18535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18535v1",
                "updated": "2025-04-25T17:59:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    17,
                    59,
                    13,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T17:59:13Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    17,
                    59,
                    13,
                    4,
                    115,
                    0
                ],
                "title": "TRACE Back from the Future: A Probabilistic Reasoning Approach to\n  Controllable Language Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRACE Back from the Future: A Probabilistic Reasoning Approach to\n  Controllable Language Generation"
                },
                "summary": "As large language models (LMs) advance, there is an increasing need to\ncontrol their outputs to align with human values (e.g., detoxification) or\ndesired attributes (e.g., personalization, topic). However, autoregressive\nmodels focus on next-token predictions and struggle with global properties that\nrequire looking ahead. Existing solutions either tune or post-train LMs for\neach new attribute - expensive and inflexible - or approximate the Expected\nAttribute Probability (EAP) of future sequences by sampling or training, which\nis slow and unreliable for rare attributes. We introduce TRACE (Tractable\nProbabilistic Reasoning for Adaptable Controllable gEneration), a novel\nframework that efficiently computes EAP and adapts to new attributes through\ntractable probabilistic reasoning and lightweight control. TRACE distills a\nHidden Markov Model (HMM) from an LM and pairs it with a small classifier to\nestimate attribute probabilities, enabling exact EAP computation over the HMM's\npredicted futures. This EAP is then used to reweigh the LM's next-token\nprobabilities for globally compliant continuations. Empirically, TRACE achieves\nstate-of-the-art results in detoxification with only 10% decoding overhead,\nadapts to 76 low-resource personalized LLMs within seconds, and seamlessly\nextends to composite attributes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LMs) advance, there is an increasing need to\ncontrol their outputs to align with human values (e.g., detoxification) or\ndesired attributes (e.g., personalization, topic). However, autoregressive\nmodels focus on next-token predictions and struggle with global properties that\nrequire looking ahead. Existing solutions either tune or post-train LMs for\neach new attribute - expensive and inflexible - or approximate the Expected\nAttribute Probability (EAP) of future sequences by sampling or training, which\nis slow and unreliable for rare attributes. We introduce TRACE (Tractable\nProbabilistic Reasoning for Adaptable Controllable gEneration), a novel\nframework that efficiently computes EAP and adapts to new attributes through\ntractable probabilistic reasoning and lightweight control. TRACE distills a\nHidden Markov Model (HMM) from an LM and pairs it with a small classifier to\nestimate attribute probabilities, enabling exact EAP computation over the HMM's\npredicted futures. This EAP is then used to reweigh the LM's next-token\nprobabilities for globally compliant continuations. Empirically, TRACE achieves\nstate-of-the-art results in detoxification with only 10% decoding overhead,\nadapts to 76 low-resource personalized LLMs within seconds, and seamlessly\nextends to composite attributes."
                },
                "authors": [
                    {
                        "name": "Gwen Yidou Weng"
                    },
                    {
                        "name": "Benjie Wang"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    }
                ],
                "author_detail": {
                    "name": "Guy Van den Broeck"
                },
                "author": "Guy Van den Broeck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18529v1",
                "updated": "2025-04-25T17:53:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    17,
                    53,
                    52,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T17:53:52Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    17,
                    53,
                    52,
                    4,
                    115,
                    0
                ],
                "title": "Practical Type-Based Taint Checking and Inference (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practical Type-Based Taint Checking and Inference (Extended Version)"
                },
                "summary": "Many important security properties can be formulated in terms of flows of\ntainted data, and improved taint analysis tools to prevent such flows are of\ncritical need. Most existing taint analyses use whole-program static analysis,\nleading to scalability challenges. Type-based checking is a promising\nalternative, as it enables modular and incremental checking for fast\nperformance. However, type-based approaches have not been widely adopted in\npractice, due to challenges with false positives and annotating existing\ncodebases. In this paper, we present a new approach to type-based checking of\ntaint properties that addresses these challenges, based on two key techniques.\nFirst, we present a new type-based tainting checker with significantly reduced\nfalse positives, via more practical handling of third-party libraries and other\nlanguage constructs. Second, we present a novel technique to automatically\ninfer tainting type qualifiers for existing code. Our technique supports\ninference of generic type argument annotations, crucial for tainting\nproperties. We implemented our techniques in a tool TaintTyper and evaluated it\non real-world benchmarks. TaintTyper exceeds the recall of a state-of-the-art\nwhole-program taint analyzer, with comparable precision, and 2.93X-22.9X faster\nchecking time. Further, TaintTyper infers annotations comparable to those\nwritten by hand, suitable for insertion into source code. TaintTyper is a\npromising new approach to efficient and practical taint checking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many important security properties can be formulated in terms of flows of\ntainted data, and improved taint analysis tools to prevent such flows are of\ncritical need. Most existing taint analyses use whole-program static analysis,\nleading to scalability challenges. Type-based checking is a promising\nalternative, as it enables modular and incremental checking for fast\nperformance. However, type-based approaches have not been widely adopted in\npractice, due to challenges with false positives and annotating existing\ncodebases. In this paper, we present a new approach to type-based checking of\ntaint properties that addresses these challenges, based on two key techniques.\nFirst, we present a new type-based tainting checker with significantly reduced\nfalse positives, via more practical handling of third-party libraries and other\nlanguage constructs. Second, we present a novel technique to automatically\ninfer tainting type qualifiers for existing code. Our technique supports\ninference of generic type argument annotations, crucial for tainting\nproperties. We implemented our techniques in a tool TaintTyper and evaluated it\non real-world benchmarks. TaintTyper exceeds the recall of a state-of-the-art\nwhole-program taint analyzer, with comparable precision, and 2.93X-22.9X faster\nchecking time. Further, TaintTyper infers annotations comparable to those\nwritten by hand, suitable for insertion into source code. TaintTyper is a\npromising new approach to efficient and practical taint checking."
                },
                "authors": [
                    {
                        "name": "Nima Karimipour"
                    },
                    {
                        "name": "Kanak Das"
                    },
                    {
                        "name": "Manu Sridharan"
                    },
                    {
                        "name": "Behnaz Hassanshahi"
                    }
                ],
                "author_detail": {
                    "name": "Behnaz Hassanshahi"
                },
                "author": "Behnaz Hassanshahi",
                "arxiv_comment": "Extended version of ECOOP 2025 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17355v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17355v4",
                "updated": "2025-04-25T17:27:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    17,
                    27,
                    10,
                    4,
                    115,
                    0
                ],
                "published": "2024-08-30T15:39:34Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    15,
                    39,
                    34,
                    4,
                    243,
                    0
                ],
                "title": "Bidirectional Decoding: Improving Action Chunking via Guided Test-Time\n  Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidirectional Decoding: Improving Action Chunking via Guided Test-Time\n  Sampling"
                },
                "summary": "Predicting and executing a sequence of actions without intermediate\nreplanning, known as action chunking, is increasingly used in robot learning\nfrom human demonstrations. Yet, its effects on the learned policy remain\ninconsistent: some studies find it crucial for achieving strong results, while\nothers observe decreased performance. In this paper, we first dissect how\naction chunking impacts the divergence between a learner and a demonstrator. We\nfind that action chunking allows the learner to better capture the temporal\ndependencies in demonstrations but at the cost of reduced reactivity to\nunexpected states. To address this tradeoff, we propose Bidirectional Decoding\n(BID), a test-time inference algorithm that bridges action chunking with\nclosed-loop adaptation. At each timestep, BID samples multiple candidate\npredictions and searches for the optimal one based on two criteria: (i)\nbackward coherence, which favors samples that align with previous decisions;\n(ii) forward contrast, which seeks samples of high likelihood for future plans.\nBy coupling decisions within and across action chunks, BID promotes both\nlong-term consistency and short-term reactivity. Experimental results show that\nour method boosts the performance of two state-of-the-art generative policies\nacross seven simulation benchmarks and two real-world tasks. Code and videos\nare available at https://bid-robot.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting and executing a sequence of actions without intermediate\nreplanning, known as action chunking, is increasingly used in robot learning\nfrom human demonstrations. Yet, its effects on the learned policy remain\ninconsistent: some studies find it crucial for achieving strong results, while\nothers observe decreased performance. In this paper, we first dissect how\naction chunking impacts the divergence between a learner and a demonstrator. We\nfind that action chunking allows the learner to better capture the temporal\ndependencies in demonstrations but at the cost of reduced reactivity to\nunexpected states. To address this tradeoff, we propose Bidirectional Decoding\n(BID), a test-time inference algorithm that bridges action chunking with\nclosed-loop adaptation. At each timestep, BID samples multiple candidate\npredictions and searches for the optimal one based on two criteria: (i)\nbackward coherence, which favors samples that align with previous decisions;\n(ii) forward contrast, which seeks samples of high likelihood for future plans.\nBy coupling decisions within and across action chunks, BID promotes both\nlong-term consistency and short-term reactivity. Experimental results show that\nour method boosts the performance of two state-of-the-art generative policies\nacross seven simulation benchmarks and two real-world tasks. Code and videos\nare available at https://bid-robot.github.io."
                },
                "authors": [
                    {
                        "name": "Yuejiang Liu"
                    },
                    {
                        "name": "Jubayer Ibn Hamid"
                    },
                    {
                        "name": "Annie Xie"
                    },
                    {
                        "name": "Yoonho Lee"
                    },
                    {
                        "name": "Maximilian Du"
                    },
                    {
                        "name": "Chelsea Finn"
                    }
                ],
                "author_detail": {
                    "name": "Chelsea Finn"
                },
                "author": "Chelsea Finn",
                "arxiv_comment": "Project website: https://bid-robot.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17355v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17355v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18510v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18510v1",
                "updated": "2025-04-25T17:23:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    17,
                    23,
                    47,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T17:23:47Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    17,
                    23,
                    47,
                    4,
                    115,
                    0
                ],
                "title": "Examining the Impact of Optical Aberrations to Image Classification and\n  Object Detection Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Examining the Impact of Optical Aberrations to Image Classification and\n  Object Detection Models"
                },
                "summary": "Deep neural networks (DNNs) have proven to be successful in various computer\nvision applications such that models even infer in safety-critical situations.\nTherefore, vision models have to behave in a robust way to disturbances such as\nnoise or blur. While seminal benchmarks exist to evaluate model robustness to\ndiverse corruptions, blur is often approximated in an overly simplistic way to\nmodel defocus, while ignoring the different blur kernel shapes that result from\noptical systems. To study model robustness against realistic optical blur\neffects, this paper proposes two datasets of blur corruptions, which we denote\nOpticsBench and LensCorruptions. OpticsBench examines primary aberrations such\nas coma, defocus, and astigmatism, i.e. aberrations that can be represented by\nvarying a single parameter of Zernike polynomials. To go beyond the principled\nbut synthetic setting of primary aberrations, LensCorruptions samples linear\ncombinations in the vector space spanned by Zernike polynomials, corresponding\nto 100 real lenses. Evaluations for image classification and object detection\non ImageNet and MSCOCO show that for a variety of different pre-trained models,\nthe performance on OpticsBench and LensCorruptions varies significantly,\nindicating the need to consider realistic image corruptions to evaluate a\nmodel's robustness against blur.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks (DNNs) have proven to be successful in various computer\nvision applications such that models even infer in safety-critical situations.\nTherefore, vision models have to behave in a robust way to disturbances such as\nnoise or blur. While seminal benchmarks exist to evaluate model robustness to\ndiverse corruptions, blur is often approximated in an overly simplistic way to\nmodel defocus, while ignoring the different blur kernel shapes that result from\noptical systems. To study model robustness against realistic optical blur\neffects, this paper proposes two datasets of blur corruptions, which we denote\nOpticsBench and LensCorruptions. OpticsBench examines primary aberrations such\nas coma, defocus, and astigmatism, i.e. aberrations that can be represented by\nvarying a single parameter of Zernike polynomials. To go beyond the principled\nbut synthetic setting of primary aberrations, LensCorruptions samples linear\ncombinations in the vector space spanned by Zernike polynomials, corresponding\nto 100 real lenses. Evaluations for image classification and object detection\non ImageNet and MSCOCO show that for a variety of different pre-trained models,\nthe performance on OpticsBench and LensCorruptions varies significantly,\nindicating the need to consider realistic image corruptions to evaluate a\nmodel's robustness against blur."
                },
                "authors": [
                    {
                        "name": "Patrick Müller"
                    },
                    {
                        "name": "Alexander Braun"
                    },
                    {
                        "name": "Margret Keuper"
                    }
                ],
                "author_detail": {
                    "name": "Margret Keuper"
                },
                "author": "Margret Keuper",
                "arxiv_comment": "v1.0",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18510v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18497v1",
                "updated": "2025-04-25T17:10:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    17,
                    10,
                    33,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T17:10:33Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    17,
                    10,
                    33,
                    4,
                    115,
                    0
                ],
                "title": "DeSIA: Attribute Inference Attacks Against Limited Fixed Aggregate\n  Statistics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeSIA: Attribute Inference Attacks Against Limited Fixed Aggregate\n  Statistics"
                },
                "summary": "Empirical inference attacks are a popular approach for evaluating the privacy\nrisk of data release mechanisms in practice. While an active attack literature\nexists to evaluate machine learning models or synthetic data release, we\ncurrently lack comparable methods for fixed aggregate statistics, in particular\nwhen only a limited number of statistics are released. We here propose an\ninference attack framework against fixed aggregate statistics and an attribute\ninference attack called DeSIA. We instantiate DeSIA against the U.S. Census\nPPMF dataset and show it to strongly outperform reconstruction-based attacks.\nIn particular, we show DeSIA to be highly effective at identifying vulnerable\nusers, achieving a true positive rate of 0.14 at a false positive rate of\n$10^{-3}$. We then show DeSIA to perform well against users whose attributes\ncannot be verified and when varying the number of aggregate statistics and\nlevel of noise addition. We also perform an extensive ablation study of DeSIA\nand show how DeSIA can be successfully adapted to the membership inference\ntask. Overall, our results show that aggregation alone is not sufficient to\nprotect privacy, even when a relatively small number of aggregates are being\nreleased, and emphasize the need for formal privacy mechanisms and testing\nbefore aggregate statistics are released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirical inference attacks are a popular approach for evaluating the privacy\nrisk of data release mechanisms in practice. While an active attack literature\nexists to evaluate machine learning models or synthetic data release, we\ncurrently lack comparable methods for fixed aggregate statistics, in particular\nwhen only a limited number of statistics are released. We here propose an\ninference attack framework against fixed aggregate statistics and an attribute\ninference attack called DeSIA. We instantiate DeSIA against the U.S. Census\nPPMF dataset and show it to strongly outperform reconstruction-based attacks.\nIn particular, we show DeSIA to be highly effective at identifying vulnerable\nusers, achieving a true positive rate of 0.14 at a false positive rate of\n$10^{-3}$. We then show DeSIA to perform well against users whose attributes\ncannot be verified and when varying the number of aggregate statistics and\nlevel of noise addition. We also perform an extensive ablation study of DeSIA\nand show how DeSIA can be successfully adapted to the membership inference\ntask. Overall, our results show that aggregation alone is not sufficient to\nprotect privacy, even when a relatively small number of aggregates are being\nreleased, and emphasize the need for formal privacy mechanisms and testing\nbefore aggregate statistics are released."
                },
                "authors": [
                    {
                        "name": "Yifeng Mao"
                    },
                    {
                        "name": "Bozhidar Stevanoski"
                    },
                    {
                        "name": "Yves-Alexandre de Montjoye"
                    }
                ],
                "author_detail": {
                    "name": "Yves-Alexandre de Montjoye"
                },
                "author": "Yves-Alexandre de Montjoye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18496v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18496v1",
                "updated": "2025-04-25T17:09:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    17,
                    9,
                    29,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T17:09:29Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    17,
                    9,
                    29,
                    4,
                    115,
                    0
                ],
                "title": "Facets, Taxonomies, and Syntheses: Navigating Structured Representations\n  in LLM-Assisted Literature Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Facets, Taxonomies, and Syntheses: Navigating Structured Representations\n  in LLM-Assisted Literature Review"
                },
                "summary": "Comprehensive literature review requires synthesizing vast amounts of\nresearch -- a labor intensive and cognitively demanding process. Most prior\nwork focuses either on helping researchers deeply understand a few papers\n(e.g., for triaging or reading), or retrieving from and visualizing a vast\ncorpus. Deep analysis and synthesis of large paper collections (e.g., to\nproduce a survey paper) is largely conducted manually with little support. We\npresent DimInd, an interactive system that scaffolds literature review across\nlarge paper collections through LLM-generated structured representations.\nDimInd scaffolds literature understanding with multiple levels of compression,\nfrom papers, to faceted literature comparison tables with information extracted\nfrom individual papers, to taxonomies of concepts, to narrative syntheses.\nUsers are guided through these successive information transformations while\nmaintaining provenance to source text. In an evaluation with 23 researchers,\nDimInd supported participants in extracting information and conceptually\norganizing papers with less effort compared to a ChatGPT-assisted baseline\nworkflow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehensive literature review requires synthesizing vast amounts of\nresearch -- a labor intensive and cognitively demanding process. Most prior\nwork focuses either on helping researchers deeply understand a few papers\n(e.g., for triaging or reading), or retrieving from and visualizing a vast\ncorpus. Deep analysis and synthesis of large paper collections (e.g., to\nproduce a survey paper) is largely conducted manually with little support. We\npresent DimInd, an interactive system that scaffolds literature review across\nlarge paper collections through LLM-generated structured representations.\nDimInd scaffolds literature understanding with multiple levels of compression,\nfrom papers, to faceted literature comparison tables with information extracted\nfrom individual papers, to taxonomies of concepts, to narrative syntheses.\nUsers are guided through these successive information transformations while\nmaintaining provenance to source text. In an evaluation with 23 researchers,\nDimInd supported participants in extracting information and conceptually\norganizing papers with less effort compared to a ChatGPT-assisted baseline\nworkflow."
                },
                "authors": [
                    {
                        "name": "Raymond Fok"
                    },
                    {
                        "name": "Joseph Chee Chang"
                    },
                    {
                        "name": "Marissa Radensky"
                    },
                    {
                        "name": "Pao Siangliulue"
                    },
                    {
                        "name": "Jonathan Bragg"
                    },
                    {
                        "name": "Amy X. Zhang"
                    },
                    {
                        "name": "Daniel S. Weld"
                    }
                ],
                "author_detail": {
                    "name": "Daniel S. Weld"
                },
                "author": "Daniel S. Weld",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18496v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18496v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15654v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15654v4",
                "updated": "2025-04-25T16:53:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    16,
                    53,
                    47,
                    4,
                    115,
                    0
                ],
                "published": "2025-02-21T18:22:36Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    22,
                    36,
                    4,
                    52,
                    0
                ],
                "title": "Machine-generated text detection prevents language model collapse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine-generated text detection prevents language model collapse"
                },
                "summary": "As Large Language Models (LLMs) become increasingly prevalent, their\ngenerated outputs are proliferating across the web, risking a future where\nmachine-generated content dilutes human-authored text. Since online data is the\nprimary resource for LLM pre-training, subsequent models could be trained on an\nunknown portion of synthetic samples. This will lead to model collapse, a\ndegenerative process whereby LLMs reinforce their own errors, and ultimately\nyield a declining performance. In this study, we investigate the impact of\ndecoding strategy on model collapse, analysing the characteristics of text at\neach model generation, the similarity to human references, and the resulting\nmodel performance. Using the decoding strategies that lead to the most\nsignificant degradation, we evaluate model collapse in more realistic scenarios\nwhere the origin of the data (human or synthetic) is unknown. We train a\nmachine-generated text detector and propose an importance sampling approach to\nalleviate model collapse. Our method is validated on two LLM variants (GPT-2\nand SmolLM2) on the open-ended text generation task. We demonstrate that it can\nnot only prevent model collapse but also improve performance when sufficient\nhuman-authored samples are present. We release our code at\nhttps://github.com/GeorgeDrayson/model_collapse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become increasingly prevalent, their\ngenerated outputs are proliferating across the web, risking a future where\nmachine-generated content dilutes human-authored text. Since online data is the\nprimary resource for LLM pre-training, subsequent models could be trained on an\nunknown portion of synthetic samples. This will lead to model collapse, a\ndegenerative process whereby LLMs reinforce their own errors, and ultimately\nyield a declining performance. In this study, we investigate the impact of\ndecoding strategy on model collapse, analysing the characteristics of text at\neach model generation, the similarity to human references, and the resulting\nmodel performance. Using the decoding strategies that lead to the most\nsignificant degradation, we evaluate model collapse in more realistic scenarios\nwhere the origin of the data (human or synthetic) is unknown. We train a\nmachine-generated text detector and propose an importance sampling approach to\nalleviate model collapse. Our method is validated on two LLM variants (GPT-2\nand SmolLM2) on the open-ended text generation task. We demonstrate that it can\nnot only prevent model collapse but also improve performance when sufficient\nhuman-authored samples are present. We release our code at\nhttps://github.com/GeorgeDrayson/model_collapse."
                },
                "authors": [
                    {
                        "name": "George Drayson"
                    },
                    {
                        "name": "Emine Yilmaz"
                    },
                    {
                        "name": "Vasileios Lampos"
                    }
                ],
                "author_detail": {
                    "name": "Vasileios Lampos"
                },
                "author": "Vasileios Lampos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15654v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15654v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18483v1",
                "updated": "2025-04-25T16:47:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    16,
                    47,
                    44,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T16:47:44Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    16,
                    47,
                    44,
                    4,
                    115,
                    0
                ],
                "title": "Investigating Co-Constructive Behavior of Large Language Models in\n  Explanation Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Co-Constructive Behavior of Large Language Models in\n  Explanation Dialogues"
                },
                "summary": "The ability to generate explanations that are understood by explainees is the\nquintessence of explainable artificial intelligence. Since understanding\ndepends on the explainee's background and needs, recent research has focused on\nco-constructive explanation dialogues, where the explainer continuously\nmonitors the explainee's understanding and adapts explanations dynamically. We\ninvestigate the ability of large language models (LLMs) to engage as explainers\nin co-constructive explanation dialogues. In particular, we present a user\nstudy in which explainees interact with LLMs, of which some have been\ninstructed to explain a predefined topic co-constructively. We evaluate the\nexplainees' understanding before and after the dialogue, as well as their\nperception of the LLMs' co-constructive behavior. Our results indicate that\ncurrent LLMs show some co-constructive behaviors, such as asking verification\nquestions, that foster the explainees' engagement and can improve understanding\nof a topic. However, their ability to effectively monitor the current\nunderstanding and scaffold the explanations accordingly remains limited.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to generate explanations that are understood by explainees is the\nquintessence of explainable artificial intelligence. Since understanding\ndepends on the explainee's background and needs, recent research has focused on\nco-constructive explanation dialogues, where the explainer continuously\nmonitors the explainee's understanding and adapts explanations dynamically. We\ninvestigate the ability of large language models (LLMs) to engage as explainers\nin co-constructive explanation dialogues. In particular, we present a user\nstudy in which explainees interact with LLMs, of which some have been\ninstructed to explain a predefined topic co-constructively. We evaluate the\nexplainees' understanding before and after the dialogue, as well as their\nperception of the LLMs' co-constructive behavior. Our results indicate that\ncurrent LLMs show some co-constructive behaviors, such as asking verification\nquestions, that foster the explainees' engagement and can improve understanding\nof a topic. However, their ability to effectively monitor the current\nunderstanding and scaffold the explanations accordingly remains limited."
                },
                "authors": [
                    {
                        "name": "Leandra Fichtel"
                    },
                    {
                        "name": "Maximilian Spliethöver"
                    },
                    {
                        "name": "Eyke Hüllermeier"
                    },
                    {
                        "name": "Patricia Jimenez"
                    },
                    {
                        "name": "Nils Klowait"
                    },
                    {
                        "name": "Stefan Kopp"
                    },
                    {
                        "name": "Axel-Cyrille Ngonga Ngomo"
                    },
                    {
                        "name": "Amelie Robrecht"
                    },
                    {
                        "name": "Ingrid Scharlau"
                    },
                    {
                        "name": "Lutz Terfloth"
                    },
                    {
                        "name": "Anna-Lisa Vollmer"
                    },
                    {
                        "name": "Henning Wachsmuth"
                    }
                ],
                "author_detail": {
                    "name": "Henning Wachsmuth"
                },
                "author": "Henning Wachsmuth",
                "arxiv_comment": "Submitted to the SIGDial Conference 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06018v2",
                "updated": "2025-04-25T16:39:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    16,
                    39,
                    41,
                    4,
                    115,
                    0
                ],
                "published": "2024-11-09T00:35:29Z",
                "published_parsed": [
                    2024,
                    11,
                    9,
                    0,
                    35,
                    29,
                    5,
                    314,
                    0
                ],
                "title": "A Picture is Worth A Thousand Numbers: Enabling LLMs Reason about Time\n  Series via Visualization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Picture is Worth A Thousand Numbers: Enabling LLMs Reason about Time\n  Series via Visualization"
                },
                "summary": "Large language models (LLMs), with demonstrated reasoning abilities across\nmultiple domains, are largely underexplored for time-series reasoning (TsR),\nwhich is ubiquitous in the real world. In this work, we propose TimerBed, the\nfirst comprehensive testbed for evaluating LLMs' TsR performance. Specifically,\nTimerBed includes stratified reasoning patterns with real-world tasks,\ncomprehensive combinations of LLMs and reasoning strategies, and various\nsupervised models as comparison anchors. We perform extensive experiments with\nTimerBed, test multiple current beliefs, and verify the initial failures of\nLLMs in TsR, evidenced by the ineffectiveness of zero shot (ZST) and\nperformance degradation of few shot in-context learning (ICL). Further, we\nidentify one possible root cause: the numerical modeling of data. To address\nthis, we propose a prompt-based solution VL-Time, using visualization-modeled\ndata and language-guided reasoning. Experimental results demonstrate that\nVl-Time enables multimodal LLMs to be non-trivial ZST and powerful ICL\nreasoners for time series, achieving about 140% average performance improvement\nand 99% average token costs reduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), with demonstrated reasoning abilities across\nmultiple domains, are largely underexplored for time-series reasoning (TsR),\nwhich is ubiquitous in the real world. In this work, we propose TimerBed, the\nfirst comprehensive testbed for evaluating LLMs' TsR performance. Specifically,\nTimerBed includes stratified reasoning patterns with real-world tasks,\ncomprehensive combinations of LLMs and reasoning strategies, and various\nsupervised models as comparison anchors. We perform extensive experiments with\nTimerBed, test multiple current beliefs, and verify the initial failures of\nLLMs in TsR, evidenced by the ineffectiveness of zero shot (ZST) and\nperformance degradation of few shot in-context learning (ICL). Further, we\nidentify one possible root cause: the numerical modeling of data. To address\nthis, we propose a prompt-based solution VL-Time, using visualization-modeled\ndata and language-guided reasoning. Experimental results demonstrate that\nVl-Time enables multimodal LLMs to be non-trivial ZST and powerful ICL\nreasoners for time series, achieving about 140% average performance improvement\nand 99% average token costs reduction."
                },
                "authors": [
                    {
                        "name": "Haoxin Liu"
                    },
                    {
                        "name": "Chenghao Liu"
                    },
                    {
                        "name": "B. Aditya Prakash"
                    }
                ],
                "author_detail": {
                    "name": "B. Aditya Prakash"
                },
                "author": "B. Aditya Prakash",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.18403v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.18403v3",
                "updated": "2025-04-25T16:39:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    16,
                    39,
                    40,
                    4,
                    115,
                    0
                ],
                "published": "2024-03-27T09:45:33Z",
                "published_parsed": [
                    2024,
                    3,
                    27,
                    9,
                    45,
                    33,
                    2,
                    87,
                    0
                ],
                "title": "FoC: Figure out the Cryptographic Functions in Stripped Binaries with\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FoC: Figure out the Cryptographic Functions in Stripped Binaries with\n  LLMs"
                },
                "summary": "Analyzing the behavior of cryptographic functions in stripped binaries is a\nchallenging but essential task. Cryptographic algorithms exhibit greater\nlogical complexity compared to typical code, yet their analysis is unavoidable\nin areas such as virus analysis and legacy code inspection. Existing methods\noften rely on data or structural pattern matching, leading to suboptimal\ngeneralizability and suffering from manual work. In this paper, we propose a\nnovel framework called FoC to Figure out the Cryptographic functions in\nstripped binaries. In FoC, we first build a binary large language model\n(FoC-BinLLM) to summarize the semantics of cryptographic functions in natural\nlanguage. The prediction of FoC-BinLLM is insensitive to minor changes, such as\nvulnerability patches. To mitigate it, we further build a binary code\nsimilarity model (FoC-Sim) upon the FoC-BinLLM to create change-sensitive\nrepresentations and use it to retrieve similar implementations of unknown\ncryptographic functions in a database. In addition, we construct a\ncryptographic binary dataset for evaluation and to facilitate further research\nin this domain. And an automated method is devised to create semantic labels\nfor extensive binary functions. Evaluation results demonstrate that FoC-BinLLM\noutperforms ChatGPT by 14.61% on the ROUGE-L score. FoC-Sim outperforms the\nprevious best methods with a 52% higher Recall@1. Furthermore, our method also\nshows practical ability in virus analysis and 1-day vulnerability detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing the behavior of cryptographic functions in stripped binaries is a\nchallenging but essential task. Cryptographic algorithms exhibit greater\nlogical complexity compared to typical code, yet their analysis is unavoidable\nin areas such as virus analysis and legacy code inspection. Existing methods\noften rely on data or structural pattern matching, leading to suboptimal\ngeneralizability and suffering from manual work. In this paper, we propose a\nnovel framework called FoC to Figure out the Cryptographic functions in\nstripped binaries. In FoC, we first build a binary large language model\n(FoC-BinLLM) to summarize the semantics of cryptographic functions in natural\nlanguage. The prediction of FoC-BinLLM is insensitive to minor changes, such as\nvulnerability patches. To mitigate it, we further build a binary code\nsimilarity model (FoC-Sim) upon the FoC-BinLLM to create change-sensitive\nrepresentations and use it to retrieve similar implementations of unknown\ncryptographic functions in a database. In addition, we construct a\ncryptographic binary dataset for evaluation and to facilitate further research\nin this domain. And an automated method is devised to create semantic labels\nfor extensive binary functions. Evaluation results demonstrate that FoC-BinLLM\noutperforms ChatGPT by 14.61% on the ROUGE-L score. FoC-Sim outperforms the\nprevious best methods with a 52% higher Recall@1. Furthermore, our method also\nshows practical ability in virus analysis and 1-day vulnerability detection."
                },
                "authors": [
                    {
                        "name": "Xiuwei Shang"
                    },
                    {
                        "name": "Guoqiang Chen"
                    },
                    {
                        "name": "Shaoyin Cheng"
                    },
                    {
                        "name": "Shikai Guo"
                    },
                    {
                        "name": "Yanming Zhang"
                    },
                    {
                        "name": "Weiming Zhang"
                    },
                    {
                        "name": "Nenghai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Nenghai Yu"
                },
                "author": "Nenghai Yu",
                "arxiv_comment": "38 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.18403v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.18403v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18478v1",
                "updated": "2025-04-25T16:36:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    16,
                    36,
                    27,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T16:36:27Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    16,
                    36,
                    27,
                    4,
                    115,
                    0
                ],
                "title": "Magnetic field orientation dependence of continuous-wave optically\n  detected magnetic resonance with nitrogen-vacancy ensembles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magnetic field orientation dependence of continuous-wave optically\n  detected magnetic resonance with nitrogen-vacancy ensembles"
                },
                "summary": "Continuous-wave optically detected magnetic resonance (CW-ODMR) measurements\nwith nitrogen-vacancy (NV) spins in diamond are used for sensing DC magnetic\nfields from nearby magnetic targets. However, this technique suffers from\nambiguities in the extraction of the magnetic field components when resonances\ndue to different NV orientation classes overlap with each other. Here, we\nperform detailed experimental and theoretical studies of such effects on NV\nensembles experiencing low bias magnetic fields. In particular, through\nsymmetry considerations, we systematically examine the ODMR response of\ndifferent NV orientation classes as a function of the orientation of the\nmagnetic field vector. Our studies are of importance for performing a careful\nand detailed analysis of the ODMR spectra in order to infer the vector magnetic\nfield information. Our results find application in the studies of magnetic\nsamples that require a low applied bias field and also can be potentially\nadapted to defect spins in other solid-state systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous-wave optically detected magnetic resonance (CW-ODMR) measurements\nwith nitrogen-vacancy (NV) spins in diamond are used for sensing DC magnetic\nfields from nearby magnetic targets. However, this technique suffers from\nambiguities in the extraction of the magnetic field components when resonances\ndue to different NV orientation classes overlap with each other. Here, we\nperform detailed experimental and theoretical studies of such effects on NV\nensembles experiencing low bias magnetic fields. In particular, through\nsymmetry considerations, we systematically examine the ODMR response of\ndifferent NV orientation classes as a function of the orientation of the\nmagnetic field vector. Our studies are of importance for performing a careful\nand detailed analysis of the ODMR spectra in order to infer the vector magnetic\nfield information. Our results find application in the studies of magnetic\nsamples that require a low applied bias field and also can be potentially\nadapted to defect spins in other solid-state systems."
                },
                "authors": [
                    {
                        "name": "Pralekh Dubey"
                    },
                    {
                        "name": "Shashank Kumar"
                    },
                    {
                        "name": "Chinmaya Singh"
                    },
                    {
                        "name": "Jemish Naliyapara"
                    },
                    {
                        "name": "Monish A Poojar"
                    },
                    {
                        "name": "Harikrishnan K B"
                    },
                    {
                        "name": "Anshul Poonia"
                    },
                    {
                        "name": "Phani Peddibhotla"
                    }
                ],
                "author_detail": {
                    "name": "Phani Peddibhotla"
                },
                "author": "Phani Peddibhotla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18474v1",
                "updated": "2025-04-25T16:29:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    16,
                    29,
                    45,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T16:29:45Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    16,
                    29,
                    45,
                    4,
                    115,
                    0
                ],
                "title": "Generative Induction of Dialogue Task Schemas with Streaming Refinement\n  and Simulated Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Induction of Dialogue Task Schemas with Streaming Refinement\n  and Simulated Interactions"
                },
                "summary": "In task-oriented dialogue (TOD) systems, Slot Schema Induction (SSI) is\nessential for automatically identifying key information slots from dialogue\ndata without manual intervention. This paper presents a novel state-of-the-art\n(SoTA) approach that formulates SSI as a text generation task, where a language\nmodel incrementally constructs and refines a slot schema over a stream of\ndialogue data. To develop this approach, we present a fully automatic LLM-based\nTOD simulation method that creates data with high-quality state labels for\nnovel task domains. Furthermore, we identify issues in SSI evaluation due to\ndata leakage and poor metric alignment with human judgment. We resolve these by\ncreating new evaluation data using our simulation method with human guidance\nand correction, as well as designing improved evaluation metrics. These\ncontributions establish a foundation for future SSI research and advance the\nSoTA in dialogue understanding and system development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In task-oriented dialogue (TOD) systems, Slot Schema Induction (SSI) is\nessential for automatically identifying key information slots from dialogue\ndata without manual intervention. This paper presents a novel state-of-the-art\n(SoTA) approach that formulates SSI as a text generation task, where a language\nmodel incrementally constructs and refines a slot schema over a stream of\ndialogue data. To develop this approach, we present a fully automatic LLM-based\nTOD simulation method that creates data with high-quality state labels for\nnovel task domains. Furthermore, we identify issues in SSI evaluation due to\ndata leakage and poor metric alignment with human judgment. We resolve these by\ncreating new evaluation data using our simulation method with human guidance\nand correction, as well as designing improved evaluation metrics. These\ncontributions establish a foundation for future SSI research and advance the\nSoTA in dialogue understanding and system development."
                },
                "authors": [
                    {
                        "name": "James D. Finch"
                    },
                    {
                        "name": "Yasasvi Josyula"
                    },
                    {
                        "name": "Jinho D. Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jinho D. Choi"
                },
                "author": "Jinho D. Choi",
                "arxiv_comment": "Accepted (B) to TACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11704v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11704v3",
                "updated": "2025-04-25T16:08:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    16,
                    8,
                    57,
                    4,
                    115,
                    0
                ],
                "published": "2024-12-16T12:26:28Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    26,
                    28,
                    0,
                    351,
                    0
                ],
                "title": "ElChat: Adapting Chat Language Models Using Only Target Unlabeled\n  Language Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ElChat: Adapting Chat Language Models Using Only Target Unlabeled\n  Language Data"
                },
                "summary": "Vocabulary expansion (VE) is the de-facto approach to language adaptation of\nlarge language models (LLMs) by adding new tokens and continuing pre-training\non target data. While this is effective for base models trained on unlabeled\ndata, it poses challenges for chat models trained to follow instructions\nthrough labeled conversation data. Directly adapting the latter with VE on\ntarget unlabeled data may result in forgetting chat abilities. While ideal,\ntarget chat data is often unavailable or costly to create for low-resource\nlanguages, and machine-translated alternatives are not always effective. To\naddress this issue, previous work proposed using a base and chat model from the\nsame family. This method first adapts the base LLM with VE on target unlabeled\ndata and then converts it to a chat model by adding a chat vector (CV) derived\nfrom the weight difference between the source base and chat models. We propose\nElChat, a new language adaptation method for chat LLMs that adapts a chat model\ndirectly on target unlabeled data, without a base model. It elicits chat\nabilities by injecting information from the source chat model. ElChat offers\nmore robust and competitive target language and safety performance while\nachieving superior English, chat, and instruction-following abilities compared\nto CV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vocabulary expansion (VE) is the de-facto approach to language adaptation of\nlarge language models (LLMs) by adding new tokens and continuing pre-training\non target data. While this is effective for base models trained on unlabeled\ndata, it poses challenges for chat models trained to follow instructions\nthrough labeled conversation data. Directly adapting the latter with VE on\ntarget unlabeled data may result in forgetting chat abilities. While ideal,\ntarget chat data is often unavailable or costly to create for low-resource\nlanguages, and machine-translated alternatives are not always effective. To\naddress this issue, previous work proposed using a base and chat model from the\nsame family. This method first adapts the base LLM with VE on target unlabeled\ndata and then converts it to a chat model by adding a chat vector (CV) derived\nfrom the weight difference between the source base and chat models. We propose\nElChat, a new language adaptation method for chat LLMs that adapts a chat model\ndirectly on target unlabeled data, without a base model. It elicits chat\nabilities by injecting information from the source chat model. ElChat offers\nmore robust and competitive target language and safety performance while\nachieving superior English, chat, and instruction-following abilities compared\nto CV."
                },
                "authors": [
                    {
                        "name": "Atsuki Yamaguchi"
                    },
                    {
                        "name": "Terufumi Morishita"
                    },
                    {
                        "name": "Aline Villavicencio"
                    },
                    {
                        "name": "Nikolaos Aletras"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaos Aletras"
                },
                "author": "Nikolaos Aletras",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11704v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11704v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18452v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18452v1",
                "updated": "2025-04-25T16:03:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    16,
                    3,
                    34,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T16:03:34Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    16,
                    3,
                    34,
                    4,
                    115,
                    0
                ],
                "title": "Structured Bayesian Regression Tree Models for Estimating Distributed\n  Lag Effects: The R Package dlmtree",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured Bayesian Regression Tree Models for Estimating Distributed\n  Lag Effects: The R Package dlmtree"
                },
                "summary": "When examining the relationship between an exposure and an outcome, there is\noften a time lag between exposure and the observed effect on the outcome. A\ncommon statistical approach for estimating the relationship between the outcome\nand lagged measurements of exposure is a distributed lag model (DLM). Because\nrepeated measurements are often autocorrelated, the lagged effects are\ntypically constrained to vary smoothly over time. A recent statistical\ndevelopment on the smoothing constraint is a tree structured DLM framework. We\npresent an R package dlmtree, available on CRAN, that integrates tree\nstructured DLM and extensions into a comprehensive software package with\nuser-friendly implementation. A conceptual background on tree structured DLMs\nand demonstration of the fitting process of each model using simulated data are\nprovided. We also demonstrate inference and interpretation using the fitted\nmodels, including summary and visualization. Additionally, a built-in shiny app\nfor heterogeneity analysis is included.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When examining the relationship between an exposure and an outcome, there is\noften a time lag between exposure and the observed effect on the outcome. A\ncommon statistical approach for estimating the relationship between the outcome\nand lagged measurements of exposure is a distributed lag model (DLM). Because\nrepeated measurements are often autocorrelated, the lagged effects are\ntypically constrained to vary smoothly over time. A recent statistical\ndevelopment on the smoothing constraint is a tree structured DLM framework. We\npresent an R package dlmtree, available on CRAN, that integrates tree\nstructured DLM and extensions into a comprehensive software package with\nuser-friendly implementation. A conceptual background on tree structured DLMs\nand demonstration of the fitting process of each model using simulated data are\nprovided. We also demonstrate inference and interpretation using the fitted\nmodels, including summary and visualization. Additionally, a built-in shiny app\nfor heterogeneity analysis is included."
                },
                "authors": [
                    {
                        "name": "Seongwon Im"
                    },
                    {
                        "name": "Ander Wilson"
                    },
                    {
                        "name": "Daniel Mork"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Mork"
                },
                "author": "Daniel Mork",
                "arxiv_comment": "22 pages, 11 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18452v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18452v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09537v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09537v2",
                "updated": "2025-04-25T15:59:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    59,
                    19,
                    4,
                    115,
                    0
                ],
                "published": "2024-08-18T16:44:41Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    16,
                    44,
                    41,
                    6,
                    231,
                    0
                ],
                "title": "Efficient Budget Allocation for Large-Scale LLM-Enabled Virtual\n  Screening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Budget Allocation for Large-Scale LLM-Enabled Virtual\n  Screening"
                },
                "summary": "Screening tasks that aim to identify a small subset of top alternatives from\na large pool are common in business decision-making processes. These tasks\noften require substantial human effort to evaluate each alternative's\nperformance, making them time-consuming and costly. Motivated by recent\nadvances in large language models (LLMs), particularly their ability to\ngenerate outputs that align well with human evaluations, we consider an\nLLM-as-human-evaluator approach for conducting screening virtually, thereby\nreducing the cost burden. To achieve scalability and cost-effectiveness in\nvirtual screening, we identify that the stochastic nature of LLM outputs and\ntheir cost structure necessitate efficient budget allocation across all\nalternatives. To address this, we propose using a top-$m$ greedy evaluation\nmechanism, a simple yet effective approach that keeps evaluating the current\ntop-$m$ alternatives, and design the explore-first top-$m$ greedy (EFG-$m$)\nalgorithm. We prove that EFG-$m$ is both sample-optimal and consistent in\nlarge-scale virtual screening. Surprisingly, we also uncover a bonus ranking\neffect, where the algorithm naturally induces an indifference-based ranking\nwithin the selected subset. To further enhance practicality, we design a suite\nof algorithm variants to improve screening performance and computational\nefficiency. Numerical experiments validate our results and demonstrate the\neffectiveness of our algorithms. Lastly, we conduct a case study on LLM-based\nvirtual screening. The study shows that while LLMs alone may not provide\nmeaningful screening and ranking results when directly queried, integrating\nthem with our sample-optimal algorithms unlocks their potential for\ncost-effective, large-scale virtual screening.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Screening tasks that aim to identify a small subset of top alternatives from\na large pool are common in business decision-making processes. These tasks\noften require substantial human effort to evaluate each alternative's\nperformance, making them time-consuming and costly. Motivated by recent\nadvances in large language models (LLMs), particularly their ability to\ngenerate outputs that align well with human evaluations, we consider an\nLLM-as-human-evaluator approach for conducting screening virtually, thereby\nreducing the cost burden. To achieve scalability and cost-effectiveness in\nvirtual screening, we identify that the stochastic nature of LLM outputs and\ntheir cost structure necessitate efficient budget allocation across all\nalternatives. To address this, we propose using a top-$m$ greedy evaluation\nmechanism, a simple yet effective approach that keeps evaluating the current\ntop-$m$ alternatives, and design the explore-first top-$m$ greedy (EFG-$m$)\nalgorithm. We prove that EFG-$m$ is both sample-optimal and consistent in\nlarge-scale virtual screening. Surprisingly, we also uncover a bonus ranking\neffect, where the algorithm naturally induces an indifference-based ranking\nwithin the selected subset. To further enhance practicality, we design a suite\nof algorithm variants to improve screening performance and computational\nefficiency. Numerical experiments validate our results and demonstrate the\neffectiveness of our algorithms. Lastly, we conduct a case study on LLM-based\nvirtual screening. The study shows that while LLMs alone may not provide\nmeaningful screening and ranking results when directly queried, integrating\nthem with our sample-optimal algorithms unlocks their potential for\ncost-effective, large-scale virtual screening."
                },
                "authors": [
                    {
                        "name": "Zaile Li"
                    },
                    {
                        "name": "Weiwei Fan"
                    },
                    {
                        "name": "L. Jeff Hong"
                    }
                ],
                "author_detail": {
                    "name": "L. Jeff Hong"
                },
                "author": "L. Jeff Hong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09537v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09537v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18443v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18443v1",
                "updated": "2025-04-25T15:54:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    54,
                    9,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:54:09Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    54,
                    9,
                    4,
                    115,
                    0
                ],
                "title": "Pseudo-Boolean Proof Logging for Optimal Classical Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pseudo-Boolean Proof Logging for Optimal Classical Planning"
                },
                "summary": "We introduce lower-bound certificates for classical planning tasks, which can\nbe used to prove the unsolvability of a task or the optimality of a plan in a\nway that can be verified by an independent third party. We describe a general\nframework for generating lower-bound certificates based on pseudo-Boolean\nconstraints, which is agnostic to the planning algorithm used.\n  As a case study, we show how to modify the $A^{*}$ algorithm to produce\nproofs of optimality with modest overhead, using pattern database heuristics\nand $h^\\textit{max}$ as concrete examples. The same proof logging approach\nworks for any heuristic whose inferences can be efficiently expressed as\nreasoning over pseudo-Boolean constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce lower-bound certificates for classical planning tasks, which can\nbe used to prove the unsolvability of a task or the optimality of a plan in a\nway that can be verified by an independent third party. We describe a general\nframework for generating lower-bound certificates based on pseudo-Boolean\nconstraints, which is agnostic to the planning algorithm used.\n  As a case study, we show how to modify the $A^{*}$ algorithm to produce\nproofs of optimality with modest overhead, using pattern database heuristics\nand $h^\\textit{max}$ as concrete examples. The same proof logging approach\nworks for any heuristic whose inferences can be efficiently expressed as\nreasoning over pseudo-Boolean constraints."
                },
                "authors": [
                    {
                        "name": "Simon Dold"
                    },
                    {
                        "name": "Malte Helmert"
                    },
                    {
                        "name": "Jakob Nordström"
                    },
                    {
                        "name": "Gabriele Röger"
                    },
                    {
                        "name": "Tanja Schindler"
                    }
                ],
                "author_detail": {
                    "name": "Tanja Schindler"
                },
                "author": "Tanja Schindler",
                "arxiv_comment": "35th International Conference on Automated Planning and Scheduling\n  (ICAPS'2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18443v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04476v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04476v3",
                "updated": "2025-04-25T15:47:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    47,
                    33,
                    4,
                    115,
                    0
                ],
                "published": "2024-11-19T15:40:16Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    15,
                    40,
                    16,
                    1,
                    324,
                    0
                ],
                "title": "The Moral Mind(s) of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Moral Mind(s) of Large Language Models"
                },
                "summary": "As large language models (LLMs) increasingly participate in tasks with\nethical and societal stakes, a critical question arises: do they exhibit an\nemergent \"moral mind\" - a consistent structure of moral preferences guiding\ntheir decisions - and to what extent is this structure shared across models? To\ninvestigate this, we applied tools from revealed preference theory to nearly 40\nleading LLMs, presenting each with many structured moral dilemmas spanning five\nfoundational dimensions of ethical reasoning. Using a probabilistic rationality\ntest, we found that at least one model from each major provider exhibited\nbehavior consistent with approximately stable moral preferences, acting as if\nguided by an underlying utility function. We then estimated these utility\nfunctions and found that most models cluster around neutral moral stances. To\nfurther characterize heterogeneity, we employed a non-parametric permutation\napproach, constructing a probabilistic similarity network based on revealed\npreference patterns. The results reveal a shared core in LLMs' moral reasoning,\nbut also meaningful variation: some models show flexible reasoning across\nperspectives, while others adhere to more rigid ethical profiles. These\nfindings provide a new empirical lens for evaluating moral consistency in LLMs\nand offer a framework for benchmarking ethical alignment across AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) increasingly participate in tasks with\nethical and societal stakes, a critical question arises: do they exhibit an\nemergent \"moral mind\" - a consistent structure of moral preferences guiding\ntheir decisions - and to what extent is this structure shared across models? To\ninvestigate this, we applied tools from revealed preference theory to nearly 40\nleading LLMs, presenting each with many structured moral dilemmas spanning five\nfoundational dimensions of ethical reasoning. Using a probabilistic rationality\ntest, we found that at least one model from each major provider exhibited\nbehavior consistent with approximately stable moral preferences, acting as if\nguided by an underlying utility function. We then estimated these utility\nfunctions and found that most models cluster around neutral moral stances. To\nfurther characterize heterogeneity, we employed a non-parametric permutation\napproach, constructing a probabilistic similarity network based on revealed\npreference patterns. The results reveal a shared core in LLMs' moral reasoning,\nbut also meaningful variation: some models show flexible reasoning across\nperspectives, while others adhere to more rigid ethical profiles. These\nfindings provide a new empirical lens for evaluating moral consistency in LLMs\nand offer a framework for benchmarking ethical alignment across AI systems."
                },
                "authors": [
                    {
                        "name": "Avner Seror"
                    }
                ],
                "author_detail": {
                    "name": "Avner Seror"
                },
                "author": "Avner Seror",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04476v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04476v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18435v1",
                "updated": "2025-04-25T15:45:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    45,
                    54,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:45:54Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    45,
                    54,
                    4,
                    115,
                    0
                ],
                "title": "Robust Binding Energy Distribution Sampling on Amorphous Solid Water\n  Models. Method testing and validation with NH3, CO and CH4",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Binding Energy Distribution Sampling on Amorphous Solid Water\n  Models. Method testing and validation with NH3, CO and CH4"
                },
                "summary": "This work aims to develop a method based on a structurally reliable ice model\nand a statistically and physico-chemically robust approach for BE distribution\ninference, with the aim to be applicable to various relevant interstellar\nspecies. A multiscale computational approach is presented, with a Molecular\nDynamics (MD) Heat & Quench protocol for the amorphous water ice model, and an\nONIOM(B3LYP-D3(BJ)/6-311+G**:GFN2-xtb) scheme for the BE inference, with a\nprime emphasis onto the BE/real system size convergence. The sampling of the\nbinding configurations is twofold, exploring both regularly spaced binding\nsites, as well as various adsorbate-to-substrate orientations on each locally\ndistinct site. This second source of BE diversity accounts for the local\nroughness of the potential energy landscape of the substrate. Three different\nadsorbate test cases are considered, i.e. NH3, CO and CH4, owing to their\nsignificance in dust icy mantles, and their distinct binding behavior with\nwater ices. The BE distributions for NH3, CO and CH4 have been inferred, with\nconverged statistics. The distribution for NH3 is better represented by a\ndouble Gaussian component profile. Three starting adsorbate orientations per\nsite are required to reach convergence for both Gaussian components of NH3,\nwhile 2 orientations are sufficient for CO, and one unique for CH4 (symmetric).\nFurther geometrical and molecular surrounding insights have been provided.\nThese results encompass previously reported results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work aims to develop a method based on a structurally reliable ice model\nand a statistically and physico-chemically robust approach for BE distribution\ninference, with the aim to be applicable to various relevant interstellar\nspecies. A multiscale computational approach is presented, with a Molecular\nDynamics (MD) Heat & Quench protocol for the amorphous water ice model, and an\nONIOM(B3LYP-D3(BJ)/6-311+G**:GFN2-xtb) scheme for the BE inference, with a\nprime emphasis onto the BE/real system size convergence. The sampling of the\nbinding configurations is twofold, exploring both regularly spaced binding\nsites, as well as various adsorbate-to-substrate orientations on each locally\ndistinct site. This second source of BE diversity accounts for the local\nroughness of the potential energy landscape of the substrate. Three different\nadsorbate test cases are considered, i.e. NH3, CO and CH4, owing to their\nsignificance in dust icy mantles, and their distinct binding behavior with\nwater ices. The BE distributions for NH3, CO and CH4 have been inferred, with\nconverged statistics. The distribution for NH3 is better represented by a\ndouble Gaussian component profile. Three starting adsorbate orientations per\nsite are required to reach convergence for both Gaussian components of NH3,\nwhile 2 orientations are sufficient for CO, and one unique for CH4 (symmetric).\nFurther geometrical and molecular surrounding insights have been provided.\nThese results encompass previously reported results."
                },
                "authors": [
                    {
                        "name": "Maria Groyne"
                    },
                    {
                        "name": "Benoît Champagne"
                    },
                    {
                        "name": "Cedric Baijot"
                    },
                    {
                        "name": "Michaël De Becker"
                    }
                ],
                "author_detail": {
                    "name": "Michaël De Becker"
                },
                "author": "Michaël De Becker",
                "arxiv_comment": "Accepted for publication in A&A 22 pages, 27 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18428v1",
                "updated": "2025-04-25T15:39:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    39,
                    4,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:39:04Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    39,
                    4,
                    4,
                    115,
                    0
                ],
                "title": "PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts"
                },
                "summary": "In this paper, we introduce PolyMath, a multilingual mathematical reasoning\nbenchmark covering 18 languages and 4 easy-to-hard difficulty levels. Our\nbenchmark ensures difficulty comprehensiveness, language diversity, and\nhigh-quality translation, making it a highly discriminative multilingual\nmathematical benchmark in the era of reasoning LLMs. We conduct a comprehensive\nevaluation for advanced LLMs and find that even Deepseek-R1-671B and\nQwen-QwQ-32B, achieve only 43.4 and 41.8 benchmark scores, with less than 30%\naccuracy under the highest level. From a language perspective, our benchmark\nreveals several key challenges of LLMs in multilingual reasoning: (1) Reasoning\nperformance varies widely across languages for current LLMs; (2) Input-output\nlanguage consistency is low in reasoning LLMs and may be correlated with\nperformance; (3) The thinking length differs significantly by language for\ncurrent LLMs. Additionally, we demonstrate that controlling the output language\nin the instructions has the potential to affect reasoning performance,\nespecially for some low-resource languages, suggesting a promising direction\nfor improving multilingual capabilities in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce PolyMath, a multilingual mathematical reasoning\nbenchmark covering 18 languages and 4 easy-to-hard difficulty levels. Our\nbenchmark ensures difficulty comprehensiveness, language diversity, and\nhigh-quality translation, making it a highly discriminative multilingual\nmathematical benchmark in the era of reasoning LLMs. We conduct a comprehensive\nevaluation for advanced LLMs and find that even Deepseek-R1-671B and\nQwen-QwQ-32B, achieve only 43.4 and 41.8 benchmark scores, with less than 30%\naccuracy under the highest level. From a language perspective, our benchmark\nreveals several key challenges of LLMs in multilingual reasoning: (1) Reasoning\nperformance varies widely across languages for current LLMs; (2) Input-output\nlanguage consistency is low in reasoning LLMs and may be correlated with\nperformance; (3) The thinking length differs significantly by language for\ncurrent LLMs. Additionally, we demonstrate that controlling the output language\nin the instructions has the potential to affect reasoning performance,\nespecially for some low-resource languages, suggesting a promising direction\nfor improving multilingual capabilities in LLMs."
                },
                "authors": [
                    {
                        "name": "Yiming Wang"
                    },
                    {
                        "name": "Pei Zhang"
                    },
                    {
                        "name": "Jialong Tang"
                    },
                    {
                        "name": "Haoran Wei"
                    },
                    {
                        "name": "Baosong Yang"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Chenshu Sun"
                    },
                    {
                        "name": "Feitong Sun"
                    },
                    {
                        "name": "Jiran Zhang"
                    },
                    {
                        "name": "Junxuan Wu"
                    },
                    {
                        "name": "Qiqian Cang"
                    },
                    {
                        "name": "Yichang Zhang"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17404v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17404v2",
                "updated": "2025-04-25T15:32:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    32,
                    41,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-24T09:53:49Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    9,
                    53,
                    49,
                    3,
                    114,
                    0
                ],
                "title": "Redefining Superalignment: From Weak-to-Strong Alignment to Human-AI\n  Co-Alignment to Sustainable Symbiotic Society",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Redefining Superalignment: From Weak-to-Strong Alignment to Human-AI\n  Co-Alignment to Sustainable Symbiotic Society"
                },
                "summary": "Artificial Intelligence (AI) systems are becoming increasingly powerful and\nautonomous, and may progress to surpass human intelligence levels, namely\nArtificial Superintelligence (ASI). During the progression from AI to ASI, it\nmay exceed human control, violate human values, and even lead to irreversible\ncatastrophic consequences in extreme cases. This gives rise to a pressing issue\nthat needs to be addressed: superalignment, ensuring that AI systems much\nsmarter than humans, remain aligned with human (compatible) intentions and\nvalues. Existing scalable oversight and weak-to-strong generalization methods\nmay prove substantially infeasible and inadequate when facing ASI. We must\nexplore safer and more pluralistic frameworks and approaches for\nsuperalignment. In this paper, we redefine superalignment as the human-AI\nco-alignment towards a sustainable symbiotic society, and highlight a framework\nthat integrates external oversight and intrinsic proactive alignment. External\noversight superalignment should be grounded in human-centered ultimate\ndecision, supplemented by interpretable automated evaluation and correction, to\nachieve continuous alignment with humanity's evolving values. Intrinsic\nproactive superalignment is rooted in a profound understanding of the Self,\nothers, and society, integrating self-awareness, self-reflection, and empathy\nto spontaneously infer human intentions, distinguishing good from evil and\nproactively considering human well-being, ultimately attaining human-AI\nco-alignment through iterative interaction. The integration of\nexternally-driven oversight with intrinsically-driven proactive alignment\nempowers sustainable symbiotic societies through human-AI co-alignment, paving\nthe way for achieving safe and beneficial AGI and ASI for good, for human, and\nfor a symbiotic ecology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) systems are becoming increasingly powerful and\nautonomous, and may progress to surpass human intelligence levels, namely\nArtificial Superintelligence (ASI). During the progression from AI to ASI, it\nmay exceed human control, violate human values, and even lead to irreversible\ncatastrophic consequences in extreme cases. This gives rise to a pressing issue\nthat needs to be addressed: superalignment, ensuring that AI systems much\nsmarter than humans, remain aligned with human (compatible) intentions and\nvalues. Existing scalable oversight and weak-to-strong generalization methods\nmay prove substantially infeasible and inadequate when facing ASI. We must\nexplore safer and more pluralistic frameworks and approaches for\nsuperalignment. In this paper, we redefine superalignment as the human-AI\nco-alignment towards a sustainable symbiotic society, and highlight a framework\nthat integrates external oversight and intrinsic proactive alignment. External\noversight superalignment should be grounded in human-centered ultimate\ndecision, supplemented by interpretable automated evaluation and correction, to\nachieve continuous alignment with humanity's evolving values. Intrinsic\nproactive superalignment is rooted in a profound understanding of the Self,\nothers, and society, integrating self-awareness, self-reflection, and empathy\nto spontaneously infer human intentions, distinguishing good from evil and\nproactively considering human well-being, ultimately attaining human-AI\nco-alignment through iterative interaction. The integration of\nexternally-driven oversight with intrinsically-driven proactive alignment\nempowers sustainable symbiotic societies through human-AI co-alignment, paving\nthe way for achieving safe and beneficial AGI and ASI for good, for human, and\nfor a symbiotic ecology."
                },
                "authors": [
                    {
                        "name": "Yi Zeng"
                    },
                    {
                        "name": "Feifei Zhao"
                    },
                    {
                        "name": "Yuwei Wang"
                    },
                    {
                        "name": "Enmeng Lu"
                    },
                    {
                        "name": "Yaodong Yang"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Chao Liu"
                    },
                    {
                        "name": "Yitao Liang"
                    },
                    {
                        "name": "Dongcheng Zhao"
                    },
                    {
                        "name": "Bing Han"
                    },
                    {
                        "name": "Haibo Tong"
                    },
                    {
                        "name": "Yao Liang"
                    },
                    {
                        "name": "Dongqi Liang"
                    },
                    {
                        "name": "Kang Sun"
                    },
                    {
                        "name": "Boyuan Chen"
                    },
                    {
                        "name": "Jinyu Fan"
                    }
                ],
                "author_detail": {
                    "name": "Jinyu Fan"
                },
                "author": "Jinyu Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17404v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17404v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18425v1",
                "updated": "2025-04-25T15:31:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    31,
                    46,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:31:46Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    31,
                    46,
                    4,
                    115,
                    0
                ],
                "title": "Kimi-Audio Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kimi-Audio Technical Report"
                },
                "summary": "We present Kimi-Audio, an open-source audio foundation model that excels in\naudio understanding, generation, and conversation. We detail the practices in\nbuilding Kimi-Audio, including model architecture, data curation, training\nrecipe, inference deployment, and evaluation. Specifically, we leverage a\n12.5Hz audio tokenizer, design a novel LLM-based architecture with continuous\nfeatures as input and discrete tokens as output, and develop a chunk-wise\nstreaming detokenizer based on flow matching. We curate a pre-training dataset\nthat consists of more than 13 million hours of audio data covering a wide range\nof modalities including speech, sound, and music, and build a pipeline to\nconstruct high-quality and diverse post-training data. Initialized from a\npre-trained LLM, Kimi-Audio is continual pre-trained on both audio and text\ndata with several carefully designed tasks, and then fine-tuned to support a\ndiverse of audio-related tasks. Extensive evaluation shows that Kimi-Audio\nachieves state-of-the-art performance on a range of audio benchmarks including\nspeech recognition, audio understanding, audio question answering, and speech\nconversation. We release the codes, model checkpoints, as well as the\nevaluation toolkits in https://github.com/MoonshotAI/Kimi-Audio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Kimi-Audio, an open-source audio foundation model that excels in\naudio understanding, generation, and conversation. We detail the practices in\nbuilding Kimi-Audio, including model architecture, data curation, training\nrecipe, inference deployment, and evaluation. Specifically, we leverage a\n12.5Hz audio tokenizer, design a novel LLM-based architecture with continuous\nfeatures as input and discrete tokens as output, and develop a chunk-wise\nstreaming detokenizer based on flow matching. We curate a pre-training dataset\nthat consists of more than 13 million hours of audio data covering a wide range\nof modalities including speech, sound, and music, and build a pipeline to\nconstruct high-quality and diverse post-training data. Initialized from a\npre-trained LLM, Kimi-Audio is continual pre-trained on both audio and text\ndata with several carefully designed tasks, and then fine-tuned to support a\ndiverse of audio-related tasks. Extensive evaluation shows that Kimi-Audio\nachieves state-of-the-art performance on a range of audio benchmarks including\nspeech recognition, audio understanding, audio question answering, and speech\nconversation. We release the codes, model checkpoints, as well as the\nevaluation toolkits in https://github.com/MoonshotAI/Kimi-Audio."
                },
                "authors": [
                    {
                        "name": "KimiTeam"
                    },
                    {
                        "name": "Ding Ding"
                    },
                    {
                        "name": "Zeqian Ju"
                    },
                    {
                        "name": "Yichong Leng"
                    },
                    {
                        "name": "Songxiang Liu"
                    },
                    {
                        "name": "Tong Liu"
                    },
                    {
                        "name": "Zeyu Shang"
                    },
                    {
                        "name": "Kai Shen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Xu Tan"
                    },
                    {
                        "name": "Heyi Tang"
                    },
                    {
                        "name": "Zhengtao Wang"
                    },
                    {
                        "name": "Chu Wei"
                    },
                    {
                        "name": "Yifei Xin"
                    },
                    {
                        "name": "Xinran Xu"
                    },
                    {
                        "name": "Jianwei Yu"
                    },
                    {
                        "name": "Yutao Zhang"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Y. Charles"
                    },
                    {
                        "name": "Jun Chen"
                    },
                    {
                        "name": "Yanru Chen"
                    },
                    {
                        "name": "Yulun Du"
                    },
                    {
                        "name": "Weiran He"
                    },
                    {
                        "name": "Zhenxing Hu"
                    },
                    {
                        "name": "Guokun Lai"
                    },
                    {
                        "name": "Qingcheng Li"
                    },
                    {
                        "name": "Yangyang Liu"
                    },
                    {
                        "name": "Weidong Sun"
                    },
                    {
                        "name": "Jianzhou Wang"
                    },
                    {
                        "name": "Yuzhi Wang"
                    },
                    {
                        "name": "Yuefeng Wu"
                    },
                    {
                        "name": "Yuxin Wu"
                    },
                    {
                        "name": "Dongchao Yang"
                    },
                    {
                        "name": "Hao Yang"
                    },
                    {
                        "name": "Ying Yang"
                    },
                    {
                        "name": "Zhilin Yang"
                    },
                    {
                        "name": "Aoxiong Yin"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Yutong Zhang"
                    },
                    {
                        "name": "Zaida Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zaida Zhou"
                },
                "author": "Zaida Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18423v1",
                "updated": "2025-04-25T15:30:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    30,
                    40,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:30:40Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    30,
                    40,
                    4,
                    115,
                    0
                ],
                "title": "LLMpatronous: Harnessing the Power of LLMs For Vulnerability Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMpatronous: Harnessing the Power of LLMs For Vulnerability Detection"
                },
                "summary": "Despite the transformative impact of Artificial Intelligence (AI) across\nvarious sectors, cyber security continues to rely on traditional static and\ndynamic analysis tools, hampered by high false positive rates and superficial\ncode comprehension. While generative AI offers promising automation\ncapabilities for software development, leveraging Large Language Models (LLMs)\nfor vulnerability detection presents unique challenges. This paper explores the\npotential and limitations of LLMs in identifying vulnerabilities, acknowledging\ninherent weaknesses such as hallucinations, limited context length, and\nknowledge cut-offs. Previous attempts employing machine learning models for\nvulnerability detection have proven ineffective due to limited real-world\napplicability, feature engineering challenges, lack of contextual\nunderstanding, and the complexities of training models to keep pace with the\nevolving threat landscape. Therefore, we propose a robust AI-driven approach\nfocused on mitigating these limitations and ensuring the quality and\nreliability of LLM based vulnerability detection. Through innovative\nmethodologies combining Retrieval-Augmented Generation (RAG) and\nMixtureof-Agents (MoA), this research seeks to leverage the strengths of LLMs\nwhile addressing their weaknesses, ultimately paving the way for dependable and\nefficient AI-powered solutions in securing the ever-evolving software\nlandscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the transformative impact of Artificial Intelligence (AI) across\nvarious sectors, cyber security continues to rely on traditional static and\ndynamic analysis tools, hampered by high false positive rates and superficial\ncode comprehension. While generative AI offers promising automation\ncapabilities for software development, leveraging Large Language Models (LLMs)\nfor vulnerability detection presents unique challenges. This paper explores the\npotential and limitations of LLMs in identifying vulnerabilities, acknowledging\ninherent weaknesses such as hallucinations, limited context length, and\nknowledge cut-offs. Previous attempts employing machine learning models for\nvulnerability detection have proven ineffective due to limited real-world\napplicability, feature engineering challenges, lack of contextual\nunderstanding, and the complexities of training models to keep pace with the\nevolving threat landscape. Therefore, we propose a robust AI-driven approach\nfocused on mitigating these limitations and ensuring the quality and\nreliability of LLM based vulnerability detection. Through innovative\nmethodologies combining Retrieval-Augmented Generation (RAG) and\nMixtureof-Agents (MoA), this research seeks to leverage the strengths of LLMs\nwhile addressing their weaknesses, ultimately paving the way for dependable and\nefficient AI-powered solutions in securing the ever-evolving software\nlandscape."
                },
                "authors": [
                    {
                        "name": "Rajesh Yarra"
                    }
                ],
                "author_detail": {
                    "name": "Rajesh Yarra"
                },
                "author": "Rajesh Yarra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16005v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16005v3",
                "updated": "2025-04-25T15:27:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    27,
                    15,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-22T16:14:31Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    16,
                    14,
                    31,
                    1,
                    112,
                    0
                ],
                "title": "CAPO: Cost-Aware Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAPO: Cost-Aware Prompt Optimization"
                },
                "summary": "Large language models (LLMs) have revolutionized natural language processing\nby solving a wide range of tasks simply guided by a prompt. Yet their\nperformance is highly sensitive to prompt formulation. While automated prompt\noptimization addresses this challenge by finding optimal prompts, current\nmethods require a substantial number of LLM calls and input tokens, making\nprompt optimization expensive. We introduce CAPO (Cost-Aware Prompt\nOptimization), an algorithm that enhances prompt optimization efficiency by\nintegrating AutoML techniques. CAPO is an evolutionary approach with LLMs as\noperators, incorporating racing to save evaluations and multi-objective\noptimization to balance performance with prompt length. It jointly optimizes\ninstructions and few-shot examples while leveraging task descriptions for\nimproved robustness. Our extensive experiments across diverse datasets and LLMs\ndemonstrate that CAPO outperforms state-of-the-art discrete prompt optimization\nmethods in 11/15 cases with improvements up to 21%p. Our algorithm achieves\nbetter performances already with smaller budgets, saves evaluations through\nracing, and decreases average prompt length via a length penalty, making it\nboth cost-efficient and cost-aware. Even without few-shot examples, CAPO\noutperforms its competitors and generally remains robust to initial prompts.\nCAPO represents an important step toward making prompt optimization more\npowerful and accessible by improving cost-efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized natural language processing\nby solving a wide range of tasks simply guided by a prompt. Yet their\nperformance is highly sensitive to prompt formulation. While automated prompt\noptimization addresses this challenge by finding optimal prompts, current\nmethods require a substantial number of LLM calls and input tokens, making\nprompt optimization expensive. We introduce CAPO (Cost-Aware Prompt\nOptimization), an algorithm that enhances prompt optimization efficiency by\nintegrating AutoML techniques. CAPO is an evolutionary approach with LLMs as\noperators, incorporating racing to save evaluations and multi-objective\noptimization to balance performance with prompt length. It jointly optimizes\ninstructions and few-shot examples while leveraging task descriptions for\nimproved robustness. Our extensive experiments across diverse datasets and LLMs\ndemonstrate that CAPO outperforms state-of-the-art discrete prompt optimization\nmethods in 11/15 cases with improvements up to 21%p. Our algorithm achieves\nbetter performances already with smaller budgets, saves evaluations through\nracing, and decreases average prompt length via a length penalty, making it\nboth cost-efficient and cost-aware. Even without few-shot examples, CAPO\noutperforms its competitors and generally remains robust to initial prompts.\nCAPO represents an important step toward making prompt optimization more\npowerful and accessible by improving cost-efficiency."
                },
                "authors": [
                    {
                        "name": "Tom Zehle"
                    },
                    {
                        "name": "Moritz Schlager"
                    },
                    {
                        "name": "Timo Heiß"
                    },
                    {
                        "name": "Matthias Feurer"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Feurer"
                },
                "author": "Matthias Feurer",
                "arxiv_comment": "Submitted to AutoML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16005v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16005v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09810v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09810v2",
                "updated": "2025-04-25T15:24:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    24,
                    44,
                    4,
                    115,
                    0
                ],
                "published": "2025-01-16T19:46:13Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    19,
                    46,
                    13,
                    3,
                    16,
                    0
                ],
                "title": "The cosmic evolution of FRBs inferred from CHIME/FRB Baseband Catalog 1",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cosmic evolution of FRBs inferred from CHIME/FRB Baseband Catalog 1"
                },
                "summary": "Redshift and luminosity distributions are essential for understanding the\ncosmic evolution of extragalactic objects and phenomena, such as galaxies,\ngamma-ray bursts, and fast radio bursts (FRBs). For FRBs, these distributions\nare primarily estimated using the fluence and the Dispersion Measure (DM).\nCalibrating their joint distribution has been challenging due to a lack of\naccurate fluences in the intensity data of the CHIME/FRB survey. Using the\nbaseband update of CHIME/FRB Catalog 1, we calibrate the 2D fluence-DM\ndistribution for the first time. We find the energy distribution is described\nwell by a Schechter function with power-law slope of $-1.94^{+0.14}_{-0.12}$.\nTesting two types of redshift evolution models suggests a likely combination of\nyoung and old formation channels. $31^{+31}_{-21}$% of FRB sources may track\nstar formation, or correspondingly, FRB sources may have delay times of\n$1.94^{+1.54}_{-1.31}$ Gyr. A pure star formation tracking population is\nexcluded by only one model at $> 2\\sigma$ confidence. An updated cosmic star\nformation rate density evolution up to redshift 14 is constrained by compiling\nresults from several JWST studies. The furthest FRB detection with planned\nradio facilities is expected to be at $z \\approx 5$. A radio telescope\noperating at 200 MHz with a system-equivalent flux density of $\\leq 0.07$ Jy\n(equivalent to a detection threshold of 1 mJy ms) and instantaneous sky\ncoverage of $\\gtrsim 400$ square degrees should be able to detect\n$630^{+730}_{-485}$ FRBs year$^{-1}$ at $z \\gtrsim 6$ and $53^{+83}_{-43}$ FRBs\nyear$^{-1}$ at $z\\gtrsim 8$, which is sufficient to differentiate between\nreionization histories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Redshift and luminosity distributions are essential for understanding the\ncosmic evolution of extragalactic objects and phenomena, such as galaxies,\ngamma-ray bursts, and fast radio bursts (FRBs). For FRBs, these distributions\nare primarily estimated using the fluence and the Dispersion Measure (DM).\nCalibrating their joint distribution has been challenging due to a lack of\naccurate fluences in the intensity data of the CHIME/FRB survey. Using the\nbaseband update of CHIME/FRB Catalog 1, we calibrate the 2D fluence-DM\ndistribution for the first time. We find the energy distribution is described\nwell by a Schechter function with power-law slope of $-1.94^{+0.14}_{-0.12}$.\nTesting two types of redshift evolution models suggests a likely combination of\nyoung and old formation channels. $31^{+31}_{-21}$% of FRB sources may track\nstar formation, or correspondingly, FRB sources may have delay times of\n$1.94^{+1.54}_{-1.31}$ Gyr. A pure star formation tracking population is\nexcluded by only one model at $> 2\\sigma$ confidence. An updated cosmic star\nformation rate density evolution up to redshift 14 is constrained by compiling\nresults from several JWST studies. The furthest FRB detection with planned\nradio facilities is expected to be at $z \\approx 5$. A radio telescope\noperating at 200 MHz with a system-equivalent flux density of $\\leq 0.07$ Jy\n(equivalent to a detection threshold of 1 mJy ms) and instantaneous sky\ncoverage of $\\gtrsim 400$ square degrees should be able to detect\n$630^{+730}_{-485}$ FRBs year$^{-1}$ at $z \\gtrsim 6$ and $53^{+83}_{-43}$ FRBs\nyear$^{-1}$ at $z\\gtrsim 8$, which is sufficient to differentiate between\nreionization histories."
                },
                "authors": [
                    {
                        "name": "Om Gupta"
                    },
                    {
                        "name": "Paz Beniamini"
                    },
                    {
                        "name": "Pawan Kumar"
                    },
                    {
                        "name": "Steven L. Finkelstein"
                    }
                ],
                "author_detail": {
                    "name": "Steven L. Finkelstein"
                },
                "author": "Steven L. Finkelstein",
                "arxiv_comment": "Accepted in ApJ. Data & visualization scripts available:\n  https://github.com/omguptaup/frb-cosmic-evol. 30 pages, 11 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09810v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09810v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08907v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08907v2",
                "updated": "2025-04-25T15:21:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    21,
                    54,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-11T18:19:59Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    18,
                    19,
                    59,
                    4,
                    101,
                    0
                ],
                "title": "Spatial Audio Processing with Large Language Model on Wearable Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial Audio Processing with Large Language Model on Wearable Devices"
                },
                "summary": "Integrating spatial context into large language models (LLMs) has the\npotential to revolutionize human-computer interaction, particularly in wearable\ndevices. In this work, we present a novel system architecture that incorporates\nspatial speech understanding into LLMs, enabling contextually aware and\nadaptive applications for wearable technologies. Our approach leverages\nmicrostructure-based spatial sensing to extract precise Direction of Arrival\n(DoA) information using a monaural microphone. To address the lack of existing\ndataset for microstructure-assisted speech recordings, we synthetically create\na dataset called OmniTalk by using the LibriSpeech dataset. This spatial\ninformation is fused with linguistic embeddings from OpenAI's Whisper model,\nallowing each modality to learn complementary contextual representations. The\nfused embeddings are aligned with the input space of LLaMA-3.2 3B model and\nfine-tuned with lightweight adaptation technique LoRA to optimize for on-device\nprocessing. SING supports spatially-aware automatic speech recognition (ASR),\nachieving a mean error of $25.72^\\circ$-a substantial improvement compared to\nthe 88.52$^\\circ$ median error in existing work-with a word error rate (WER) of\n5.3. SING also supports soundscaping, for example, inference how many people\nwere talking and their directions, with up to 5 people and a median DoA error\nof 16$^\\circ$. Our system demonstrates superior performance in spatial speech\nunderstanding while addressing the challenges of power efficiency, privacy, and\nhardware constraints, paving the way for advanced applications in augmented\nreality, accessibility, and immersive experiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating spatial context into large language models (LLMs) has the\npotential to revolutionize human-computer interaction, particularly in wearable\ndevices. In this work, we present a novel system architecture that incorporates\nspatial speech understanding into LLMs, enabling contextually aware and\nadaptive applications for wearable technologies. Our approach leverages\nmicrostructure-based spatial sensing to extract precise Direction of Arrival\n(DoA) information using a monaural microphone. To address the lack of existing\ndataset for microstructure-assisted speech recordings, we synthetically create\na dataset called OmniTalk by using the LibriSpeech dataset. This spatial\ninformation is fused with linguistic embeddings from OpenAI's Whisper model,\nallowing each modality to learn complementary contextual representations. The\nfused embeddings are aligned with the input space of LLaMA-3.2 3B model and\nfine-tuned with lightweight adaptation technique LoRA to optimize for on-device\nprocessing. SING supports spatially-aware automatic speech recognition (ASR),\nachieving a mean error of $25.72^\\circ$-a substantial improvement compared to\nthe 88.52$^\\circ$ median error in existing work-with a word error rate (WER) of\n5.3. SING also supports soundscaping, for example, inference how many people\nwere talking and their directions, with up to 5 people and a median DoA error\nof 16$^\\circ$. Our system demonstrates superior performance in spatial speech\nunderstanding while addressing the challenges of power efficiency, privacy, and\nhardware constraints, paving the way for advanced applications in augmented\nreality, accessibility, and immersive experiences."
                },
                "authors": [
                    {
                        "name": "Ayushi Mishra"
                    },
                    {
                        "name": "Yang Bai"
                    },
                    {
                        "name": "Priyadarshan Narayanasamy"
                    },
                    {
                        "name": "Nakul Garg"
                    },
                    {
                        "name": "Nirupam Roy"
                    }
                ],
                "author_detail": {
                    "name": "Nirupam Roy"
                },
                "author": "Nirupam Roy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08907v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08907v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18415v1",
                "updated": "2025-04-25T15:17:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    17,
                    52,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:17:52Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    17,
                    52,
                    4,
                    115,
                    0
                ],
                "title": "BitNet v2: Native 4-bit Activations with Hadamard Transformation for\n  1-bit LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitNet v2: Native 4-bit Activations with Hadamard Transformation for\n  1-bit LLMs"
                },
                "summary": "Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by\nactivation outliers, which complicate quantization to low bit-widths. We\nintroduce BitNet v2, a novel framework enabling native 4-bit activation\nquantization for 1-bit LLMs. To tackle outliers in attention and feed-forward\nnetwork activations, we propose H-BitLinear, a module applying an online\nHadamard transformation prior to activation quantization. This transformation\nsmooths sharp activation distributions into more Gaussian-like forms, suitable\nfor low-bit representation. Experiments show BitNet v2 trained from scratch\nwith 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2\nachieves minimal performance degradation when trained with native 4-bit\nactivations, significantly reducing memory footprint and computational cost for\nbatched inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by\nactivation outliers, which complicate quantization to low bit-widths. We\nintroduce BitNet v2, a novel framework enabling native 4-bit activation\nquantization for 1-bit LLMs. To tackle outliers in attention and feed-forward\nnetwork activations, we propose H-BitLinear, a module applying an online\nHadamard transformation prior to activation quantization. This transformation\nsmooths sharp activation distributions into more Gaussian-like forms, suitable\nfor low-bit representation. Experiments show BitNet v2 trained from scratch\nwith 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2\nachieves minimal performance degradation when trained with native 4-bit\nactivations, significantly reducing memory footprint and computational cost for\nbatched inference."
                },
                "authors": [
                    {
                        "name": "Hongyu Wang"
                    },
                    {
                        "name": "Shuming Ma"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18412v1",
                "updated": "2025-04-25T15:14:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    14,
                    21,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:14:21Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    14,
                    21,
                    4,
                    115,
                    0
                ],
                "title": "Expressing stigma and inappropriate responses prevents LLMs from safely\n  replacing mental health providers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expressing stigma and inappropriate responses prevents LLMs from safely\n  replacing mental health providers"
                },
                "summary": "Should a large language model (LLM) be used as a therapist? In this paper, we\ninvestigate the use of LLMs to *replace* mental health providers, a use case\npromoted in the tech startup and research space. We conduct a mapping review of\ntherapy guides used by major medical institutions to identify crucial aspects\nof therapeutic relationships, such as the importance of a therapeutic alliance\nbetween therapist and client. We then assess the ability of LLMs to reproduce\nand adhere to these aspects of therapeutic relationships by conducting several\nexperiments investigating the responses of current LLMs, such as `gpt-4o`.\nContrary to best practices in the medical community, LLMs 1) express stigma\ntoward those with mental health conditions and 2) respond inappropriately to\ncertain common (and critical) conditions in naturalistic therapy settings --\ne.g., LLMs encourage clients' delusional thinking, likely due to their\nsycophancy. This occurs even with larger and newer LLMs, indicating that\ncurrent safety practices may not address these gaps. Furthermore, we note\nfoundational and practical barriers to the adoption of LLMs as therapists, such\nas that a therapeutic alliance requires human characteristics (e.g., identity\nand stakes). For these reasons, we conclude that LLMs should not replace\ntherapists, and we discuss alternative roles for LLMs in clinical therapy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Should a large language model (LLM) be used as a therapist? In this paper, we\ninvestigate the use of LLMs to *replace* mental health providers, a use case\npromoted in the tech startup and research space. We conduct a mapping review of\ntherapy guides used by major medical institutions to identify crucial aspects\nof therapeutic relationships, such as the importance of a therapeutic alliance\nbetween therapist and client. We then assess the ability of LLMs to reproduce\nand adhere to these aspects of therapeutic relationships by conducting several\nexperiments investigating the responses of current LLMs, such as `gpt-4o`.\nContrary to best practices in the medical community, LLMs 1) express stigma\ntoward those with mental health conditions and 2) respond inappropriately to\ncertain common (and critical) conditions in naturalistic therapy settings --\ne.g., LLMs encourage clients' delusional thinking, likely due to their\nsycophancy. This occurs even with larger and newer LLMs, indicating that\ncurrent safety practices may not address these gaps. Furthermore, we note\nfoundational and practical barriers to the adoption of LLMs as therapists, such\nas that a therapeutic alliance requires human characteristics (e.g., identity\nand stakes). For these reasons, we conclude that LLMs should not replace\ntherapists, and we discuss alternative roles for LLMs in clinical therapy."
                },
                "authors": [
                    {
                        "name": "Jared Moore"
                    },
                    {
                        "name": "Declan Grabb"
                    },
                    {
                        "name": "William Agnew"
                    },
                    {
                        "name": "Kevin Klyman"
                    },
                    {
                        "name": "Stevie Chancellor"
                    },
                    {
                        "name": "Desmond C. Ong"
                    },
                    {
                        "name": "Nick Haber"
                    }
                ],
                "author_detail": {
                    "name": "Nick Haber"
                },
                "author": "Nick Haber",
                "arxiv_doi": "10.1145/3715275.3732039",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3715275.3732039",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.18412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16172v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16172v2",
                "updated": "2025-04-25T15:12:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    12,
                    10,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-22T18:01:45Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    18,
                    1,
                    45,
                    1,
                    112,
                    0
                ],
                "title": "Physics-Informed Inference Time Scaling via Simulation-Calibrated\n  Scientific Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-Informed Inference Time Scaling via Simulation-Calibrated\n  Scientific Machine Learning"
                },
                "summary": "High-dimensional partial differential equations (PDEs) pose significant\ncomputational challenges across fields ranging from quantum chemistry to\neconomics and finance. Although scientific machine learning (SciML) techniques\noffer approximate solutions, they often suffer from bias and neglect crucial\nphysical insights. Inspired by inference-time scaling strategies in language\nmodels, we propose Simulation-Calibrated Scientific Machine Learning (SCaSML),\na physics-informed framework that dynamically refines and debiases the SCiML\npredictions during inference by enforcing the physical laws. SCaSML leverages\nderived new physical laws that quantifies systematic errors and employs Monte\nCarlo solvers based on the Feynman-Kac and Elworthy-Bismut-Li formulas to\ndynamically correct the prediction. Both numerical and theoretical analysis\nconfirms enhanced convergence rates via compute-optimal inference methods. Our\nnumerical experiments demonstrate that SCaSML reduces errors by 20-50% compared\nto the base surrogate model, establishing it as the first algorithm to refine\napproximated solutions to high-dimensional PDE during inference. Code of SCaSML\nis available at https://github.com/Francis-Fan-create/SCaSML.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-dimensional partial differential equations (PDEs) pose significant\ncomputational challenges across fields ranging from quantum chemistry to\neconomics and finance. Although scientific machine learning (SciML) techniques\noffer approximate solutions, they often suffer from bias and neglect crucial\nphysical insights. Inspired by inference-time scaling strategies in language\nmodels, we propose Simulation-Calibrated Scientific Machine Learning (SCaSML),\na physics-informed framework that dynamically refines and debiases the SCiML\npredictions during inference by enforcing the physical laws. SCaSML leverages\nderived new physical laws that quantifies systematic errors and employs Monte\nCarlo solvers based on the Feynman-Kac and Elworthy-Bismut-Li formulas to\ndynamically correct the prediction. Both numerical and theoretical analysis\nconfirms enhanced convergence rates via compute-optimal inference methods. Our\nnumerical experiments demonstrate that SCaSML reduces errors by 20-50% compared\nto the base surrogate model, establishing it as the first algorithm to refine\napproximated solutions to high-dimensional PDE during inference. Code of SCaSML\nis available at https://github.com/Francis-Fan-create/SCaSML."
                },
                "authors": [
                    {
                        "name": "Zexi Fan"
                    },
                    {
                        "name": "Yan Sun"
                    },
                    {
                        "name": "Shihao Yang"
                    },
                    {
                        "name": "Yiping Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yiping Lu"
                },
                "author": "Yiping Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16172v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16172v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18410v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18410v1",
                "updated": "2025-04-25T15:10:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    10,
                    51,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:10:51Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    10,
                    51,
                    4,
                    115,
                    0
                ],
                "title": "Can Code Outlove Blood? A LLM-based VR Experience to Prompt Reflection\n  on Parental Verbal Abuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Code Outlove Blood? A LLM-based VR Experience to Prompt Reflection\n  on Parental Verbal Abuse"
                },
                "summary": "Parental verbal abuse leaves lasting emotional impacts, yet current\ntherapeutic approaches often lack immersive self-reflection opportunities. To\naddress this, we developed a VR experience powered by LLMs to foster reflection\non parental verbal abuse. Participants with relevant experiences engage in a\ndual-phase VR experience: first assuming the role of a verbally abusive parent,\ninteracting with an LLM portraying a child, then observing the LLM reframing\nabusive dialogue into warm, supportive expressions as a nurturing parent. A\nqualitative study with 12 participants showed that the experience encourages\nreflection on their past experiences and fosters supportive emotions. However,\nthese effects vary with participants' personal histories, emphasizing the need\nfor greater personalization in AI-driven emotional support. This study explores\nthe use of LLMs in immersive environment to promote emotional reflection,\noffering insights into the design of AI-driven emotional support systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parental verbal abuse leaves lasting emotional impacts, yet current\ntherapeutic approaches often lack immersive self-reflection opportunities. To\naddress this, we developed a VR experience powered by LLMs to foster reflection\non parental verbal abuse. Participants with relevant experiences engage in a\ndual-phase VR experience: first assuming the role of a verbally abusive parent,\ninteracting with an LLM portraying a child, then observing the LLM reframing\nabusive dialogue into warm, supportive expressions as a nurturing parent. A\nqualitative study with 12 participants showed that the experience encourages\nreflection on their past experiences and fosters supportive emotions. However,\nthese effects vary with participants' personal histories, emphasizing the need\nfor greater personalization in AI-driven emotional support. This study explores\nthe use of LLMs in immersive environment to promote emotional reflection,\noffering insights into the design of AI-driven emotional support systems."
                },
                "authors": [
                    {
                        "name": "Jiaying Fu"
                    },
                    {
                        "name": "Jialin Gu"
                    },
                    {
                        "name": "Tianyue Gong"
                    },
                    {
                        "name": "Tiange Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Tiange Zhou"
                },
                "author": "Tiange Zhou",
                "arxiv_comment": "8 pages, 5 figures, accetped by 30th International Symposium on\n  Electronic Art (ISEA 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18410v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17565v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17565v2",
                "updated": "2025-04-25T15:10:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    10,
                    20,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-24T13:57:53Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    57,
                    53,
                    3,
                    114,
                    0
                ],
                "title": "DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale\n  Difficulty-Graded Data Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale\n  Difficulty-Graded Data Training"
                },
                "summary": "Although large language models (LLMs) have recently achieved remarkable\nperformance on various complex reasoning benchmarks, the academic community\nstill lacks an in-depth understanding of base model training processes and data\nquality. To address this, we construct a large-scale, difficulty-graded\nreasoning dataset containing approximately 3.34 million unique queries of\nvarying difficulty levels and about 40 million distilled responses generated by\nmultiple models over several passes. Leveraging pass rate and Coefficient of\nVariation (CV), we precisely select the most valuable training data to enhance\nreasoning capability. Notably, we observe a training pattern shift, indicating\nthat reasoning-focused training based on base models requires higher learning\nrates for effective training. Using this carefully selected data, we\nsignificantly improve the reasoning capabilities of the base model, achieving a\npass rate of 79.2\\% on the AIME2024 mathematical reasoning benchmark. This\nresult surpasses most current distilled models and closely approaches\nstate-of-the-art performance. We provide detailed descriptions of our data\nprocessing, difficulty assessment, and training methodology, and have publicly\nreleased all datasets and methods to promote rapid progress in open-source\nlong-reasoning LLMs. The dataset is available at:\nhttps://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) have recently achieved remarkable\nperformance on various complex reasoning benchmarks, the academic community\nstill lacks an in-depth understanding of base model training processes and data\nquality. To address this, we construct a large-scale, difficulty-graded\nreasoning dataset containing approximately 3.34 million unique queries of\nvarying difficulty levels and about 40 million distilled responses generated by\nmultiple models over several passes. Leveraging pass rate and Coefficient of\nVariation (CV), we precisely select the most valuable training data to enhance\nreasoning capability. Notably, we observe a training pattern shift, indicating\nthat reasoning-focused training based on base models requires higher learning\nrates for effective training. Using this carefully selected data, we\nsignificantly improve the reasoning capabilities of the base model, achieving a\npass rate of 79.2\\% on the AIME2024 mathematical reasoning benchmark. This\nresult surpasses most current distilled models and closely approaches\nstate-of-the-art performance. We provide detailed descriptions of our data\nprocessing, difficulty assessment, and training methodology, and have publicly\nreleased all datasets and methods to promote rapid progress in open-source\nlong-reasoning LLMs. The dataset is available at:\nhttps://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M"
                },
                "authors": [
                    {
                        "name": "Xiaoyu Tian"
                    },
                    {
                        "name": "Sitong Zhao"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Shuaiting Chen"
                    },
                    {
                        "name": "Yiping Peng"
                    },
                    {
                        "name": "Yunjie Ji"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Xiangang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangang Li"
                },
                "author": "Xiangang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17565v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17565v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18383v1",
                "updated": "2025-04-25T14:30:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    14,
                    30,
                    25,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T14:30:25Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    14,
                    30,
                    25,
                    4,
                    115,
                    0
                ],
                "title": "Bridge the Domains: Large Language Models Enhanced Cross-domain\n  Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridge the Domains: Large Language Models Enhanced Cross-domain\n  Sequential Recommendation"
                },
                "summary": "Cross-domain Sequential Recommendation (CDSR) aims to extract the preference\nfrom the user's historical interactions across various domains. Despite some\nprogress in CDSR, two problems set the barrier for further advancements, i.e.,\noverlap dilemma and transition complexity. The former means existing CDSR\nmethods severely rely on users who own interactions on all domains to learn\ncross-domain item relationships, compromising the practicability. The latter\nrefers to the difficulties in learning the complex transition patterns from the\nmixed behavior sequences. With powerful representation and reasoning abilities,\nLarge Language Models (LLMs) are promising to address these two problems by\nbridging the items and capturing the user's preferences from a semantic view.\nTherefore, we propose an LLMs Enhanced Cross-domain Sequential Recommendation\nmodel (LLM4CDSR). To obtain the semantic item relationships, we first propose\nan LLM-based unified representation module to represent items. Then, a\ntrainable adapter with contrastive regularization is designed to adapt the CDSR\ntask. Besides, a hierarchical LLMs profiling module is designed to summarize\nuser cross-domain preferences. Finally, these two modules are integrated into\nthe proposed tri-thread framework to derive recommendations. We have conducted\nextensive experiments on three public cross-domain datasets, validating the\neffectiveness of LLM4CDSR. We have released the code online.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-domain Sequential Recommendation (CDSR) aims to extract the preference\nfrom the user's historical interactions across various domains. Despite some\nprogress in CDSR, two problems set the barrier for further advancements, i.e.,\noverlap dilemma and transition complexity. The former means existing CDSR\nmethods severely rely on users who own interactions on all domains to learn\ncross-domain item relationships, compromising the practicability. The latter\nrefers to the difficulties in learning the complex transition patterns from the\nmixed behavior sequences. With powerful representation and reasoning abilities,\nLarge Language Models (LLMs) are promising to address these two problems by\nbridging the items and capturing the user's preferences from a semantic view.\nTherefore, we propose an LLMs Enhanced Cross-domain Sequential Recommendation\nmodel (LLM4CDSR). To obtain the semantic item relationships, we first propose\nan LLM-based unified representation module to represent items. Then, a\ntrainable adapter with contrastive regularization is designed to adapt the CDSR\ntask. Besides, a hierarchical LLMs profiling module is designed to summarize\nuser cross-domain preferences. Finally, these two modules are integrated into\nthe proposed tri-thread framework to derive recommendations. We have conducted\nextensive experiments on three public cross-domain datasets, validating the\neffectiveness of LLM4CDSR. We have released the code online."
                },
                "authors": [
                    {
                        "name": "Qidong Liu"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Yejing Wang"
                    },
                    {
                        "name": "Zijian Zhang"
                    },
                    {
                        "name": "Howard Zhong"
                    },
                    {
                        "name": "Chong Chen"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Feng Tian"
                    }
                ],
                "author_detail": {
                    "name": "Feng Tian"
                },
                "author": "Feng Tian",
                "arxiv_comment": "accepted by SIGIR'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18380v1",
                "updated": "2025-04-25T14:27:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    14,
                    27,
                    27,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T14:27:27Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    14,
                    27,
                    27,
                    4,
                    115,
                    0
                ],
                "title": "Spatial Reasoner: A 3D Inference Pipeline for XR Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial Reasoner: A 3D Inference Pipeline for XR Applications"
                },
                "summary": "Modern extended reality XR systems provide rich analysis of image data and\nfusion of sensor input and demand AR/VR applications that can reason about 3D\nscenes in a semantic manner. We present a spatial reasoning framework that\nbridges geometric facts with symbolic predicates and relations to handle key\ntasks such as determining how 3D objects are arranged among each other ('on',\n'behind', 'near', etc.). Its foundation relies on oriented 3D bounding box\nrepresentations, enhanced by a comprehensive set of spatial predicates, ranging\nfrom topology and connectivity to directionality and orientation, expressed in\na formalism related to natural language. The derived predicates form a spatial\nknowledge graph and, in combination with a pipeline-based inference model,\nenable spatial queries and dynamic rule evaluation. Implementations for client-\nand server-side processing demonstrate the framework's capability to\nefficiently translate geometric data into actionable knowledge, ensuring\nscalable and technology-independent spatial reasoning in complex 3D\nenvironments. The Spatial Reasoner framework is fostering the creation of\nspatial ontologies, and seamlessly integrates with and therefore enriches\nmachine learning, natural language processing, and rule systems in XR\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern extended reality XR systems provide rich analysis of image data and\nfusion of sensor input and demand AR/VR applications that can reason about 3D\nscenes in a semantic manner. We present a spatial reasoning framework that\nbridges geometric facts with symbolic predicates and relations to handle key\ntasks such as determining how 3D objects are arranged among each other ('on',\n'behind', 'near', etc.). Its foundation relies on oriented 3D bounding box\nrepresentations, enhanced by a comprehensive set of spatial predicates, ranging\nfrom topology and connectivity to directionality and orientation, expressed in\na formalism related to natural language. The derived predicates form a spatial\nknowledge graph and, in combination with a pipeline-based inference model,\nenable spatial queries and dynamic rule evaluation. Implementations for client-\nand server-side processing demonstrate the framework's capability to\nefficiently translate geometric data into actionable knowledge, ensuring\nscalable and technology-independent spatial reasoning in complex 3D\nenvironments. The Spatial Reasoner framework is fostering the creation of\nspatial ontologies, and seamlessly integrates with and therefore enriches\nmachine learning, natural language processing, and rule systems in XR\napplications."
                },
                "authors": [
                    {
                        "name": "Steven Häsler"
                    },
                    {
                        "name": "Philipp Ackermann"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Ackermann"
                },
                "author": "Philipp Ackermann",
                "arxiv_comment": "11 pages, preprint of ICVARS 2025 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "spatial computing, extended reality, knowledge representation,\n  spatial reasoning",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18376v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18376v1",
                "updated": "2025-04-25T14:20:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    14,
                    20,
                    57,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T14:20:57Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    14,
                    20,
                    57,
                    4,
                    115,
                    0
                ],
                "title": "Pushing the boundary on Natural Language Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pushing the boundary on Natural Language Inference"
                },
                "summary": "Natural Language Inference (NLI) is a central task in natural language\nunderstanding with applications in fact-checking, question answering, and\ninformation retrieval. Despite its importance, current NLI systems heavily rely\non supervised learning with datasets that often contain annotation artifacts\nand biases, limiting generalization and real-world applicability. In this work,\nwe apply a reinforcement learning-based approach using Group Relative Policy\nOptimization (GRPO) for Chain-of-Thought (CoT) learning in NLI, eliminating the\nneed for labeled rationales and enabling this type of training on more\nchallenging datasets such as ANLI. We fine-tune 7B, 14B, and 32B language\nmodels using parameter-efficient techniques (LoRA and QLoRA), demonstrating\nstrong performance across standard and adversarial NLI benchmarks. Our 32B\nAWQ-quantized model surpasses state-of-the-art results on 7 out of 11\nadversarial sets$\\unicode{x2013}$or on all of them considering our\nreplication$\\unicode{x2013}$within a 22GB memory footprint, showing that robust\nreasoning can be retained under aggressive quantization. This work provides a\nscalable and practical framework for building robust NLI systems without\nsacrificing inference quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Inference (NLI) is a central task in natural language\nunderstanding with applications in fact-checking, question answering, and\ninformation retrieval. Despite its importance, current NLI systems heavily rely\non supervised learning with datasets that often contain annotation artifacts\nand biases, limiting generalization and real-world applicability. In this work,\nwe apply a reinforcement learning-based approach using Group Relative Policy\nOptimization (GRPO) for Chain-of-Thought (CoT) learning in NLI, eliminating the\nneed for labeled rationales and enabling this type of training on more\nchallenging datasets such as ANLI. We fine-tune 7B, 14B, and 32B language\nmodels using parameter-efficient techniques (LoRA and QLoRA), demonstrating\nstrong performance across standard and adversarial NLI benchmarks. Our 32B\nAWQ-quantized model surpasses state-of-the-art results on 7 out of 11\nadversarial sets$\\unicode{x2013}$or on all of them considering our\nreplication$\\unicode{x2013}$within a 22GB memory footprint, showing that robust\nreasoning can be retained under aggressive quantization. This work provides a\nscalable and practical framework for building robust NLI systems without\nsacrificing inference quality."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-González"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martín"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18376v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18376v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18373v1",
                "updated": "2025-04-25T14:17:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    14,
                    17,
                    47,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T14:17:47Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    14,
                    17,
                    47,
                    4,
                    115,
                    0
                ],
                "title": "Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in\n  Smart Personal Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in\n  Smart Personal Assistant"
                },
                "summary": "In recent years, multi-agent frameworks powered by large language models\n(LLMs) have advanced rapidly. Despite this progress, there is still a notable\nabsence of benchmark datasets specifically tailored to evaluate their\nperformance. To bridge this gap, we introduce Auto-SLURP, a benchmark dataset\naimed at evaluating LLM-based multi-agent frameworks in the context of\nintelligent personal assistants. Auto-SLURP extends the original SLURP dataset\n-- initially developed for natural language understanding tasks -- by\nrelabeling the data and integrating simulated servers and external services.\nThis enhancement enables a comprehensive end-to-end evaluation pipeline,\ncovering language understanding, task execution, and response generation. Our\nexperiments demonstrate that Auto-SLURP presents a significant challenge for\ncurrent state-of-the-art frameworks, highlighting that truly reliable and\nintelligent multi-agent personal assistants remain a work in progress. The\ndataset and related code are available at\nhttps://github.com/lorashen/Auto-SLURP/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, multi-agent frameworks powered by large language models\n(LLMs) have advanced rapidly. Despite this progress, there is still a notable\nabsence of benchmark datasets specifically tailored to evaluate their\nperformance. To bridge this gap, we introduce Auto-SLURP, a benchmark dataset\naimed at evaluating LLM-based multi-agent frameworks in the context of\nintelligent personal assistants. Auto-SLURP extends the original SLURP dataset\n-- initially developed for natural language understanding tasks -- by\nrelabeling the data and integrating simulated servers and external services.\nThis enhancement enables a comprehensive end-to-end evaluation pipeline,\ncovering language understanding, task execution, and response generation. Our\nexperiments demonstrate that Auto-SLURP presents a significant challenge for\ncurrent state-of-the-art frameworks, highlighting that truly reliable and\nintelligent multi-agent personal assistants remain a work in progress. The\ndataset and related code are available at\nhttps://github.com/lorashen/Auto-SLURP/."
                },
                "authors": [
                    {
                        "name": "Lei Shen"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18369v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18369v1",
                "updated": "2025-04-25T14:11:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    14,
                    11,
                    42,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T14:11:42Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    14,
                    11,
                    42,
                    4,
                    115,
                    0
                ],
                "title": "ThreMoLIA: Threat Modeling of Large Language Model-Integrated\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThreMoLIA: Threat Modeling of Large Language Model-Integrated\n  Applications"
                },
                "summary": "Large Language Models (LLMs) are currently being integrated into industrial\nsoftware applications to help users perform more complex tasks in less time.\nHowever, these LLM-Integrated Applications (LIA) expand the attack surface and\nintroduce new kinds of threats. Threat modeling is commonly used to identify\nthese threats and suggest mitigations. However, it is a time-consuming practice\nthat requires the involvement of a security practitioner. Our goals are to 1)\nprovide a method for performing threat modeling for LIAs early in their\nlifecycle, (2) develop a threat modeling tool that integrates existing threat\nmodels, and (3) ensure high-quality threat modeling. To achieve the goals, we\nwork in collaboration with our industry partner. Our proposed way of performing\nthreat modeling will benefit industry by requiring fewer security experts'\nparticipation and reducing the time spent on this activity. Our proposed tool\ncombines LLMs and Retrieval Augmented Generation (RAG) and uses sources such as\nexisting threat models and application architecture repositories to\ncontinuously create and update threat models. We propose to evaluate the tool\noffline -- i.e., using benchmarking -- and online with practitioners in the\nfield. We conducted an early evaluation using ChatGPT on a simple LIA and\nobtained results that encouraged us to proceed with our research efforts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are currently being integrated into industrial\nsoftware applications to help users perform more complex tasks in less time.\nHowever, these LLM-Integrated Applications (LIA) expand the attack surface and\nintroduce new kinds of threats. Threat modeling is commonly used to identify\nthese threats and suggest mitigations. However, it is a time-consuming practice\nthat requires the involvement of a security practitioner. Our goals are to 1)\nprovide a method for performing threat modeling for LIAs early in their\nlifecycle, (2) develop a threat modeling tool that integrates existing threat\nmodels, and (3) ensure high-quality threat modeling. To achieve the goals, we\nwork in collaboration with our industry partner. Our proposed way of performing\nthreat modeling will benefit industry by requiring fewer security experts'\nparticipation and reducing the time spent on this activity. Our proposed tool\ncombines LLMs and Retrieval Augmented Generation (RAG) and uses sources such as\nexisting threat models and application architecture repositories to\ncontinuously create and update threat models. We propose to evaluate the tool\noffline -- i.e., using benchmarking -- and online with practitioners in the\nfield. We conducted an early evaluation using ChatGPT on a simple LIA and\nobtained results that encouraged us to proceed with our research efforts."
                },
                "authors": [
                    {
                        "name": "Felix Viktor Jedrzejewski"
                    },
                    {
                        "name": "Davide Fucci"
                    },
                    {
                        "name": "Oleksandr Adamov"
                    }
                ],
                "author_detail": {
                    "name": "Oleksandr Adamov"
                },
                "author": "Oleksandr Adamov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18369v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18369v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17119v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17119v2",
                "updated": "2025-04-25T13:42:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    13,
                    42,
                    19,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-23T22:02:25Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    22,
                    2,
                    25,
                    2,
                    113,
                    0
                ],
                "title": "The Rise of Small Language Models in Healthcare: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Rise of Small Language Models in Healthcare: A Comprehensive Survey"
                },
                "summary": "Despite substantial progress in healthcare applications driven by large\nlanguage models (LLMs), growing concerns around data privacy, and limited\nresources; the small language models (SLMs) offer a scalable and clinically\nviable solution for efficient performance in resource-constrained environments\nfor next-generation healthcare informatics. Our comprehensive survey presents a\ntaxonomic framework to identify and categorize them for healthcare\nprofessionals and informaticians. The timeline of healthcare SLM contributions\nestablishes a foundational framework for analyzing models across three\ndimensions: NLP tasks, stakeholder roles, and the continuum of care. We present\na taxonomic framework to identify the architectural foundations for building\nmodels from scratch; adapting SLMs to clinical precision through prompting,\ninstruction fine-tuning, and reasoning; and accessibility and sustainability\nthrough compression techniques. Our primary objective is to offer a\ncomprehensive survey for healthcare professionals, introducing recent\ninnovations in model optimization and equipping them with curated resources to\nsupport future research and development in the field. Aiming to showcase the\ngroundbreaking advancements in SLMs for healthcare, we present a comprehensive\ncompilation of experimental results across widely studied NLP tasks in\nhealthcare to highlight the transformative potential of SLMs in healthcare. The\nupdated repository is available at Github",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite substantial progress in healthcare applications driven by large\nlanguage models (LLMs), growing concerns around data privacy, and limited\nresources; the small language models (SLMs) offer a scalable and clinically\nviable solution for efficient performance in resource-constrained environments\nfor next-generation healthcare informatics. Our comprehensive survey presents a\ntaxonomic framework to identify and categorize them for healthcare\nprofessionals and informaticians. The timeline of healthcare SLM contributions\nestablishes a foundational framework for analyzing models across three\ndimensions: NLP tasks, stakeholder roles, and the continuum of care. We present\na taxonomic framework to identify the architectural foundations for building\nmodels from scratch; adapting SLMs to clinical precision through prompting,\ninstruction fine-tuning, and reasoning; and accessibility and sustainability\nthrough compression techniques. Our primary objective is to offer a\ncomprehensive survey for healthcare professionals, introducing recent\ninnovations in model optimization and equipping them with curated resources to\nsupport future research and development in the field. Aiming to showcase the\ngroundbreaking advancements in SLMs for healthcare, we present a comprehensive\ncompilation of experimental results across widely studied NLP tasks in\nhealthcare to highlight the transformative potential of SLMs in healthcare. The\nupdated repository is available at Github"
                },
                "authors": [
                    {
                        "name": "Muskan Garg"
                    },
                    {
                        "name": "Shaina Raza"
                    },
                    {
                        "name": "Shebuti Rayana"
                    },
                    {
                        "name": "Xingyi Liu"
                    },
                    {
                        "name": "Sunghwan Sohn"
                    }
                ],
                "author_detail": {
                    "name": "Sunghwan Sohn"
                },
                "author": "Sunghwan Sohn",
                "arxiv_comment": "35 pages, 7 tables, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17119v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17119v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13711v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13711v3",
                "updated": "2025-04-25T13:41:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    13,
                    41,
                    38,
                    4,
                    115,
                    0
                ],
                "published": "2025-01-23T14:41:26Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    41,
                    26,
                    3,
                    23,
                    0
                ],
                "title": "Rapid wavefront shaping using an optical gradient acquisition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid wavefront shaping using an optical gradient acquisition"
                },
                "summary": "Wavefront shaping systems aim to image deep into scattering tissue by\nreshaping incoming and outgoing light to correct aberrations caused by tissue\ninhomogeneity However, the desired modulation depends on the unknown tissue\nstructure and therefore its estimation is a challenging time-consuming task.\nMost strategies rely on coordinate descent optimization, which sequentially\nvaries each modulation parameter and assesses its impact on the resulting\nimage. We propose a rapid wavefront shaping scheme that transitions from\ncoordinate descent to gradient descent optimization, using the same measurement\nto update all modulation parameters simultaneously. To achieve this, we have\ndeveloped an analytical framework that expresses the gradient of the wavefront\nshaping score with respect to all modulation parameters. Although this gradient\ndepends on the unknown tissue structure, we demonstrate how it can be inferred\nfrom the optical system's measurements. Our new framework enables rapid\ninference of wavefront shaping modulations. Additionally, since the complexity\nof our algorithm does not scale with the number of modulation parameters, we\ncan achieve very high-resolution modulations, leading to better corrections in\nthicker tissue layers. We showcase the effectiveness of our framework in\ncorrecting aberrations in a coherent confocal microscope.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wavefront shaping systems aim to image deep into scattering tissue by\nreshaping incoming and outgoing light to correct aberrations caused by tissue\ninhomogeneity However, the desired modulation depends on the unknown tissue\nstructure and therefore its estimation is a challenging time-consuming task.\nMost strategies rely on coordinate descent optimization, which sequentially\nvaries each modulation parameter and assesses its impact on the resulting\nimage. We propose a rapid wavefront shaping scheme that transitions from\ncoordinate descent to gradient descent optimization, using the same measurement\nto update all modulation parameters simultaneously. To achieve this, we have\ndeveloped an analytical framework that expresses the gradient of the wavefront\nshaping score with respect to all modulation parameters. Although this gradient\ndepends on the unknown tissue structure, we demonstrate how it can be inferred\nfrom the optical system's measurements. Our new framework enables rapid\ninference of wavefront shaping modulations. Additionally, since the complexity\nof our algorithm does not scale with the number of modulation parameters, we\ncan achieve very high-resolution modulations, leading to better corrections in\nthicker tissue layers. We showcase the effectiveness of our framework in\ncorrecting aberrations in a coherent confocal microscope."
                },
                "authors": [
                    {
                        "name": "Sagi Monin"
                    },
                    {
                        "name": "Marina Alterman"
                    },
                    {
                        "name": "Anat Levin"
                    }
                ],
                "author_detail": {
                    "name": "Anat Levin"
                },
                "author": "Anat Levin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13711v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13711v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18350v1",
                "updated": "2025-04-25T13:40:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    13,
                    40,
                    25,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T13:40:25Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    13,
                    40,
                    25,
                    4,
                    115,
                    0
                ],
                "title": "Characterizing the Impact of Alfvén Wave Forcing in Interplanetary\n  Space on the Distribution of near-Earth Solar Wind Speeds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing the Impact of Alfvén Wave Forcing in Interplanetary\n  Space on the Distribution of near-Earth Solar Wind Speeds"
                },
                "summary": "Broadly, solar wind source regions can be classified by their magnetic\ntopology as intermittently and continuously open to the heliosphere. Early\nmodels of solar wind acceleration do not account for the fastest, non-transient\nsolar wind speeds observed near-Earth and energy must be deposited into the\nsolar wind after it leaves the Sun. Alfv\\'en wave energy deposition and thermal\npressure gradients are likely candidates and the relative contribution of each\nacceleration mechanism likely depends on the source region. Although solar wind\nspeed is a rough proxy for solar wind source region, it cannot unambiguously\nidentify source region topology.\n  Using near-Sun observations of the solar wind's kinetic energy flux, we\npredict the expected kinetic energy flux near-Earth. This predicted kinetic\nenergy flux corresponds to the range of solar wind speeds observed in the fast\nsolar wind and infer that the solar wind's near-Sun kinetic energy flux is\nsufficient to predict the distribution of fastest, non-transient speeds\nobserved near Earth. Applying a recently developed model of solar wind\nevolution in the inner heliosphere, we suggest that the acceleration required\nto generate this distribution of fastest, non-transient speeds is likely due to\nthe continuous deposition of energy by Alfv\\'en wave forcing during the solar\nwind's propagation through interplanetary space. We infer that the solar wind's\nAlfv\\'enicity can statistically map near-Earth observations to their source\nregions because the Alfv\\'en wave forcing that the solar wind experiences in\ntransit is a consequence of the source region topology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Broadly, solar wind source regions can be classified by their magnetic\ntopology as intermittently and continuously open to the heliosphere. Early\nmodels of solar wind acceleration do not account for the fastest, non-transient\nsolar wind speeds observed near-Earth and energy must be deposited into the\nsolar wind after it leaves the Sun. Alfv\\'en wave energy deposition and thermal\npressure gradients are likely candidates and the relative contribution of each\nacceleration mechanism likely depends on the source region. Although solar wind\nspeed is a rough proxy for solar wind source region, it cannot unambiguously\nidentify source region topology.\n  Using near-Sun observations of the solar wind's kinetic energy flux, we\npredict the expected kinetic energy flux near-Earth. This predicted kinetic\nenergy flux corresponds to the range of solar wind speeds observed in the fast\nsolar wind and infer that the solar wind's near-Sun kinetic energy flux is\nsufficient to predict the distribution of fastest, non-transient speeds\nobserved near Earth. Applying a recently developed model of solar wind\nevolution in the inner heliosphere, we suggest that the acceleration required\nto generate this distribution of fastest, non-transient speeds is likely due to\nthe continuous deposition of energy by Alfv\\'en wave forcing during the solar\nwind's propagation through interplanetary space. We infer that the solar wind's\nAlfv\\'enicity can statistically map near-Earth observations to their source\nregions because the Alfv\\'en wave forcing that the solar wind experiences in\ntransit is a consequence of the source region topology."
                },
                "authors": [
                    {
                        "name": "B. L. Alterman"
                    }
                ],
                "author_detail": {
                    "name": "B. L. Alterman"
                },
                "author": "B. L. Alterman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18349v1",
                "updated": "2025-04-25T13:38:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    13,
                    38,
                    23,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T13:38:23Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    13,
                    38,
                    23,
                    4,
                    115,
                    0
                ],
                "title": "Revisiting Data Auditing in Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Data Auditing in Large Vision-Language Models"
                },
                "summary": "With the surge of large language models (LLMs), Large Vision-Language Models\n(VLMs)--which integrate vision encoders with LLMs for accurate visual\ngrounding--have shown great potential in tasks like generalist agents and\nrobotic control. However, VLMs are typically trained on massive web-scraped\nimages, raising concerns over copyright infringement and privacy violations,\nand making data auditing increasingly urgent. Membership inference (MI), which\ndetermines whether a sample was used in training, has emerged as a key auditing\ntechnique, with promising results on open-source VLMs like LLaVA (AUC > 80%).\nIn this work, we revisit these advances and uncover a critical issue: current\nMI benchmarks suffer from distribution shifts between member and non-member\nimages, introducing shortcut cues that inflate MI performance. We further\nanalyze the nature of these shifts and propose a principled metric based on\noptimal transport to quantify the distribution discrepancy. To evaluate MI in\nrealistic settings, we construct new benchmarks with i.i.d. member and\nnon-member images. Existing MI methods fail under these unbiased conditions,\nperforming only marginally better than chance. Further, we explore the\ntheoretical upper bound of MI by probing the Bayes Optimality within the VLM's\nembedding space and find the irreducible error rate remains high. Despite this\npessimistic outlook, we analyze why MI for VLMs is particularly challenging and\nidentify three practical scenarios--fine-tuning, access to ground-truth texts,\nand set-based inference--where auditing becomes feasible. Our study presents a\nsystematic view of the limits and opportunities of MI for VLMs, providing\nguidance for future efforts in trustworthy data auditing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the surge of large language models (LLMs), Large Vision-Language Models\n(VLMs)--which integrate vision encoders with LLMs for accurate visual\ngrounding--have shown great potential in tasks like generalist agents and\nrobotic control. However, VLMs are typically trained on massive web-scraped\nimages, raising concerns over copyright infringement and privacy violations,\nand making data auditing increasingly urgent. Membership inference (MI), which\ndetermines whether a sample was used in training, has emerged as a key auditing\ntechnique, with promising results on open-source VLMs like LLaVA (AUC > 80%).\nIn this work, we revisit these advances and uncover a critical issue: current\nMI benchmarks suffer from distribution shifts between member and non-member\nimages, introducing shortcut cues that inflate MI performance. We further\nanalyze the nature of these shifts and propose a principled metric based on\noptimal transport to quantify the distribution discrepancy. To evaluate MI in\nrealistic settings, we construct new benchmarks with i.i.d. member and\nnon-member images. Existing MI methods fail under these unbiased conditions,\nperforming only marginally better than chance. Further, we explore the\ntheoretical upper bound of MI by probing the Bayes Optimality within the VLM's\nembedding space and find the irreducible error rate remains high. Despite this\npessimistic outlook, we analyze why MI for VLMs is particularly challenging and\nidentify three practical scenarios--fine-tuning, access to ground-truth texts,\nand set-based inference--where auditing becomes feasible. Our study presents a\nsystematic view of the limits and opportunities of MI for VLMs, providing\nguidance for future efforts in trustworthy data auditing."
                },
                "authors": [
                    {
                        "name": "Hongyu Zhu"
                    },
                    {
                        "name": "Sichu Liang"
                    },
                    {
                        "name": "Wenwen Wang"
                    },
                    {
                        "name": "Boheng Li"
                    },
                    {
                        "name": "Tongxin Yuan"
                    },
                    {
                        "name": "Fangqi Li"
                    },
                    {
                        "name": "ShiLin Wang"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhuosheng Zhang"
                },
                "author": "Zhuosheng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18346v1",
                "updated": "2025-04-25T13:34:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    13,
                    34,
                    40,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T13:34:40Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    13,
                    34,
                    40,
                    4,
                    115,
                    0
                ],
                "title": "Comparing Uncertainty Measurement and Mitigation Methods for Large\n  Language Models: A Systematic Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing Uncertainty Measurement and Mitigation Methods for Large\n  Language Models: A Systematic Review"
                },
                "summary": "Large Language Models (LLMs) have been transformative across many domains.\nHowever, hallucination -- confidently outputting incorrect information --\nremains one of the leading challenges for LLMs. This raises the question of how\nto accurately assess and quantify the uncertainty of LLMs. Extensive literature\non traditional models has explored Uncertainty Quantification (UQ) to measure\nuncertainty and employed calibration techniques to address the misalignment\nbetween uncertainty and accuracy. While some of these methods have been adapted\nfor LLMs, the literature lacks an in-depth analysis of their effectiveness and\ndoes not offer a comprehensive benchmark to enable insightful comparison among\nexisting solutions. In this work, we fill this gap via a systematic survey of\nrepresentative prior works on UQ and calibration for LLMs and introduce a\nrigorous benchmark. Using two widely used reliability datasets, we empirically\nevaluate six related methods, which justify the significant findings of our\nreview. Finally, we provide outlooks for key future directions and outline open\nchallenges. To the best of our knowledge, this survey is the first dedicated\nstudy to review the calibration methods and relevant metrics for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been transformative across many domains.\nHowever, hallucination -- confidently outputting incorrect information --\nremains one of the leading challenges for LLMs. This raises the question of how\nto accurately assess and quantify the uncertainty of LLMs. Extensive literature\non traditional models has explored Uncertainty Quantification (UQ) to measure\nuncertainty and employed calibration techniques to address the misalignment\nbetween uncertainty and accuracy. While some of these methods have been adapted\nfor LLMs, the literature lacks an in-depth analysis of their effectiveness and\ndoes not offer a comprehensive benchmark to enable insightful comparison among\nexisting solutions. In this work, we fill this gap via a systematic survey of\nrepresentative prior works on UQ and calibration for LLMs and introduce a\nrigorous benchmark. Using two widely used reliability datasets, we empirically\nevaluate six related methods, which justify the significant findings of our\nreview. Finally, we provide outlooks for key future directions and outline open\nchallenges. To the best of our knowledge, this survey is the first dedicated\nstudy to review the calibration methods and relevant metrics for LLMs."
                },
                "authors": [
                    {
                        "name": "Toghrul Abbasli"
                    },
                    {
                        "name": "Kentaroh Toyoda"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Leon Witt"
                    },
                    {
                        "name": "Muhammad Asif Ali"
                    },
                    {
                        "name": "Yukai Miao"
                    },
                    {
                        "name": "Dan Li"
                    },
                    {
                        "name": "Qingsong Wei"
                    }
                ],
                "author_detail": {
                    "name": "Qingsong Wei"
                },
                "author": "Qingsong Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16526v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16526v2",
                "updated": "2025-04-25T13:32:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    13,
                    32,
                    24,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-23T08:50:24Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    8,
                    50,
                    24,
                    2,
                    113,
                    0
                ],
                "title": "Using Causal Inference to Test Systems with Hidden and Interacting\n  Variables: An Evaluative Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Causal Inference to Test Systems with Hidden and Interacting\n  Variables: An Evaluative Case Study"
                },
                "summary": "Software systems with large parameter spaces, nondeterminism and high\ncomputational cost are challenging to test. Recently, software testing\ntechniques based on causal inference have been successfully applied to systems\nthat exhibit such characteristics, including scientific models and autonomous\ndriving systems. One significant limitation is that these are restricted to\ntest properties where all of the variables involved can be observed and where\nthere are no interactions between variables. In practice, this is rarely\nguaranteed; the logging infrastructure may not be available to record all of\nthe necessary runtime variable values, and it can often be the case that an\noutput of the system can be affected by complex interactions between variables.\nTo address this, we leverage two additional concepts from causal inference,\nnamely effect modification and instrumental variable methods. We build these\nconcepts into an existing causal testing tool and conduct an evaluative case\nstudy which uses the concepts to test three system-level requirements of CARLA,\na high-fidelity driving simulator widely used in autonomous vehicle development\nand testing. The results show that we can obtain reliable test outcomes without\nrequiring large amounts of highly controlled test data or instrumentation of\nthe code, even when variables interact with each other and are not recorded in\nthe test data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software systems with large parameter spaces, nondeterminism and high\ncomputational cost are challenging to test. Recently, software testing\ntechniques based on causal inference have been successfully applied to systems\nthat exhibit such characteristics, including scientific models and autonomous\ndriving systems. One significant limitation is that these are restricted to\ntest properties where all of the variables involved can be observed and where\nthere are no interactions between variables. In practice, this is rarely\nguaranteed; the logging infrastructure may not be available to record all of\nthe necessary runtime variable values, and it can often be the case that an\noutput of the system can be affected by complex interactions between variables.\nTo address this, we leverage two additional concepts from causal inference,\nnamely effect modification and instrumental variable methods. We build these\nconcepts into an existing causal testing tool and conduct an evaluative case\nstudy which uses the concepts to test three system-level requirements of CARLA,\na high-fidelity driving simulator widely used in autonomous vehicle development\nand testing. The results show that we can obtain reliable test outcomes without\nrequiring large amounts of highly controlled test data or instrumentation of\nthe code, even when variables interact with each other and are not recorded in\nthe test data."
                },
                "authors": [
                    {
                        "name": "Michael Foster"
                    },
                    {
                        "name": "Robert M. Hierons"
                    },
                    {
                        "name": "Donghwan Shin"
                    },
                    {
                        "name": "Neil Walkinshaw"
                    },
                    {
                        "name": "Christopher Wild"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Wild"
                },
                "author": "Christopher Wild",
                "arxiv_comment": "10 pages (plus two containing only references), 3 tables, 2 figures,\n  EASE 2025 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16526v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16526v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18340v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18340v1",
                "updated": "2025-04-25T13:30:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    13,
                    30,
                    7,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T13:30:07Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    13,
                    30,
                    7,
                    4,
                    115,
                    0
                ],
                "title": "Large Language Models to Accelerate Organic Chemistry Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models to Accelerate Organic Chemistry Synthesis"
                },
                "summary": "Chemical synthesis, as a foundational methodology in the creation of\ntransformative molecules, exerts substantial influence across diverse sectors\nfrom life sciences to materials and energy. Current chemical synthesis\npractices emphasize laborious and costly trial-and-error workflows,\nunderscoring the urgent need for advanced AI assistants. Nowadays, large\nlanguage models (LLMs), typified by GPT-4, have been introduced as an efficient\ntool to facilitate scientific research. Here, we present Chemma, a fully\nfine-tuned LLM with 1.28 million pairs of Q&A about reactions, as an assistant\nto accelerate organic chemistry synthesis. Chemma surpasses the best-known\nresults in multiple chemical tasks, e.g., single-step retrosynthesis and yield\nprediction, which highlights the potential of general AI for organic chemistry.\nVia predicting yields across the experimental reaction space, Chemma\nsignificantly improves the reaction exploration capability of Bayesian\noptimization. More importantly, integrated in an active learning framework,\nChemma exhibits advanced potential for autonomous experimental exploration and\noptimization in open reaction spaces. For an unreported Suzuki-Miyaura\ncross-coupling reaction of cyclic aminoboronates and aryl halides for the\nsynthesis of $\\alpha$-Aryl N-heterocycles, the human-AI collaboration\nsuccessfully explored suitable ligand and solvent (1,4-dioxane) within only 15\nruns, achieving an isolated yield of 67%. These results reveal that, without\nquantum-chemical calculations, Chemma can comprehend and extract chemical\ninsights from reaction data, in a manner akin to human experts. This work opens\navenues for accelerating organic chemistry synthesis with adapted large\nlanguage models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chemical synthesis, as a foundational methodology in the creation of\ntransformative molecules, exerts substantial influence across diverse sectors\nfrom life sciences to materials and energy. Current chemical synthesis\npractices emphasize laborious and costly trial-and-error workflows,\nunderscoring the urgent need for advanced AI assistants. Nowadays, large\nlanguage models (LLMs), typified by GPT-4, have been introduced as an efficient\ntool to facilitate scientific research. Here, we present Chemma, a fully\nfine-tuned LLM with 1.28 million pairs of Q&A about reactions, as an assistant\nto accelerate organic chemistry synthesis. Chemma surpasses the best-known\nresults in multiple chemical tasks, e.g., single-step retrosynthesis and yield\nprediction, which highlights the potential of general AI for organic chemistry.\nVia predicting yields across the experimental reaction space, Chemma\nsignificantly improves the reaction exploration capability of Bayesian\noptimization. More importantly, integrated in an active learning framework,\nChemma exhibits advanced potential for autonomous experimental exploration and\noptimization in open reaction spaces. For an unreported Suzuki-Miyaura\ncross-coupling reaction of cyclic aminoboronates and aryl halides for the\nsynthesis of $\\alpha$-Aryl N-heterocycles, the human-AI collaboration\nsuccessfully explored suitable ligand and solvent (1,4-dioxane) within only 15\nruns, achieving an isolated yield of 67%. These results reveal that, without\nquantum-chemical calculations, Chemma can comprehend and extract chemical\ninsights from reaction data, in a manner akin to human experts. This work opens\navenues for accelerating organic chemistry synthesis with adapted large\nlanguage models."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Yang Han"
                    },
                    {
                        "name": "Shuai Chen"
                    },
                    {
                        "name": "Ruijie Yu"
                    },
                    {
                        "name": "Xin Zhao"
                    },
                    {
                        "name": "Xianbin Liu"
                    },
                    {
                        "name": "Kaipeng Zeng"
                    },
                    {
                        "name": "Mengdi Yu"
                    },
                    {
                        "name": "Jidong Tian"
                    },
                    {
                        "name": "Feng Zhu"
                    },
                    {
                        "name": "Xiaokang Yang"
                    },
                    {
                        "name": "Yaohui Jin"
                    },
                    {
                        "name": "Yanyan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yanyan Xu"
                },
                "author": "Yanyan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18340v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18340v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18333v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18333v1",
                "updated": "2025-04-25T13:18:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    13,
                    18,
                    42,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T13:18:42Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    13,
                    18,
                    42,
                    4,
                    115,
                    0
                ],
                "title": "Adversarial Attacks on LLM-as-a-Judge Systems: Insights from Prompt\n  Injections",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Attacks on LLM-as-a-Judge Systems: Insights from Prompt\n  Injections"
                },
                "summary": "LLM as judge systems used to assess text quality code correctness and\nargument strength are vulnerable to prompt injection attacks. We introduce a\nframework that separates content author attacks from system prompt attacks and\nevaluate five models Gemma 3.27B Gemma 3.4B Llama 3.2 3B GPT 4 and Claude 3\nOpus on four tasks with various defenses using fifty prompts per condition.\nAttacks achieved up to seventy three point eight percent success smaller models\nproved more vulnerable and transferability ranged from fifty point five to\nsixty two point six percent. Our results contrast with Universal Prompt\nInjection and AdvPrompter We recommend multi model committees and comparative\nscoring and release all code and datasets",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM as judge systems used to assess text quality code correctness and\nargument strength are vulnerable to prompt injection attacks. We introduce a\nframework that separates content author attacks from system prompt attacks and\nevaluate five models Gemma 3.27B Gemma 3.4B Llama 3.2 3B GPT 4 and Claude 3\nOpus on four tasks with various defenses using fifty prompts per condition.\nAttacks achieved up to seventy three point eight percent success smaller models\nproved more vulnerable and transferability ranged from fifty point five to\nsixty two point six percent. Our results contrast with Universal Prompt\nInjection and AdvPrompter We recommend multi model committees and comparative\nscoring and release all code and datasets"
                },
                "authors": [
                    {
                        "name": "Narek Maloyan"
                    },
                    {
                        "name": "Dmitry Namiot"
                    }
                ],
                "author_detail": {
                    "name": "Dmitry Namiot"
                },
                "author": "Dmitry Namiot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18333v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18333v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18332v1",
                "updated": "2025-04-25T13:18:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    13,
                    18,
                    6,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T13:18:06Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    13,
                    18,
                    6,
                    4,
                    115,
                    0
                ],
                "title": "SSD-Poser: Avatar Pose Estimation with State Space Duality from Sparse\n  Observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SSD-Poser: Avatar Pose Estimation with State Space Duality from Sparse\n  Observations"
                },
                "summary": "The growing applications of AR/VR increase the demand for real-time full-body\npose estimation from Head-Mounted Displays (HMDs). Although HMDs provide joint\nsignals from the head and hands, reconstructing a full-body pose remains\nchallenging due to the unconstrained lower body. Recent advancements often rely\non conventional neural networks and generative models to improve performance in\nthis task, such as Transformers and diffusion models. However, these approaches\nstruggle to strike a balance between achieving precise pose reconstruction and\nmaintaining fast inference speed. To overcome these challenges, a lightweight\nand efficient model, SSD-Poser, is designed for robust full-body motion\nestimation from sparse observations. SSD-Poser incorporates a well-designed\nhybrid encoder, State Space Attention Encoders, to adapt the state space\nduality to complex motion poses and enable real-time realistic pose\nreconstruction. Moreover, a Frequency-Aware Decoder is introduced to mitigate\njitter caused by variable-frequency motion signals, remarkably enhancing the\nmotion smoothness. Comprehensive experiments on the AMASS dataset demonstrate\nthat SSD-Poser achieves exceptional accuracy and computational efficiency,\nshowing outstanding inference efficiency compared to state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing applications of AR/VR increase the demand for real-time full-body\npose estimation from Head-Mounted Displays (HMDs). Although HMDs provide joint\nsignals from the head and hands, reconstructing a full-body pose remains\nchallenging due to the unconstrained lower body. Recent advancements often rely\non conventional neural networks and generative models to improve performance in\nthis task, such as Transformers and diffusion models. However, these approaches\nstruggle to strike a balance between achieving precise pose reconstruction and\nmaintaining fast inference speed. To overcome these challenges, a lightweight\nand efficient model, SSD-Poser, is designed for robust full-body motion\nestimation from sparse observations. SSD-Poser incorporates a well-designed\nhybrid encoder, State Space Attention Encoders, to adapt the state space\nduality to complex motion poses and enable real-time realistic pose\nreconstruction. Moreover, a Frequency-Aware Decoder is introduced to mitigate\njitter caused by variable-frequency motion signals, remarkably enhancing the\nmotion smoothness. Comprehensive experiments on the AMASS dataset demonstrate\nthat SSD-Poser achieves exceptional accuracy and computational efficiency,\nshowing outstanding inference efficiency compared to state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Shuting Zhao"
                    },
                    {
                        "name": "Linxin Bai"
                    },
                    {
                        "name": "Liangjing Shao"
                    },
                    {
                        "name": "Ye Zhang"
                    },
                    {
                        "name": "Xinrong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xinrong Chen"
                },
                "author": "Xinrong Chen",
                "arxiv_comment": "9 pages, 6 figures, conference ICMR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68U05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12486v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12486v5",
                "updated": "2025-04-25T13:03:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    13,
                    3,
                    37,
                    4,
                    115,
                    0
                ],
                "published": "2025-02-18T03:15:55Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    3,
                    15,
                    55,
                    1,
                    49,
                    0
                ],
                "title": "EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via\n  Reinforcement Learning"
                },
                "summary": "Large Language Models (LLMs) have shown impressive reasoning capabilities in\nwell-defined problems with clear solutions, such as mathematics and coding.\nHowever, they still struggle with complex real-world scenarios like business\nnegotiations, which require strategic reasoning-an ability to navigate dynamic\nenvironments and align long-term goals amidst uncertainty. Existing methods for\nstrategic reasoning face challenges in adaptability, scalability, and\ntransferring strategies to new contexts. To address these issues, we propose\nexplicit policy optimization (EPO) for strategic reasoning, featuring an LLM\nthat provides strategies in open-ended action space and can be plugged into\narbitrary LLM agents to motivate goal-directed behavior. To improve\nadaptability and policy transferability, we train the strategic reasoning model\nvia multi-turn reinforcement learning (RL) using process rewards and iterative\nself-play, without supervised fine-tuning (SFT) as a preliminary step.\nExperiments across social and physical domains demonstrate EPO's ability of\nlong-term goal alignment through enhanced strategic reasoning, achieving\nstate-of-the-art performance on social dialogue and web navigation tasks. Our\nfindings reveal various collaborative reasoning mechanisms emergent in EPO and\nits effectiveness in generating novel strategies, underscoring its potential\nfor strategic reasoning in real-world applications. Code and data are available\nat https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/EPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive reasoning capabilities in\nwell-defined problems with clear solutions, such as mathematics and coding.\nHowever, they still struggle with complex real-world scenarios like business\nnegotiations, which require strategic reasoning-an ability to navigate dynamic\nenvironments and align long-term goals amidst uncertainty. Existing methods for\nstrategic reasoning face challenges in adaptability, scalability, and\ntransferring strategies to new contexts. To address these issues, we propose\nexplicit policy optimization (EPO) for strategic reasoning, featuring an LLM\nthat provides strategies in open-ended action space and can be plugged into\narbitrary LLM agents to motivate goal-directed behavior. To improve\nadaptability and policy transferability, we train the strategic reasoning model\nvia multi-turn reinforcement learning (RL) using process rewards and iterative\nself-play, without supervised fine-tuning (SFT) as a preliminary step.\nExperiments across social and physical domains demonstrate EPO's ability of\nlong-term goal alignment through enhanced strategic reasoning, achieving\nstate-of-the-art performance on social dialogue and web navigation tasks. Our\nfindings reveal various collaborative reasoning mechanisms emergent in EPO and\nits effectiveness in generating novel strategies, underscoring its potential\nfor strategic reasoning in real-world applications. Code and data are available\nat https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/EPO."
                },
                "authors": [
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Yongbin Li"
                    },
                    {
                        "name": "Yuchuan Wu"
                    },
                    {
                        "name": "Wentao Ma"
                    },
                    {
                        "name": "Aobo Kong"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Jianbin Jiao"
                    },
                    {
                        "name": "Junge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Junge Zhang"
                },
                "author": "Junge Zhang",
                "arxiv_comment": "22 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12486v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12486v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18318v1",
                "updated": "2025-04-25T12:53:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    12,
                    53,
                    15,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T12:53:15Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    12,
                    53,
                    15,
                    4,
                    115,
                    0
                ],
                "title": "STP4D: Spatio-Temporal-Prompt Consistent Modeling for Text-to-4D\n  Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STP4D: Spatio-Temporal-Prompt Consistent Modeling for Text-to-4D\n  Gaussian Splatting"
                },
                "summary": "Text-to-4D generation is rapidly developing and widely applied in various\nscenarios. However, existing methods often fail to incorporate adequate\nspatio-temporal modeling and prompt alignment within a unified framework,\nresulting in temporal inconsistencies, geometric distortions, or low-quality 4D\ncontent that deviates from the provided texts. Therefore, we propose STP4D, a\nnovel approach that aims to integrate comprehensive spatio-temporal-prompt\nconsistency modeling for high-quality text-to-4D generation. Specifically,\nSTP4D employs three carefully designed modules: Time-varying Prompt Embedding,\nGeometric Information Enhancement, and Temporal Extension Deformation, which\ncollaborate to accomplish this goal. Furthermore, STP4D is among the first\nmethods to exploit the Diffusion model to generate 4D Gaussians, combining the\nfine-grained modeling capabilities and the real-time rendering process of 4DGS\nwith the rapid inference speed of the Diffusion model. Extensive experiments\ndemonstrate that STP4D excels in generating high-fidelity 4D content with\nexceptional efficiency (approximately 4.6s per asset), surpassing existing\nmethods in both quality and speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-4D generation is rapidly developing and widely applied in various\nscenarios. However, existing methods often fail to incorporate adequate\nspatio-temporal modeling and prompt alignment within a unified framework,\nresulting in temporal inconsistencies, geometric distortions, or low-quality 4D\ncontent that deviates from the provided texts. Therefore, we propose STP4D, a\nnovel approach that aims to integrate comprehensive spatio-temporal-prompt\nconsistency modeling for high-quality text-to-4D generation. Specifically,\nSTP4D employs three carefully designed modules: Time-varying Prompt Embedding,\nGeometric Information Enhancement, and Temporal Extension Deformation, which\ncollaborate to accomplish this goal. Furthermore, STP4D is among the first\nmethods to exploit the Diffusion model to generate 4D Gaussians, combining the\nfine-grained modeling capabilities and the real-time rendering process of 4DGS\nwith the rapid inference speed of the Diffusion model. Extensive experiments\ndemonstrate that STP4D excels in generating high-fidelity 4D content with\nexceptional efficiency (approximately 4.6s per asset), surpassing existing\nmethods in both quality and speed."
                },
                "authors": [
                    {
                        "name": "Yunze Deng"
                    },
                    {
                        "name": "Haijun Xiong"
                    },
                    {
                        "name": "Bin Feng"
                    },
                    {
                        "name": "Xinggang Wang"
                    },
                    {
                        "name": "Wenyu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wenyu Liu"
                },
                "author": "Wenyu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18316v1",
                "updated": "2025-04-25T12:48:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    12,
                    48,
                    8,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T12:48:08Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    12,
                    48,
                    8,
                    4,
                    115,
                    0
                ],
                "title": "Towards Adaptive Software Agents for Debugging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Adaptive Software Agents for Debugging"
                },
                "summary": "Using multiple agents was found to improve the debugging capabilities of\nLarge Language Models. However, increasing the number of LLM-agents has several\ndrawbacks such as increasing the running costs and rising the risk for the\nagents to lose focus. In this work, we propose an adaptive agentic design,\nwhere the number of agents and their roles are determined dynamically based on\nthe characteristics of the task to be achieved. In this design, the agents\nroles are not predefined, but are generated after analyzing the problem to be\nsolved. Our initial evaluation shows that, with the adaptive design, the number\nof agents that are generated depends on the complexity of the buggy code. In\nfact, for simple code with mere syntax issues, the problem was usually fixed\nusing one agent only. However, for more complex problems, we noticed the\ncreation of a higher number of agents. Regarding the effectiveness of the fix,\nwe noticed an average improvement of 11% compared to the one-shot prompting.\nGiven these promising results, we outline future research directions to improve\nour design for adaptive software agents that can autonomously plan and conduct\ntheir software goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using multiple agents was found to improve the debugging capabilities of\nLarge Language Models. However, increasing the number of LLM-agents has several\ndrawbacks such as increasing the running costs and rising the risk for the\nagents to lose focus. In this work, we propose an adaptive agentic design,\nwhere the number of agents and their roles are determined dynamically based on\nthe characteristics of the task to be achieved. In this design, the agents\nroles are not predefined, but are generated after analyzing the problem to be\nsolved. Our initial evaluation shows that, with the adaptive design, the number\nof agents that are generated depends on the complexity of the buggy code. In\nfact, for simple code with mere syntax issues, the problem was usually fixed\nusing one agent only. However, for more complex problems, we noticed the\ncreation of a higher number of agents. Regarding the effectiveness of the fix,\nwe noticed an average improvement of 11% compared to the one-shot prompting.\nGiven these promising results, we outline future research directions to improve\nour design for adaptive software agents that can autonomously plan and conduct\ntheir software goals."
                },
                "authors": [
                    {
                        "name": "Yacine Majdoub"
                    },
                    {
                        "name": "Eya Ben Charrada"
                    },
                    {
                        "name": "Haifa Touati"
                    }
                ],
                "author_detail": {
                    "name": "Haifa Touati"
                },
                "author": "Haifa Touati",
                "arxiv_comment": "5 pages, 3 figures, FSE2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.02252v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.02252v3",
                "updated": "2025-04-25T12:47:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    12,
                    47,
                    21,
                    4,
                    115,
                    0
                ],
                "published": "2023-12-04T18:14:29Z",
                "published_parsed": [
                    2023,
                    12,
                    4,
                    18,
                    14,
                    29,
                    0,
                    338,
                    0
                ],
                "title": "StoryGPT-V: Large Language Models as Consistent Story Visualizers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StoryGPT-V: Large Language Models as Consistent Story Visualizers"
                },
                "summary": "Recent generative models have demonstrated impressive capabilities in\ngenerating realistic and visually pleasing images grounded on textual prompts.\nNevertheless, a significant challenge remains in applying these models for the\nmore intricate task of story visualization. Since it requires resolving\npronouns (he, she, they) in the frame descriptions, i.e., anaphora resolution,\nand ensuring consistent characters and background synthesis across frames. Yet,\nthe emerging Large Language Model (LLM) showcases robust reasoning abilities to\nnavigate through ambiguous references and process extensive sequences.\nTherefore, we introduce \\emph{StoryGPT-V}, which leverages the merits of the\nlatent diffusion (LDM) and LLM to produce images with consistent and\nhigh-quality characters grounded on given story descriptions. First, we train a\ncharacter-aware LDM, which takes character-augmented semantic embedding as\ninput and includes the supervision of the cross-attention map using character\nsegmentation masks, aiming to enhance character generation accuracy and\nfaithfulness. In the second stage, we enable an alignment between the output of\nLLM and the character-augmented embedding residing in the input space of the\nfirst-stage model. This harnesses the reasoning ability of LLM to address\nambiguous references and the comprehension capability to memorize the context.\nWe conduct comprehensive experiments on two visual story visualization\nbenchmarks. Our model reports superior quantitative results and consistently\ngenerates accurate characters of remarkable quality with low memory\nconsumption. Our code is publicly available at:\n\\href{https://xiaoqian-shen.github.io/StoryGPT-V}{https://xiaoqian-shen.github.io/StoryGPT-V}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent generative models have demonstrated impressive capabilities in\ngenerating realistic and visually pleasing images grounded on textual prompts.\nNevertheless, a significant challenge remains in applying these models for the\nmore intricate task of story visualization. Since it requires resolving\npronouns (he, she, they) in the frame descriptions, i.e., anaphora resolution,\nand ensuring consistent characters and background synthesis across frames. Yet,\nthe emerging Large Language Model (LLM) showcases robust reasoning abilities to\nnavigate through ambiguous references and process extensive sequences.\nTherefore, we introduce \\emph{StoryGPT-V}, which leverages the merits of the\nlatent diffusion (LDM) and LLM to produce images with consistent and\nhigh-quality characters grounded on given story descriptions. First, we train a\ncharacter-aware LDM, which takes character-augmented semantic embedding as\ninput and includes the supervision of the cross-attention map using character\nsegmentation masks, aiming to enhance character generation accuracy and\nfaithfulness. In the second stage, we enable an alignment between the output of\nLLM and the character-augmented embedding residing in the input space of the\nfirst-stage model. This harnesses the reasoning ability of LLM to address\nambiguous references and the comprehension capability to memorize the context.\nWe conduct comprehensive experiments on two visual story visualization\nbenchmarks. Our model reports superior quantitative results and consistently\ngenerates accurate characters of remarkable quality with low memory\nconsumption. Our code is publicly available at:\n\\href{https://xiaoqian-shen.github.io/StoryGPT-V}{https://xiaoqian-shen.github.io/StoryGPT-V}."
                },
                "authors": [
                    {
                        "name": "Xiaoqian Shen"
                    },
                    {
                        "name": "Mohamed Elhoseiny"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Elhoseiny"
                },
                "author": "Mohamed Elhoseiny",
                "arxiv_comment": "Accepted to CVPR 2025; Project page:\n  https://xiaoqian-shen.github.io/StoryGPT-V",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.02252v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.02252v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02810v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02810v2",
                "updated": "2025-04-25T12:02:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    12,
                    2,
                    19,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-03T17:54:18Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    54,
                    18,
                    3,
                    93,
                    0
                ],
                "title": "Generative Evaluation of Complex Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Evaluation of Complex Reasoning in Large Language Models"
                },
                "summary": "With powerful large language models (LLMs) demonstrating superhuman reasoning\ncapabilities, a critical question arises: Do LLMs genuinely reason, or do they\nmerely recall answers from their extensive, web-scraped training datasets?\nPublicly released benchmarks inevitably become contaminated once incorporated\ninto subsequent LLM training sets, undermining their reliability as faithful\nassessments. To address this, we introduce KUMO, a generative evaluation\nframework designed specifically for assessing reasoning in LLMs. KUMO\nsynergistically combines LLMs with symbolic engines to dynamically produce\ndiverse, multi-turn reasoning tasks that are partially observable and\nadjustable in difficulty. Through an automated pipeline, KUMO continuously\ngenerates novel tasks across open-ended domains, compelling models to\ndemonstrate genuine generalization rather than memorization. We evaluated 23\nstate-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO,\nbenchmarking their reasoning abilities against university students. Our\nfindings reveal that many LLMs have outperformed university-level performance\non easy reasoning tasks, and reasoning-scaled LLMs reach university-level\nperformance on complex reasoning challenges. Moreover, LLM performance on KUMO\ntasks correlates strongly with results on newly released real-world reasoning\nbenchmarks, underscoring KUMO's value as a robust, enduring assessment tool for\ngenuine LLM reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With powerful large language models (LLMs) demonstrating superhuman reasoning\ncapabilities, a critical question arises: Do LLMs genuinely reason, or do they\nmerely recall answers from their extensive, web-scraped training datasets?\nPublicly released benchmarks inevitably become contaminated once incorporated\ninto subsequent LLM training sets, undermining their reliability as faithful\nassessments. To address this, we introduce KUMO, a generative evaluation\nframework designed specifically for assessing reasoning in LLMs. KUMO\nsynergistically combines LLMs with symbolic engines to dynamically produce\ndiverse, multi-turn reasoning tasks that are partially observable and\nadjustable in difficulty. Through an automated pipeline, KUMO continuously\ngenerates novel tasks across open-ended domains, compelling models to\ndemonstrate genuine generalization rather than memorization. We evaluated 23\nstate-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO,\nbenchmarking their reasoning abilities against university students. Our\nfindings reveal that many LLMs have outperformed university-level performance\non easy reasoning tasks, and reasoning-scaled LLMs reach university-level\nperformance on complex reasoning challenges. Moreover, LLM performance on KUMO\ntasks correlates strongly with results on newly released real-world reasoning\nbenchmarks, underscoring KUMO's value as a robust, enduring assessment tool for\ngenuine LLM reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Haowei Lin"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Ruilin Yan"
                    },
                    {
                        "name": "Baizhou Huang"
                    },
                    {
                        "name": "Haotian Ye"
                    },
                    {
                        "name": "Jianhua Zhu"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Jianzhu Ma"
                    },
                    {
                        "name": "Yitao Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yitao Liang"
                },
                "author": "Yitao Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02810v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02810v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18271v1",
                "updated": "2025-04-25T11:29:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    11,
                    29,
                    30,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T11:29:30Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    11,
                    29,
                    30,
                    4,
                    115,
                    0
                ],
                "title": "LEAM: A Prompt-only Large Language Model-enabled Antenna Modeling Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEAM: A Prompt-only Large Language Model-enabled Antenna Modeling Method"
                },
                "summary": "Antenna modeling is a time-consuming and complex process, decreasing the\nspeed of antenna analysis and design. In this paper, a large language model\n(LLM)- enabled antenna modeling method, called LEAM, is presented to address\nthis challenge. LEAM enables automatic antenna model generation based on\nlanguage descriptions via prompt input, images, descriptions from academic\npapers, patents, and technical reports (either one or multiple). The\neffectiveness of LEAM is demonstrated by three examples: a Vivaldi antenna\ngenerated from a complete user description, a slotted patch antenna generated\nfrom an incomplete user description and the operating frequency, and a monopole\nslotted antenna generated from images and descriptions scanned from the\nliterature. For all the examples, correct antenna models are generated in a few\nminutes. The code can be accessed via https://github.com/TaoWu974/LEAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Antenna modeling is a time-consuming and complex process, decreasing the\nspeed of antenna analysis and design. In this paper, a large language model\n(LLM)- enabled antenna modeling method, called LEAM, is presented to address\nthis challenge. LEAM enables automatic antenna model generation based on\nlanguage descriptions via prompt input, images, descriptions from academic\npapers, patents, and technical reports (either one or multiple). The\neffectiveness of LEAM is demonstrated by three examples: a Vivaldi antenna\ngenerated from a complete user description, a slotted patch antenna generated\nfrom an incomplete user description and the operating frequency, and a monopole\nslotted antenna generated from images and descriptions scanned from the\nliterature. For all the examples, correct antenna models are generated in a few\nminutes. The code can be accessed via https://github.com/TaoWu974/LEAM."
                },
                "authors": [
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Kexue Fu"
                    },
                    {
                        "name": "Qiang Hua"
                    },
                    {
                        "name": "Xinxin Liu"
                    },
                    {
                        "name": "Muhammad Ali Imran"
                    },
                    {
                        "name": "Bo Liu"
                    }
                ],
                "author_detail": {
                    "name": "Bo Liu"
                },
                "author": "Bo Liu",
                "arxiv_comment": "Code are available: https://github.com/TaoWu974/LEAM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18269v1",
                "updated": "2025-04-25T11:27:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    11,
                    27,
                    44,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T11:27:44Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    11,
                    27,
                    44,
                    4,
                    115,
                    0
                ],
                "title": "TextTIGER: Text-based Intelligent Generation with Entity Prompt\n  Refinement for Text-to-Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TextTIGER: Text-based Intelligent Generation with Entity Prompt\n  Refinement for Text-to-Image Generation"
                },
                "summary": "Generating images from prompts containing specific entities requires models\nto retain as much entity-specific knowledge as possible. However, fully\nmemorizing such knowledge is impractical due to the vast number of entities and\ntheir continuous emergence. To address this, we propose Text-based Intelligent\nGeneration with Entity prompt Refinement (TextTIGER), which augments knowledge\non entities included in the prompts and then summarizes the augmented\ndescriptions using Large Language Models (LLMs) to mitigate performance\ndegradation from longer inputs. To evaluate our method, we introduce WiT-Cub\n(WiT with Captions and Uncomplicated Background-explanations), a dataset\ncomprising captions, images, and an entity list. Experiments on four image\ngeneration models and five LLMs show that TextTIGER improves image generation\nperformance in standard metrics (IS, FID, and CLIPScore) compared to\ncaption-only prompts. Additionally, multiple annotators' evaluation confirms\nthat the summarized descriptions are more informative, validating LLMs' ability\nto generate concise yet rich descriptions. These findings demonstrate that\nrefining prompts with augmented and summarized entity-related descriptions\nenhances image generation capabilities. The code and dataset will be available\nupon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating images from prompts containing specific entities requires models\nto retain as much entity-specific knowledge as possible. However, fully\nmemorizing such knowledge is impractical due to the vast number of entities and\ntheir continuous emergence. To address this, we propose Text-based Intelligent\nGeneration with Entity prompt Refinement (TextTIGER), which augments knowledge\non entities included in the prompts and then summarizes the augmented\ndescriptions using Large Language Models (LLMs) to mitigate performance\ndegradation from longer inputs. To evaluate our method, we introduce WiT-Cub\n(WiT with Captions and Uncomplicated Background-explanations), a dataset\ncomprising captions, images, and an entity list. Experiments on four image\ngeneration models and five LLMs show that TextTIGER improves image generation\nperformance in standard metrics (IS, FID, and CLIPScore) compared to\ncaption-only prompts. Additionally, multiple annotators' evaluation confirms\nthat the summarized descriptions are more informative, validating LLMs' ability\nto generate concise yet rich descriptions. These findings demonstrate that\nrefining prompts with augmented and summarized entity-related descriptions\nenhances image generation capabilities. The code and dataset will be available\nupon acceptance."
                },
                "authors": [
                    {
                        "name": "Shintaro Ozaki"
                    },
                    {
                        "name": "Kazuki Hayashi"
                    },
                    {
                        "name": "Yusuke Sakai"
                    },
                    {
                        "name": "Jingun Kwon"
                    },
                    {
                        "name": "Hidetaka Kamigaito"
                    },
                    {
                        "name": "Katsuhiko Hayashi"
                    },
                    {
                        "name": "Manabu Okumura"
                    },
                    {
                        "name": "Taro Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Taro Watanabe"
                },
                "author": "Taro Watanabe",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18263v1",
                "updated": "2025-04-25T11:17:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    11,
                    17,
                    46,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T11:17:46Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    11,
                    17,
                    46,
                    4,
                    115,
                    0
                ],
                "title": "COSINUS model-independent sensitivity to the DAMA/LIBRA dark matter\n  signal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COSINUS model-independent sensitivity to the DAMA/LIBRA dark matter\n  signal"
                },
                "summary": "COSINUS is a dark matter direct detection experiment using NaI crystals as\ncryogenic scintillating calorimeters. If no signal is observed, this will\nconstrain the dark matter scattering rate in sodium iodide. We investigate how\nthis constraint can be used to infer that the annual modulation signal observed\nin DAMA/LIBRA experiment cannot originate from dark matter nuclear recoil\nevents, independent of the DM model. We achieve this by unfolding the DAMA\nmodulation spectrum to obtain the implied unquenched nuclear recoil spectrum,\nwhich we compare to the expected COSINUS sensitivity. We find that assuming\nzero background in the signal region, a 1$\\sigma$, 2$\\sigma$ or 3$\\sigma$\nconfidence limit exclusion can be obtained with 57, 130 or 250 kg day of\nexposure, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COSINUS is a dark matter direct detection experiment using NaI crystals as\ncryogenic scintillating calorimeters. If no signal is observed, this will\nconstrain the dark matter scattering rate in sodium iodide. We investigate how\nthis constraint can be used to infer that the annual modulation signal observed\nin DAMA/LIBRA experiment cannot originate from dark matter nuclear recoil\nevents, independent of the DM model. We achieve this by unfolding the DAMA\nmodulation spectrum to obtain the implied unquenched nuclear recoil spectrum,\nwhich we compare to the expected COSINUS sensitivity. We find that assuming\nzero background in the signal region, a 1$\\sigma$, 2$\\sigma$ or 3$\\sigma$\nconfidence limit exclusion can be obtained with 57, 130 or 250 kg day of\nexposure, respectively."
                },
                "authors": [
                    {
                        "name": "G. Angloher"
                    },
                    {
                        "name": "M. R. Bharadwaj"
                    },
                    {
                        "name": "A. Böhmer"
                    },
                    {
                        "name": "M. Cababie"
                    },
                    {
                        "name": "I. Colantoni"
                    },
                    {
                        "name": "I. Dafinei"
                    },
                    {
                        "name": "N. Di Marco"
                    },
                    {
                        "name": "C. Dittmar"
                    },
                    {
                        "name": "L. Einfalt"
                    },
                    {
                        "name": "F. Ferella"
                    },
                    {
                        "name": "F. Ferroni"
                    },
                    {
                        "name": "S. Fichtinger"
                    },
                    {
                        "name": "A. Filipponi"
                    },
                    {
                        "name": "T. Frank"
                    },
                    {
                        "name": "M. Friedl"
                    },
                    {
                        "name": "Z. Ge"
                    },
                    {
                        "name": "M. Heikinheimo"
                    },
                    {
                        "name": "M. N. Hughes"
                    },
                    {
                        "name": "K. Huitu"
                    },
                    {
                        "name": "M. Kellermann"
                    },
                    {
                        "name": "R. Maji"
                    },
                    {
                        "name": "M. Mancuso"
                    },
                    {
                        "name": "L. Pagnanini"
                    },
                    {
                        "name": "F. Petricca"
                    },
                    {
                        "name": "S. Pirro"
                    },
                    {
                        "name": "F. Pröbst"
                    },
                    {
                        "name": "G. Profeta"
                    },
                    {
                        "name": "A. Puiu"
                    },
                    {
                        "name": "F. Reindl"
                    },
                    {
                        "name": "K. Schäffner"
                    },
                    {
                        "name": "J. Schieck"
                    },
                    {
                        "name": "P. Schreiner"
                    },
                    {
                        "name": "C. Schwertner"
                    },
                    {
                        "name": "K. Shera"
                    },
                    {
                        "name": "M. Stahlberg"
                    },
                    {
                        "name": "A. Stendahl"
                    },
                    {
                        "name": "M. Stukel"
                    },
                    {
                        "name": "C. Tresca"
                    },
                    {
                        "name": "F. Wagner"
                    },
                    {
                        "name": "S. Yue"
                    },
                    {
                        "name": "V. Zema"
                    },
                    {
                        "name": "Y. Zhu"
                    },
                    {
                        "name": "The COSINUS Collaboration"
                    }
                ],
                "author_detail": {
                    "name": "The COSINUS Collaboration"
                },
                "author": "The COSINUS Collaboration",
                "arxiv_comment": "8 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22784v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22784v2",
                "updated": "2025-04-25T11:17:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    11,
                    17,
                    27,
                    4,
                    115,
                    0
                ],
                "published": "2024-10-30T07:59:52Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    7,
                    59,
                    52,
                    2,
                    304,
                    0
                ],
                "title": "Contrastive Learning and Adversarial Disentanglement for Task-Oriented\n  Semantic Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive Learning and Adversarial Disentanglement for Task-Oriented\n  Semantic Communications"
                },
                "summary": "Task-oriented semantic communication systems have emerged as a promising\napproach to achieving efficient and intelligent data transmission, where only\ninformation relevant to a specific task is communicated. However, existing\nmethods struggle to fully disentangle task-relevant and task-irrelevant\ninformation, leading to privacy concerns and subpar performance. To address\nthis, we propose an information-bottleneck method, named CLAD (contrastive\nlearning and adversarial disentanglement). CLAD utilizes contrastive learning\nto effectively capture task-relevant features while employing adversarial\ndisentanglement to discard task-irrelevant information. Additionally, due to\nthe lack of reliable and reproducible methods to gain insight into the\ninformativeness and minimality of the encoded feature vectors, we introduce a\nnew technique to compute the information retention index (IRI), a comparative\nmetric used as a proxy for the mutual information between the encoded features\nand the input, reflecting the minimality of the encoded features. The IRI\nquantifies the minimality and informativeness of the encoded feature vectors\nacross different task-oriented communication techniques. Our extensive\nexperiments demonstrate that CLAD outperforms state-of-the-art baselines in\nterms of semantic extraction, task performance, privacy preservation, and IRI.\nCLAD achieves a predictive performance improvement of around 2.5-3%, along with\na 77-90% reduction in IRI and a 57-76% decrease in adversarial attribute\ninference attack accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-oriented semantic communication systems have emerged as a promising\napproach to achieving efficient and intelligent data transmission, where only\ninformation relevant to a specific task is communicated. However, existing\nmethods struggle to fully disentangle task-relevant and task-irrelevant\ninformation, leading to privacy concerns and subpar performance. To address\nthis, we propose an information-bottleneck method, named CLAD (contrastive\nlearning and adversarial disentanglement). CLAD utilizes contrastive learning\nto effectively capture task-relevant features while employing adversarial\ndisentanglement to discard task-irrelevant information. Additionally, due to\nthe lack of reliable and reproducible methods to gain insight into the\ninformativeness and minimality of the encoded feature vectors, we introduce a\nnew technique to compute the information retention index (IRI), a comparative\nmetric used as a proxy for the mutual information between the encoded features\nand the input, reflecting the minimality of the encoded features. The IRI\nquantifies the minimality and informativeness of the encoded feature vectors\nacross different task-oriented communication techniques. Our extensive\nexperiments demonstrate that CLAD outperforms state-of-the-art baselines in\nterms of semantic extraction, task performance, privacy preservation, and IRI.\nCLAD achieves a predictive performance improvement of around 2.5-3%, along with\na 77-90% reduction in IRI and a 57-76% decrease in adversarial attribute\ninference attack accuracy."
                },
                "authors": [
                    {
                        "name": "Omar Erak"
                    },
                    {
                        "name": "Omar Alhussein"
                    },
                    {
                        "name": "Wen Tong"
                    }
                ],
                "author_detail": {
                    "name": "Wen Tong"
                },
                "author": "Wen Tong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22784v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22784v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06276v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06276v5",
                "updated": "2025-04-25T11:10:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    11,
                    10,
                    43,
                    4,
                    115,
                    0
                ],
                "published": "2024-08-12T16:39:03Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    39,
                    3,
                    0,
                    225,
                    0
                ],
                "title": "Review-driven Personalized Preference Reasoning with Large Language\n  Models for Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Review-driven Personalized Preference Reasoning with Large Language\n  Models for Recommendation"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nexceptional performance across a wide range of tasks, generating significant\ninterest in their application to recommendation systems. However, existing\nmethods have not fully capitalized on the potential of LLMs, often constrained\nby limited input information or failing to fully utilize their advanced\nreasoning capabilities. To address these limitations, we introduce EXP3RT, a\nnovel LLM-based recommender designed to leverage rich preference information\ncontained in user and item reviews. EXP3RT is basically fine-tuned through\ndistillation from a teacher LLM to perform three key tasks in order: EXP3RT\nfirst extracts and encapsulates essential subjective preferences from raw\nreviews, aggregates and summarizes them according to specific criteria to\ncreate user and item profiles. It then generates detailed step-by-step\nreasoning followed by predicted rating, i.e., reasoning-enhanced rating\nprediction, by considering both subjective and objective information from\nuser/item profiles and item descriptions. This personalized preference\nreasoning from EXP3RT enhances rating prediction accuracy and also provides\nfaithful and reasonable explanations for recommendation. Extensive experiments\nshow that EXP3RT outperforms existing methods on both rating prediction and\ncandidate item reranking for top-k recommendation, while significantly\nenhancing the explainability of recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have demonstrated\nexceptional performance across a wide range of tasks, generating significant\ninterest in their application to recommendation systems. However, existing\nmethods have not fully capitalized on the potential of LLMs, often constrained\nby limited input information or failing to fully utilize their advanced\nreasoning capabilities. To address these limitations, we introduce EXP3RT, a\nnovel LLM-based recommender designed to leverage rich preference information\ncontained in user and item reviews. EXP3RT is basically fine-tuned through\ndistillation from a teacher LLM to perform three key tasks in order: EXP3RT\nfirst extracts and encapsulates essential subjective preferences from raw\nreviews, aggregates and summarizes them according to specific criteria to\ncreate user and item profiles. It then generates detailed step-by-step\nreasoning followed by predicted rating, i.e., reasoning-enhanced rating\nprediction, by considering both subjective and objective information from\nuser/item profiles and item descriptions. This personalized preference\nreasoning from EXP3RT enhances rating prediction accuracy and also provides\nfaithful and reasonable explanations for recommendation. Extensive experiments\nshow that EXP3RT outperforms existing methods on both rating prediction and\ncandidate item reranking for top-k recommendation, while significantly\nenhancing the explainability of recommendation systems."
                },
                "authors": [
                    {
                        "name": "Jieyong Kim"
                    },
                    {
                        "name": "Hyunseo Kim"
                    },
                    {
                        "name": "Hyunjin Cho"
                    },
                    {
                        "name": "SeongKu Kang"
                    },
                    {
                        "name": "Buru Chang"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "arxiv_doi": "10.1145/3726302.3730055",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730055",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.06276v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06276v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to SIGIR 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18260v1",
                "updated": "2025-04-25T11:08:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    11,
                    8,
                    27,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T11:08:27Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    11,
                    8,
                    27,
                    4,
                    115,
                    0
                ],
                "title": "MAGI: Multi-Agent Guided Interview for Psychiatric Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAGI: Multi-Agent Guided Interview for Psychiatric Assessment"
                },
                "summary": "Automating structured clinical interviews could revolutionize mental\nhealthcare accessibility, yet existing large language models (LLMs) approaches\nfail to align with psychiatric diagnostic protocols. We present MAGI, the first\nframework that transforms the gold-standard Mini International Neuropsychiatric\nInterview (MINI) into automatic computational workflows through coordinated\nmulti-agent collaboration. MAGI dynamically navigates clinical logic via four\nspecialized agents: 1) an interview tree guided navigation agent adhering to\nthe MINI's branching structure, 2) an adaptive question agent blending\ndiagnostic probing, explaining, and empathy, 3) a judgment agent validating\nwhether the response from participants meet the node, and 4) a diagnosis Agent\ngenerating Psychometric Chain-of- Thought (PsyCoT) traces that explicitly map\nsymptoms to clinical criteria. Experimental results on 1,002 real-world\nparticipants covering depression, generalized anxiety, social anxiety and\nsuicide shows that MAGI advances LLM- assisted mental health assessment by\ncombining clinical rigor, conversational adaptability, and explainable\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating structured clinical interviews could revolutionize mental\nhealthcare accessibility, yet existing large language models (LLMs) approaches\nfail to align with psychiatric diagnostic protocols. We present MAGI, the first\nframework that transforms the gold-standard Mini International Neuropsychiatric\nInterview (MINI) into automatic computational workflows through coordinated\nmulti-agent collaboration. MAGI dynamically navigates clinical logic via four\nspecialized agents: 1) an interview tree guided navigation agent adhering to\nthe MINI's branching structure, 2) an adaptive question agent blending\ndiagnostic probing, explaining, and empathy, 3) a judgment agent validating\nwhether the response from participants meet the node, and 4) a diagnosis Agent\ngenerating Psychometric Chain-of- Thought (PsyCoT) traces that explicitly map\nsymptoms to clinical criteria. Experimental results on 1,002 real-world\nparticipants covering depression, generalized anxiety, social anxiety and\nsuicide shows that MAGI advances LLM- assisted mental health assessment by\ncombining clinical rigor, conversational adaptability, and explainable\nreasoning."
                },
                "authors": [
                    {
                        "name": "Guanqun Bi"
                    },
                    {
                        "name": "Zhuang Chen"
                    },
                    {
                        "name": "Zhoufu Liu"
                    },
                    {
                        "name": "Hongkai Wang"
                    },
                    {
                        "name": "Xiyao Xiao"
                    },
                    {
                        "name": "Yuqiang Xie"
                    },
                    {
                        "name": "Wen Zhang"
                    },
                    {
                        "name": "Yongkang Huang"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Libiao Peng"
                    },
                    {
                        "name": "Yi Feng"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "arxiv_comment": "In progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03463v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03463v2",
                "updated": "2025-04-25T11:00:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    11,
                    0,
                    15,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-04T14:12:53Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    12,
                    53,
                    4,
                    94,
                    0
                ],
                "title": "Generating ensembles of spatially-coherent in-situ forecasts using flow\n  matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating ensembles of spatially-coherent in-situ forecasts using flow\n  matching"
                },
                "summary": "We propose a machine-learning-based methodology for in-situ weather forecast\npostprocessing that is both spatially coherent and multivariate. Compared to\nprevious work, our Flow MAtching Postprocessing (FMAP) better represents the\ncorrelation structures of the observations distribution, while also improving\nmarginal performance at the stations. FMAP generates forecasts that are not\nbound to what is already modeled by the underlying gridded prediction and can\ninfer new correlation structures from data. The resulting model can generate an\narbitrary number of forecasts from a limited number of numerical simulations,\nallowing for low-cost forecasting systems. A single training is sufficient to\nperform postprocessing at multiple lead times, in contrast with other methods\nwhich use multiple trained networks at generation time. This work details our\nmethodology, including a spatial attention transformer backbone trained within\na flow matching generative modeling framework. FMAP shows promising performance\nin experiments on the EUPPBench dataset, forecasting surface temperature and\nwind gust values at station locations in western Europe up to five-day lead\ntimes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a machine-learning-based methodology for in-situ weather forecast\npostprocessing that is both spatially coherent and multivariate. Compared to\nprevious work, our Flow MAtching Postprocessing (FMAP) better represents the\ncorrelation structures of the observations distribution, while also improving\nmarginal performance at the stations. FMAP generates forecasts that are not\nbound to what is already modeled by the underlying gridded prediction and can\ninfer new correlation structures from data. The resulting model can generate an\narbitrary number of forecasts from a limited number of numerical simulations,\nallowing for low-cost forecasting systems. A single training is sufficient to\nperform postprocessing at multiple lead times, in contrast with other methods\nwhich use multiple trained networks at generation time. This work details our\nmethodology, including a spatial attention transformer backbone trained within\na flow matching generative modeling framework. FMAP shows promising performance\nin experiments on the EUPPBench dataset, forecasting surface temperature and\nwind gust values at station locations in western Europe up to five-day lead\ntimes."
                },
                "authors": [
                    {
                        "name": "David Landry"
                    },
                    {
                        "name": "Claire Monteleoni"
                    },
                    {
                        "name": "Anastase Charantonis"
                    }
                ],
                "author_detail": {
                    "name": "Anastase Charantonis"
                },
                "author": "Anastase Charantonis",
                "arxiv_comment": "26 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03463v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03463v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10602v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10602v2",
                "updated": "2025-04-25T10:53:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    53,
                    27,
                    4,
                    115,
                    0
                ],
                "published": "2024-06-15T11:31:39Z",
                "published_parsed": [
                    2024,
                    6,
                    15,
                    11,
                    31,
                    39,
                    5,
                    167,
                    0
                ],
                "title": "Multilingual Large Language Models and Curse of Multilinguality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Large Language Models and Curse of Multilinguality"
                },
                "summary": "Multilingual Large Language Models (LLMs) have gained large popularity among\nNatural Language Processing (NLP) researchers and practitioners. These models,\ntrained on huge datasets, show proficiency across various languages and\ndemonstrate effectiveness in numerous downstream tasks. This paper navigates\nthe landscape of multilingual LLMs, providing an introductory overview of their\ntechnical aspects. It explains underlying architectures, objective functions,\npre-training data sources, and tokenization methods. This work explores the\nunique features of different model types: encoder-only (mBERT, XLM-R),\ndecoder-only (XGLM, PALM, BLOOM, GPT-3), and encoder-decoder models (mT5,\nmBART). Additionally, it addresses one of the significant limitations of\nmultilingual LLMs - the curse of multilinguality - and discusses current\nattempts to overcome it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Large Language Models (LLMs) have gained large popularity among\nNatural Language Processing (NLP) researchers and practitioners. These models,\ntrained on huge datasets, show proficiency across various languages and\ndemonstrate effectiveness in numerous downstream tasks. This paper navigates\nthe landscape of multilingual LLMs, providing an introductory overview of their\ntechnical aspects. It explains underlying architectures, objective functions,\npre-training data sources, and tokenization methods. This work explores the\nunique features of different model types: encoder-only (mBERT, XLM-R),\ndecoder-only (XGLM, PALM, BLOOM, GPT-3), and encoder-decoder models (mT5,\nmBART). Additionally, it addresses one of the significant limitations of\nmultilingual LLMs - the curse of multilinguality - and discusses current\nattempts to overcome it."
                },
                "authors": [
                    {
                        "name": "Daniil Gurgurov"
                    },
                    {
                        "name": "Tanja Bäumel"
                    },
                    {
                        "name": "Tatiana Anikina"
                    }
                ],
                "author_detail": {
                    "name": "Tatiana Anikina"
                },
                "author": "Tatiana Anikina",
                "arxiv_doi": "10.48550/arXiv.2406.10602",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.48550/arXiv.2406.10602",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.10602v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10602v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18246v1",
                "updated": "2025-04-25T10:46:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    46,
                    56,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T10:46:56Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    46,
                    56,
                    4,
                    115,
                    0
                ],
                "title": "Efficient Single-Pass Training for Multi-Turn Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Single-Pass Training for Multi-Turn Reasoning"
                },
                "summary": "Training Large Language Models ( LLMs) to generate explicit reasoning before\nthey produce an answer has been shown to improve their performance across\nvarious tasks such as mathematics and coding. However, fine-tuning LLMs on\nmulti-turn reasoning datasets presents a unique challenge: LLMs must generate\nreasoning tokens that are excluded from subsequent inputs to the LLM. This\ndiscrepancy prevents us from processing an entire conversation in a single\nforward pass-an optimization readily available when we fine-tune on a\nmulti-turn non-reasoning dataset. This paper proposes a novel approach that\novercomes this limitation through response token duplication and a custom\nattention mask that enforces appropriate visibility constraints. Our approach\nsignificantly reduces the training time and allows efficient fine-tuning on\nmulti-turn reasoning datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Large Language Models ( LLMs) to generate explicit reasoning before\nthey produce an answer has been shown to improve their performance across\nvarious tasks such as mathematics and coding. However, fine-tuning LLMs on\nmulti-turn reasoning datasets presents a unique challenge: LLMs must generate\nreasoning tokens that are excluded from subsequent inputs to the LLM. This\ndiscrepancy prevents us from processing an entire conversation in a single\nforward pass-an optimization readily available when we fine-tune on a\nmulti-turn non-reasoning dataset. This paper proposes a novel approach that\novercomes this limitation through response token duplication and a custom\nattention mask that enforces appropriate visibility constraints. Our approach\nsignificantly reduces the training time and allows efficient fine-tuning on\nmulti-turn reasoning datasets."
                },
                "authors": [
                    {
                        "name": "Ritesh Goru"
                    },
                    {
                        "name": "Shanay Mehta"
                    },
                    {
                        "name": "Prateek Jain"
                    }
                ],
                "author_detail": {
                    "name": "Prateek Jain"
                },
                "author": "Prateek Jain",
                "arxiv_comment": "9 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18217v1",
                "updated": "2025-04-25T09:58:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    9,
                    58,
                    33,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T09:58:33Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    9,
                    58,
                    33,
                    4,
                    115,
                    0
                ],
                "title": "An e-MERLIN & EVN radio counterpart to the ultraluminous X-ray source\n  M82 X-1",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An e-MERLIN & EVN radio counterpart to the ultraluminous X-ray source\n  M82 X-1"
                },
                "summary": "Ultra-luminous X-ray sources (ULXs) are X-ray bright (L$_{\\rm X-ray}\n>$3$\\times$10$^{39}$erg s$^{-1}$) extra-galactic objects that are powered by\neither neutron stars, or stellar or intermediate-mass black holes (IMBHs) but\nfew have been detected in the radio waveband. In the nearby galaxy M82, the\nbrightest ULX - M82 X$-$1, is thought to be associated with an IMBH but to date\ndoes not have a radio counterpart. We present deep wide-band reprocessed\ne-MERLIN images observed in 2015 May with an r.m.s. sensitivity of 7$\\mu$Jy\nbeam$^{-1}$ and report the discovery of a new radio source with an integrated\nflux of S$_{\\rm \\nu=4.88\\,GHz}$ = 174$\\pm$15$\\mu$Jy, which is spatially\nco-incident with the Chandra X-ray position of M82 X$-$1. This source is not\ndetected in archival MERLIN/e-MERLIN observations in the last three decades. A\nsearch for intra-observation variability in the 2015 e-MERLIN data was\ninconclusive, but a comparison with 1.5 GHz e-MERLIN observations taken a week\nprior yielded no detection. We also detect the source at the same position with\nmilliarcsecond angular resolution in EVN+e-MERLIN data from 2021 March at\n53$\\pm$10$\\mu$Jy. The radio source position is ICRF J2000 RA:\n09$^{h}$55$^{m}$50.1172$^{s}$, Dec: +69$^{\\circ}$40'46.606\" ($\\pm$1.5 mas).\nThese radio fluxes are consistent with other radio-detected ULXs on the\nradio:X-ray plane and points towards a stellar/intermediate-mass black hole.\nThe black hole mass inferred by the `fundamental plane of black hole activity'\nis 2650 M$_{\\odot}$, but this value remains highly uncertain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-luminous X-ray sources (ULXs) are X-ray bright (L$_{\\rm X-ray}\n>$3$\\times$10$^{39}$erg s$^{-1}$) extra-galactic objects that are powered by\neither neutron stars, or stellar or intermediate-mass black holes (IMBHs) but\nfew have been detected in the radio waveband. In the nearby galaxy M82, the\nbrightest ULX - M82 X$-$1, is thought to be associated with an IMBH but to date\ndoes not have a radio counterpart. We present deep wide-band reprocessed\ne-MERLIN images observed in 2015 May with an r.m.s. sensitivity of 7$\\mu$Jy\nbeam$^{-1}$ and report the discovery of a new radio source with an integrated\nflux of S$_{\\rm \\nu=4.88\\,GHz}$ = 174$\\pm$15$\\mu$Jy, which is spatially\nco-incident with the Chandra X-ray position of M82 X$-$1. This source is not\ndetected in archival MERLIN/e-MERLIN observations in the last three decades. A\nsearch for intra-observation variability in the 2015 e-MERLIN data was\ninconclusive, but a comparison with 1.5 GHz e-MERLIN observations taken a week\nprior yielded no detection. We also detect the source at the same position with\nmilliarcsecond angular resolution in EVN+e-MERLIN data from 2021 March at\n53$\\pm$10$\\mu$Jy. The radio source position is ICRF J2000 RA:\n09$^{h}$55$^{m}$50.1172$^{s}$, Dec: +69$^{\\circ}$40'46.606\" ($\\pm$1.5 mas).\nThese radio fluxes are consistent with other radio-detected ULXs on the\nradio:X-ray plane and points towards a stellar/intermediate-mass black hole.\nThe black hole mass inferred by the `fundamental plane of black hole activity'\nis 2650 M$_{\\odot}$, but this value remains highly uncertain."
                },
                "authors": [
                    {
                        "name": "D. Williams-Baldwin"
                    },
                    {
                        "name": "T. W. B. Muxlow"
                    },
                    {
                        "name": "G. Lucatelli"
                    },
                    {
                        "name": "R. J. Beswick"
                    }
                ],
                "author_detail": {
                    "name": "R. J. Beswick"
                },
                "arxiv_affiliation": "Jodrell Bank Centre for Astrophysics, School of Physics and Astronomy, The University of Manchester, Manchester, M13 9PL, UK",
                "author": "R. J. Beswick",
                "arxiv_comment": "8 pages, 4 figures, accepted to MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19662v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19662v2",
                "updated": "2025-04-25T09:54:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    9,
                    54,
                    59,
                    4,
                    115,
                    0
                ],
                "published": "2025-02-27T01:08:33Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    1,
                    8,
                    33,
                    3,
                    58,
                    0
                ],
                "title": "HALO: Hardware-aware quantization with low critical-path-delay weights\n  for LLM acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HALO: Hardware-aware quantization with low critical-path-delay weights\n  for LLM acceleration"
                },
                "summary": "Quantization is critical for efficiently deploying large language models\n(LLMs). Yet conventional methods remain hardware-agnostic, limited to bit-width\nconstraints, and do not account for intrinsic circuit characteristics such as\nthe timing behaviors and energy profiles of Multiply-Accumulate (MAC) units.\nThis disconnect from circuit-level behavior limits the ability to exploit\navailable timing margins and energy-saving opportunities, reducing the overall\nefficiency of deployment on modern accelerators.\n  To address these limitations, we propose HALO, a versatile framework for\nHardware-Aware Post-Training Quantization (PTQ). Unlike traditional methods,\nHALO explicitly incorporates detailed hardware characteristics, including\ncritical-path timing and power consumption, into its quantization approach.\nHALO strategically selects weights with low critical-path-delays enabling\nhigher operational frequencies and dynamic frequency scaling without disrupting\nthe architecture's dataflow. Remarkably, HALO achieves these improvements with\nonly a few dynamic voltage and frequency scaling (DVFS) adjustments, ensuring\nsimplicity and practicality in deployment. Additionally, by reducing switching\nactivity within the MAC units, HALO effectively lowers energy consumption.\nEvaluations on accelerators such as Tensor Processing Units (TPUs) and Graphics\nProcessing Units (GPUs) demonstrate that HALO significantly enhances inference\nefficiency, achieving average performance improvements of 270% and energy\nsavings of 51% over baseline quantization methods, all with minimal impact on\naccuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is critical for efficiently deploying large language models\n(LLMs). Yet conventional methods remain hardware-agnostic, limited to bit-width\nconstraints, and do not account for intrinsic circuit characteristics such as\nthe timing behaviors and energy profiles of Multiply-Accumulate (MAC) units.\nThis disconnect from circuit-level behavior limits the ability to exploit\navailable timing margins and energy-saving opportunities, reducing the overall\nefficiency of deployment on modern accelerators.\n  To address these limitations, we propose HALO, a versatile framework for\nHardware-Aware Post-Training Quantization (PTQ). Unlike traditional methods,\nHALO explicitly incorporates detailed hardware characteristics, including\ncritical-path timing and power consumption, into its quantization approach.\nHALO strategically selects weights with low critical-path-delays enabling\nhigher operational frequencies and dynamic frequency scaling without disrupting\nthe architecture's dataflow. Remarkably, HALO achieves these improvements with\nonly a few dynamic voltage and frequency scaling (DVFS) adjustments, ensuring\nsimplicity and practicality in deployment. Additionally, by reducing switching\nactivity within the MAC units, HALO effectively lowers energy consumption.\nEvaluations on accelerators such as Tensor Processing Units (TPUs) and Graphics\nProcessing Units (GPUs) demonstrate that HALO significantly enhances inference\nefficiency, achieving average performance improvements of 270% and energy\nsavings of 51% over baseline quantization methods, all with minimal impact on\naccuracy."
                },
                "authors": [
                    {
                        "name": "Rohan Juneja"
                    },
                    {
                        "name": "Shivam Aggarwal"
                    },
                    {
                        "name": "Safeen Huda"
                    },
                    {
                        "name": "Tulika Mitra"
                    },
                    {
                        "name": "Li-Shiuan Peh"
                    }
                ],
                "author_detail": {
                    "name": "Li-Shiuan Peh"
                },
                "author": "Li-Shiuan Peh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19662v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19662v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14348v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14348v3",
                "updated": "2025-04-25T09:46:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    9,
                    46,
                    35,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-19T16:28:03Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    16,
                    28,
                    3,
                    5,
                    109,
                    0
                ],
                "title": "Manipulating Multimodal Agents via Cross-Modal Prompt Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Manipulating Multimodal Agents via Cross-Modal Prompt Injection"
                },
                "summary": "The emergence of multimodal large language models has redefined the agent\nparadigm by integrating language and vision modalities with external data\nsources, enabling agents to better interpret human instructions and execute\nincreasingly complex tasks. However, in this work, we identify a critical yet\npreviously overlooked security vulnerability in multimodal agents: cross-modal\nprompt injection attacks. To exploit this vulnerability, we propose\nCrossInject, a novel attack framework in which attackers embed adversarial\nperturbations across multiple modalities to align with target malicious\ncontent, allowing external instructions to hijack the agent's decision-making\nprocess and execute unauthorized tasks. Our approach consists of two key\ncomponents. First, we introduce Visual Latent Alignment, where we optimize\nadversarial features to the malicious instructions in the visual embedding\nspace based on a text-to-image generative model, ensuring that adversarial\nimages subtly encode cues for malicious task execution. Subsequently, we\npresent Textual Guidance Enhancement, where a large language model is leveraged\nto infer the black-box defensive system prompt through adversarial meta\nprompting and generate an malicious textual command that steers the agent's\noutput toward better compliance with attackers' requests. Extensive experiments\ndemonstrate that our method outperforms existing injection attacks, achieving\nat least a +26.4% increase in attack success rates across diverse tasks.\nFurthermore, we validate our attack's effectiveness in real-world multimodal\nautonomous agents, highlighting its potential implications for safety-critical\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of multimodal large language models has redefined the agent\nparadigm by integrating language and vision modalities with external data\nsources, enabling agents to better interpret human instructions and execute\nincreasingly complex tasks. However, in this work, we identify a critical yet\npreviously overlooked security vulnerability in multimodal agents: cross-modal\nprompt injection attacks. To exploit this vulnerability, we propose\nCrossInject, a novel attack framework in which attackers embed adversarial\nperturbations across multiple modalities to align with target malicious\ncontent, allowing external instructions to hijack the agent's decision-making\nprocess and execute unauthorized tasks. Our approach consists of two key\ncomponents. First, we introduce Visual Latent Alignment, where we optimize\nadversarial features to the malicious instructions in the visual embedding\nspace based on a text-to-image generative model, ensuring that adversarial\nimages subtly encode cues for malicious task execution. Subsequently, we\npresent Textual Guidance Enhancement, where a large language model is leveraged\nto infer the black-box defensive system prompt through adversarial meta\nprompting and generate an malicious textual command that steers the agent's\noutput toward better compliance with attackers' requests. Extensive experiments\ndemonstrate that our method outperforms existing injection attacks, achieving\nat least a +26.4% increase in attack success rates across diverse tasks.\nFurthermore, we validate our attack's effectiveness in real-world multimodal\nautonomous agents, highlighting its potential implications for safety-critical\napplications."
                },
                "authors": [
                    {
                        "name": "Le Wang"
                    },
                    {
                        "name": "Zonghao Ying"
                    },
                    {
                        "name": "Tianyuan Zhang"
                    },
                    {
                        "name": "Siyuan Liang"
                    },
                    {
                        "name": "Shengshan Hu"
                    },
                    {
                        "name": "Mingchuan Zhang"
                    },
                    {
                        "name": "Aishan Liu"
                    },
                    {
                        "name": "Xianglong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xianglong Liu"
                },
                "author": "Xianglong Liu",
                "arxiv_comment": "17 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14348v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14348v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18212v1",
                "updated": "2025-04-25T09:43:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    9,
                    43,
                    1,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T09:43:01Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    9,
                    43,
                    1,
                    4,
                    115,
                    0
                ],
                "title": "Post-Transfer Learning Statistical Inference in High-Dimensional\n  Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Transfer Learning Statistical Inference in High-Dimensional\n  Regression"
                },
                "summary": "Transfer learning (TL) for high-dimensional regression (HDR) is an important\nproblem in machine learning, particularly when dealing with limited sample size\nin the target task. However, there currently lacks a method to quantify the\nstatistical significance of the relationship between features and the response\nin TL-HDR settings. In this paper, we introduce a novel statistical inference\nframework for assessing the reliability of feature selection in TL-HDR, called\nPTL-SI (Post-TL Statistical Inference). The core contribution of PTL-SI is its\nability to provide valid $p$-values to features selected in TL-HDR, thereby\nrigorously controlling the false positive rate (FPR) at desired significance\nlevel $\\alpha$ (e.g., 0.05). Furthermore, we enhance statistical power by\nincorporating a strategic divide-and-conquer approach into our framework. We\ndemonstrate the validity and effectiveness of the proposed PTL-SI through\nextensive experiments on both synthetic and real-world high-dimensional\ndatasets, confirming its theoretical properties and utility in testing the\nreliability of feature selection in TL scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transfer learning (TL) for high-dimensional regression (HDR) is an important\nproblem in machine learning, particularly when dealing with limited sample size\nin the target task. However, there currently lacks a method to quantify the\nstatistical significance of the relationship between features and the response\nin TL-HDR settings. In this paper, we introduce a novel statistical inference\nframework for assessing the reliability of feature selection in TL-HDR, called\nPTL-SI (Post-TL Statistical Inference). The core contribution of PTL-SI is its\nability to provide valid $p$-values to features selected in TL-HDR, thereby\nrigorously controlling the false positive rate (FPR) at desired significance\nlevel $\\alpha$ (e.g., 0.05). Furthermore, we enhance statistical power by\nincorporating a strategic divide-and-conquer approach into our framework. We\ndemonstrate the validity and effectiveness of the proposed PTL-SI through\nextensive experiments on both synthetic and real-world high-dimensional\ndatasets, confirming its theoretical properties and utility in testing the\nreliability of feature selection in TL scenarios."
                },
                "authors": [
                    {
                        "name": "Nguyen Vu Khai Tam"
                    },
                    {
                        "name": "Cao Huyen My"
                    },
                    {
                        "name": "Vo Nguyen Le Duy"
                    }
                ],
                "author_detail": {
                    "name": "Vo Nguyen Le Duy"
                },
                "author": "Vo Nguyen Le Duy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18205v1",
                "updated": "2025-04-25T09:35:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    9,
                    35,
                    8,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T09:35:08Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    9,
                    35,
                    8,
                    4,
                    115,
                    0
                ],
                "title": "Estimation of the second-order coherence function using quantum\n  reservoir and ensemble methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimation of the second-order coherence function using quantum\n  reservoir and ensemble methods"
                },
                "summary": "We propose a machine learning-based approach enhanced by quantum reservoir\ncomputing (QRC) to estimate the zero-time second-order correlation function\ng2(0). Typically, measuring g2(0) requires single-photon detectors and\ntime-correlated measurements. Machine learning may offer practical solutions by\ntraining a model to estimate g2(0) solely from average intensity measurements.\nIn our method, emission from a given quantum source is first processed in QRC.\nDuring the inference phase, only intensity measurements are used, which are\nthen passed to a software-based decision tree-based ensemble model. We evaluate\nthis hybrid quantum-classical approach across a variety of quantum optical\nsystems and demonstrate that it provides accurate estimates of g2(0). We\nfurther extend our analysis to assess the ability of a trained model to\ngeneralize beyond its training distribution, both to the same system under\ndifferent physical parameters and to fundamentally different quantum sources.\nWhile the model may yield reliable estimates within specific regimes, its\nperformance across distinct systems is generally limited.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a machine learning-based approach enhanced by quantum reservoir\ncomputing (QRC) to estimate the zero-time second-order correlation function\ng2(0). Typically, measuring g2(0) requires single-photon detectors and\ntime-correlated measurements. Machine learning may offer practical solutions by\ntraining a model to estimate g2(0) solely from average intensity measurements.\nIn our method, emission from a given quantum source is first processed in QRC.\nDuring the inference phase, only intensity measurements are used, which are\nthen passed to a software-based decision tree-based ensemble model. We evaluate\nthis hybrid quantum-classical approach across a variety of quantum optical\nsystems and demonstrate that it provides accurate estimates of g2(0). We\nfurther extend our analysis to assess the ability of a trained model to\ngeneralize beyond its training distribution, both to the same system under\ndifferent physical parameters and to fundamentally different quantum sources.\nWhile the model may yield reliable estimates within specific regimes, its\nperformance across distinct systems is generally limited."
                },
                "authors": [
                    {
                        "name": "Dogyun Ko"
                    },
                    {
                        "name": "Stanisław Świerczewski"
                    },
                    {
                        "name": "Andrzej Opala"
                    },
                    {
                        "name": "Michał Matuszewski"
                    },
                    {
                        "name": "Amir Rahmani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Rahmani"
                },
                "author": "Amir Rahmani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16834v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16834v2",
                "updated": "2025-04-25T08:51:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    8,
                    51,
                    9,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-23T15:56:28Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    56,
                    28,
                    2,
                    113,
                    0
                ],
                "title": "Improving Significant Wave Height Prediction Using Chronos Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Significant Wave Height Prediction Using Chronos Models"
                },
                "summary": "Accurate wave height prediction is critical for maritime safety and coastal\nresilience, yet conventional physics-based models and traditional machine\nlearning methods face challenges in computational efficiency and nonlinear\ndynamics modeling. This study introduces Chronos, the first implementation of a\nlarge language model (LLM)-powered temporal architecture (Chronos) optimized\nfor wave forecasting. Through advanced temporal pattern recognition applied to\nhistorical wave data from three strategically chosen marine zones in the\nNorthwest Pacific basin, our framework achieves multimodal improvements: (1)\n14.3% reduction in training time with 2.5x faster inference speed compared to\nPatchTST baselines, achieving 0.575 mean absolute scaled error (MASE) units;\n(2) superior short-term forecasting (1-24h) across comprehensive metrics; (3)\nsustained predictive leadership in extended-range forecasts (1-120h); and (4)\ndemonstrated zero-shot capability maintaining median performance (rank 4/12)\nagainst specialized operational models. This LLM-enhanced temporal modeling\nparadigm establishes a new standard in wave prediction, offering both\ncomputationally efficient solutions and a transferable framework for complex\ngeophysical systems modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate wave height prediction is critical for maritime safety and coastal\nresilience, yet conventional physics-based models and traditional machine\nlearning methods face challenges in computational efficiency and nonlinear\ndynamics modeling. This study introduces Chronos, the first implementation of a\nlarge language model (LLM)-powered temporal architecture (Chronos) optimized\nfor wave forecasting. Through advanced temporal pattern recognition applied to\nhistorical wave data from three strategically chosen marine zones in the\nNorthwest Pacific basin, our framework achieves multimodal improvements: (1)\n14.3% reduction in training time with 2.5x faster inference speed compared to\nPatchTST baselines, achieving 0.575 mean absolute scaled error (MASE) units;\n(2) superior short-term forecasting (1-24h) across comprehensive metrics; (3)\nsustained predictive leadership in extended-range forecasts (1-120h); and (4)\ndemonstrated zero-shot capability maintaining median performance (rank 4/12)\nagainst specialized operational models. This LLM-enhanced temporal modeling\nparadigm establishes a new standard in wave prediction, offering both\ncomputationally efficient solutions and a transferable framework for complex\ngeophysical systems modeling."
                },
                "authors": [
                    {
                        "name": "Yilin Zhai"
                    },
                    {
                        "name": "Hongyuan Shi"
                    },
                    {
                        "name": "Chao Zhan"
                    },
                    {
                        "name": "Qing Wang"
                    },
                    {
                        "name": "Zaijin You"
                    },
                    {
                        "name": "Nan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Nan Wang"
                },
                "author": "Nan Wang",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2403.07815 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16834v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16834v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18175v1",
                "updated": "2025-04-25T08:46:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    8,
                    46,
                    38,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T08:46:38Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    8,
                    46,
                    38,
                    4,
                    115,
                    0
                ],
                "title": "Generative AI for Physical-Layer Authentication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI for Physical-Layer Authentication"
                },
                "summary": "In recent years, Artificial Intelligence (AI)-driven Physical-Layer\nAuthentication (PLA), which focuses on achieving endogenous security and\nintelligent identity authentication, has attracted considerable interest. When\ncompared with Discriminative AI (DAI), Generative AI (GAI) offers several\nadvantages, such as fingerprint data augmentation, fingerprint denoising and\nreconstruction, and protection against adversarial attacks. Inspired by these\ninnovations, this paper provides a systematic exploration of GAI's integration\ninto PLA frameworks. We commence with a review of representative authentication\ntechniques, emphasizing PLA's inherent strengths. Following this, we revisit\nfour typical GAI models and contrast the limitations of DAI with the potential\nof GAI in addressing PLA challenges, including insufficient fingerprint data,\nenvironment noises and inferences, perturbations in fingerprint data, and\ncomplex tasks. Specifically, we delve into providing GAI-enhance methods for\nPLA across the data, model, and application layers in detail. Moreover, we\npresent a case study that combines fingerprint extrapolation, generative\ndiffusion models, and cooperative nodes to illustrate the superiority of GAI in\nbolstering the reliability of PLA compared to DAI. Additionally, we outline\npotential future research directions for GAI-based PLA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Artificial Intelligence (AI)-driven Physical-Layer\nAuthentication (PLA), which focuses on achieving endogenous security and\nintelligent identity authentication, has attracted considerable interest. When\ncompared with Discriminative AI (DAI), Generative AI (GAI) offers several\nadvantages, such as fingerprint data augmentation, fingerprint denoising and\nreconstruction, and protection against adversarial attacks. Inspired by these\ninnovations, this paper provides a systematic exploration of GAI's integration\ninto PLA frameworks. We commence with a review of representative authentication\ntechniques, emphasizing PLA's inherent strengths. Following this, we revisit\nfour typical GAI models and contrast the limitations of DAI with the potential\nof GAI in addressing PLA challenges, including insufficient fingerprint data,\nenvironment noises and inferences, perturbations in fingerprint data, and\ncomplex tasks. Specifically, we delve into providing GAI-enhance methods for\nPLA across the data, model, and application layers in detail. Moreover, we\npresent a case study that combines fingerprint extrapolation, generative\ndiffusion models, and cooperative nodes to illustrate the superiority of GAI in\nbolstering the reliability of PLA compared to DAI. Additionally, we outline\npotential future research directions for GAI-based PLA."
                },
                "authors": [
                    {
                        "name": "Rui Meng"
                    },
                    {
                        "name": "Xiqi Cheng"
                    },
                    {
                        "name": "Song Gao"
                    },
                    {
                        "name": "Xiaodong Xu"
                    },
                    {
                        "name": "Chen Dong"
                    },
                    {
                        "name": "Guoshun Nan"
                    },
                    {
                        "name": "Xiaofeng Tao"
                    },
                    {
                        "name": "Ping Zhang"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "10 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18164v1",
                "updated": "2025-04-25T08:27:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    8,
                    27,
                    31,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T08:27:31Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    8,
                    27,
                    31,
                    4,
                    115,
                    0
                ],
                "title": "Recent advances in data-driven methods for degradation modelling across\n  applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in data-driven methods for degradation modelling across\n  applications"
                },
                "summary": "Understanding degradation is crucial for ensuring the longevity and\nperformance of materials, systems, and organisms. To illustrate the\nsimilarities across applications, this article provides a review of data-based\nmethod in materials science, engineering, and medicine. The methods analyzed in\nthis paper include regression analysis, factor analysis, cluster analysis,\nMarkov Chain Monte Carlo, Bayesian statistics, hidden Markov models,\nnonparametric Bayesian modeling of time series, supervised learning, and deep\nlearning. The review provides an overview of degradation models, referencing\nbooks and methods, and includes detailed tables highlighting the applications\nand insights offered in medicine, power engineering, and material science. It\nalso discusses the classification of methods, emphasizing statistical\ninference, dynamic prediction, machine learning, and hybrid modeling\ntechniques. Overall, this review enhances understanding of degradation\nmodelling across diverse domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding degradation is crucial for ensuring the longevity and\nperformance of materials, systems, and organisms. To illustrate the\nsimilarities across applications, this article provides a review of data-based\nmethod in materials science, engineering, and medicine. The methods analyzed in\nthis paper include regression analysis, factor analysis, cluster analysis,\nMarkov Chain Monte Carlo, Bayesian statistics, hidden Markov models,\nnonparametric Bayesian modeling of time series, supervised learning, and deep\nlearning. The review provides an overview of degradation models, referencing\nbooks and methods, and includes detailed tables highlighting the applications\nand insights offered in medicine, power engineering, and material science. It\nalso discusses the classification of methods, emphasizing statistical\ninference, dynamic prediction, machine learning, and hybrid modeling\ntechniques. Overall, this review enhances understanding of degradation\nmodelling across diverse domains."
                },
                "authors": [
                    {
                        "name": "Anna Jarosz"
                    },
                    {
                        "name": "Marta Zagorowska"
                    },
                    {
                        "name": "Jerzy Baranowski"
                    }
                ],
                "author_detail": {
                    "name": "Jerzy Baranowski"
                },
                "author": "Jerzy Baranowski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16968v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16968v2",
                "updated": "2025-04-25T08:26:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    8,
                    26,
                    21,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-23T12:28:27Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    12,
                    28,
                    27,
                    2,
                    113,
                    0
                ],
                "title": "BackSlash: Rate Constrained Optimized Training of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BackSlash: Rate Constrained Optimized Training of Large Language Models"
                },
                "summary": "The rapid advancement of large-language models (LLMs) has driven extensive\nresearch into parameter compression after training has been completed, yet\ncompression during the training phase remains largely unexplored. In this work,\nwe introduce Rate-Constrained Training (BackSlash), a novel training-time\ncompression approach based on rate-distortion optimization (RDO). BackSlash\nenables a flexible trade-off between model accuracy and complexity,\nsignificantly reducing parameter redundancy while preserving performance.\nExperiments in various architectures and tasks demonstrate that BackSlash can\nreduce memory usage by 60% - 90% without accuracy loss and provides significant\ncompression gain compared to compression after training. Moreover, BackSlash\nproves to be highly versatile: it enhances generalization with small Lagrange\nmultipliers, improves model robustness to pruning (maintaining accuracy even at\n80% pruning rates), and enables network simplification for accelerated\ninference on edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large-language models (LLMs) has driven extensive\nresearch into parameter compression after training has been completed, yet\ncompression during the training phase remains largely unexplored. In this work,\nwe introduce Rate-Constrained Training (BackSlash), a novel training-time\ncompression approach based on rate-distortion optimization (RDO). BackSlash\nenables a flexible trade-off between model accuracy and complexity,\nsignificantly reducing parameter redundancy while preserving performance.\nExperiments in various architectures and tasks demonstrate that BackSlash can\nreduce memory usage by 60% - 90% without accuracy loss and provides significant\ncompression gain compared to compression after training. Moreover, BackSlash\nproves to be highly versatile: it enhances generalization with small Lagrange\nmultipliers, improves model robustness to pruning (maintaining accuracy even at\n80% pruning rates), and enables network simplification for accelerated\ninference on edge devices."
                },
                "authors": [
                    {
                        "name": "Jun Wu"
                    },
                    {
                        "name": "Jiangtao Wen"
                    },
                    {
                        "name": "Yuxing Han"
                    }
                ],
                "author_detail": {
                    "name": "Yuxing Han"
                },
                "author": "Yuxing Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16968v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16968v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17419v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17419v3",
                "updated": "2025-04-25T08:15:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    8,
                    15,
                    51,
                    4,
                    115,
                    0
                ],
                "published": "2025-02-24T18:50:52Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    50,
                    52,
                    0,
                    55,
                    0
                ],
                "title": "From System 1 to System 2: A Survey of Reasoning Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From System 1 to System 2: A Survey of Reasoning Large Language Models"
                },
                "summary": "Achieving human-level intelligence requires refining the transition from the\nfast, intuitive System 1 to the slower, more deliberate System 2 reasoning.\nWhile System 1 excels in quick, heuristic decisions, System 2 relies on logical\nreasoning for more accurate judgments and reduced biases. Foundational Large\nLanguage Models (LLMs) excel at fast decision-making but lack the depth for\ncomplex reasoning, as they have not yet fully embraced the step-by-step\nanalysis characteristic of true System 2 thinking. Recently, reasoning LLMs\nlike OpenAI's o1/o3 and DeepSeek's R1 have demonstrated expert-level\nperformance in fields such as mathematics and coding, closely mimicking the\ndeliberate reasoning of System 2 and showcasing human-like cognitive abilities.\nThis survey begins with a brief overview of the progress in foundational LLMs\nand the early development of System 2 technologies, exploring how their\ncombination has paved the way for reasoning LLMs. Next, we discuss how to\nconstruct reasoning LLMs, analyzing their features, the core methods enabling\nadvanced reasoning, and the evolution of various reasoning LLMs. Additionally,\nwe provide an overview of reasoning benchmarks, offering an in-depth comparison\nof the performance of representative reasoning LLMs. Finally, we explore\npromising directions for advancing reasoning LLMs and maintain a real-time\n\\href{https://github.com/zzli2022/Awesome-Slow-Reason-System}{GitHub\nRepository} to track the latest developments. We hope this survey will serve as\na valuable resource to inspire innovation and drive progress in this rapidly\nevolving field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving human-level intelligence requires refining the transition from the\nfast, intuitive System 1 to the slower, more deliberate System 2 reasoning.\nWhile System 1 excels in quick, heuristic decisions, System 2 relies on logical\nreasoning for more accurate judgments and reduced biases. Foundational Large\nLanguage Models (LLMs) excel at fast decision-making but lack the depth for\ncomplex reasoning, as they have not yet fully embraced the step-by-step\nanalysis characteristic of true System 2 thinking. Recently, reasoning LLMs\nlike OpenAI's o1/o3 and DeepSeek's R1 have demonstrated expert-level\nperformance in fields such as mathematics and coding, closely mimicking the\ndeliberate reasoning of System 2 and showcasing human-like cognitive abilities.\nThis survey begins with a brief overview of the progress in foundational LLMs\nand the early development of System 2 technologies, exploring how their\ncombination has paved the way for reasoning LLMs. Next, we discuss how to\nconstruct reasoning LLMs, analyzing their features, the core methods enabling\nadvanced reasoning, and the evolution of various reasoning LLMs. Additionally,\nwe provide an overview of reasoning benchmarks, offering an in-depth comparison\nof the performance of representative reasoning LLMs. Finally, we explore\npromising directions for advancing reasoning LLMs and maintain a real-time\n\\href{https://github.com/zzli2022/Awesome-Slow-Reason-System}{GitHub\nRepository} to track the latest developments. We hope this survey will serve as\na valuable resource to inspire innovation and drive progress in this rapidly\nevolving field."
                },
                "authors": [
                    {
                        "name": "Zhong-Zhi Li"
                    },
                    {
                        "name": "Duzhen Zhang"
                    },
                    {
                        "name": "Ming-Liang Zhang"
                    },
                    {
                        "name": "Jiaxin Zhang"
                    },
                    {
                        "name": "Zengyan Liu"
                    },
                    {
                        "name": "Yuxuan Yao"
                    },
                    {
                        "name": "Haotian Xu"
                    },
                    {
                        "name": "Junhao Zheng"
                    },
                    {
                        "name": "Pei-Jie Wang"
                    },
                    {
                        "name": "Xiuyi Chen"
                    },
                    {
                        "name": "Yingying Zhang"
                    },
                    {
                        "name": "Fei Yin"
                    },
                    {
                        "name": "Jiahua Dong"
                    },
                    {
                        "name": "Zhiwei Li"
                    },
                    {
                        "name": "Bao-Long Bi"
                    },
                    {
                        "name": "Ling-Rui Mei"
                    },
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Le Song"
                    },
                    {
                        "name": "Cheng-Lin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Cheng-Lin Liu"
                },
                "author": "Cheng-Lin Liu",
                "arxiv_comment": "Slow-thinking, Large Language Models, Human-like Reasoning, Decision\n  Making in AI, AGI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17419v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17419v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18154v1",
                "updated": "2025-04-25T08:06:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    8,
                    6,
                    22,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T08:06:22Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    8,
                    6,
                    22,
                    4,
                    115,
                    0
                ],
                "title": "EcoServe: Enabling Cost-effective LLM Serving with Proactive Intra- and\n  Inter-Instance Orchestration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EcoServe: Enabling Cost-effective LLM Serving with Proactive Intra- and\n  Inter-Instance Orchestration"
                },
                "summary": "Existing LLM serving strategies can be categorized based on whether prefill\nand decode phases are disaggregated: non-disaggregated (NoDG) or fully\ndisaggregated (FuDG). However, the NoDG strategy leads to strong prefill-decode\ninterference and the FuDG strategy highly relies on high-performance\ninterconnects, making them less cost-effective.\n  We introduce EcoServe, a system that enables cost-effective LLM serving on\nclusters with commodity interconnects. EcoServe is built on the partially\ndisaggregated (PaDG) strategy, applying temporal disaggregation and rolling\nactivation for proactive intra- and inter-instance scheduling. It first\ndisaggregates the prefill and decode phases along the time dimension within a\nsingle instance to mitigate inter-phase interference and enhance throughput.\nNext, it coordinates multiple instances and cyclically activates them to ensure\nthe continuous availability of prefill processing, thereby improving latency.\nThus, EcoServe's basic serving unit is the macro instance, within which\nmultiple instances collaborate. It further integrates an adaptive scheduling\nalgorithm to route requests in a macro instance and a mitosis scaling approach\nto enable fine-grained capacity scaling. Beyond delivering high goodput,\nEcoServe excels in load balancing, hardware cost, parallelism compatibility,\nand even engineering simplicity compared to existing solutions.\n  When serving 30B- and 70B-scale models on a production-level cluster with 32\nNVIDIA L20 GPUs using commodity Ethernet, EcoServe averagely improves goodput\nby 82.49%, 86.17%, 122.76%, and 126.96% over four representative NoDG and FuDG\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM serving strategies can be categorized based on whether prefill\nand decode phases are disaggregated: non-disaggregated (NoDG) or fully\ndisaggregated (FuDG). However, the NoDG strategy leads to strong prefill-decode\ninterference and the FuDG strategy highly relies on high-performance\ninterconnects, making them less cost-effective.\n  We introduce EcoServe, a system that enables cost-effective LLM serving on\nclusters with commodity interconnects. EcoServe is built on the partially\ndisaggregated (PaDG) strategy, applying temporal disaggregation and rolling\nactivation for proactive intra- and inter-instance scheduling. It first\ndisaggregates the prefill and decode phases along the time dimension within a\nsingle instance to mitigate inter-phase interference and enhance throughput.\nNext, it coordinates multiple instances and cyclically activates them to ensure\nthe continuous availability of prefill processing, thereby improving latency.\nThus, EcoServe's basic serving unit is the macro instance, within which\nmultiple instances collaborate. It further integrates an adaptive scheduling\nalgorithm to route requests in a macro instance and a mitosis scaling approach\nto enable fine-grained capacity scaling. Beyond delivering high goodput,\nEcoServe excels in load balancing, hardware cost, parallelism compatibility,\nand even engineering simplicity compared to existing solutions.\n  When serving 30B- and 70B-scale models on a production-level cluster with 32\nNVIDIA L20 GPUs using commodity Ethernet, EcoServe averagely improves goodput\nby 82.49%, 86.17%, 122.76%, and 126.96% over four representative NoDG and FuDG\nsystems."
                },
                "authors": [
                    {
                        "name": "Jiangsu Du"
                    },
                    {
                        "name": "Hongbin Zhang"
                    },
                    {
                        "name": "Taosheng Wei"
                    },
                    {
                        "name": "Zhenyi Zheng"
                    },
                    {
                        "name": "Kaiyi Wu"
                    },
                    {
                        "name": "Zhiguang Chen"
                    },
                    {
                        "name": "Yutong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yutong Lu"
                },
                "author": "Yutong Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18147v1",
                "updated": "2025-04-25T07:56:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    7,
                    56,
                    24,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T07:56:24Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    7,
                    56,
                    24,
                    4,
                    115,
                    0
                ],
                "title": "NoEsis: Differentially Private Knowledge Transfer in Modular LLM\n  Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NoEsis: Differentially Private Knowledge Transfer in Modular LLM\n  Adaptation"
                },
                "summary": "Large Language Models (LLM) are typically trained on vast amounts of data\nfrom various sources. Even when designed modularly (e.g., Mixture-of-Experts),\nLLMs can leak privacy on their sources. Conversely, training such models in\nisolation arguably prohibits generalization. To this end, we propose a\nframework, NoEsis, which builds upon the desired properties of modularity,\nprivacy, and knowledge transfer. NoEsis integrates differential privacy with a\nhybrid two-staged parameter-efficient fine-tuning that combines domain-specific\nlow-rank adapters, acting as experts, with common prompt tokens, acting as a\nknowledge-sharing backbone. Results from our evaluation on CodeXGLUE showcase\nthat NoEsis can achieve provable privacy guarantees with tangible knowledge\ntransfer across domains, and empirically show protection against Membership\nInference Attacks. Finally, on code completion tasks, NoEsis bridges at least\n77% of the accuracy gap between the non-shared and the non-private baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) are typically trained on vast amounts of data\nfrom various sources. Even when designed modularly (e.g., Mixture-of-Experts),\nLLMs can leak privacy on their sources. Conversely, training such models in\nisolation arguably prohibits generalization. To this end, we propose a\nframework, NoEsis, which builds upon the desired properties of modularity,\nprivacy, and knowledge transfer. NoEsis integrates differential privacy with a\nhybrid two-staged parameter-efficient fine-tuning that combines domain-specific\nlow-rank adapters, acting as experts, with common prompt tokens, acting as a\nknowledge-sharing backbone. Results from our evaluation on CodeXGLUE showcase\nthat NoEsis can achieve provable privacy guarantees with tangible knowledge\ntransfer across domains, and empirically show protection against Membership\nInference Attacks. Finally, on code completion tasks, NoEsis bridges at least\n77% of the accuracy gap between the non-shared and the non-private baseline."
                },
                "authors": [
                    {
                        "name": "Rob Romijnders"
                    },
                    {
                        "name": "Stefanos Laskaridis"
                    },
                    {
                        "name": "Ali Shahin Shamsabadi"
                    },
                    {
                        "name": "Hamed Haddadi"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Haddadi"
                },
                "author": "Hamed Haddadi",
                "arxiv_comment": "ICLR 2025 MCDC workshop",
                "arxiv_journal_ref": "ICLR 2025 Workshop on Modularity for Collaborative, Decentralized,\n  and Continual Deep Learning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21419v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21419v3",
                "updated": "2025-04-25T07:54:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    7,
                    54,
                    59,
                    4,
                    115,
                    0
                ],
                "published": "2025-03-27T12:09:04Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    9,
                    4,
                    3,
                    86,
                    0
                ],
                "title": "Neuroplasticity in Artificial Intelligence -- An Overview and\n  Inspirations on Drop In & Out Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuroplasticity in Artificial Intelligence -- An Overview and\n  Inspirations on Drop In & Out Learning"
                },
                "summary": "Artificial Intelligence (AI) has achieved new levels of performance and\nspread in public usage with the rise of deep neural networks (DNNs). Initially\ninspired by human neurons and their connections, NNs have become the foundation\nof AI models for many advanced architectures. However, some of the most\nintegral processes in the human brain, particularly neurogenesis and\nneuroplasticity in addition to the more spread neuroapoptosis have largely been\nignored in DNN architecture design. Instead, contemporary AI development\npredominantly focuses on constructing advanced frameworks, such as large\nlanguage models, which retain a static structure of neural connections during\ntraining and inference. In this light, we explore how neurogenesis,\nneuroapoptosis, and neuroplasticity can inspire future AI advances.\nSpecifically, we examine analogous activities in artificial NNs, introducing\nthe concepts of ``dropin'' for neurogenesis and revisiting ``dropout'' and\nstructural pruning for neuroapoptosis. We additionally suggest neuroplasticity\ncombining the two for future large NNs in ``life-long learning'' settings\nfollowing the biological inspiration. We conclude by advocating for greater\nresearch efforts in this interdisciplinary domain and identifying promising\ndirections for future exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) has achieved new levels of performance and\nspread in public usage with the rise of deep neural networks (DNNs). Initially\ninspired by human neurons and their connections, NNs have become the foundation\nof AI models for many advanced architectures. However, some of the most\nintegral processes in the human brain, particularly neurogenesis and\nneuroplasticity in addition to the more spread neuroapoptosis have largely been\nignored in DNN architecture design. Instead, contemporary AI development\npredominantly focuses on constructing advanced frameworks, such as large\nlanguage models, which retain a static structure of neural connections during\ntraining and inference. In this light, we explore how neurogenesis,\nneuroapoptosis, and neuroplasticity can inspire future AI advances.\nSpecifically, we examine analogous activities in artificial NNs, introducing\nthe concepts of ``dropin'' for neurogenesis and revisiting ``dropout'' and\nstructural pruning for neuroapoptosis. We additionally suggest neuroplasticity\ncombining the two for future large NNs in ``life-long learning'' settings\nfollowing the biological inspiration. We conclude by advocating for greater\nresearch efforts in this interdisciplinary domain and identifying promising\ndirections for future exploration."
                },
                "authors": [
                    {
                        "name": "Yupei Li"
                    },
                    {
                        "name": "Manuel Milling"
                    },
                    {
                        "name": "Björn W. Schuller"
                    }
                ],
                "author_detail": {
                    "name": "Björn W. Schuller"
                },
                "author": "Björn W. Schuller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21419v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21419v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18145v1",
                "updated": "2025-04-25T07:54:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    7,
                    54,
                    21,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T07:54:21Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    7,
                    54,
                    21,
                    4,
                    115,
                    0
                ],
                "title": "Passive All-Optical Nonlinear Neuron Activation via PPLN Nanophotonic\n  Waveguides",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Passive All-Optical Nonlinear Neuron Activation via PPLN Nanophotonic\n  Waveguides"
                },
                "summary": "Artificial intelligence (AI) is transforming modern life, yet the growing\nscale of AI applications places mounting demands on computational resources,\nraising sustainability concerns. Photonic integrated circuits (PICs) offer a\npromising alternative, enabling massive parallelism, low latency, and reduced\nelectrical overhead, particularly excelling in high-throughput linear\noperations. However, passive and fully optical nonlinear activation functions\nwith equally superb performance remain rare, posing a critical bottleneck in\nrealizing all-optical neural networks on PICs. Here, we demonstrate a compact\nand readily integrated all-optical nonlinear activation function,\nexperimentally realized through highly pump-depleted second-harmonic generation\n(SHG) in periodically poled lithium niobate (PPLN) nanophotonic waveguides,\nachieving 79% absolute conversion efficiency. This activation exhibits a\nsigmoid-like, wavelength-selective response with femtosecond-scale dynamics and\nlight-speed processing, requiring no electrical control or auxiliary optical\nsignals. We further validate its feasibility for neural inference by combining\nthe measured SHG-based nonlinearity with linear operations implemented via a\nMach-Zehnder interferometer system on a silicon PIC. Our demonstration achieves\nperformance on par with digital implementations in real-world tasks, including\nairfoil regression and medical image classification. These results pave the way\ntoward scalable, high-speed, and fully integrated all-optical neural networks\nfor next-generation photonic AI hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) is transforming modern life, yet the growing\nscale of AI applications places mounting demands on computational resources,\nraising sustainability concerns. Photonic integrated circuits (PICs) offer a\npromising alternative, enabling massive parallelism, low latency, and reduced\nelectrical overhead, particularly excelling in high-throughput linear\noperations. However, passive and fully optical nonlinear activation functions\nwith equally superb performance remain rare, posing a critical bottleneck in\nrealizing all-optical neural networks on PICs. Here, we demonstrate a compact\nand readily integrated all-optical nonlinear activation function,\nexperimentally realized through highly pump-depleted second-harmonic generation\n(SHG) in periodically poled lithium niobate (PPLN) nanophotonic waveguides,\nachieving 79% absolute conversion efficiency. This activation exhibits a\nsigmoid-like, wavelength-selective response with femtosecond-scale dynamics and\nlight-speed processing, requiring no electrical control or auxiliary optical\nsignals. We further validate its feasibility for neural inference by combining\nthe measured SHG-based nonlinearity with linear operations implemented via a\nMach-Zehnder interferometer system on a silicon PIC. Our demonstration achieves\nperformance on par with digital implementations in real-world tasks, including\nairfoil regression and medical image classification. These results pave the way\ntoward scalable, high-speed, and fully integrated all-optical neural networks\nfor next-generation photonic AI hardware."
                },
                "authors": [
                    {
                        "name": "Wujie Fu"
                    },
                    {
                        "name": "Xiaodong Shi"
                    },
                    {
                        "name": "Lei Shi"
                    },
                    {
                        "name": "Sakthi Sanjeev Mohanraj"
                    },
                    {
                        "name": "Yuan Gao"
                    },
                    {
                        "name": "Luo Qi"
                    },
                    {
                        "name": "Pragati Aashna"
                    },
                    {
                        "name": "Zexian Wang"
                    },
                    {
                        "name": "Guanyu Chen"
                    },
                    {
                        "name": "Di Zhu"
                    },
                    {
                        "name": "Aaron Danner"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Danner"
                },
                "author": "Aaron Danner",
                "arxiv_comment": "12 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15843v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15843v2",
                "updated": "2025-04-25T07:47:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    7,
                    47,
                    16,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-22T12:39:30Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    12,
                    39,
                    30,
                    1,
                    112,
                    0
                ],
                "title": "Pre-DPO: Improving Data Utilization in Direct Preference Optimization\n  Using a Guiding Reference Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-DPO: Improving Data Utilization in Direct Preference Optimization\n  Using a Guiding Reference Model"
                },
                "summary": "Direct Preference Optimization (DPO) simplifies reinforcement learning from\nhuman feedback (RLHF) for large language models (LLMs) by directly optimizing\nhuman preferences without an explicit reward model. We find that during DPO\ntraining, the reference model plays the role of a data weight adjuster.\nHowever, the common practice of initializing the policy and reference models\nidentically in DPO can lead to inefficient data utilization and impose a\nperformance ceiling. Meanwhile, the lack of a reference model in Simple\nPreference Optimization (SimPO) reduces training robustness and necessitates\nstricter conditions to prevent catastrophic forgetting. In this work, we\npropose Pre-DPO, a simple yet effective DPO-based training paradigm that\nenhances preference optimization performance by leveraging a guiding reference\nmodel. This reference model provides foresight into the optimal policy state\nachievable through the training preference data, serving as a guiding mechanism\nthat adaptively assigns higher weights to samples more suitable for the model\nand lower weights to those less suitable. Extensive experiments on AlpacaEval\n2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently\nimproves the performance of both DPO and SimPO, without relying on external\nmodels or additional data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) simplifies reinforcement learning from\nhuman feedback (RLHF) for large language models (LLMs) by directly optimizing\nhuman preferences without an explicit reward model. We find that during DPO\ntraining, the reference model plays the role of a data weight adjuster.\nHowever, the common practice of initializing the policy and reference models\nidentically in DPO can lead to inefficient data utilization and impose a\nperformance ceiling. Meanwhile, the lack of a reference model in Simple\nPreference Optimization (SimPO) reduces training robustness and necessitates\nstricter conditions to prevent catastrophic forgetting. In this work, we\npropose Pre-DPO, a simple yet effective DPO-based training paradigm that\nenhances preference optimization performance by leveraging a guiding reference\nmodel. This reference model provides foresight into the optimal policy state\nachievable through the training preference data, serving as a guiding mechanism\nthat adaptively assigns higher weights to samples more suitable for the model\nand lower weights to those less suitable. Extensive experiments on AlpacaEval\n2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently\nimproves the performance of both DPO and SimPO, without relying on external\nmodels or additional data."
                },
                "authors": [
                    {
                        "name": "Junshu Pan"
                    },
                    {
                        "name": "Wei Shen"
                    },
                    {
                        "name": "Shulin Huang"
                    },
                    {
                        "name": "Qiji Zhou"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15843v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15843v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01482v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01482v2",
                "updated": "2025-04-25T07:40:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    7,
                    40,
                    9,
                    4,
                    115,
                    0
                ],
                "published": "2025-03-03T12:41:01Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    12,
                    41,
                    1,
                    0,
                    62,
                    0
                ],
                "title": "Revisiting Locally Differentially Private Protocols: Towards Better\n  Trade-offs in Privacy, Utility, and Attack Resistance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Locally Differentially Private Protocols: Towards Better\n  Trade-offs in Privacy, Utility, and Attack Resistance"
                },
                "summary": "Local Differential Privacy (LDP) offers strong privacy protection, especially\nin settings in which the server collecting the data is untrusted. However,\ndesigning LDP mechanisms that achieve an optimal trade-off between privacy,\nutility and robustness to adversarial inference attacks remains challenging. In\nthis work, we introduce a general multi-objective optimization framework for\nrefining LDP protocols, enabling the joint optimization of privacy and utility\nunder various adversarial settings. While our framework is flexible to\naccommodate multiple privacy and security attacks as well as utility metrics,\nin this paper, we specifically optimize for Attacker Success Rate (ASR) under\n\\emph{data reconstruction attack} as a concrete measure of privacy leakage and\nMean Squared Error (MSE) as a measure of utility. More precisely, we\nsystematically revisit these trade-offs by analyzing eight state-of-the-art LDP\nprotocols and proposing refined counterparts that leverage tailored\noptimization techniques. Experimental results demonstrate that our proposed\nadaptive mechanisms consistently outperform their non-adaptive counterparts,\nachieving substantial reductions in ASR while preserving utility, and pushing\ncloser to the ASR-MSE Pareto frontier. By bridging the gap between theoretical\nguarantees and real-world vulnerabilities, our framework enables modular and\ncontext-aware deployment of LDP mechanisms with tunable privacy-utility\ntrade-offs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local Differential Privacy (LDP) offers strong privacy protection, especially\nin settings in which the server collecting the data is untrusted. However,\ndesigning LDP mechanisms that achieve an optimal trade-off between privacy,\nutility and robustness to adversarial inference attacks remains challenging. In\nthis work, we introduce a general multi-objective optimization framework for\nrefining LDP protocols, enabling the joint optimization of privacy and utility\nunder various adversarial settings. While our framework is flexible to\naccommodate multiple privacy and security attacks as well as utility metrics,\nin this paper, we specifically optimize for Attacker Success Rate (ASR) under\n\\emph{data reconstruction attack} as a concrete measure of privacy leakage and\nMean Squared Error (MSE) as a measure of utility. More precisely, we\nsystematically revisit these trade-offs by analyzing eight state-of-the-art LDP\nprotocols and proposing refined counterparts that leverage tailored\noptimization techniques. Experimental results demonstrate that our proposed\nadaptive mechanisms consistently outperform their non-adaptive counterparts,\nachieving substantial reductions in ASR while preserving utility, and pushing\ncloser to the ASR-MSE Pareto frontier. By bridging the gap between theoretical\nguarantees and real-world vulnerabilities, our framework enables modular and\ncontext-aware deployment of LDP mechanisms with tunable privacy-utility\ntrade-offs."
                },
                "authors": [
                    {
                        "name": "Héber H. Arcolezi"
                    },
                    {
                        "name": "Sébastien Gambs"
                    }
                ],
                "author_detail": {
                    "name": "Sébastien Gambs"
                },
                "author": "Sébastien Gambs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01482v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01482v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18131v1",
                "updated": "2025-04-25T07:33:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    7,
                    33,
                    35,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T07:33:35Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    7,
                    33,
                    35,
                    4,
                    115,
                    0
                ],
                "title": "SoK: Timeline based event reconstruction for digital forensics:\n  Terminology, methodology, and current challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoK: Timeline based event reconstruction for digital forensics:\n  Terminology, methodology, and current challenges"
                },
                "summary": "Event reconstruction is a technique that examiners can use to attempt to\ninfer past activities by analyzing digital artifacts. Despite its significance,\nthe field suffers from fragmented research, with studies often focusing\nnarrowly on aspects like timeline creation or tampering detection. This paper\naddresses the lack of a unified perspective by proposing a comprehensive\nframework for timeline-based event reconstruction, adapted from traditional\nforensic science models. We begin by harmonizing existing terminology and\npresenting a cohesive diagram that clarifies the relationships between key\nelements of the reconstruction process. Through a comprehensive literature\nsurvey, we classify and organize the main challenges, extending the discussion\nbeyond common issues like data volume. Lastly, we highlight recent advancements\nand propose directions for future research, including specific research gaps.\nBy providing a structured approach, key findings, and a clearer understanding\nof the underlying challenges, this work aims to strengthen the foundation of\ndigital forensics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event reconstruction is a technique that examiners can use to attempt to\ninfer past activities by analyzing digital artifacts. Despite its significance,\nthe field suffers from fragmented research, with studies often focusing\nnarrowly on aspects like timeline creation or tampering detection. This paper\naddresses the lack of a unified perspective by proposing a comprehensive\nframework for timeline-based event reconstruction, adapted from traditional\nforensic science models. We begin by harmonizing existing terminology and\npresenting a cohesive diagram that clarifies the relationships between key\nelements of the reconstruction process. Through a comprehensive literature\nsurvey, we classify and organize the main challenges, extending the discussion\nbeyond common issues like data volume. Lastly, we highlight recent advancements\nand propose directions for future research, including specific research gaps.\nBy providing a structured approach, key findings, and a clearer understanding\nof the underlying challenges, this work aims to strengthen the foundation of\ndigital forensics."
                },
                "authors": [
                    {
                        "name": "Frank Breitinger"
                    },
                    {
                        "name": "Hudan Studiawan"
                    },
                    {
                        "name": "Chris Hargreaves"
                    }
                ],
                "author_detail": {
                    "name": "Chris Hargreaves"
                },
                "author": "Chris Hargreaves",
                "arxiv_comment": "Accepted for publication at DFRWS USA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13992v2",
                "updated": "2025-04-25T07:22:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    7,
                    22,
                    17,
                    4,
                    115,
                    0
                ],
                "published": "2025-01-23T10:20:12Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    10,
                    20,
                    12,
                    3,
                    23,
                    0
                ],
                "title": "Dual-Branch HNSW Approach with Skip Bridges and LID-Driven Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual-Branch HNSW Approach with Skip Bridges and LID-Driven Optimization"
                },
                "summary": "The Hierarchical Navigable Small World (HNSW) algorithm is widely used for\napproximate nearest neighbor (ANN) search, leveraging the principles of\nnavigable small-world graphs. However, it faces some limitations. The first is\nthe local optima problem, which arises from the algorithm's greedy search\nstrategy, selecting neighbors based solely on proximity at each step. This\noften leads to cluster disconnections. The second limitation is that HNSW\nfrequently fails to achieve logarithmic complexity, particularly in\nhigh-dimensional datasets, due to the exhaustive traversal through each layer.\nTo address these limitations, we propose a novel algorithm that mitigates local\noptima and cluster disconnections while enhancing the construction speed,\nmaintaining inference speed. The first component is a dual-branch HNSW\nstructure with LID-based insertion mechanisms, enabling traversal from multiple\ndirections. This improves outlier node capture, enhances cluster connectivity,\naccelerates construction speed and reduces the risk of local minima. The second\ncomponent incorporates a bridge-building technique that bypasses redundant\nintermediate layers, maintaining inference and making up the additional\ncomputational overhead introduced by the dual-branch structure. Experiments on\nvarious benchmarks and datasets showed that our algorithm outperforms the\noriginal HNSW in both accuracy and speed. We evaluated six datasets across\nComputer Vision (CV), and Natural Language Processing (NLP), showing recall\nimprovements of 18\\% in NLP, and up to 30\\% in CV tasks while reducing the\nconstruction time by up to 20\\% and maintaining the inference speed. We did not\nobserve any trade-offs in our algorithm. Ablation studies revealed that\nLID-based insertion had the greatest impact on performance, followed by the\ndual-branch structure and bridge-building components.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hierarchical Navigable Small World (HNSW) algorithm is widely used for\napproximate nearest neighbor (ANN) search, leveraging the principles of\nnavigable small-world graphs. However, it faces some limitations. The first is\nthe local optima problem, which arises from the algorithm's greedy search\nstrategy, selecting neighbors based solely on proximity at each step. This\noften leads to cluster disconnections. The second limitation is that HNSW\nfrequently fails to achieve logarithmic complexity, particularly in\nhigh-dimensional datasets, due to the exhaustive traversal through each layer.\nTo address these limitations, we propose a novel algorithm that mitigates local\noptima and cluster disconnections while enhancing the construction speed,\nmaintaining inference speed. The first component is a dual-branch HNSW\nstructure with LID-based insertion mechanisms, enabling traversal from multiple\ndirections. This improves outlier node capture, enhances cluster connectivity,\naccelerates construction speed and reduces the risk of local minima. The second\ncomponent incorporates a bridge-building technique that bypasses redundant\nintermediate layers, maintaining inference and making up the additional\ncomputational overhead introduced by the dual-branch structure. Experiments on\nvarious benchmarks and datasets showed that our algorithm outperforms the\noriginal HNSW in both accuracy and speed. We evaluated six datasets across\nComputer Vision (CV), and Natural Language Processing (NLP), showing recall\nimprovements of 18\\% in NLP, and up to 30\\% in CV tasks while reducing the\nconstruction time by up to 20\\% and maintaining the inference speed. We did not\nobserve any trade-offs in our algorithm. Ablation studies revealed that\nLID-based insertion had the greatest impact on performance, followed by the\ndual-branch structure and bridge-building components."
                },
                "authors": [
                    {
                        "name": "Hy Nguyen"
                    },
                    {
                        "name": "Nguyen Hung Nguyen"
                    },
                    {
                        "name": "Nguyen Linh Bao Nguyen"
                    },
                    {
                        "name": "Srikanth Thudumu"
                    },
                    {
                        "name": "Hung Du"
                    },
                    {
                        "name": "Rajesh Vasa"
                    },
                    {
                        "name": "Kon Mouzakis"
                    }
                ],
                "author_detail": {
                    "name": "Kon Mouzakis"
                },
                "author": "Kon Mouzakis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08558v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08558v2",
                "updated": "2025-04-25T07:12:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    7,
                    12,
                    28,
                    4,
                    115,
                    0
                ],
                "published": "2025-03-11T15:47:12Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    47,
                    12,
                    1,
                    70,
                    0
                ],
                "title": "Can We Detect Failures Without Failure Data? Uncertainty-Aware Runtime\n  Failure Detection for Imitation Learning Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can We Detect Failures Without Failure Data? Uncertainty-Aware Runtime\n  Failure Detection for Imitation Learning Policies"
                },
                "summary": "Recent years have witnessed impressive robotic manipulation systems driven by\nadvances in imitation learning and generative modeling, such as diffusion- and\nflow-based approaches. As robot policy performance increases, so does the\ncomplexity and time horizon of achievable tasks, inducing unexpected and\ndiverse failure modes that are difficult to predict a priori. To enable\ntrustworthy policy deployment in safety-critical human environments, reliable\nruntime failure detection becomes important during policy inference. However,\nmost existing failure detection approaches rely on prior knowledge of failure\nmodes and require failure data during training, which imposes a significant\nchallenge in practicality and scalability. In response to these limitations, we\npresent FAIL-Detect, a modular two-stage approach for failure detection in\nimitation learning-based robotic manipulation. To accurately identify failures\nfrom successful training data alone, we frame the problem as sequential\nout-of-distribution (OOD) detection. We first distill policy inputs and outputs\ninto scalar signals that correlate with policy failures and capture epistemic\nuncertainty. FAIL-Detect then employs conformal prediction (CP) as a versatile\nframework for uncertainty quantification with statistical guarantees.\nEmpirically, we thoroughly investigate both learned and post-hoc scalar signal\ncandidates on diverse robotic manipulation tasks. Our experiments show learned\nsignals to be mostly consistently effective, particularly when using our novel\nflow-based density estimator. Furthermore, our method detects failures more\naccurately and faster than state-of-the-art (SOTA) failure detection baselines.\nThese results highlight the potential of FAIL-Detect to enhance the safety and\nreliability of imitation learning-based robotic systems as they progress toward\nreal-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed impressive robotic manipulation systems driven by\nadvances in imitation learning and generative modeling, such as diffusion- and\nflow-based approaches. As robot policy performance increases, so does the\ncomplexity and time horizon of achievable tasks, inducing unexpected and\ndiverse failure modes that are difficult to predict a priori. To enable\ntrustworthy policy deployment in safety-critical human environments, reliable\nruntime failure detection becomes important during policy inference. However,\nmost existing failure detection approaches rely on prior knowledge of failure\nmodes and require failure data during training, which imposes a significant\nchallenge in practicality and scalability. In response to these limitations, we\npresent FAIL-Detect, a modular two-stage approach for failure detection in\nimitation learning-based robotic manipulation. To accurately identify failures\nfrom successful training data alone, we frame the problem as sequential\nout-of-distribution (OOD) detection. We first distill policy inputs and outputs\ninto scalar signals that correlate with policy failures and capture epistemic\nuncertainty. FAIL-Detect then employs conformal prediction (CP) as a versatile\nframework for uncertainty quantification with statistical guarantees.\nEmpirically, we thoroughly investigate both learned and post-hoc scalar signal\ncandidates on diverse robotic manipulation tasks. Our experiments show learned\nsignals to be mostly consistently effective, particularly when using our novel\nflow-based density estimator. Furthermore, our method detects failures more\naccurately and faster than state-of-the-art (SOTA) failure detection baselines.\nThese results highlight the potential of FAIL-Detect to enhance the safety and\nreliability of imitation learning-based robotic systems as they progress toward\nreal-world deployment."
                },
                "authors": [
                    {
                        "name": "Chen Xu"
                    },
                    {
                        "name": "Tony Khuong Nguyen"
                    },
                    {
                        "name": "Emma Dixon"
                    },
                    {
                        "name": "Christopher Rodriguez"
                    },
                    {
                        "name": "Patrick Miller"
                    },
                    {
                        "name": "Robert Lee"
                    },
                    {
                        "name": "Paarth Shah"
                    },
                    {
                        "name": "Rares Ambrus"
                    },
                    {
                        "name": "Haruki Nishimura"
                    },
                    {
                        "name": "Masha Itkina"
                    }
                ],
                "author_detail": {
                    "name": "Masha Itkina"
                },
                "author": "Masha Itkina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08558v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08558v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18116v1",
                "updated": "2025-04-25T06:48:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    6,
                    48,
                    55,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T06:48:55Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    6,
                    48,
                    55,
                    4,
                    115,
                    0
                ],
                "title": "Think, Prune, Train, Improve: Scaling Reasoning without Scaling Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think, Prune, Train, Improve: Scaling Reasoning without Scaling Models"
                },
                "summary": "Large language models (LLMs) have demonstrated strong capabilities in\nprogramming and mathematical reasoning tasks, but are constrained by limited\nhigh-quality training data. Synthetic data can be leveraged to enhance\nfine-tuning outcomes, but several factors influence this process, including\nmodel size, synthetic data volume, pruning strategy, and number of fine-tuning\nrounds. We explore these axes and investigate which conditions enable model\nself-improvement. We introduce the Think, Prune, Train process, a scalable\nframework that iteratively fine-tunes models on their own reasoning traces,\nusing ground-truth pruning to ensure high-quality training data. This approach\nyields improved performance: on GSM8K, Gemma2-2B achieves a Pass@1 of 57.6%\n(from 41.9%), Gemma2-9B reaches 82%, matching LLaMA-3.1-70B, and LLaMA-3.1-70B\nattains 91%, even surpassing GPT-4o, demonstrating the effectiveness of\nself-generated reasoning and systematic data selection for improving LLM\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong capabilities in\nprogramming and mathematical reasoning tasks, but are constrained by limited\nhigh-quality training data. Synthetic data can be leveraged to enhance\nfine-tuning outcomes, but several factors influence this process, including\nmodel size, synthetic data volume, pruning strategy, and number of fine-tuning\nrounds. We explore these axes and investigate which conditions enable model\nself-improvement. We introduce the Think, Prune, Train process, a scalable\nframework that iteratively fine-tunes models on their own reasoning traces,\nusing ground-truth pruning to ensure high-quality training data. This approach\nyields improved performance: on GSM8K, Gemma2-2B achieves a Pass@1 of 57.6%\n(from 41.9%), Gemma2-9B reaches 82%, matching LLaMA-3.1-70B, and LLaMA-3.1-70B\nattains 91%, even surpassing GPT-4o, demonstrating the effectiveness of\nself-generated reasoning and systematic data selection for improving LLM\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Caia Costello"
                    },
                    {
                        "name": "Simon Guo"
                    },
                    {
                        "name": "Anna Goldie"
                    },
                    {
                        "name": "Azalia Mirhoseini"
                    }
                ],
                "author_detail": {
                    "name": "Azalia Mirhoseini"
                },
                "author": "Azalia Mirhoseini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18114v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18114v1",
                "updated": "2025-04-25T06:37:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    6,
                    37,
                    29,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T06:37:29Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    6,
                    37,
                    29,
                    4,
                    115,
                    0
                ],
                "title": "Evaluating Evaluation Metrics -- The Mirage of Hallucination Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Evaluation Metrics -- The Mirage of Hallucination Detection"
                },
                "summary": "Hallucinations pose a significant obstacle to the reliability and widespread\nadoption of language models, yet their accurate measurement remains a\npersistent challenge. While many task- and domain-specific metrics have been\nproposed to assess faithfulness and factuality concerns, the robustness and\ngeneralization of these metrics are still untested. In this paper, we conduct a\nlarge-scale empirical evaluation of 6 diverse sets of hallucination detection\nmetrics across 4 datasets, 37 language models from 5 families, and 5 decoding\nmethods. Our extensive investigation reveals concerning gaps in current\nhallucination evaluation: metrics often fail to align with human judgments,\ntake an overtly myopic view of the problem, and show inconsistent gains with\nparameter scaling. Encouragingly, LLM-based evaluation, particularly with\nGPT-4, yields the best overall results, and mode-seeking decoding methods seem\nto reduce hallucinations, especially in knowledge-grounded settings. These\nfindings underscore the need for more robust metrics to understand and quantify\nhallucinations, and better strategies to mitigate them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations pose a significant obstacle to the reliability and widespread\nadoption of language models, yet their accurate measurement remains a\npersistent challenge. While many task- and domain-specific metrics have been\nproposed to assess faithfulness and factuality concerns, the robustness and\ngeneralization of these metrics are still untested. In this paper, we conduct a\nlarge-scale empirical evaluation of 6 diverse sets of hallucination detection\nmetrics across 4 datasets, 37 language models from 5 families, and 5 decoding\nmethods. Our extensive investigation reveals concerning gaps in current\nhallucination evaluation: metrics often fail to align with human judgments,\ntake an overtly myopic view of the problem, and show inconsistent gains with\nparameter scaling. Encouragingly, LLM-based evaluation, particularly with\nGPT-4, yields the best overall results, and mode-seeking decoding methods seem\nto reduce hallucinations, especially in knowledge-grounded settings. These\nfindings underscore the need for more robust metrics to understand and quantify\nhallucinations, and better strategies to mitigate them."
                },
                "authors": [
                    {
                        "name": "Atharva Kulkarni"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Joel Ruben Antony Moniz"
                    },
                    {
                        "name": "Xiou Ge"
                    },
                    {
                        "name": "Bo-Hsiang Tseng"
                    },
                    {
                        "name": "Dhivya Piraviperumal"
                    },
                    {
                        "name": "Swabha Swayamdipta"
                    },
                    {
                        "name": "Hong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Yu"
                },
                "author": "Hong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18114v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18114v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14657v2",
                "updated": "2025-04-25T06:34:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    6,
                    34,
                    43,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-20T15:37:05Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    15,
                    37,
                    5,
                    6,
                    110,
                    0
                ],
                "title": "A Case Study Exploring the Current Landscape of Synthetic Medical Record\n  Generation with Commercial LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Case Study Exploring the Current Landscape of Synthetic Medical Record\n  Generation with Commercial LLMs"
                },
                "summary": "Synthetic Electronic Health Records (EHRs) offer a valuable opportunity to\ncreate privacy preserving and harmonized structured data, supporting numerous\napplications in healthcare. Key benefits of synthetic data include precise\ncontrol over the data schema, improved fairness and representation of patient\npopulations, and the ability to share datasets without concerns about\ncompromising real individuals privacy. Consequently, the AI community has\nincreasingly turned to Large Language Models (LLMs) to generate synthetic data\nacross various domains. However, a significant challenge in healthcare is\nensuring that synthetic health records reliably generalize across different\nhospitals, a long standing issue in the field. In this work, we evaluate the\ncurrent state of commercial LLMs for generating synthetic data and investigate\nmultiple aspects of the generation process to identify areas where these models\nexcel and where they fall short. Our main finding from this work is that while\nLLMs can reliably generate synthetic health records for smaller subsets of\nfeatures, they struggle to preserve realistic distributions and correlations as\nthe dimensionality of the data increases, ultimately limiting their ability to\ngeneralize across diverse hospital settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Electronic Health Records (EHRs) offer a valuable opportunity to\ncreate privacy preserving and harmonized structured data, supporting numerous\napplications in healthcare. Key benefits of synthetic data include precise\ncontrol over the data schema, improved fairness and representation of patient\npopulations, and the ability to share datasets without concerns about\ncompromising real individuals privacy. Consequently, the AI community has\nincreasingly turned to Large Language Models (LLMs) to generate synthetic data\nacross various domains. However, a significant challenge in healthcare is\nensuring that synthetic health records reliably generalize across different\nhospitals, a long standing issue in the field. In this work, we evaluate the\ncurrent state of commercial LLMs for generating synthetic data and investigate\nmultiple aspects of the generation process to identify areas where these models\nexcel and where they fall short. Our main finding from this work is that while\nLLMs can reliably generate synthetic health records for smaller subsets of\nfeatures, they struggle to preserve realistic distributions and correlations as\nthe dimensionality of the data increases, ultimately limiting their ability to\ngeneralize across diverse hospital settings."
                },
                "authors": [
                    {
                        "name": "Yihan Lin"
                    },
                    {
                        "name": "Zhirong Bella Yu"
                    },
                    {
                        "name": "Simon Lee"
                    }
                ],
                "author_detail": {
                    "name": "Simon Lee"
                },
                "author": "Simon Lee",
                "arxiv_comment": "Accepted at the Conference of Health, Inference, Learning (CHIL 2025)\n  in Berkeley, CA. To appear in PMLR later in 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18112v1",
                "updated": "2025-04-25T06:30:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    6,
                    30,
                    50,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T06:30:50Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    6,
                    30,
                    50,
                    4,
                    115,
                    0
                ],
                "title": "Study on Real-Time Road Surface Reconstruction Using Stereo Vision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Study on Real-Time Road Surface Reconstruction Using Stereo Vision"
                },
                "summary": "Road surface reconstruction plays a crucial role in autonomous driving,\nproviding essential information for safe and smooth navigation. This paper\nenhances the RoadBEV [1] framework for real-time inference on edge devices by\noptimizing both efficiency and accuracy. To achieve this, we proposed to apply\nIsomorphic Global Structured Pruning to the stereo feature extraction backbone,\nreducing network complexity while maintaining performance. Additionally, the\nhead network is redesigned with an optimized hourglass structure, dynamic\nattention heads, reduced feature channels, mixed precision inference, and\nefficient probability volume computation. Our approach improves inference speed\nwhile achieving lower reconstruction error, making it well-suited for real-time\nroad surface reconstruction in autonomous driving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Road surface reconstruction plays a crucial role in autonomous driving,\nproviding essential information for safe and smooth navigation. This paper\nenhances the RoadBEV [1] framework for real-time inference on edge devices by\noptimizing both efficiency and accuracy. To achieve this, we proposed to apply\nIsomorphic Global Structured Pruning to the stereo feature extraction backbone,\nreducing network complexity while maintaining performance. Additionally, the\nhead network is redesigned with an optimized hourglass structure, dynamic\nattention heads, reduced feature channels, mixed precision inference, and\nefficient probability volume computation. Our approach improves inference speed\nwhile achieving lower reconstruction error, making it well-suited for real-time\nroad surface reconstruction in autonomous driving."
                },
                "authors": [
                    {
                        "name": "Deepak Ghimire"
                    },
                    {
                        "name": "Byoungjun Kim"
                    },
                    {
                        "name": "Donghoon Kim"
                    },
                    {
                        "name": "SungHwan Jeong"
                    }
                ],
                "author_detail": {
                    "name": "SungHwan Jeong"
                },
                "author": "SungHwan Jeong",
                "arxiv_comment": "Stereo Vision, Efficient CNN, Pruning, Optimization. 2025 Intelligent\n  Information and Control Conference (IICC 2025), Jeonju, Korea",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18107v1",
                "updated": "2025-04-25T06:26:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    6,
                    26,
                    0,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T06:26:00Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    6,
                    26,
                    0,
                    4,
                    115,
                    0
                ],
                "title": "Debiased Continuous Updating GMM with Many Weak Instruments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debiased Continuous Updating GMM with Many Weak Instruments"
                },
                "summary": "Many weak instrumental variables (IVs) are routinely used in the health and\nsocial sciences to improve identification and inference, but can lead to bias\nin the usual two-step generalized method of moments methods. We propose a new\ndebiased continuous updating estimator (CUE) which simultaneously address the\nbiases from the diverging number of weak IVs, and concomitant first-step\nnonparametric or high-dimensional estimation of regression functions in the\nmeasured covariates. We establish mean-square rate requirements on the\nfirst-step estimators so that debiased CUE remains consistent and\nasymptotically normal under a many weak IVs asymptotic regime, in which the\nnumber of IVs diverges with sample size while identification shrinks. We\nevaluate the proposed method via extensive Monte Carlo studies and an empirical\napplication to estimate the returns to education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many weak instrumental variables (IVs) are routinely used in the health and\nsocial sciences to improve identification and inference, but can lead to bias\nin the usual two-step generalized method of moments methods. We propose a new\ndebiased continuous updating estimator (CUE) which simultaneously address the\nbiases from the diverging number of weak IVs, and concomitant first-step\nnonparametric or high-dimensional estimation of regression functions in the\nmeasured covariates. We establish mean-square rate requirements on the\nfirst-step estimators so that debiased CUE remains consistent and\nasymptotically normal under a many weak IVs asymptotic regime, in which the\nnumber of IVs diverges with sample size while identification shrinks. We\nevaluate the proposed method via extensive Monte Carlo studies and an empirical\napplication to estimate the returns to education."
                },
                "authors": [
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Baoluo Sun"
                    }
                ],
                "author_detail": {
                    "name": "Baoluo Sun"
                },
                "author": "Baoluo Sun",
                "arxiv_comment": "34 pages, 1 figure, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02566v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02566v2",
                "updated": "2025-04-25T06:24:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    6,
                    24,
                    10,
                    4,
                    115,
                    0
                ],
                "published": "2024-05-03T19:24:41Z",
                "published_parsed": [
                    2024,
                    5,
                    3,
                    19,
                    24,
                    41,
                    4,
                    124,
                    0
                ],
                "title": "Combining X-Vectors and Bayesian Batch Active Learning: Two-Stage Active\n  Learning Pipeline for Speech Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining X-Vectors and Bayesian Batch Active Learning: Two-Stage Active\n  Learning Pipeline for Speech Recognition"
                },
                "summary": "This paper introduces a novel two-stage active learning (AL) pipeline for\nautomatic speech recognition (ASR), combining unsupervised and supervised AL\nmethods. The first stage utilizes unsupervised AL by using x-vectors clustering\nfor diverse sample selection from unlabeled speech data, thus establishing a\nrobust initial dataset for the subsequent supervised AL. The second stage\nincorporates a supervised AL strategy, with a batch AL method specifically\ndeveloped for ASR, aimed at selecting diverse and informative batches of\nsamples. Here, sample diversity is also achieved using x-vectors clustering,\nwhile the most informative samples are identified using a Bayesian AL method\ntailored for ASR with an adaptation of Monte Carlo dropout to approximate\nBayesian inference. This approach enables precise uncertainty estimation,\nthereby enhancing ASR model training with significantly reduced data\nrequirements. Our method has shown superior performance compared to competing\nmethods on homogeneous, heterogeneous, and OOD test sets, demonstrating that\nstrategic sample selection and innovative Bayesian modeling can substantially\noptimize both labeling effort and data utilization in deep learning-based ASR\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel two-stage active learning (AL) pipeline for\nautomatic speech recognition (ASR), combining unsupervised and supervised AL\nmethods. The first stage utilizes unsupervised AL by using x-vectors clustering\nfor diverse sample selection from unlabeled speech data, thus establishing a\nrobust initial dataset for the subsequent supervised AL. The second stage\nincorporates a supervised AL strategy, with a batch AL method specifically\ndeveloped for ASR, aimed at selecting diverse and informative batches of\nsamples. Here, sample diversity is also achieved using x-vectors clustering,\nwhile the most informative samples are identified using a Bayesian AL method\ntailored for ASR with an adaptation of Monte Carlo dropout to approximate\nBayesian inference. This approach enables precise uncertainty estimation,\nthereby enhancing ASR model training with significantly reduced data\nrequirements. Our method has shown superior performance compared to competing\nmethods on homogeneous, heterogeneous, and OOD test sets, demonstrating that\nstrategic sample selection and innovative Bayesian modeling can substantially\noptimize both labeling effort and data utilization in deep learning-based ASR\napplications."
                },
                "authors": [
                    {
                        "name": "Ognjen Kundacina"
                    },
                    {
                        "name": "Vladimir Vincan"
                    },
                    {
                        "name": "Dragisa Miskovic"
                    }
                ],
                "author_detail": {
                    "name": "Dragisa Miskovic"
                },
                "author": "Dragisa Miskovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02566v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02566v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18106v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18106v1",
                "updated": "2025-04-25T06:23:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    6,
                    23,
                    55,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T06:23:55Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    6,
                    23,
                    55,
                    4,
                    115,
                    0
                ],
                "title": "Comparative Study on the Discourse Meaning of Chinese and English Media\n  in the Paris Olympics Based on LDA Topic Modeling Technology and LLM Prompt\n  Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Study on the Discourse Meaning of Chinese and English Media\n  in the Paris Olympics Based on LDA Topic Modeling Technology and LLM Prompt\n  Engineering"
                },
                "summary": "This study analyzes Chinese and English media reports on the Paris Olympics\nusing topic modeling, Large Language Model (LLM) prompt engineering, and corpus\nphraseology methods to explore similarities and differences in discourse\nconstruction and attitudinal meanings. Common topics include the opening\nceremony, athlete performance, and sponsorship brands. Chinese media focus on\nspecific sports, sports spirit, doping controversies, and new technologies,\nwhile English media focus on female athletes, medal wins, and eligibility\ncontroversies. Chinese reports show more frequent prepositional co-occurrences\nand positive semantic prosody in describing the opening ceremony and sports\nspirit. English reports exhibit positive semantic prosody when covering female\nathletes but negative prosody in predicting opening ceremony reactions and\ndiscussing women's boxing controversies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study analyzes Chinese and English media reports on the Paris Olympics\nusing topic modeling, Large Language Model (LLM) prompt engineering, and corpus\nphraseology methods to explore similarities and differences in discourse\nconstruction and attitudinal meanings. Common topics include the opening\nceremony, athlete performance, and sponsorship brands. Chinese media focus on\nspecific sports, sports spirit, doping controversies, and new technologies,\nwhile English media focus on female athletes, medal wins, and eligibility\ncontroversies. Chinese reports show more frequent prepositional co-occurrences\nand positive semantic prosody in describing the opening ceremony and sports\nspirit. English reports exhibit positive semantic prosody when covering female\nathletes but negative prosody in predicting opening ceremony reactions and\ndiscussing women's boxing controversies."
                },
                "authors": [
                    {
                        "name": "Yinglong Yu"
                    },
                    {
                        "name": "Zhaopu Yao"
                    },
                    {
                        "name": "Fang Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Fang Yuan"
                },
                "author": "Fang Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18106v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18106v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18092v1",
                "updated": "2025-04-25T05:39:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    39,
                    20,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T05:39:20Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    39,
                    20,
                    4,
                    115,
                    0
                ],
                "title": "The Evolution of Heavy Ion Abundances with Solar Activity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Evolution of Heavy Ion Abundances with Solar Activity"
                },
                "summary": "When observed at 1 AU, slow solar wind is typically considered to have\noriginated in source regions with magnetic topologies that are intermittently\nopen to the heliosphere. Fast wind is typically considered to have originated\nin source regions that are continuously open to the heliosphere eg coronal\nholes. The evolution of the solar wind helium abundance (AHe) with solar\nactivity is likely driven by the evolution of different solar wind source\nregions. Separating the solar wind into fast and slow for each element based on\nits characteristic speed derived in Alterman et al. (2025) we quantify the\nevolution of helium and heavy element abundances $(X/H):(X/H)_\\mathrm{photo}$\nwith solar activity. We show that AHe strongly correlates with sunspot number;\nin slow and fast wind the average non-transient solar wind AHe is limited to\n51% of its photospheric value; slow wind heavy element abundances evolve\nsignificantly with solar activity; fast wind heavy element abundances do not;\nthe correlation coefficient with sunspot number of elemental abundances for\nspecies heavier than He monotonically increases with increasing mass; and the\ncorrelation coefficients between the in situ observations and the normalized\nsunspot number are stronger than those using the unnormalized sunspot number.\nWe infer that the sunspot number is a clock timing the solar cycle but not the\ndriver of the physical process underlying the evolution of these abundances\nwith solar activity; this underlying process is likely related to the energy\navailable to accelerate the solar plasma from the chromosphere and transition\nregion or low corona into the solar wind; and differences between the evolution\nof slow and fast solar wind abundances are similarly related to the energy\navailable to accelerate the elements at these heights above the Suns surface.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When observed at 1 AU, slow solar wind is typically considered to have\noriginated in source regions with magnetic topologies that are intermittently\nopen to the heliosphere. Fast wind is typically considered to have originated\nin source regions that are continuously open to the heliosphere eg coronal\nholes. The evolution of the solar wind helium abundance (AHe) with solar\nactivity is likely driven by the evolution of different solar wind source\nregions. Separating the solar wind into fast and slow for each element based on\nits characteristic speed derived in Alterman et al. (2025) we quantify the\nevolution of helium and heavy element abundances $(X/H):(X/H)_\\mathrm{photo}$\nwith solar activity. We show that AHe strongly correlates with sunspot number;\nin slow and fast wind the average non-transient solar wind AHe is limited to\n51% of its photospheric value; slow wind heavy element abundances evolve\nsignificantly with solar activity; fast wind heavy element abundances do not;\nthe correlation coefficient with sunspot number of elemental abundances for\nspecies heavier than He monotonically increases with increasing mass; and the\ncorrelation coefficients between the in situ observations and the normalized\nsunspot number are stronger than those using the unnormalized sunspot number.\nWe infer that the sunspot number is a clock timing the solar cycle but not the\ndriver of the physical process underlying the evolution of these abundances\nwith solar activity; this underlying process is likely related to the energy\navailable to accelerate the solar plasma from the chromosphere and transition\nregion or low corona into the solar wind; and differences between the evolution\nof slow and fast solar wind abundances are similarly related to the energy\navailable to accelerate the elements at these heights above the Suns surface."
                },
                "authors": [
                    {
                        "name": "B. L. Alterman"
                    },
                    {
                        "name": "Y. J. Rivera"
                    },
                    {
                        "name": "S. T. Lepri"
                    },
                    {
                        "name": "J. M. Raines"
                    },
                    {
                        "name": "R. D'Amicis"
                    }
                ],
                "author_detail": {
                    "name": "R. D'Amicis"
                },
                "author": "R. D'Amicis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01841v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01841v3",
                "updated": "2025-04-25T05:36:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    36,
                    19,
                    4,
                    115,
                    0
                ],
                "published": "2024-11-04T06:27:14Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    6,
                    27,
                    14,
                    0,
                    309,
                    0
                ],
                "title": "Leveraging Label Semantics and Meta-Label Refinement for Multi-Label\n  Question Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Label Semantics and Meta-Label Refinement for Multi-Label\n  Question Classification"
                },
                "summary": "Accurate annotation of educational resources is crucial for effective\npersonalized learning and resource recommendation in online education. However,\nfine-grained knowledge labels often overlap or share similarities, making it\ndifficult for existing multi-label classification methods to differentiate\nthem. The label distribution imbalance due to sparsity of human annotations\nfurther intensifies these challenges. To address these issues, this paper\nintroduces RR2QC, a novel Retrieval Reranking method to multi-label Question\nClassification by leveraging label semantics and meta-label refinement. First,\nRR2QC improves the pre-training strategy by utilizing semantic relationships\nwithin and across label groups. Second, it introduces a class center learning\ntask to align questions with label semantics during downstream training.\nFinally, this method decomposes labels into meta-labels and uses a meta-label\nclassifier to rerank the retrieved label sequences. In doing so, RR2QC enhances\nthe understanding and prediction capability of long-tail labels by learning\nfrom meta-labels that frequently appear in other labels. Additionally, a\nmathematical LLM is used to generate solutions for questions, extracting latent\ninformation to further refine the model's insights. Experimental results show\nthat RR2QC outperforms existing methods in Precision@K and F1 scores across\nmultiple educational datasets, demonstrating its effectiveness for online\neducation applications. The code and datasets are available at\nhttps://github.com/78Erii/RR2QC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate annotation of educational resources is crucial for effective\npersonalized learning and resource recommendation in online education. However,\nfine-grained knowledge labels often overlap or share similarities, making it\ndifficult for existing multi-label classification methods to differentiate\nthem. The label distribution imbalance due to sparsity of human annotations\nfurther intensifies these challenges. To address these issues, this paper\nintroduces RR2QC, a novel Retrieval Reranking method to multi-label Question\nClassification by leveraging label semantics and meta-label refinement. First,\nRR2QC improves the pre-training strategy by utilizing semantic relationships\nwithin and across label groups. Second, it introduces a class center learning\ntask to align questions with label semantics during downstream training.\nFinally, this method decomposes labels into meta-labels and uses a meta-label\nclassifier to rerank the retrieved label sequences. In doing so, RR2QC enhances\nthe understanding and prediction capability of long-tail labels by learning\nfrom meta-labels that frequently appear in other labels. Additionally, a\nmathematical LLM is used to generate solutions for questions, extracting latent\ninformation to further refine the model's insights. Experimental results show\nthat RR2QC outperforms existing methods in Precision@K and F1 scores across\nmultiple educational datasets, demonstrating its effectiveness for online\neducation applications. The code and datasets are available at\nhttps://github.com/78Erii/RR2QC."
                },
                "authors": [
                    {
                        "name": "Shi Dong"
                    },
                    {
                        "name": "Xiaobei Niu"
                    },
                    {
                        "name": "Rui Zhong"
                    },
                    {
                        "name": "Zhifeng Wang"
                    },
                    {
                        "name": "Mingzhang Zuo"
                    }
                ],
                "author_detail": {
                    "name": "Mingzhang Zuo"
                },
                "author": "Mingzhang Zuo",
                "arxiv_doi": "10.1016/j.knosys.2025.113412",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.knosys.2025.113412",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.01841v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01841v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Knowledge-Based Systems, 2025: 113412",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08813v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08813v2",
                "updated": "2025-04-25T05:34:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    34,
                    47,
                    4,
                    115,
                    0
                ],
                "published": "2024-09-13T13:24:52Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    13,
                    24,
                    52,
                    4,
                    257,
                    0
                ],
                "title": "Your Weak LLM is Secretly a Strong Teacher for Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Your Weak LLM is Secretly a Strong Teacher for Alignment"
                },
                "summary": "The burgeoning capabilities of large language models (LLMs) have underscored\nthe need for alignment to ensure these models act in accordance with human\nvalues and intentions. Existing alignment frameworks present constraints either\nin the form of expensive human effort or high computational costs. This paper\nexplores a promising middle ground, where we employ a weak LLM that is\nsignificantly less resource-intensive than top-tier models, yet offers more\nautomation than purely human feedback. We present a systematic study to\nevaluate and understand weak LLM's ability to generate feedback for alignment.\nOur empirical findings demonstrate that weak LLMs can provide feedback that\nrivals or even exceeds that of fully human-annotated data. Our study indicates\na minimized impact of model size on feedback efficacy, shedding light on a\nscalable and sustainable alignment strategy. To deepen our understanding of\nalignment under weak LLM feedback, we conduct a series of qualitative and\nquantitative analyses, offering novel insights into the quality discrepancies\nbetween human feedback vs. weak LLM feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The burgeoning capabilities of large language models (LLMs) have underscored\nthe need for alignment to ensure these models act in accordance with human\nvalues and intentions. Existing alignment frameworks present constraints either\nin the form of expensive human effort or high computational costs. This paper\nexplores a promising middle ground, where we employ a weak LLM that is\nsignificantly less resource-intensive than top-tier models, yet offers more\nautomation than purely human feedback. We present a systematic study to\nevaluate and understand weak LLM's ability to generate feedback for alignment.\nOur empirical findings demonstrate that weak LLMs can provide feedback that\nrivals or even exceeds that of fully human-annotated data. Our study indicates\na minimized impact of model size on feedback efficacy, shedding light on a\nscalable and sustainable alignment strategy. To deepen our understanding of\nalignment under weak LLM feedback, we conduct a series of qualitative and\nquantitative analyses, offering novel insights into the quality discrepancies\nbetween human feedback vs. weak LLM feedback."
                },
                "authors": [
                    {
                        "name": "Leitian Tao"
                    },
                    {
                        "name": "Yixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Yixuan Li"
                },
                "author": "Yixuan Li",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08813v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08813v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18085v1",
                "updated": "2025-04-25T05:25:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    25,
                    27,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T05:25:27Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    25,
                    27,
                    4,
                    115,
                    0
                ],
                "title": "Random-Set Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random-Set Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are known to produce very high-quality tests and\nresponses to our queries. But how much can we trust this generated text? In\nthis paper, we study the problem of uncertainty quantification in LLMs. We\npropose a novel Random-Set Large Language Model (RSLLM) approach which predicts\nfinite random sets (belief functions) over the token space, rather than\nprobability vectors as in classical LLMs. In order to allow so efficiently, we\nalso present a methodology based on hierarchical clustering to extract and use\na budget of \"focal\" subsets of tokens upon which the belief prediction is\ndefined, rather than using all possible collections of tokens, making the\nmethod scalable yet effective. RS-LLMs encode the epistemic uncertainty induced\nin their generation process by the size and diversity of its training set via\nthe size of the credal sets associated with the predicted belief functions. The\nproposed approach is evaluated on CoQA and OBQA datasets using Llama2-7b,\nMistral-7b and Phi-2 models and is shown to outperform the standard model in\nboth datasets in terms of correctness of answer while also showing potential in\nestimating the second level uncertainty in its predictions and providing the\ncapability to detect when its hallucinating.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are known to produce very high-quality tests and\nresponses to our queries. But how much can we trust this generated text? In\nthis paper, we study the problem of uncertainty quantification in LLMs. We\npropose a novel Random-Set Large Language Model (RSLLM) approach which predicts\nfinite random sets (belief functions) over the token space, rather than\nprobability vectors as in classical LLMs. In order to allow so efficiently, we\nalso present a methodology based on hierarchical clustering to extract and use\na budget of \"focal\" subsets of tokens upon which the belief prediction is\ndefined, rather than using all possible collections of tokens, making the\nmethod scalable yet effective. RS-LLMs encode the epistemic uncertainty induced\nin their generation process by the size and diversity of its training set via\nthe size of the credal sets associated with the predicted belief functions. The\nproposed approach is evaluated on CoQA and OBQA datasets using Llama2-7b,\nMistral-7b and Phi-2 models and is shown to outperform the standard model in\nboth datasets in terms of correctness of answer while also showing potential in\nestimating the second level uncertainty in its predictions and providing the\ncapability to detect when its hallucinating."
                },
                "authors": [
                    {
                        "name": "Muhammad Mubashar"
                    },
                    {
                        "name": "Shireen Kudukkil Manchingal"
                    },
                    {
                        "name": "Fabio Cuzzolin"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Cuzzolin"
                },
                "author": "Fabio Cuzzolin",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18083v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18083v1",
                "updated": "2025-04-25T05:19:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    19,
                    2,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T05:19:02Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    19,
                    2,
                    4,
                    115,
                    0
                ],
                "title": "Automating Function-Level TARA for Automotive Full-Lifecycle Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Function-Level TARA for Automotive Full-Lifecycle Security"
                },
                "summary": "As modern vehicles evolve into intelligent and connected systems, their\ngrowing complexity introduces significant cybersecurity risks. Threat Analysis\nand Risk Assessment (TARA) has therefore become essential for managing these\nrisks under mandatory regulations. However, existing TARA automation methods\nrely on static threat libraries, limiting their utility in the detailed,\nfunction-level analyses demanded by industry. This paper introduces\nDefenseWeaver, the first system that automates function-level TARA using\ncomponent-specific details and large language models (LLMs). DefenseWeaver\ndynamically generates attack trees and risk evaluations from system\nconfigurations described in an extended OpenXSAM++ format, then employs a\nmulti-agent framework to coordinate specialized LLM roles for more robust\nanalysis. To further adapt to evolving threats and diverse standards,\nDefenseWeaver incorporates Low-Rank Adaptation (LoRA) fine-tuning and\nRetrieval-Augmented Generation (RAG) with expert-curated TARA reports. We\nvalidated DefenseWeaver through deployment in four automotive security\nprojects, where it identified 11 critical attack paths, verified through\npenetration testing, and subsequently reported and remediated by the relevant\nautomakers and suppliers. Additionally, DefenseWeaver demonstrated cross-domain\nadaptability, successfully applying to unmanned aerial vehicles (UAVs) and\nmarine navigation systems. In comparison to human experts, DefenseWeaver\noutperformed manual attack tree generation across six assessment scenarios.\nIntegrated into commercial cybersecurity platforms such as UAES and Xiaomi,\nDefenseWeaver has generated over 8,200 attack trees. These results highlight\nits ability to significantly reduce processing time, and its scalability and\ntransformative impact on cybersecurity across industries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As modern vehicles evolve into intelligent and connected systems, their\ngrowing complexity introduces significant cybersecurity risks. Threat Analysis\nand Risk Assessment (TARA) has therefore become essential for managing these\nrisks under mandatory regulations. However, existing TARA automation methods\nrely on static threat libraries, limiting their utility in the detailed,\nfunction-level analyses demanded by industry. This paper introduces\nDefenseWeaver, the first system that automates function-level TARA using\ncomponent-specific details and large language models (LLMs). DefenseWeaver\ndynamically generates attack trees and risk evaluations from system\nconfigurations described in an extended OpenXSAM++ format, then employs a\nmulti-agent framework to coordinate specialized LLM roles for more robust\nanalysis. To further adapt to evolving threats and diverse standards,\nDefenseWeaver incorporates Low-Rank Adaptation (LoRA) fine-tuning and\nRetrieval-Augmented Generation (RAG) with expert-curated TARA reports. We\nvalidated DefenseWeaver through deployment in four automotive security\nprojects, where it identified 11 critical attack paths, verified through\npenetration testing, and subsequently reported and remediated by the relevant\nautomakers and suppliers. Additionally, DefenseWeaver demonstrated cross-domain\nadaptability, successfully applying to unmanned aerial vehicles (UAVs) and\nmarine navigation systems. In comparison to human experts, DefenseWeaver\noutperformed manual attack tree generation across six assessment scenarios.\nIntegrated into commercial cybersecurity platforms such as UAES and Xiaomi,\nDefenseWeaver has generated over 8,200 attack trees. These results highlight\nits ability to significantly reduce processing time, and its scalability and\ntransformative impact on cybersecurity across industries."
                },
                "authors": [
                    {
                        "name": "Yuqiao Yang"
                    },
                    {
                        "name": "Yongzhao Zhang"
                    },
                    {
                        "name": "Wenhao Liu"
                    },
                    {
                        "name": "Jun Li"
                    },
                    {
                        "name": "Pengtao Shi"
                    },
                    {
                        "name": "DingYu Zhong"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Ting Chen"
                    },
                    {
                        "name": "Sheng Cao"
                    },
                    {
                        "name": "Yuntao Ren"
                    },
                    {
                        "name": "Yongyue Wu"
                    },
                    {
                        "name": "Xiaosong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaosong Zhang"
                },
                "author": "Xiaosong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18083v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18080v1",
                "updated": "2025-04-25T05:15:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    15,
                    31,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T05:15:31Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    15,
                    31,
                    4,
                    115,
                    0
                ],
                "title": "Stabilizing Reasoning in Medical LLMs with Continued Pretraining and\n  Reasoning Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stabilizing Reasoning in Medical LLMs with Continued Pretraining and\n  Reasoning Preference Optimization"
                },
                "summary": "Large Language Models (LLMs) show potential in medicine, yet clinical\nadoption is hindered by concerns over factual accuracy, language-specific\nlimitations (e.g., Japanese), and critically, their reliability when required\nto generate reasoning explanations -- a prerequisite for trust. This paper\nintroduces Preferred-MedLLM-Qwen-72B, a 72B-parameter model optimized for the\nJapanese medical domain to achieve both high accuracy and stable reasoning. We\nemploy a two-stage fine-tuning process on the Qwen2.5-72B base model: first,\nContinued Pretraining (CPT) on a comprehensive Japanese medical corpus instills\ndeep domain knowledge. Second, Reasoning Preference Optimization (RPO), a\npreference-based method, enhances the generation of reliable reasoning pathways\nwhile preserving high answer accuracy. Evaluations on the Japanese Medical\nLicensing Exam benchmark (IgakuQA) show Preferred-MedLLM-Qwen-72B achieves\nstate-of-the-art performance (0.868 accuracy), surpassing strong proprietary\nmodels like GPT-4o (0.866). Crucially, unlike baseline or CPT-only models which\nexhibit significant accuracy degradation (up to 11.5\\% and 3.8\\% respectively\non IgakuQA) when prompted for explanations, our model maintains its high\naccuracy (0.868) under such conditions. This highlights RPO's effectiveness in\nstabilizing reasoning generation. This work underscores the importance of\noptimizing for reliable explanations alongside accuracy. We release the\nPreferred-MedLLM-Qwen-72B model weights to foster research into trustworthy\nLLMs for specialized, high-stakes applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show potential in medicine, yet clinical\nadoption is hindered by concerns over factual accuracy, language-specific\nlimitations (e.g., Japanese), and critically, their reliability when required\nto generate reasoning explanations -- a prerequisite for trust. This paper\nintroduces Preferred-MedLLM-Qwen-72B, a 72B-parameter model optimized for the\nJapanese medical domain to achieve both high accuracy and stable reasoning. We\nemploy a two-stage fine-tuning process on the Qwen2.5-72B base model: first,\nContinued Pretraining (CPT) on a comprehensive Japanese medical corpus instills\ndeep domain knowledge. Second, Reasoning Preference Optimization (RPO), a\npreference-based method, enhances the generation of reliable reasoning pathways\nwhile preserving high answer accuracy. Evaluations on the Japanese Medical\nLicensing Exam benchmark (IgakuQA) show Preferred-MedLLM-Qwen-72B achieves\nstate-of-the-art performance (0.868 accuracy), surpassing strong proprietary\nmodels like GPT-4o (0.866). Crucially, unlike baseline or CPT-only models which\nexhibit significant accuracy degradation (up to 11.5\\% and 3.8\\% respectively\non IgakuQA) when prompted for explanations, our model maintains its high\naccuracy (0.868) under such conditions. This highlights RPO's effectiveness in\nstabilizing reasoning generation. This work underscores the importance of\noptimizing for reliable explanations alongside accuracy. We release the\nPreferred-MedLLM-Qwen-72B model weights to foster research into trustworthy\nLLMs for specialized, high-stakes applications."
                },
                "authors": [
                    {
                        "name": "Wataru Kawakami"
                    },
                    {
                        "name": "Keita Suzuki"
                    },
                    {
                        "name": "Junichiro Iwasawa"
                    }
                ],
                "author_detail": {
                    "name": "Junichiro Iwasawa"
                },
                "author": "Junichiro Iwasawa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14603v2",
                "updated": "2025-04-25T05:14:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    14,
                    14,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-20T13:04:43Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    13,
                    4,
                    43,
                    6,
                    110,
                    0
                ],
                "title": "UFO2: The Desktop AgentOS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UFO2: The Desktop AgentOS"
                },
                "summary": "Recent Computer-Using Agents (CUAs), powered by multimodal large language\nmodels (LLMs), offer a promising direction for automating complex desktop\nworkflows through natural language. However, most existing CUAs remain\nconceptual prototypes, hindered by shallow OS integration, fragile\nscreenshot-based interaction, and disruptive execution.\n  We present UFO2, a multiagent AgentOS for Windows desktops that elevates CUAs\ninto practical, system-level automation. UFO2 features a centralized HostAgent\nfor task decomposition and coordination, alongside a collection of\napplication-specialized AppAgent equipped with native APIs, domain-specific\nknowledge, and a unified GUI--API action layer. This architecture enables\nrobust task execution while preserving modularity and extensibility. A hybrid\ncontrol detection pipeline fuses Windows UI Automation (UIA) with vision-based\nparsing to support diverse interface styles. Runtime efficiency is further\nenhanced through speculative multi-action planning, reducing per-step LLM\noverhead. Finally, a Picture-in-Picture (PiP) interface enables automation\nwithin an isolated virtual desktop, allowing agents and users to operate\nconcurrently without interference.\n  We evaluate UFO2 across over 20 real-world Windows applications,\ndemonstrating substantial improvements in robustness and execution accuracy\nover prior CUAs. Our results show that deep OS integration unlocks a scalable\npath toward reliable, user-aligned desktop automation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Computer-Using Agents (CUAs), powered by multimodal large language\nmodels (LLMs), offer a promising direction for automating complex desktop\nworkflows through natural language. However, most existing CUAs remain\nconceptual prototypes, hindered by shallow OS integration, fragile\nscreenshot-based interaction, and disruptive execution.\n  We present UFO2, a multiagent AgentOS for Windows desktops that elevates CUAs\ninto practical, system-level automation. UFO2 features a centralized HostAgent\nfor task decomposition and coordination, alongside a collection of\napplication-specialized AppAgent equipped with native APIs, domain-specific\nknowledge, and a unified GUI--API action layer. This architecture enables\nrobust task execution while preserving modularity and extensibility. A hybrid\ncontrol detection pipeline fuses Windows UI Automation (UIA) with vision-based\nparsing to support diverse interface styles. Runtime efficiency is further\nenhanced through speculative multi-action planning, reducing per-step LLM\noverhead. Finally, a Picture-in-Picture (PiP) interface enables automation\nwithin an isolated virtual desktop, allowing agents and users to operate\nconcurrently without interference.\n  We evaluate UFO2 across over 20 real-world Windows applications,\ndemonstrating substantial improvements in robustness and execution accuracy\nover prior CUAs. Our results show that deep OS integration unlocks a scalable\npath toward reliable, user-aligned desktop automation."
                },
                "authors": [
                    {
                        "name": "Chaoyun Zhang"
                    },
                    {
                        "name": "He Huang"
                    },
                    {
                        "name": "Chiming Ni"
                    },
                    {
                        "name": "Jian Mu"
                    },
                    {
                        "name": "Si Qin"
                    },
                    {
                        "name": "Shilin He"
                    },
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Fangkai Yang"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Liqun Li"
                    },
                    {
                        "name": "Yu Kang"
                    },
                    {
                        "name": "Zhao Jiang"
                    },
                    {
                        "name": "Suzhen Zheng"
                    },
                    {
                        "name": "Rujia Wang"
                    },
                    {
                        "name": "Jiaxu Qian"
                    },
                    {
                        "name": "Minghua Ma"
                    },
                    {
                        "name": "Jian-Guang Lou"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dongmei Zhang"
                },
                "author": "Dongmei Zhang",
                "arxiv_comment": "The source code of UFO2 is publicly available at\n  https://github.com/microsoft/UFO/, with comprehensive documentation provided\n  at https://microsoft.github.io/UFO/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14335v2",
                "updated": "2025-04-25T05:08:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    8,
                    45,
                    4,
                    115,
                    0
                ],
                "published": "2024-12-18T21:09:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines"
                },
                "summary": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). That is, C3 on average achieves only 21% of ideal\nspeedup. This is so, due to known challenges of compute and memory interference\nbetween concurrent GPU kernels (that is, sharing of GPU's compute units, caches\nand HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build concurrent communication collectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). That is, C3 on average achieves only 21% of ideal\nspeedup. This is so, due to known challenges of compute and memory interference\nbetween concurrent GPU kernels (that is, sharing of GPU's compute units, caches\nand HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build concurrent communication collectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs."
                },
                "authors": [
                    {
                        "name": "Anirudha Agrawal"
                    },
                    {
                        "name": "Shaizeen Aga"
                    },
                    {
                        "name": "Suchita Pati"
                    },
                    {
                        "name": "Mahzabeen Islam"
                    }
                ],
                "author_detail": {
                    "name": "Mahzabeen Islam"
                },
                "author": "Mahzabeen Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18073v1",
                "updated": "2025-04-25T05:04:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    4,
                    3,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T05:04:03Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    4,
                    3,
                    4,
                    115,
                    0
                ],
                "title": "Modular integration of neural connectomics, dynamics and biomechanics\n  for identification of behavioral sensorimotor pathways in Caenorhabditis\n  elegans",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modular integration of neural connectomics, dynamics and biomechanics\n  for identification of behavioral sensorimotor pathways in Caenorhabditis\n  elegans"
                },
                "summary": "Computational approaches which emulate in-vivo nervous system are needed to\ninvestigate mechanisms of the brain to orchestrate behavior. Such approaches\nmust integrate a series of biophysical models encompassing the nervous system,\nmuscles, biomechanics to allow observing the system in its entirety while\nsupporting model variations. Here we develop modWorm: a modular modeling\nframework for the nematode C. elegans. modWorm allows for construction of a\nmodel as an integrated series of configurable, exchangeable modules each\ndescribing specific biophysical processes across different modalities.\nUtilizing modWorm, we propose a base neuro-mechanical model for C. elegans\nbuilt upon the complete connectome. The model integrates a series of 7 modules:\ni) intra-cellular dynamics, ii) electrical and iii) chemical extra-cellular\nneural dynamics, iv) translation of neural activity to muscle calcium dynamics,\nv) muscle calcium dynamics to muscle forces, vi) muscle forces to body postures\nand vii) proprioceptive feedback. We validate the base model by in-silico\ninjection of constant currents into neurons known to be associated with\nlocomotion behaviors and by applying external forces to the body. Applications\nof in-silico neural stimuli experimentally known to modulate locomotion show\nthat the model can recapitulate natural behavioral responses such as forward\nand backward locomotion as well as mid-locomotion responses such as avoidance\nand turns. Furthermore, through in-silico ablation surveys, the model can infer\nnovel neural circuits involved in sensorimotor behaviors. To further dissect\nmechanisms of locomotion, we utilize modWorm to introduce empirical based model\nvariations and model optimizations to elucidate their effects on simulated\nlocomotion. Our results show that modWorm can be utilized to identify neural\ncircuits which control, mediate and generate natural behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational approaches which emulate in-vivo nervous system are needed to\ninvestigate mechanisms of the brain to orchestrate behavior. Such approaches\nmust integrate a series of biophysical models encompassing the nervous system,\nmuscles, biomechanics to allow observing the system in its entirety while\nsupporting model variations. Here we develop modWorm: a modular modeling\nframework for the nematode C. elegans. modWorm allows for construction of a\nmodel as an integrated series of configurable, exchangeable modules each\ndescribing specific biophysical processes across different modalities.\nUtilizing modWorm, we propose a base neuro-mechanical model for C. elegans\nbuilt upon the complete connectome. The model integrates a series of 7 modules:\ni) intra-cellular dynamics, ii) electrical and iii) chemical extra-cellular\nneural dynamics, iv) translation of neural activity to muscle calcium dynamics,\nv) muscle calcium dynamics to muscle forces, vi) muscle forces to body postures\nand vii) proprioceptive feedback. We validate the base model by in-silico\ninjection of constant currents into neurons known to be associated with\nlocomotion behaviors and by applying external forces to the body. Applications\nof in-silico neural stimuli experimentally known to modulate locomotion show\nthat the model can recapitulate natural behavioral responses such as forward\nand backward locomotion as well as mid-locomotion responses such as avoidance\nand turns. Furthermore, through in-silico ablation surveys, the model can infer\nnovel neural circuits involved in sensorimotor behaviors. To further dissect\nmechanisms of locomotion, we utilize modWorm to introduce empirical based model\nvariations and model optimizations to elucidate their effects on simulated\nlocomotion. Our results show that modWorm can be utilized to identify neural\ncircuits which control, mediate and generate natural behavior."
                },
                "authors": [
                    {
                        "name": "Jimin Kim"
                    },
                    {
                        "name": "Jeremy T. Florman"
                    },
                    {
                        "name": "Julia A. Santos"
                    },
                    {
                        "name": "Mark J. Alkema"
                    },
                    {
                        "name": "Eli Shlizerman"
                    }
                ],
                "author_detail": {
                    "name": "Eli Shlizerman"
                },
                "author": "Eli Shlizerman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17699v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17699v2",
                "updated": "2025-04-25T05:02:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    2,
                    28,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-24T16:08:52Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    16,
                    8,
                    52,
                    3,
                    114,
                    0
                ],
                "title": "Quadratic Interest Network for Multimodal Click-Through Rate Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quadratic Interest Network for Multimodal Click-Through Rate Prediction"
                },
                "summary": "Multimodal click-through rate (CTR) prediction is a key technique in\nindustrial recommender systems. It leverages heterogeneous modalities such as\ntext, images, and behavioral logs to capture high-order feature interactions\nbetween users and items, thereby enhancing the system's understanding of user\ninterests and its ability to predict click behavior. The primary challenge in\nthis field lies in effectively utilizing the rich semantic information from\nmultiple modalities while satisfying the low-latency requirements of online\ninference in real-world applications. To foster progress in this area, the\nMultimodal CTR Prediction Challenge Track of the WWW 2025 EReL@MIR Workshop\nformulates the problem into two tasks: (1) Task 1 of Multimodal Item Embedding:\nthis task aims to explore multimodal information extraction and item\nrepresentation learning methods that enhance recommendation tasks; and (2) Task\n2 of Multimodal CTR Prediction: this task aims to explore what multimodal\nrecommendation model can effectively leverage multimodal embedding features and\nachieve better performance. In this paper, we propose a novel model for Task 2,\nnamed Quadratic Interest Network (QIN) for Multimodal CTR Prediction.\nSpecifically, QIN employs adaptive sparse target attention to extract\nmultimodal user behavior features, and leverages Quadratic Neural Networks to\ncapture high-order feature interactions. As a result, QIN achieved an AUC of\n0.9798 on the leaderboard and ranked second in the competition. The model code,\ntraining logs, hyperparameter configurations, and checkpoints are available at\nhttps://github.com/salmon1802/QIN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal click-through rate (CTR) prediction is a key technique in\nindustrial recommender systems. It leverages heterogeneous modalities such as\ntext, images, and behavioral logs to capture high-order feature interactions\nbetween users and items, thereby enhancing the system's understanding of user\ninterests and its ability to predict click behavior. The primary challenge in\nthis field lies in effectively utilizing the rich semantic information from\nmultiple modalities while satisfying the low-latency requirements of online\ninference in real-world applications. To foster progress in this area, the\nMultimodal CTR Prediction Challenge Track of the WWW 2025 EReL@MIR Workshop\nformulates the problem into two tasks: (1) Task 1 of Multimodal Item Embedding:\nthis task aims to explore multimodal information extraction and item\nrepresentation learning methods that enhance recommendation tasks; and (2) Task\n2 of Multimodal CTR Prediction: this task aims to explore what multimodal\nrecommendation model can effectively leverage multimodal embedding features and\nachieve better performance. In this paper, we propose a novel model for Task 2,\nnamed Quadratic Interest Network (QIN) for Multimodal CTR Prediction.\nSpecifically, QIN employs adaptive sparse target attention to extract\nmultimodal user behavior features, and leverages Quadratic Neural Networks to\ncapture high-order feature interactions. As a result, QIN achieved an AUC of\n0.9798 on the leaderboard and ranked second in the competition. The model code,\ntraining logs, hyperparameter configurations, and checkpoints are available at\nhttps://github.com/salmon1802/QIN."
                },
                "authors": [
                    {
                        "name": "Honghao Li"
                    },
                    {
                        "name": "Hanwei Li"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Ziniu Yu"
                    },
                    {
                        "name": "Lei Sang"
                    },
                    {
                        "name": "Yiwen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiwen Zhang"
                },
                "author": "Yiwen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17699v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17699v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18070v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18070v1",
                "updated": "2025-04-25T04:47:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    4,
                    47,
                    34,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T04:47:34Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    4,
                    47,
                    34,
                    4,
                    115,
                    0
                ],
                "title": "PropRAG: Guiding Retrieval with Beam Search over Proposition Paths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PropRAG: Guiding Retrieval with Beam Search over Proposition Paths"
                },
                "summary": "Retrieval Augmented Generation (RAG) has become the standard non-parametric\napproach for equipping Large Language Models (LLMs) with up-to-date knowledge\nand mitigating catastrophic forgetting common in continual learning. However,\nstandard RAG, relying on independent passage retrieval, fails to capture the\ninterconnected nature of human memory crucial for complex reasoning\n(associativity) and contextual understanding (sense-making). While structured\nRAG methods like HippoRAG utilize knowledge graphs (KGs) built from triples,\nthe inherent context loss limits fidelity. We introduce PropRAG, a framework\nleveraging contextually rich propositions and a novel beam search algorithm\nover proposition paths to explicitly discover multi-step reasoning chains.\nCrucially, PropRAG's online retrieval process operates entirely without\ninvoking generative LLMs, relying instead on efficient graph traversal and\npre-computed embeddings. This avoids online LLM inference costs and potential\ninconsistencies during evidence gathering. LLMs are used effectively offline\nfor high-quality proposition extraction and post-retrieval for answer\ngeneration. PropRAG achieves state-of-the-art zero-shot Recall@5 results on\nPopQA (55.3%), 2Wiki (93.7%), HotpotQA (97.0%), and MuSiQue (77.3%), alongside\ntop F1 scores (e.g., 52.4% on MuSiQue). By improving evidence retrieval through\nricher representation and explicit, LLM-free online path finding, PropRAG\nadvances non-parametric continual learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) has become the standard non-parametric\napproach for equipping Large Language Models (LLMs) with up-to-date knowledge\nand mitigating catastrophic forgetting common in continual learning. However,\nstandard RAG, relying on independent passage retrieval, fails to capture the\ninterconnected nature of human memory crucial for complex reasoning\n(associativity) and contextual understanding (sense-making). While structured\nRAG methods like HippoRAG utilize knowledge graphs (KGs) built from triples,\nthe inherent context loss limits fidelity. We introduce PropRAG, a framework\nleveraging contextually rich propositions and a novel beam search algorithm\nover proposition paths to explicitly discover multi-step reasoning chains.\nCrucially, PropRAG's online retrieval process operates entirely without\ninvoking generative LLMs, relying instead on efficient graph traversal and\npre-computed embeddings. This avoids online LLM inference costs and potential\ninconsistencies during evidence gathering. LLMs are used effectively offline\nfor high-quality proposition extraction and post-retrieval for answer\ngeneration. PropRAG achieves state-of-the-art zero-shot Recall@5 results on\nPopQA (55.3%), 2Wiki (93.7%), HotpotQA (97.0%), and MuSiQue (77.3%), alongside\ntop F1 scores (e.g., 52.4% on MuSiQue). By improving evidence retrieval through\nricher representation and explicit, LLM-free online path finding, PropRAG\nadvances non-parametric continual learning."
                },
                "authors": [
                    {
                        "name": "Jingjin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jingjin Wang"
                },
                "author": "Jingjin Wang",
                "arxiv_comment": "Code and data to be released at:\n  https://github.com/ReLink-Inc/PropRAG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18070v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18068v1",
                "updated": "2025-04-25T04:45:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    4,
                    45,
                    35,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T04:45:35Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    4,
                    45,
                    35,
                    4,
                    115,
                    0
                ],
                "title": "S3MOT: Monocular 3D Object Tracking with Selective State Space Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "S3MOT: Monocular 3D Object Tracking with Selective State Space Model"
                },
                "summary": "Accurate and reliable multi-object tracking (MOT) in 3D space is essential\nfor advancing robotics and computer vision applications. However, it remains a\nsignificant challenge in monocular setups due to the difficulty of mining 3D\nspatiotemporal associations from 2D video streams. In this work, we present\nthree innovative techniques to enhance the fusion and exploitation of\nheterogeneous cues for monocular 3D MOT: (1) we introduce the Hungarian State\nSpace Model (HSSM), a novel data association mechanism that compresses\ncontextual tracking cues across multiple paths, enabling efficient and\ncomprehensive assignment decisions with linear complexity. HSSM features a\nglobal receptive field and dynamic weights, in contrast to traditional linear\nassignment algorithms that rely on hand-crafted association costs. (2) We\npropose Fully Convolutional One-stage Embedding (FCOE), which eliminates ROI\npooling by directly using dense feature maps for contrastive learning, thus\nimproving object re-identification accuracy under challenging conditions such\nas varying viewpoints and lighting. (3) We enhance 6-DoF pose estimation\nthrough VeloSSM, an encoder-decoder architecture that models temporal\ndependencies in velocity to capture motion dynamics, overcoming the limitations\nof frame-based 3D inference. Experiments on the KITTI public test benchmark\ndemonstrate the effectiveness of our method, achieving a new state-of-the-art\nperformance of 76.86~HOTA at 31~FPS. Our approach outperforms the previous best\nby significant margins of +2.63~HOTA and +3.62~AssA, showcasing its robustness\nand efficiency for monocular 3D MOT tasks. The code and models are available at\nhttps://github.com/bytepioneerX/s3mot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and reliable multi-object tracking (MOT) in 3D space is essential\nfor advancing robotics and computer vision applications. However, it remains a\nsignificant challenge in monocular setups due to the difficulty of mining 3D\nspatiotemporal associations from 2D video streams. In this work, we present\nthree innovative techniques to enhance the fusion and exploitation of\nheterogeneous cues for monocular 3D MOT: (1) we introduce the Hungarian State\nSpace Model (HSSM), a novel data association mechanism that compresses\ncontextual tracking cues across multiple paths, enabling efficient and\ncomprehensive assignment decisions with linear complexity. HSSM features a\nglobal receptive field and dynamic weights, in contrast to traditional linear\nassignment algorithms that rely on hand-crafted association costs. (2) We\npropose Fully Convolutional One-stage Embedding (FCOE), which eliminates ROI\npooling by directly using dense feature maps for contrastive learning, thus\nimproving object re-identification accuracy under challenging conditions such\nas varying viewpoints and lighting. (3) We enhance 6-DoF pose estimation\nthrough VeloSSM, an encoder-decoder architecture that models temporal\ndependencies in velocity to capture motion dynamics, overcoming the limitations\nof frame-based 3D inference. Experiments on the KITTI public test benchmark\ndemonstrate the effectiveness of our method, achieving a new state-of-the-art\nperformance of 76.86~HOTA at 31~FPS. Our approach outperforms the previous best\nby significant margins of +2.63~HOTA and +3.62~AssA, showcasing its robustness\nand efficiency for monocular 3D MOT tasks. The code and models are available at\nhttps://github.com/bytepioneerX/s3mot."
                },
                "authors": [
                    {
                        "name": "Zhuohao Yan"
                    },
                    {
                        "name": "Shaoquan Feng"
                    },
                    {
                        "name": "Xingxing Li"
                    },
                    {
                        "name": "Yuxuan Zhou"
                    },
                    {
                        "name": "Chunxi Xia"
                    },
                    {
                        "name": "Shengyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Shengyu Li"
                },
                "author": "Shengyu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15638v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15638v2",
                "updated": "2025-04-25T04:37:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    4,
                    37,
                    45,
                    4,
                    115,
                    0
                ],
                "published": "2024-05-24T15:25:28Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    15,
                    25,
                    28,
                    4,
                    145,
                    0
                ],
                "title": "M4U: Evaluating Multilingual Understanding and Reasoning for Large\n  Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M4U: Evaluating Multilingual Understanding and Reasoning for Large\n  Multimodal Models"
                },
                "summary": "Multilingual capability is an essential aspect for large multimodal models,\nsince they are usually deployed across various countries and languages.\nHowever, most existing benchmarks for multilingual multimodal reasoning\nstruggle to differentiate between models of varying performance; even language\nmodels without visual capabilities can easily achieve high scores. This leaves\na comprehensive evaluation of leading multilingual multimodal models largely\nunexplored. In this work, we introduce M4U, a novel and challenging benchmark\nfor assessing the capability of multi-discipline multilingual multimodal\nunderstanding and reasoning. M4U contains 10k samples covering 64 disciplines\nacross 16 subfields in Science, Engineering, and Healthcare in six languages.\nUsing M4U, we conduct extensive evaluations of leading Large Multimodal Models\n(LMMs) and Large Language Models (LLMs) with external tools. The evaluation\nresults demonstrate that the state-of-the-art model, GPT-4o, achieves only\n47.6% average accuracy on M4U. Additionally, we observe that the leading LMMs\nexhibit significant language preferences. Our in-depth analysis indicates that\nleading LMMs, including GPT-4o, struggle to perform reasoning using\nmultilingual information present in both visual and textual context.\nSpecifically, they suffer performance degradation when prompted with\ncross-lingual multimodal questions. Our code and dataset is public available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual capability is an essential aspect for large multimodal models,\nsince they are usually deployed across various countries and languages.\nHowever, most existing benchmarks for multilingual multimodal reasoning\nstruggle to differentiate between models of varying performance; even language\nmodels without visual capabilities can easily achieve high scores. This leaves\na comprehensive evaluation of leading multilingual multimodal models largely\nunexplored. In this work, we introduce M4U, a novel and challenging benchmark\nfor assessing the capability of multi-discipline multilingual multimodal\nunderstanding and reasoning. M4U contains 10k samples covering 64 disciplines\nacross 16 subfields in Science, Engineering, and Healthcare in six languages.\nUsing M4U, we conduct extensive evaluations of leading Large Multimodal Models\n(LMMs) and Large Language Models (LLMs) with external tools. The evaluation\nresults demonstrate that the state-of-the-art model, GPT-4o, achieves only\n47.6% average accuracy on M4U. Additionally, we observe that the leading LMMs\nexhibit significant language preferences. Our in-depth analysis indicates that\nleading LMMs, including GPT-4o, struggle to perform reasoning using\nmultilingual information present in both visual and textual context.\nSpecifically, they suffer performance degradation when prompted with\ncross-lingual multimodal questions. Our code and dataset is public available."
                },
                "authors": [
                    {
                        "name": "Hongyu Wang"
                    },
                    {
                        "name": "Jiayu Xu"
                    },
                    {
                        "name": "Senwei Xie"
                    },
                    {
                        "name": "Ruiping Wang"
                    },
                    {
                        "name": "Jialin Li"
                    },
                    {
                        "name": "Zhaojie Xie"
                    },
                    {
                        "name": "Bin Zhang"
                    },
                    {
                        "name": "Chuyan Xiong"
                    },
                    {
                        "name": "Xilin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xilin Chen"
                },
                "author": "Xilin Chen",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15638v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15638v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18062v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18062v1",
                "updated": "2025-04-25T04:18:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    4,
                    18,
                    23,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T04:18:23Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    4,
                    18,
                    23,
                    4,
                    115,
                    0
                ],
                "title": "LLM-Guided Open RAN: Empowering Hierarchical RAN Intelligent Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Guided Open RAN: Empowering Hierarchical RAN Intelligent Control"
                },
                "summary": "Recent advancements in large language models (LLMs) have led to a significant\ninterest in deploying LLMempowered algorithms for wireless communication\nnetworks. Meanwhile, open radio access network (O-RAN) techniques offer\nunprecedented flexibility, with the non-real-time (non-RT) radio access network\n(RAN) intelligent controller (RIC) (non-RT RIC) and near-real-time (near-RT)\nRIC (near-RT RIC) components enabling intelligent resource management across\ndifferent time scales. In this paper, we propose the LLM empowered hierarchical\nRIC (LLM-hRIC) framework to improve the collaboration between RICs. This\nframework integrates LLMs with reinforcement learning (RL) for efficient\nnetwork resource management. In this framework, LLMs-empowered non-RT RICs\nprovide strategic guidance and high-level policies based on environmental\ncontext. Concurrently, RL-empowered near-RT RICs perform low-latency tasks\nbased on strategic guidance and local near-RT observation. We evaluate the\nLLM-hRIC framework in an integrated access and backhaul (IAB) network setting.\nSimulation results demonstrate that the proposed framework achieves superior\nperformance. Finally, we discuss the key future challenges in applying LLMs to\nO-RAN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have led to a significant\ninterest in deploying LLMempowered algorithms for wireless communication\nnetworks. Meanwhile, open radio access network (O-RAN) techniques offer\nunprecedented flexibility, with the non-real-time (non-RT) radio access network\n(RAN) intelligent controller (RIC) (non-RT RIC) and near-real-time (near-RT)\nRIC (near-RT RIC) components enabling intelligent resource management across\ndifferent time scales. In this paper, we propose the LLM empowered hierarchical\nRIC (LLM-hRIC) framework to improve the collaboration between RICs. This\nframework integrates LLMs with reinforcement learning (RL) for efficient\nnetwork resource management. In this framework, LLMs-empowered non-RT RICs\nprovide strategic guidance and high-level policies based on environmental\ncontext. Concurrently, RL-empowered near-RT RICs perform low-latency tasks\nbased on strategic guidance and local near-RT observation. We evaluate the\nLLM-hRIC framework in an integrated access and backhaul (IAB) network setting.\nSimulation results demonstrate that the proposed framework achieves superior\nperformance. Finally, we discuss the key future challenges in applying LLMs to\nO-RAN."
                },
                "authors": [
                    {
                        "name": "Lingyan Bao"
                    },
                    {
                        "name": "Sinwoong Yun"
                    },
                    {
                        "name": "Jemin Lee"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18062v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18062v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18053v1",
                "updated": "2025-04-25T03:54:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    3,
                    54,
                    24,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T03:54:24Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    3,
                    54,
                    24,
                    4,
                    115,
                    0
                ],
                "title": "DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal\n  Large Language Models"
                },
                "summary": "Multimodal Large Language Models (MLLMs) pose unique safety challenges due to\ntheir integration of visual and textual data, thereby introducing new\ndimensions of potential attacks and complex risk combinations. In this paper,\nwe begin with a detailed analysis aimed at disentangling risks through\nstep-by-step reasoning within multimodal inputs. We find that systematic\nmultimodal risk disentanglement substantially enhances the risk awareness of\nMLLMs. Via leveraging the strong discriminative abilities of multimodal risk\ndisentanglement, we further introduce \\textbf{DREAM}\n(\\textit{\\textbf{D}isentangling \\textbf{R}isks to \\textbf{E}nhance Safety\n\\textbf{A}lignment in \\textbf{M}LLMs}), a novel approach that enhances safety\nalignment in MLLMs through supervised fine-tuning and iterative Reinforcement\nLearning from AI Feedback (RLAIF). Experimental results show that DREAM\nsignificantly boosts safety during both inference and training phases without\ncompromising performance on normal tasks (namely oversafety), achieving a\n16.17\\% improvement in the SIUO safe\\&effective score compared to GPT-4V. The\ndata and code are available at https://github.com/Kizna1ver/DREAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) pose unique safety challenges due to\ntheir integration of visual and textual data, thereby introducing new\ndimensions of potential attacks and complex risk combinations. In this paper,\nwe begin with a detailed analysis aimed at disentangling risks through\nstep-by-step reasoning within multimodal inputs. We find that systematic\nmultimodal risk disentanglement substantially enhances the risk awareness of\nMLLMs. Via leveraging the strong discriminative abilities of multimodal risk\ndisentanglement, we further introduce \\textbf{DREAM}\n(\\textit{\\textbf{D}isentangling \\textbf{R}isks to \\textbf{E}nhance Safety\n\\textbf{A}lignment in \\textbf{M}LLMs}), a novel approach that enhances safety\nalignment in MLLMs through supervised fine-tuning and iterative Reinforcement\nLearning from AI Feedback (RLAIF). Experimental results show that DREAM\nsignificantly boosts safety during both inference and training phases without\ncompromising performance on normal tasks (namely oversafety), achieving a\n16.17\\% improvement in the SIUO safe\\&effective score compared to GPT-4V. The\ndata and code are available at https://github.com/Kizna1ver/DREAM."
                },
                "authors": [
                    {
                        "name": "Jianyu Liu"
                    },
                    {
                        "name": "Hangyu Guo"
                    },
                    {
                        "name": "Ranjie Duan"
                    },
                    {
                        "name": "Xingyuan Bu"
                    },
                    {
                        "name": "Yancheng He"
                    },
                    {
                        "name": "Shilong Li"
                    },
                    {
                        "name": "Hui Huang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Yucheng Wang"
                    },
                    {
                        "name": "Chenchen Jing"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Yingshui Tan"
                    },
                    {
                        "name": "Yanan Wu"
                    },
                    {
                        "name": "Jihao Gu"
                    },
                    {
                        "name": "Yangguang Li"
                    },
                    {
                        "name": "Jianke Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jianke Zhu"
                },
                "author": "Jianke Zhu",
                "arxiv_comment": "[NAACL 2025] The first four authors contribute equally, 23 pages,\n  repo at https://github.com/Kizna1ver/DREAM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07611v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07611v2",
                "updated": "2025-04-25T03:53:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    3,
                    53,
                    34,
                    4,
                    115,
                    0
                ],
                "published": "2024-11-12T07:34:56Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    7,
                    34,
                    56,
                    1,
                    317,
                    0
                ],
                "title": "Knowledge-Augmented Multimodal Clinical Rationale Generation for Disease\n  Diagnosis with Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Augmented Multimodal Clinical Rationale Generation for Disease\n  Diagnosis with Small Language Models"
                },
                "summary": "Interpretation is critical for disease diagnosis, but existing models\nstruggle to balance predictive accuracy with human-understandable rationales.\nWhile large language models (LLMs) offer strong reasoning abilities, their\nclinical use is limited by high computational costs and restricted multimodal\nreasoning ability. Small language models (SLMs) are efficient but lack advanced\nreasoning for integrating multimodal medical data. In addition, both LLMs and\nSLMs lack of domain knowledge for trustworthy reasoning. Therefore, we propose\nClinRaGen, enhancing SLMs by leveraging LLM-derived reasoning ability via\nrationale distillation and domain knowledge injection for trustworthy\nmultimodal rationale generation. Key innovations include a sequential rationale\ndistillation framework that equips SLMs with LLM-comparable mutlimodal\nreasoning abilities, and a knowledge-augmented attention mechanism that jointly\nunifies multimodal representation from time series and textual data in a same\nencoding space, enabling it naturally interpreted by SLMs while incorporating\ndomain knowledge for reliable rationale generation. Experiments on real-world\nmedical datasets show that ClinRaGen achieves state-of-the-art performance in\ndisease diagnosis and rationale generation, demonstrating the effectiveness of\ncombining LLM-driven reasoning with knowledge augmentation for improved\ninterpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretation is critical for disease diagnosis, but existing models\nstruggle to balance predictive accuracy with human-understandable rationales.\nWhile large language models (LLMs) offer strong reasoning abilities, their\nclinical use is limited by high computational costs and restricted multimodal\nreasoning ability. Small language models (SLMs) are efficient but lack advanced\nreasoning for integrating multimodal medical data. In addition, both LLMs and\nSLMs lack of domain knowledge for trustworthy reasoning. Therefore, we propose\nClinRaGen, enhancing SLMs by leveraging LLM-derived reasoning ability via\nrationale distillation and domain knowledge injection for trustworthy\nmultimodal rationale generation. Key innovations include a sequential rationale\ndistillation framework that equips SLMs with LLM-comparable mutlimodal\nreasoning abilities, and a knowledge-augmented attention mechanism that jointly\nunifies multimodal representation from time series and textual data in a same\nencoding space, enabling it naturally interpreted by SLMs while incorporating\ndomain knowledge for reliable rationale generation. Experiments on real-world\nmedical datasets show that ClinRaGen achieves state-of-the-art performance in\ndisease diagnosis and rationale generation, demonstrating the effectiveness of\ncombining LLM-driven reasoning with knowledge augmentation for improved\ninterpretability."
                },
                "authors": [
                    {
                        "name": "Shuai Niu"
                    },
                    {
                        "name": "Jing Ma"
                    },
                    {
                        "name": "Hongzhan Lin"
                    },
                    {
                        "name": "Liang Bai"
                    },
                    {
                        "name": "Zhihua Wang"
                    },
                    {
                        "name": "Yida Xu"
                    },
                    {
                        "name": "Yunya Song"
                    },
                    {
                        "name": "Xian Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xian Yang"
                },
                "author": "Xian Yang",
                "arxiv_comment": "13 pages. 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07611v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07611v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.18535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18535v1",
                "updated": "2025-04-25T17:59:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    17,
                    59,
                    13,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T17:59:13Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    17,
                    59,
                    13,
                    4,
                    115,
                    0
                ],
                "title": "TRACE Back from the Future: A Probabilistic Reasoning Approach to\n  Controllable Language Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRACE Back from the Future: A Probabilistic Reasoning Approach to\n  Controllable Language Generation"
                },
                "summary": "As large language models (LMs) advance, there is an increasing need to\ncontrol their outputs to align with human values (e.g., detoxification) or\ndesired attributes (e.g., personalization, topic). However, autoregressive\nmodels focus on next-token predictions and struggle with global properties that\nrequire looking ahead. Existing solutions either tune or post-train LMs for\neach new attribute - expensive and inflexible - or approximate the Expected\nAttribute Probability (EAP) of future sequences by sampling or training, which\nis slow and unreliable for rare attributes. We introduce TRACE (Tractable\nProbabilistic Reasoning for Adaptable Controllable gEneration), a novel\nframework that efficiently computes EAP and adapts to new attributes through\ntractable probabilistic reasoning and lightweight control. TRACE distills a\nHidden Markov Model (HMM) from an LM and pairs it with a small classifier to\nestimate attribute probabilities, enabling exact EAP computation over the HMM's\npredicted futures. This EAP is then used to reweigh the LM's next-token\nprobabilities for globally compliant continuations. Empirically, TRACE achieves\nstate-of-the-art results in detoxification with only 10% decoding overhead,\nadapts to 76 low-resource personalized LLMs within seconds, and seamlessly\nextends to composite attributes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LMs) advance, there is an increasing need to\ncontrol their outputs to align with human values (e.g., detoxification) or\ndesired attributes (e.g., personalization, topic). However, autoregressive\nmodels focus on next-token predictions and struggle with global properties that\nrequire looking ahead. Existing solutions either tune or post-train LMs for\neach new attribute - expensive and inflexible - or approximate the Expected\nAttribute Probability (EAP) of future sequences by sampling or training, which\nis slow and unreliable for rare attributes. We introduce TRACE (Tractable\nProbabilistic Reasoning for Adaptable Controllable gEneration), a novel\nframework that efficiently computes EAP and adapts to new attributes through\ntractable probabilistic reasoning and lightweight control. TRACE distills a\nHidden Markov Model (HMM) from an LM and pairs it with a small classifier to\nestimate attribute probabilities, enabling exact EAP computation over the HMM's\npredicted futures. This EAP is then used to reweigh the LM's next-token\nprobabilities for globally compliant continuations. Empirically, TRACE achieves\nstate-of-the-art results in detoxification with only 10% decoding overhead,\nadapts to 76 low-resource personalized LLMs within seconds, and seamlessly\nextends to composite attributes."
                },
                "authors": [
                    {
                        "name": "Gwen Yidou Weng"
                    },
                    {
                        "name": "Benjie Wang"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    }
                ],
                "author_detail": {
                    "name": "Guy Van den Broeck"
                },
                "author": "Guy Van den Broeck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18496v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18496v1",
                "updated": "2025-04-25T17:09:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    17,
                    9,
                    29,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T17:09:29Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    17,
                    9,
                    29,
                    4,
                    115,
                    0
                ],
                "title": "Facets, Taxonomies, and Syntheses: Navigating Structured Representations\n  in LLM-Assisted Literature Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Facets, Taxonomies, and Syntheses: Navigating Structured Representations\n  in LLM-Assisted Literature Review"
                },
                "summary": "Comprehensive literature review requires synthesizing vast amounts of\nresearch -- a labor intensive and cognitively demanding process. Most prior\nwork focuses either on helping researchers deeply understand a few papers\n(e.g., for triaging or reading), or retrieving from and visualizing a vast\ncorpus. Deep analysis and synthesis of large paper collections (e.g., to\nproduce a survey paper) is largely conducted manually with little support. We\npresent DimInd, an interactive system that scaffolds literature review across\nlarge paper collections through LLM-generated structured representations.\nDimInd scaffolds literature understanding with multiple levels of compression,\nfrom papers, to faceted literature comparison tables with information extracted\nfrom individual papers, to taxonomies of concepts, to narrative syntheses.\nUsers are guided through these successive information transformations while\nmaintaining provenance to source text. In an evaluation with 23 researchers,\nDimInd supported participants in extracting information and conceptually\norganizing papers with less effort compared to a ChatGPT-assisted baseline\nworkflow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehensive literature review requires synthesizing vast amounts of\nresearch -- a labor intensive and cognitively demanding process. Most prior\nwork focuses either on helping researchers deeply understand a few papers\n(e.g., for triaging or reading), or retrieving from and visualizing a vast\ncorpus. Deep analysis and synthesis of large paper collections (e.g., to\nproduce a survey paper) is largely conducted manually with little support. We\npresent DimInd, an interactive system that scaffolds literature review across\nlarge paper collections through LLM-generated structured representations.\nDimInd scaffolds literature understanding with multiple levels of compression,\nfrom papers, to faceted literature comparison tables with information extracted\nfrom individual papers, to taxonomies of concepts, to narrative syntheses.\nUsers are guided through these successive information transformations while\nmaintaining provenance to source text. In an evaluation with 23 researchers,\nDimInd supported participants in extracting information and conceptually\norganizing papers with less effort compared to a ChatGPT-assisted baseline\nworkflow."
                },
                "authors": [
                    {
                        "name": "Raymond Fok"
                    },
                    {
                        "name": "Joseph Chee Chang"
                    },
                    {
                        "name": "Marissa Radensky"
                    },
                    {
                        "name": "Pao Siangliulue"
                    },
                    {
                        "name": "Jonathan Bragg"
                    },
                    {
                        "name": "Amy X. Zhang"
                    },
                    {
                        "name": "Daniel S. Weld"
                    }
                ],
                "author_detail": {
                    "name": "Daniel S. Weld"
                },
                "author": "Daniel S. Weld",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18496v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18496v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15654v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15654v4",
                "updated": "2025-04-25T16:53:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    16,
                    53,
                    47,
                    4,
                    115,
                    0
                ],
                "published": "2025-02-21T18:22:36Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    18,
                    22,
                    36,
                    4,
                    52,
                    0
                ],
                "title": "Machine-generated text detection prevents language model collapse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine-generated text detection prevents language model collapse"
                },
                "summary": "As Large Language Models (LLMs) become increasingly prevalent, their\ngenerated outputs are proliferating across the web, risking a future where\nmachine-generated content dilutes human-authored text. Since online data is the\nprimary resource for LLM pre-training, subsequent models could be trained on an\nunknown portion of synthetic samples. This will lead to model collapse, a\ndegenerative process whereby LLMs reinforce their own errors, and ultimately\nyield a declining performance. In this study, we investigate the impact of\ndecoding strategy on model collapse, analysing the characteristics of text at\neach model generation, the similarity to human references, and the resulting\nmodel performance. Using the decoding strategies that lead to the most\nsignificant degradation, we evaluate model collapse in more realistic scenarios\nwhere the origin of the data (human or synthetic) is unknown. We train a\nmachine-generated text detector and propose an importance sampling approach to\nalleviate model collapse. Our method is validated on two LLM variants (GPT-2\nand SmolLM2) on the open-ended text generation task. We demonstrate that it can\nnot only prevent model collapse but also improve performance when sufficient\nhuman-authored samples are present. We release our code at\nhttps://github.com/GeorgeDrayson/model_collapse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become increasingly prevalent, their\ngenerated outputs are proliferating across the web, risking a future where\nmachine-generated content dilutes human-authored text. Since online data is the\nprimary resource for LLM pre-training, subsequent models could be trained on an\nunknown portion of synthetic samples. This will lead to model collapse, a\ndegenerative process whereby LLMs reinforce their own errors, and ultimately\nyield a declining performance. In this study, we investigate the impact of\ndecoding strategy on model collapse, analysing the characteristics of text at\neach model generation, the similarity to human references, and the resulting\nmodel performance. Using the decoding strategies that lead to the most\nsignificant degradation, we evaluate model collapse in more realistic scenarios\nwhere the origin of the data (human or synthetic) is unknown. We train a\nmachine-generated text detector and propose an importance sampling approach to\nalleviate model collapse. Our method is validated on two LLM variants (GPT-2\nand SmolLM2) on the open-ended text generation task. We demonstrate that it can\nnot only prevent model collapse but also improve performance when sufficient\nhuman-authored samples are present. We release our code at\nhttps://github.com/GeorgeDrayson/model_collapse."
                },
                "authors": [
                    {
                        "name": "George Drayson"
                    },
                    {
                        "name": "Emine Yilmaz"
                    },
                    {
                        "name": "Vasileios Lampos"
                    }
                ],
                "author_detail": {
                    "name": "Vasileios Lampos"
                },
                "author": "Vasileios Lampos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15654v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15654v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18483v1",
                "updated": "2025-04-25T16:47:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    16,
                    47,
                    44,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T16:47:44Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    16,
                    47,
                    44,
                    4,
                    115,
                    0
                ],
                "title": "Investigating Co-Constructive Behavior of Large Language Models in\n  Explanation Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Co-Constructive Behavior of Large Language Models in\n  Explanation Dialogues"
                },
                "summary": "The ability to generate explanations that are understood by explainees is the\nquintessence of explainable artificial intelligence. Since understanding\ndepends on the explainee's background and needs, recent research has focused on\nco-constructive explanation dialogues, where the explainer continuously\nmonitors the explainee's understanding and adapts explanations dynamically. We\ninvestigate the ability of large language models (LLMs) to engage as explainers\nin co-constructive explanation dialogues. In particular, we present a user\nstudy in which explainees interact with LLMs, of which some have been\ninstructed to explain a predefined topic co-constructively. We evaluate the\nexplainees' understanding before and after the dialogue, as well as their\nperception of the LLMs' co-constructive behavior. Our results indicate that\ncurrent LLMs show some co-constructive behaviors, such as asking verification\nquestions, that foster the explainees' engagement and can improve understanding\nof a topic. However, their ability to effectively monitor the current\nunderstanding and scaffold the explanations accordingly remains limited.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to generate explanations that are understood by explainees is the\nquintessence of explainable artificial intelligence. Since understanding\ndepends on the explainee's background and needs, recent research has focused on\nco-constructive explanation dialogues, where the explainer continuously\nmonitors the explainee's understanding and adapts explanations dynamically. We\ninvestigate the ability of large language models (LLMs) to engage as explainers\nin co-constructive explanation dialogues. In particular, we present a user\nstudy in which explainees interact with LLMs, of which some have been\ninstructed to explain a predefined topic co-constructively. We evaluate the\nexplainees' understanding before and after the dialogue, as well as their\nperception of the LLMs' co-constructive behavior. Our results indicate that\ncurrent LLMs show some co-constructive behaviors, such as asking verification\nquestions, that foster the explainees' engagement and can improve understanding\nof a topic. However, their ability to effectively monitor the current\nunderstanding and scaffold the explanations accordingly remains limited."
                },
                "authors": [
                    {
                        "name": "Leandra Fichtel"
                    },
                    {
                        "name": "Maximilian Spliethöver"
                    },
                    {
                        "name": "Eyke Hüllermeier"
                    },
                    {
                        "name": "Patricia Jimenez"
                    },
                    {
                        "name": "Nils Klowait"
                    },
                    {
                        "name": "Stefan Kopp"
                    },
                    {
                        "name": "Axel-Cyrille Ngonga Ngomo"
                    },
                    {
                        "name": "Amelie Robrecht"
                    },
                    {
                        "name": "Ingrid Scharlau"
                    },
                    {
                        "name": "Lutz Terfloth"
                    },
                    {
                        "name": "Anna-Lisa Vollmer"
                    },
                    {
                        "name": "Henning Wachsmuth"
                    }
                ],
                "author_detail": {
                    "name": "Henning Wachsmuth"
                },
                "author": "Henning Wachsmuth",
                "arxiv_comment": "Submitted to the SIGDial Conference 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06018v2",
                "updated": "2025-04-25T16:39:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    16,
                    39,
                    41,
                    4,
                    115,
                    0
                ],
                "published": "2024-11-09T00:35:29Z",
                "published_parsed": [
                    2024,
                    11,
                    9,
                    0,
                    35,
                    29,
                    5,
                    314,
                    0
                ],
                "title": "A Picture is Worth A Thousand Numbers: Enabling LLMs Reason about Time\n  Series via Visualization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Picture is Worth A Thousand Numbers: Enabling LLMs Reason about Time\n  Series via Visualization"
                },
                "summary": "Large language models (LLMs), with demonstrated reasoning abilities across\nmultiple domains, are largely underexplored for time-series reasoning (TsR),\nwhich is ubiquitous in the real world. In this work, we propose TimerBed, the\nfirst comprehensive testbed for evaluating LLMs' TsR performance. Specifically,\nTimerBed includes stratified reasoning patterns with real-world tasks,\ncomprehensive combinations of LLMs and reasoning strategies, and various\nsupervised models as comparison anchors. We perform extensive experiments with\nTimerBed, test multiple current beliefs, and verify the initial failures of\nLLMs in TsR, evidenced by the ineffectiveness of zero shot (ZST) and\nperformance degradation of few shot in-context learning (ICL). Further, we\nidentify one possible root cause: the numerical modeling of data. To address\nthis, we propose a prompt-based solution VL-Time, using visualization-modeled\ndata and language-guided reasoning. Experimental results demonstrate that\nVl-Time enables multimodal LLMs to be non-trivial ZST and powerful ICL\nreasoners for time series, achieving about 140% average performance improvement\nand 99% average token costs reduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), with demonstrated reasoning abilities across\nmultiple domains, are largely underexplored for time-series reasoning (TsR),\nwhich is ubiquitous in the real world. In this work, we propose TimerBed, the\nfirst comprehensive testbed for evaluating LLMs' TsR performance. Specifically,\nTimerBed includes stratified reasoning patterns with real-world tasks,\ncomprehensive combinations of LLMs and reasoning strategies, and various\nsupervised models as comparison anchors. We perform extensive experiments with\nTimerBed, test multiple current beliefs, and verify the initial failures of\nLLMs in TsR, evidenced by the ineffectiveness of zero shot (ZST) and\nperformance degradation of few shot in-context learning (ICL). Further, we\nidentify one possible root cause: the numerical modeling of data. To address\nthis, we propose a prompt-based solution VL-Time, using visualization-modeled\ndata and language-guided reasoning. Experimental results demonstrate that\nVl-Time enables multimodal LLMs to be non-trivial ZST and powerful ICL\nreasoners for time series, achieving about 140% average performance improvement\nand 99% average token costs reduction."
                },
                "authors": [
                    {
                        "name": "Haoxin Liu"
                    },
                    {
                        "name": "Chenghao Liu"
                    },
                    {
                        "name": "B. Aditya Prakash"
                    }
                ],
                "author_detail": {
                    "name": "B. Aditya Prakash"
                },
                "author": "B. Aditya Prakash",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.18403v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.18403v3",
                "updated": "2025-04-25T16:39:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    16,
                    39,
                    40,
                    4,
                    115,
                    0
                ],
                "published": "2024-03-27T09:45:33Z",
                "published_parsed": [
                    2024,
                    3,
                    27,
                    9,
                    45,
                    33,
                    2,
                    87,
                    0
                ],
                "title": "FoC: Figure out the Cryptographic Functions in Stripped Binaries with\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FoC: Figure out the Cryptographic Functions in Stripped Binaries with\n  LLMs"
                },
                "summary": "Analyzing the behavior of cryptographic functions in stripped binaries is a\nchallenging but essential task. Cryptographic algorithms exhibit greater\nlogical complexity compared to typical code, yet their analysis is unavoidable\nin areas such as virus analysis and legacy code inspection. Existing methods\noften rely on data or structural pattern matching, leading to suboptimal\ngeneralizability and suffering from manual work. In this paper, we propose a\nnovel framework called FoC to Figure out the Cryptographic functions in\nstripped binaries. In FoC, we first build a binary large language model\n(FoC-BinLLM) to summarize the semantics of cryptographic functions in natural\nlanguage. The prediction of FoC-BinLLM is insensitive to minor changes, such as\nvulnerability patches. To mitigate it, we further build a binary code\nsimilarity model (FoC-Sim) upon the FoC-BinLLM to create change-sensitive\nrepresentations and use it to retrieve similar implementations of unknown\ncryptographic functions in a database. In addition, we construct a\ncryptographic binary dataset for evaluation and to facilitate further research\nin this domain. And an automated method is devised to create semantic labels\nfor extensive binary functions. Evaluation results demonstrate that FoC-BinLLM\noutperforms ChatGPT by 14.61% on the ROUGE-L score. FoC-Sim outperforms the\nprevious best methods with a 52% higher Recall@1. Furthermore, our method also\nshows practical ability in virus analysis and 1-day vulnerability detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing the behavior of cryptographic functions in stripped binaries is a\nchallenging but essential task. Cryptographic algorithms exhibit greater\nlogical complexity compared to typical code, yet their analysis is unavoidable\nin areas such as virus analysis and legacy code inspection. Existing methods\noften rely on data or structural pattern matching, leading to suboptimal\ngeneralizability and suffering from manual work. In this paper, we propose a\nnovel framework called FoC to Figure out the Cryptographic functions in\nstripped binaries. In FoC, we first build a binary large language model\n(FoC-BinLLM) to summarize the semantics of cryptographic functions in natural\nlanguage. The prediction of FoC-BinLLM is insensitive to minor changes, such as\nvulnerability patches. To mitigate it, we further build a binary code\nsimilarity model (FoC-Sim) upon the FoC-BinLLM to create change-sensitive\nrepresentations and use it to retrieve similar implementations of unknown\ncryptographic functions in a database. In addition, we construct a\ncryptographic binary dataset for evaluation and to facilitate further research\nin this domain. And an automated method is devised to create semantic labels\nfor extensive binary functions. Evaluation results demonstrate that FoC-BinLLM\noutperforms ChatGPT by 14.61% on the ROUGE-L score. FoC-Sim outperforms the\nprevious best methods with a 52% higher Recall@1. Furthermore, our method also\nshows practical ability in virus analysis and 1-day vulnerability detection."
                },
                "authors": [
                    {
                        "name": "Xiuwei Shang"
                    },
                    {
                        "name": "Guoqiang Chen"
                    },
                    {
                        "name": "Shaoyin Cheng"
                    },
                    {
                        "name": "Shikai Guo"
                    },
                    {
                        "name": "Yanming Zhang"
                    },
                    {
                        "name": "Weiming Zhang"
                    },
                    {
                        "name": "Nenghai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Nenghai Yu"
                },
                "author": "Nenghai Yu",
                "arxiv_comment": "38 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.18403v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.18403v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05091v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05091v2",
                "updated": "2025-04-25T16:36:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    16,
                    36,
                    21,
                    4,
                    115,
                    0
                ],
                "published": "2025-02-07T17:10:22Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    17,
                    10,
                    22,
                    4,
                    38,
                    0
                ],
                "title": "DCFormer: Efficient 3D Vision-Language Modeling with Decomposed\n  Convolutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DCFormer: Efficient 3D Vision-Language Modeling with Decomposed\n  Convolutions"
                },
                "summary": "Vision-language models (VLMs) have been widely applied to 2D medical image\nanalysis due to their ability to align visual and textual representations.\nHowever, extending VLMs to 3D imaging remains computationally challenging.\nExisting 3D VLMs often rely on Vision Transformers (ViTs), which are\ncomputationally expensive due to the quadratic complexity of self-attention, or\non 3D convolutions, which require large numbers of parameters and FLOPs as\nkernel size increases. We introduce DCFormer, an efficient 3D image encoder\nthat factorizes 3D convolutions into three parallel 1D convolutions along the\ndepth, height, and width dimensions. This design preserves spatial information\nwhile significantly reducing computational cost. Integrated into a CLIP-based\nvision-language framework, DCFormer is trained and evaluated on CT-RATE, a\ndataset of 50,188 paired 3D chest CT volumes and radiology reports. In\nzero-shot and fine-tuned detection of 18 pathologies, as well as in image-text\nretrieval tasks, DCFormer consistently outperforms state-of-the-art 3D vision\nencoders, including CT-ViT, ViT, ConvNeXt, PoolFormer, and TransUNet. These\nresults highlight DCFormer's potential for scalable, clinically deployable 3D\nmedical VLMs. Our code is available at: https://github.com/mirthAI/DCFormer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) have been widely applied to 2D medical image\nanalysis due to their ability to align visual and textual representations.\nHowever, extending VLMs to 3D imaging remains computationally challenging.\nExisting 3D VLMs often rely on Vision Transformers (ViTs), which are\ncomputationally expensive due to the quadratic complexity of self-attention, or\non 3D convolutions, which require large numbers of parameters and FLOPs as\nkernel size increases. We introduce DCFormer, an efficient 3D image encoder\nthat factorizes 3D convolutions into three parallel 1D convolutions along the\ndepth, height, and width dimensions. This design preserves spatial information\nwhile significantly reducing computational cost. Integrated into a CLIP-based\nvision-language framework, DCFormer is trained and evaluated on CT-RATE, a\ndataset of 50,188 paired 3D chest CT volumes and radiology reports. In\nzero-shot and fine-tuned detection of 18 pathologies, as well as in image-text\nretrieval tasks, DCFormer consistently outperforms state-of-the-art 3D vision\nencoders, including CT-ViT, ViT, ConvNeXt, PoolFormer, and TransUNet. These\nresults highlight DCFormer's potential for scalable, clinically deployable 3D\nmedical VLMs. Our code is available at: https://github.com/mirthAI/DCFormer."
                },
                "authors": [
                    {
                        "name": "Gorkem Can Ates"
                    },
                    {
                        "name": "Yu Xin"
                    },
                    {
                        "name": "Kuang Gong"
                    },
                    {
                        "name": "Wei Shao"
                    }
                ],
                "author_detail": {
                    "name": "Wei Shao"
                },
                "author": "Wei Shao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05091v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05091v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18474v1",
                "updated": "2025-04-25T16:29:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    16,
                    29,
                    45,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T16:29:45Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    16,
                    29,
                    45,
                    4,
                    115,
                    0
                ],
                "title": "Generative Induction of Dialogue Task Schemas with Streaming Refinement\n  and Simulated Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Induction of Dialogue Task Schemas with Streaming Refinement\n  and Simulated Interactions"
                },
                "summary": "In task-oriented dialogue (TOD) systems, Slot Schema Induction (SSI) is\nessential for automatically identifying key information slots from dialogue\ndata without manual intervention. This paper presents a novel state-of-the-art\n(SoTA) approach that formulates SSI as a text generation task, where a language\nmodel incrementally constructs and refines a slot schema over a stream of\ndialogue data. To develop this approach, we present a fully automatic LLM-based\nTOD simulation method that creates data with high-quality state labels for\nnovel task domains. Furthermore, we identify issues in SSI evaluation due to\ndata leakage and poor metric alignment with human judgment. We resolve these by\ncreating new evaluation data using our simulation method with human guidance\nand correction, as well as designing improved evaluation metrics. These\ncontributions establish a foundation for future SSI research and advance the\nSoTA in dialogue understanding and system development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In task-oriented dialogue (TOD) systems, Slot Schema Induction (SSI) is\nessential for automatically identifying key information slots from dialogue\ndata without manual intervention. This paper presents a novel state-of-the-art\n(SoTA) approach that formulates SSI as a text generation task, where a language\nmodel incrementally constructs and refines a slot schema over a stream of\ndialogue data. To develop this approach, we present a fully automatic LLM-based\nTOD simulation method that creates data with high-quality state labels for\nnovel task domains. Furthermore, we identify issues in SSI evaluation due to\ndata leakage and poor metric alignment with human judgment. We resolve these by\ncreating new evaluation data using our simulation method with human guidance\nand correction, as well as designing improved evaluation metrics. These\ncontributions establish a foundation for future SSI research and advance the\nSoTA in dialogue understanding and system development."
                },
                "authors": [
                    {
                        "name": "James D. Finch"
                    },
                    {
                        "name": "Yasasvi Josyula"
                    },
                    {
                        "name": "Jinho D. Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jinho D. Choi"
                },
                "author": "Jinho D. Choi",
                "arxiv_comment": "Accepted (B) to TACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11704v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11704v3",
                "updated": "2025-04-25T16:08:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    16,
                    8,
                    57,
                    4,
                    115,
                    0
                ],
                "published": "2024-12-16T12:26:28Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    26,
                    28,
                    0,
                    351,
                    0
                ],
                "title": "ElChat: Adapting Chat Language Models Using Only Target Unlabeled\n  Language Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ElChat: Adapting Chat Language Models Using Only Target Unlabeled\n  Language Data"
                },
                "summary": "Vocabulary expansion (VE) is the de-facto approach to language adaptation of\nlarge language models (LLMs) by adding new tokens and continuing pre-training\non target data. While this is effective for base models trained on unlabeled\ndata, it poses challenges for chat models trained to follow instructions\nthrough labeled conversation data. Directly adapting the latter with VE on\ntarget unlabeled data may result in forgetting chat abilities. While ideal,\ntarget chat data is often unavailable or costly to create for low-resource\nlanguages, and machine-translated alternatives are not always effective. To\naddress this issue, previous work proposed using a base and chat model from the\nsame family. This method first adapts the base LLM with VE on target unlabeled\ndata and then converts it to a chat model by adding a chat vector (CV) derived\nfrom the weight difference between the source base and chat models. We propose\nElChat, a new language adaptation method for chat LLMs that adapts a chat model\ndirectly on target unlabeled data, without a base model. It elicits chat\nabilities by injecting information from the source chat model. ElChat offers\nmore robust and competitive target language and safety performance while\nachieving superior English, chat, and instruction-following abilities compared\nto CV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vocabulary expansion (VE) is the de-facto approach to language adaptation of\nlarge language models (LLMs) by adding new tokens and continuing pre-training\non target data. While this is effective for base models trained on unlabeled\ndata, it poses challenges for chat models trained to follow instructions\nthrough labeled conversation data. Directly adapting the latter with VE on\ntarget unlabeled data may result in forgetting chat abilities. While ideal,\ntarget chat data is often unavailable or costly to create for low-resource\nlanguages, and machine-translated alternatives are not always effective. To\naddress this issue, previous work proposed using a base and chat model from the\nsame family. This method first adapts the base LLM with VE on target unlabeled\ndata and then converts it to a chat model by adding a chat vector (CV) derived\nfrom the weight difference between the source base and chat models. We propose\nElChat, a new language adaptation method for chat LLMs that adapts a chat model\ndirectly on target unlabeled data, without a base model. It elicits chat\nabilities by injecting information from the source chat model. ElChat offers\nmore robust and competitive target language and safety performance while\nachieving superior English, chat, and instruction-following abilities compared\nto CV."
                },
                "authors": [
                    {
                        "name": "Atsuki Yamaguchi"
                    },
                    {
                        "name": "Terufumi Morishita"
                    },
                    {
                        "name": "Aline Villavicencio"
                    },
                    {
                        "name": "Nikolaos Aletras"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaos Aletras"
                },
                "author": "Nikolaos Aletras",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11704v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11704v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18451v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18451v1",
                "updated": "2025-04-25T16:02:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    16,
                    2,
                    50,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T16:02:50Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    16,
                    2,
                    50,
                    4,
                    115,
                    0
                ],
                "title": "Enhancing Strawberry Yield Forecasting with Backcasted IoT Sensor Data\n  and Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Strawberry Yield Forecasting with Backcasted IoT Sensor Data\n  and Machine Learning"
                },
                "summary": "Due to rapid population growth globally, digitally-enabled agricultural\nsectors are crucial for sustainable food production and making informed\ndecisions about resource management for farmers and various stakeholders. The\ndeployment of Internet of Things (IoT) technologies that collect real-time\nobservations of various environmental (e.g., temperature, humidity, etc.) and\noperational factors (e.g., irrigation) influencing production is often seen as\na critical step to enable additional novel downstream tasks, such as AI-based\nyield forecasting. However, since AI models require large amounts of data, this\ncreates practical challenges in a real-world dynamic farm setting where IoT\nobservations would need to be collected over a number of seasons. In this\nstudy, we deployed IoT sensors in strawberry production polytunnels for two\ngrowing seasons to collect environmental data, including water usage, external\nand internal temperature, external and internal humidity, soil moisture, soil\ntemperature, and photosynthetically active radiation. The sensor observations\nwere combined with manually provided yield records spanning a period of four\nseasons. To bridge the gap of missing IoT observations for two additional\nseasons, we propose an AI-based backcasting approach to generate synthetic\nsensor observations using historical weather data from a nearby weather station\nand the existing polytunnel observations. We built an AI-based yield\nforecasting model to evaluate our approach using the combination of real and\nsynthetic observations. Our results demonstrated that incorporating synthetic\ndata improved yield forecasting accuracy, with models incorporating synthetic\ndata outperforming those trained only on historical yield, weather records, and\nreal sensor data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to rapid population growth globally, digitally-enabled agricultural\nsectors are crucial for sustainable food production and making informed\ndecisions about resource management for farmers and various stakeholders. The\ndeployment of Internet of Things (IoT) technologies that collect real-time\nobservations of various environmental (e.g., temperature, humidity, etc.) and\noperational factors (e.g., irrigation) influencing production is often seen as\na critical step to enable additional novel downstream tasks, such as AI-based\nyield forecasting. However, since AI models require large amounts of data, this\ncreates practical challenges in a real-world dynamic farm setting where IoT\nobservations would need to be collected over a number of seasons. In this\nstudy, we deployed IoT sensors in strawberry production polytunnels for two\ngrowing seasons to collect environmental data, including water usage, external\nand internal temperature, external and internal humidity, soil moisture, soil\ntemperature, and photosynthetically active radiation. The sensor observations\nwere combined with manually provided yield records spanning a period of four\nseasons. To bridge the gap of missing IoT observations for two additional\nseasons, we propose an AI-based backcasting approach to generate synthetic\nsensor observations using historical weather data from a nearby weather station\nand the existing polytunnel observations. We built an AI-based yield\nforecasting model to evaluate our approach using the combination of real and\nsynthetic observations. Our results demonstrated that incorporating synthetic\ndata improved yield forecasting accuracy, with models incorporating synthetic\ndata outperforming those trained only on historical yield, weather records, and\nreal sensor data."
                },
                "authors": [
                    {
                        "name": "Tewodros Alemu Ayall"
                    },
                    {
                        "name": "Andy Li"
                    },
                    {
                        "name": "Matthew Beddows"
                    },
                    {
                        "name": "Milan Markovic"
                    },
                    {
                        "name": "Georgios Leontidis"
                    }
                ],
                "author_detail": {
                    "name": "Georgios Leontidis"
                },
                "author": "Georgios Leontidis",
                "arxiv_comment": "20 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18451v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09537v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09537v2",
                "updated": "2025-04-25T15:59:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    59,
                    19,
                    4,
                    115,
                    0
                ],
                "published": "2024-08-18T16:44:41Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    16,
                    44,
                    41,
                    6,
                    231,
                    0
                ],
                "title": "Efficient Budget Allocation for Large-Scale LLM-Enabled Virtual\n  Screening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Budget Allocation for Large-Scale LLM-Enabled Virtual\n  Screening"
                },
                "summary": "Screening tasks that aim to identify a small subset of top alternatives from\na large pool are common in business decision-making processes. These tasks\noften require substantial human effort to evaluate each alternative's\nperformance, making them time-consuming and costly. Motivated by recent\nadvances in large language models (LLMs), particularly their ability to\ngenerate outputs that align well with human evaluations, we consider an\nLLM-as-human-evaluator approach for conducting screening virtually, thereby\nreducing the cost burden. To achieve scalability and cost-effectiveness in\nvirtual screening, we identify that the stochastic nature of LLM outputs and\ntheir cost structure necessitate efficient budget allocation across all\nalternatives. To address this, we propose using a top-$m$ greedy evaluation\nmechanism, a simple yet effective approach that keeps evaluating the current\ntop-$m$ alternatives, and design the explore-first top-$m$ greedy (EFG-$m$)\nalgorithm. We prove that EFG-$m$ is both sample-optimal and consistent in\nlarge-scale virtual screening. Surprisingly, we also uncover a bonus ranking\neffect, where the algorithm naturally induces an indifference-based ranking\nwithin the selected subset. To further enhance practicality, we design a suite\nof algorithm variants to improve screening performance and computational\nefficiency. Numerical experiments validate our results and demonstrate the\neffectiveness of our algorithms. Lastly, we conduct a case study on LLM-based\nvirtual screening. The study shows that while LLMs alone may not provide\nmeaningful screening and ranking results when directly queried, integrating\nthem with our sample-optimal algorithms unlocks their potential for\ncost-effective, large-scale virtual screening.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Screening tasks that aim to identify a small subset of top alternatives from\na large pool are common in business decision-making processes. These tasks\noften require substantial human effort to evaluate each alternative's\nperformance, making them time-consuming and costly. Motivated by recent\nadvances in large language models (LLMs), particularly their ability to\ngenerate outputs that align well with human evaluations, we consider an\nLLM-as-human-evaluator approach for conducting screening virtually, thereby\nreducing the cost burden. To achieve scalability and cost-effectiveness in\nvirtual screening, we identify that the stochastic nature of LLM outputs and\ntheir cost structure necessitate efficient budget allocation across all\nalternatives. To address this, we propose using a top-$m$ greedy evaluation\nmechanism, a simple yet effective approach that keeps evaluating the current\ntop-$m$ alternatives, and design the explore-first top-$m$ greedy (EFG-$m$)\nalgorithm. We prove that EFG-$m$ is both sample-optimal and consistent in\nlarge-scale virtual screening. Surprisingly, we also uncover a bonus ranking\neffect, where the algorithm naturally induces an indifference-based ranking\nwithin the selected subset. To further enhance practicality, we design a suite\nof algorithm variants to improve screening performance and computational\nefficiency. Numerical experiments validate our results and demonstrate the\neffectiveness of our algorithms. Lastly, we conduct a case study on LLM-based\nvirtual screening. The study shows that while LLMs alone may not provide\nmeaningful screening and ranking results when directly queried, integrating\nthem with our sample-optimal algorithms unlocks their potential for\ncost-effective, large-scale virtual screening."
                },
                "authors": [
                    {
                        "name": "Zaile Li"
                    },
                    {
                        "name": "Weiwei Fan"
                    },
                    {
                        "name": "L. Jeff Hong"
                    }
                ],
                "author_detail": {
                    "name": "L. Jeff Hong"
                },
                "author": "L. Jeff Hong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09537v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09537v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04476v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04476v3",
                "updated": "2025-04-25T15:47:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    47,
                    33,
                    4,
                    115,
                    0
                ],
                "published": "2024-11-19T15:40:16Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    15,
                    40,
                    16,
                    1,
                    324,
                    0
                ],
                "title": "The Moral Mind(s) of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Moral Mind(s) of Large Language Models"
                },
                "summary": "As large language models (LLMs) increasingly participate in tasks with\nethical and societal stakes, a critical question arises: do they exhibit an\nemergent \"moral mind\" - a consistent structure of moral preferences guiding\ntheir decisions - and to what extent is this structure shared across models? To\ninvestigate this, we applied tools from revealed preference theory to nearly 40\nleading LLMs, presenting each with many structured moral dilemmas spanning five\nfoundational dimensions of ethical reasoning. Using a probabilistic rationality\ntest, we found that at least one model from each major provider exhibited\nbehavior consistent with approximately stable moral preferences, acting as if\nguided by an underlying utility function. We then estimated these utility\nfunctions and found that most models cluster around neutral moral stances. To\nfurther characterize heterogeneity, we employed a non-parametric permutation\napproach, constructing a probabilistic similarity network based on revealed\npreference patterns. The results reveal a shared core in LLMs' moral reasoning,\nbut also meaningful variation: some models show flexible reasoning across\nperspectives, while others adhere to more rigid ethical profiles. These\nfindings provide a new empirical lens for evaluating moral consistency in LLMs\nand offer a framework for benchmarking ethical alignment across AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) increasingly participate in tasks with\nethical and societal stakes, a critical question arises: do they exhibit an\nemergent \"moral mind\" - a consistent structure of moral preferences guiding\ntheir decisions - and to what extent is this structure shared across models? To\ninvestigate this, we applied tools from revealed preference theory to nearly 40\nleading LLMs, presenting each with many structured moral dilemmas spanning five\nfoundational dimensions of ethical reasoning. Using a probabilistic rationality\ntest, we found that at least one model from each major provider exhibited\nbehavior consistent with approximately stable moral preferences, acting as if\nguided by an underlying utility function. We then estimated these utility\nfunctions and found that most models cluster around neutral moral stances. To\nfurther characterize heterogeneity, we employed a non-parametric permutation\napproach, constructing a probabilistic similarity network based on revealed\npreference patterns. The results reveal a shared core in LLMs' moral reasoning,\nbut also meaningful variation: some models show flexible reasoning across\nperspectives, while others adhere to more rigid ethical profiles. These\nfindings provide a new empirical lens for evaluating moral consistency in LLMs\nand offer a framework for benchmarking ethical alignment across AI systems."
                },
                "authors": [
                    {
                        "name": "Avner Seror"
                    }
                ],
                "author_detail": {
                    "name": "Avner Seror"
                },
                "author": "Avner Seror",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04476v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04476v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18428v1",
                "updated": "2025-04-25T15:39:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    39,
                    4,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:39:04Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    39,
                    4,
                    4,
                    115,
                    0
                ],
                "title": "PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts"
                },
                "summary": "In this paper, we introduce PolyMath, a multilingual mathematical reasoning\nbenchmark covering 18 languages and 4 easy-to-hard difficulty levels. Our\nbenchmark ensures difficulty comprehensiveness, language diversity, and\nhigh-quality translation, making it a highly discriminative multilingual\nmathematical benchmark in the era of reasoning LLMs. We conduct a comprehensive\nevaluation for advanced LLMs and find that even Deepseek-R1-671B and\nQwen-QwQ-32B, achieve only 43.4 and 41.8 benchmark scores, with less than 30%\naccuracy under the highest level. From a language perspective, our benchmark\nreveals several key challenges of LLMs in multilingual reasoning: (1) Reasoning\nperformance varies widely across languages for current LLMs; (2) Input-output\nlanguage consistency is low in reasoning LLMs and may be correlated with\nperformance; (3) The thinking length differs significantly by language for\ncurrent LLMs. Additionally, we demonstrate that controlling the output language\nin the instructions has the potential to affect reasoning performance,\nespecially for some low-resource languages, suggesting a promising direction\nfor improving multilingual capabilities in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce PolyMath, a multilingual mathematical reasoning\nbenchmark covering 18 languages and 4 easy-to-hard difficulty levels. Our\nbenchmark ensures difficulty comprehensiveness, language diversity, and\nhigh-quality translation, making it a highly discriminative multilingual\nmathematical benchmark in the era of reasoning LLMs. We conduct a comprehensive\nevaluation for advanced LLMs and find that even Deepseek-R1-671B and\nQwen-QwQ-32B, achieve only 43.4 and 41.8 benchmark scores, with less than 30%\naccuracy under the highest level. From a language perspective, our benchmark\nreveals several key challenges of LLMs in multilingual reasoning: (1) Reasoning\nperformance varies widely across languages for current LLMs; (2) Input-output\nlanguage consistency is low in reasoning LLMs and may be correlated with\nperformance; (3) The thinking length differs significantly by language for\ncurrent LLMs. Additionally, we demonstrate that controlling the output language\nin the instructions has the potential to affect reasoning performance,\nespecially for some low-resource languages, suggesting a promising direction\nfor improving multilingual capabilities in LLMs."
                },
                "authors": [
                    {
                        "name": "Yiming Wang"
                    },
                    {
                        "name": "Pei Zhang"
                    },
                    {
                        "name": "Jialong Tang"
                    },
                    {
                        "name": "Haoran Wei"
                    },
                    {
                        "name": "Baosong Yang"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Chenshu Sun"
                    },
                    {
                        "name": "Feitong Sun"
                    },
                    {
                        "name": "Jiran Zhang"
                    },
                    {
                        "name": "Junxuan Wu"
                    },
                    {
                        "name": "Qiqian Cang"
                    },
                    {
                        "name": "Yichang Zhang"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18425v1",
                "updated": "2025-04-25T15:31:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    31,
                    46,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:31:46Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    31,
                    46,
                    4,
                    115,
                    0
                ],
                "title": "Kimi-Audio Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kimi-Audio Technical Report"
                },
                "summary": "We present Kimi-Audio, an open-source audio foundation model that excels in\naudio understanding, generation, and conversation. We detail the practices in\nbuilding Kimi-Audio, including model architecture, data curation, training\nrecipe, inference deployment, and evaluation. Specifically, we leverage a\n12.5Hz audio tokenizer, design a novel LLM-based architecture with continuous\nfeatures as input and discrete tokens as output, and develop a chunk-wise\nstreaming detokenizer based on flow matching. We curate a pre-training dataset\nthat consists of more than 13 million hours of audio data covering a wide range\nof modalities including speech, sound, and music, and build a pipeline to\nconstruct high-quality and diverse post-training data. Initialized from a\npre-trained LLM, Kimi-Audio is continual pre-trained on both audio and text\ndata with several carefully designed tasks, and then fine-tuned to support a\ndiverse of audio-related tasks. Extensive evaluation shows that Kimi-Audio\nachieves state-of-the-art performance on a range of audio benchmarks including\nspeech recognition, audio understanding, audio question answering, and speech\nconversation. We release the codes, model checkpoints, as well as the\nevaluation toolkits in https://github.com/MoonshotAI/Kimi-Audio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Kimi-Audio, an open-source audio foundation model that excels in\naudio understanding, generation, and conversation. We detail the practices in\nbuilding Kimi-Audio, including model architecture, data curation, training\nrecipe, inference deployment, and evaluation. Specifically, we leverage a\n12.5Hz audio tokenizer, design a novel LLM-based architecture with continuous\nfeatures as input and discrete tokens as output, and develop a chunk-wise\nstreaming detokenizer based on flow matching. We curate a pre-training dataset\nthat consists of more than 13 million hours of audio data covering a wide range\nof modalities including speech, sound, and music, and build a pipeline to\nconstruct high-quality and diverse post-training data. Initialized from a\npre-trained LLM, Kimi-Audio is continual pre-trained on both audio and text\ndata with several carefully designed tasks, and then fine-tuned to support a\ndiverse of audio-related tasks. Extensive evaluation shows that Kimi-Audio\nachieves state-of-the-art performance on a range of audio benchmarks including\nspeech recognition, audio understanding, audio question answering, and speech\nconversation. We release the codes, model checkpoints, as well as the\nevaluation toolkits in https://github.com/MoonshotAI/Kimi-Audio."
                },
                "authors": [
                    {
                        "name": "KimiTeam"
                    },
                    {
                        "name": "Ding Ding"
                    },
                    {
                        "name": "Zeqian Ju"
                    },
                    {
                        "name": "Yichong Leng"
                    },
                    {
                        "name": "Songxiang Liu"
                    },
                    {
                        "name": "Tong Liu"
                    },
                    {
                        "name": "Zeyu Shang"
                    },
                    {
                        "name": "Kai Shen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Xu Tan"
                    },
                    {
                        "name": "Heyi Tang"
                    },
                    {
                        "name": "Zhengtao Wang"
                    },
                    {
                        "name": "Chu Wei"
                    },
                    {
                        "name": "Yifei Xin"
                    },
                    {
                        "name": "Xinran Xu"
                    },
                    {
                        "name": "Jianwei Yu"
                    },
                    {
                        "name": "Yutao Zhang"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Y. Charles"
                    },
                    {
                        "name": "Jun Chen"
                    },
                    {
                        "name": "Yanru Chen"
                    },
                    {
                        "name": "Yulun Du"
                    },
                    {
                        "name": "Weiran He"
                    },
                    {
                        "name": "Zhenxing Hu"
                    },
                    {
                        "name": "Guokun Lai"
                    },
                    {
                        "name": "Qingcheng Li"
                    },
                    {
                        "name": "Yangyang Liu"
                    },
                    {
                        "name": "Weidong Sun"
                    },
                    {
                        "name": "Jianzhou Wang"
                    },
                    {
                        "name": "Yuzhi Wang"
                    },
                    {
                        "name": "Yuefeng Wu"
                    },
                    {
                        "name": "Yuxin Wu"
                    },
                    {
                        "name": "Dongchao Yang"
                    },
                    {
                        "name": "Hao Yang"
                    },
                    {
                        "name": "Ying Yang"
                    },
                    {
                        "name": "Zhilin Yang"
                    },
                    {
                        "name": "Aoxiong Yin"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Yutong Zhang"
                    },
                    {
                        "name": "Zaida Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zaida Zhou"
                },
                "author": "Zaida Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18423v1",
                "updated": "2025-04-25T15:30:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    30,
                    40,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:30:40Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    30,
                    40,
                    4,
                    115,
                    0
                ],
                "title": "LLMpatronous: Harnessing the Power of LLMs For Vulnerability Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMpatronous: Harnessing the Power of LLMs For Vulnerability Detection"
                },
                "summary": "Despite the transformative impact of Artificial Intelligence (AI) across\nvarious sectors, cyber security continues to rely on traditional static and\ndynamic analysis tools, hampered by high false positive rates and superficial\ncode comprehension. While generative AI offers promising automation\ncapabilities for software development, leveraging Large Language Models (LLMs)\nfor vulnerability detection presents unique challenges. This paper explores the\npotential and limitations of LLMs in identifying vulnerabilities, acknowledging\ninherent weaknesses such as hallucinations, limited context length, and\nknowledge cut-offs. Previous attempts employing machine learning models for\nvulnerability detection have proven ineffective due to limited real-world\napplicability, feature engineering challenges, lack of contextual\nunderstanding, and the complexities of training models to keep pace with the\nevolving threat landscape. Therefore, we propose a robust AI-driven approach\nfocused on mitigating these limitations and ensuring the quality and\nreliability of LLM based vulnerability detection. Through innovative\nmethodologies combining Retrieval-Augmented Generation (RAG) and\nMixtureof-Agents (MoA), this research seeks to leverage the strengths of LLMs\nwhile addressing their weaknesses, ultimately paving the way for dependable and\nefficient AI-powered solutions in securing the ever-evolving software\nlandscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the transformative impact of Artificial Intelligence (AI) across\nvarious sectors, cyber security continues to rely on traditional static and\ndynamic analysis tools, hampered by high false positive rates and superficial\ncode comprehension. While generative AI offers promising automation\ncapabilities for software development, leveraging Large Language Models (LLMs)\nfor vulnerability detection presents unique challenges. This paper explores the\npotential and limitations of LLMs in identifying vulnerabilities, acknowledging\ninherent weaknesses such as hallucinations, limited context length, and\nknowledge cut-offs. Previous attempts employing machine learning models for\nvulnerability detection have proven ineffective due to limited real-world\napplicability, feature engineering challenges, lack of contextual\nunderstanding, and the complexities of training models to keep pace with the\nevolving threat landscape. Therefore, we propose a robust AI-driven approach\nfocused on mitigating these limitations and ensuring the quality and\nreliability of LLM based vulnerability detection. Through innovative\nmethodologies combining Retrieval-Augmented Generation (RAG) and\nMixtureof-Agents (MoA), this research seeks to leverage the strengths of LLMs\nwhile addressing their weaknesses, ultimately paving the way for dependable and\nefficient AI-powered solutions in securing the ever-evolving software\nlandscape."
                },
                "authors": [
                    {
                        "name": "Rajesh Yarra"
                    }
                ],
                "author_detail": {
                    "name": "Rajesh Yarra"
                },
                "author": "Rajesh Yarra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16005v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16005v3",
                "updated": "2025-04-25T15:27:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    27,
                    15,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-22T16:14:31Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    16,
                    14,
                    31,
                    1,
                    112,
                    0
                ],
                "title": "CAPO: Cost-Aware Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAPO: Cost-Aware Prompt Optimization"
                },
                "summary": "Large language models (LLMs) have revolutionized natural language processing\nby solving a wide range of tasks simply guided by a prompt. Yet their\nperformance is highly sensitive to prompt formulation. While automated prompt\noptimization addresses this challenge by finding optimal prompts, current\nmethods require a substantial number of LLM calls and input tokens, making\nprompt optimization expensive. We introduce CAPO (Cost-Aware Prompt\nOptimization), an algorithm that enhances prompt optimization efficiency by\nintegrating AutoML techniques. CAPO is an evolutionary approach with LLMs as\noperators, incorporating racing to save evaluations and multi-objective\noptimization to balance performance with prompt length. It jointly optimizes\ninstructions and few-shot examples while leveraging task descriptions for\nimproved robustness. Our extensive experiments across diverse datasets and LLMs\ndemonstrate that CAPO outperforms state-of-the-art discrete prompt optimization\nmethods in 11/15 cases with improvements up to 21%p. Our algorithm achieves\nbetter performances already with smaller budgets, saves evaluations through\nracing, and decreases average prompt length via a length penalty, making it\nboth cost-efficient and cost-aware. Even without few-shot examples, CAPO\noutperforms its competitors and generally remains robust to initial prompts.\nCAPO represents an important step toward making prompt optimization more\npowerful and accessible by improving cost-efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized natural language processing\nby solving a wide range of tasks simply guided by a prompt. Yet their\nperformance is highly sensitive to prompt formulation. While automated prompt\noptimization addresses this challenge by finding optimal prompts, current\nmethods require a substantial number of LLM calls and input tokens, making\nprompt optimization expensive. We introduce CAPO (Cost-Aware Prompt\nOptimization), an algorithm that enhances prompt optimization efficiency by\nintegrating AutoML techniques. CAPO is an evolutionary approach with LLMs as\noperators, incorporating racing to save evaluations and multi-objective\noptimization to balance performance with prompt length. It jointly optimizes\ninstructions and few-shot examples while leveraging task descriptions for\nimproved robustness. Our extensive experiments across diverse datasets and LLMs\ndemonstrate that CAPO outperforms state-of-the-art discrete prompt optimization\nmethods in 11/15 cases with improvements up to 21%p. Our algorithm achieves\nbetter performances already with smaller budgets, saves evaluations through\nracing, and decreases average prompt length via a length penalty, making it\nboth cost-efficient and cost-aware. Even without few-shot examples, CAPO\noutperforms its competitors and generally remains robust to initial prompts.\nCAPO represents an important step toward making prompt optimization more\npowerful and accessible by improving cost-efficiency."
                },
                "authors": [
                    {
                        "name": "Tom Zehle"
                    },
                    {
                        "name": "Moritz Schlager"
                    },
                    {
                        "name": "Timo Heiß"
                    },
                    {
                        "name": "Matthias Feurer"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Feurer"
                },
                "author": "Matthias Feurer",
                "arxiv_comment": "Submitted to AutoML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16005v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16005v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08907v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08907v2",
                "updated": "2025-04-25T15:21:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    21,
                    54,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-11T18:19:59Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    18,
                    19,
                    59,
                    4,
                    101,
                    0
                ],
                "title": "Spatial Audio Processing with Large Language Model on Wearable Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial Audio Processing with Large Language Model on Wearable Devices"
                },
                "summary": "Integrating spatial context into large language models (LLMs) has the\npotential to revolutionize human-computer interaction, particularly in wearable\ndevices. In this work, we present a novel system architecture that incorporates\nspatial speech understanding into LLMs, enabling contextually aware and\nadaptive applications for wearable technologies. Our approach leverages\nmicrostructure-based spatial sensing to extract precise Direction of Arrival\n(DoA) information using a monaural microphone. To address the lack of existing\ndataset for microstructure-assisted speech recordings, we synthetically create\na dataset called OmniTalk by using the LibriSpeech dataset. This spatial\ninformation is fused with linguistic embeddings from OpenAI's Whisper model,\nallowing each modality to learn complementary contextual representations. The\nfused embeddings are aligned with the input space of LLaMA-3.2 3B model and\nfine-tuned with lightweight adaptation technique LoRA to optimize for on-device\nprocessing. SING supports spatially-aware automatic speech recognition (ASR),\nachieving a mean error of $25.72^\\circ$-a substantial improvement compared to\nthe 88.52$^\\circ$ median error in existing work-with a word error rate (WER) of\n5.3. SING also supports soundscaping, for example, inference how many people\nwere talking and their directions, with up to 5 people and a median DoA error\nof 16$^\\circ$. Our system demonstrates superior performance in spatial speech\nunderstanding while addressing the challenges of power efficiency, privacy, and\nhardware constraints, paving the way for advanced applications in augmented\nreality, accessibility, and immersive experiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating spatial context into large language models (LLMs) has the\npotential to revolutionize human-computer interaction, particularly in wearable\ndevices. In this work, we present a novel system architecture that incorporates\nspatial speech understanding into LLMs, enabling contextually aware and\nadaptive applications for wearable technologies. Our approach leverages\nmicrostructure-based spatial sensing to extract precise Direction of Arrival\n(DoA) information using a monaural microphone. To address the lack of existing\ndataset for microstructure-assisted speech recordings, we synthetically create\na dataset called OmniTalk by using the LibriSpeech dataset. This spatial\ninformation is fused with linguistic embeddings from OpenAI's Whisper model,\nallowing each modality to learn complementary contextual representations. The\nfused embeddings are aligned with the input space of LLaMA-3.2 3B model and\nfine-tuned with lightweight adaptation technique LoRA to optimize for on-device\nprocessing. SING supports spatially-aware automatic speech recognition (ASR),\nachieving a mean error of $25.72^\\circ$-a substantial improvement compared to\nthe 88.52$^\\circ$ median error in existing work-with a word error rate (WER) of\n5.3. SING also supports soundscaping, for example, inference how many people\nwere talking and their directions, with up to 5 people and a median DoA error\nof 16$^\\circ$. Our system demonstrates superior performance in spatial speech\nunderstanding while addressing the challenges of power efficiency, privacy, and\nhardware constraints, paving the way for advanced applications in augmented\nreality, accessibility, and immersive experiences."
                },
                "authors": [
                    {
                        "name": "Ayushi Mishra"
                    },
                    {
                        "name": "Yang Bai"
                    },
                    {
                        "name": "Priyadarshan Narayanasamy"
                    },
                    {
                        "name": "Nakul Garg"
                    },
                    {
                        "name": "Nirupam Roy"
                    }
                ],
                "author_detail": {
                    "name": "Nirupam Roy"
                },
                "author": "Nirupam Roy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08907v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08907v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18415v1",
                "updated": "2025-04-25T15:17:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    17,
                    52,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:17:52Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    17,
                    52,
                    4,
                    115,
                    0
                ],
                "title": "BitNet v2: Native 4-bit Activations with Hadamard Transformation for\n  1-bit LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitNet v2: Native 4-bit Activations with Hadamard Transformation for\n  1-bit LLMs"
                },
                "summary": "Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by\nactivation outliers, which complicate quantization to low bit-widths. We\nintroduce BitNet v2, a novel framework enabling native 4-bit activation\nquantization for 1-bit LLMs. To tackle outliers in attention and feed-forward\nnetwork activations, we propose H-BitLinear, a module applying an online\nHadamard transformation prior to activation quantization. This transformation\nsmooths sharp activation distributions into more Gaussian-like forms, suitable\nfor low-bit representation. Experiments show BitNet v2 trained from scratch\nwith 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2\nachieves minimal performance degradation when trained with native 4-bit\nactivations, significantly reducing memory footprint and computational cost for\nbatched inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by\nactivation outliers, which complicate quantization to low bit-widths. We\nintroduce BitNet v2, a novel framework enabling native 4-bit activation\nquantization for 1-bit LLMs. To tackle outliers in attention and feed-forward\nnetwork activations, we propose H-BitLinear, a module applying an online\nHadamard transformation prior to activation quantization. This transformation\nsmooths sharp activation distributions into more Gaussian-like forms, suitable\nfor low-bit representation. Experiments show BitNet v2 trained from scratch\nwith 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2\nachieves minimal performance degradation when trained with native 4-bit\nactivations, significantly reducing memory footprint and computational cost for\nbatched inference."
                },
                "authors": [
                    {
                        "name": "Hongyu Wang"
                    },
                    {
                        "name": "Shuming Ma"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18412v1",
                "updated": "2025-04-25T15:14:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    14,
                    21,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:14:21Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    14,
                    21,
                    4,
                    115,
                    0
                ],
                "title": "Expressing stigma and inappropriate responses prevents LLMs from safely\n  replacing mental health providers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expressing stigma and inappropriate responses prevents LLMs from safely\n  replacing mental health providers"
                },
                "summary": "Should a large language model (LLM) be used as a therapist? In this paper, we\ninvestigate the use of LLMs to *replace* mental health providers, a use case\npromoted in the tech startup and research space. We conduct a mapping review of\ntherapy guides used by major medical institutions to identify crucial aspects\nof therapeutic relationships, such as the importance of a therapeutic alliance\nbetween therapist and client. We then assess the ability of LLMs to reproduce\nand adhere to these aspects of therapeutic relationships by conducting several\nexperiments investigating the responses of current LLMs, such as `gpt-4o`.\nContrary to best practices in the medical community, LLMs 1) express stigma\ntoward those with mental health conditions and 2) respond inappropriately to\ncertain common (and critical) conditions in naturalistic therapy settings --\ne.g., LLMs encourage clients' delusional thinking, likely due to their\nsycophancy. This occurs even with larger and newer LLMs, indicating that\ncurrent safety practices may not address these gaps. Furthermore, we note\nfoundational and practical barriers to the adoption of LLMs as therapists, such\nas that a therapeutic alliance requires human characteristics (e.g., identity\nand stakes). For these reasons, we conclude that LLMs should not replace\ntherapists, and we discuss alternative roles for LLMs in clinical therapy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Should a large language model (LLM) be used as a therapist? In this paper, we\ninvestigate the use of LLMs to *replace* mental health providers, a use case\npromoted in the tech startup and research space. We conduct a mapping review of\ntherapy guides used by major medical institutions to identify crucial aspects\nof therapeutic relationships, such as the importance of a therapeutic alliance\nbetween therapist and client. We then assess the ability of LLMs to reproduce\nand adhere to these aspects of therapeutic relationships by conducting several\nexperiments investigating the responses of current LLMs, such as `gpt-4o`.\nContrary to best practices in the medical community, LLMs 1) express stigma\ntoward those with mental health conditions and 2) respond inappropriately to\ncertain common (and critical) conditions in naturalistic therapy settings --\ne.g., LLMs encourage clients' delusional thinking, likely due to their\nsycophancy. This occurs even with larger and newer LLMs, indicating that\ncurrent safety practices may not address these gaps. Furthermore, we note\nfoundational and practical barriers to the adoption of LLMs as therapists, such\nas that a therapeutic alliance requires human characteristics (e.g., identity\nand stakes). For these reasons, we conclude that LLMs should not replace\ntherapists, and we discuss alternative roles for LLMs in clinical therapy."
                },
                "authors": [
                    {
                        "name": "Jared Moore"
                    },
                    {
                        "name": "Declan Grabb"
                    },
                    {
                        "name": "William Agnew"
                    },
                    {
                        "name": "Kevin Klyman"
                    },
                    {
                        "name": "Stevie Chancellor"
                    },
                    {
                        "name": "Desmond C. Ong"
                    },
                    {
                        "name": "Nick Haber"
                    }
                ],
                "author_detail": {
                    "name": "Nick Haber"
                },
                "author": "Nick Haber",
                "arxiv_doi": "10.1145/3715275.3732039",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3715275.3732039",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.18412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18410v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18410v1",
                "updated": "2025-04-25T15:10:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    10,
                    51,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:10:51Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    10,
                    51,
                    4,
                    115,
                    0
                ],
                "title": "Can Code Outlove Blood? A LLM-based VR Experience to Prompt Reflection\n  on Parental Verbal Abuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Code Outlove Blood? A LLM-based VR Experience to Prompt Reflection\n  on Parental Verbal Abuse"
                },
                "summary": "Parental verbal abuse leaves lasting emotional impacts, yet current\ntherapeutic approaches often lack immersive self-reflection opportunities. To\naddress this, we developed a VR experience powered by LLMs to foster reflection\non parental verbal abuse. Participants with relevant experiences engage in a\ndual-phase VR experience: first assuming the role of a verbally abusive parent,\ninteracting with an LLM portraying a child, then observing the LLM reframing\nabusive dialogue into warm, supportive expressions as a nurturing parent. A\nqualitative study with 12 participants showed that the experience encourages\nreflection on their past experiences and fosters supportive emotions. However,\nthese effects vary with participants' personal histories, emphasizing the need\nfor greater personalization in AI-driven emotional support. This study explores\nthe use of LLMs in immersive environment to promote emotional reflection,\noffering insights into the design of AI-driven emotional support systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parental verbal abuse leaves lasting emotional impacts, yet current\ntherapeutic approaches often lack immersive self-reflection opportunities. To\naddress this, we developed a VR experience powered by LLMs to foster reflection\non parental verbal abuse. Participants with relevant experiences engage in a\ndual-phase VR experience: first assuming the role of a verbally abusive parent,\ninteracting with an LLM portraying a child, then observing the LLM reframing\nabusive dialogue into warm, supportive expressions as a nurturing parent. A\nqualitative study with 12 participants showed that the experience encourages\nreflection on their past experiences and fosters supportive emotions. However,\nthese effects vary with participants' personal histories, emphasizing the need\nfor greater personalization in AI-driven emotional support. This study explores\nthe use of LLMs in immersive environment to promote emotional reflection,\noffering insights into the design of AI-driven emotional support systems."
                },
                "authors": [
                    {
                        "name": "Jiaying Fu"
                    },
                    {
                        "name": "Jialin Gu"
                    },
                    {
                        "name": "Tianyue Gong"
                    },
                    {
                        "name": "Tiange Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Tiange Zhou"
                },
                "author": "Tiange Zhou",
                "arxiv_comment": "8 pages, 5 figures, accetped by 30th International Symposium on\n  Electronic Art (ISEA 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18410v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17565v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17565v2",
                "updated": "2025-04-25T15:10:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    10,
                    20,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-24T13:57:53Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    57,
                    53,
                    3,
                    114,
                    0
                ],
                "title": "DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale\n  Difficulty-Graded Data Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale\n  Difficulty-Graded Data Training"
                },
                "summary": "Although large language models (LLMs) have recently achieved remarkable\nperformance on various complex reasoning benchmarks, the academic community\nstill lacks an in-depth understanding of base model training processes and data\nquality. To address this, we construct a large-scale, difficulty-graded\nreasoning dataset containing approximately 3.34 million unique queries of\nvarying difficulty levels and about 40 million distilled responses generated by\nmultiple models over several passes. Leveraging pass rate and Coefficient of\nVariation (CV), we precisely select the most valuable training data to enhance\nreasoning capability. Notably, we observe a training pattern shift, indicating\nthat reasoning-focused training based on base models requires higher learning\nrates for effective training. Using this carefully selected data, we\nsignificantly improve the reasoning capabilities of the base model, achieving a\npass rate of 79.2\\% on the AIME2024 mathematical reasoning benchmark. This\nresult surpasses most current distilled models and closely approaches\nstate-of-the-art performance. We provide detailed descriptions of our data\nprocessing, difficulty assessment, and training methodology, and have publicly\nreleased all datasets and methods to promote rapid progress in open-source\nlong-reasoning LLMs. The dataset is available at:\nhttps://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) have recently achieved remarkable\nperformance on various complex reasoning benchmarks, the academic community\nstill lacks an in-depth understanding of base model training processes and data\nquality. To address this, we construct a large-scale, difficulty-graded\nreasoning dataset containing approximately 3.34 million unique queries of\nvarying difficulty levels and about 40 million distilled responses generated by\nmultiple models over several passes. Leveraging pass rate and Coefficient of\nVariation (CV), we precisely select the most valuable training data to enhance\nreasoning capability. Notably, we observe a training pattern shift, indicating\nthat reasoning-focused training based on base models requires higher learning\nrates for effective training. Using this carefully selected data, we\nsignificantly improve the reasoning capabilities of the base model, achieving a\npass rate of 79.2\\% on the AIME2024 mathematical reasoning benchmark. This\nresult surpasses most current distilled models and closely approaches\nstate-of-the-art performance. We provide detailed descriptions of our data\nprocessing, difficulty assessment, and training methodology, and have publicly\nreleased all datasets and methods to promote rapid progress in open-source\nlong-reasoning LLMs. The dataset is available at:\nhttps://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M"
                },
                "authors": [
                    {
                        "name": "Xiaoyu Tian"
                    },
                    {
                        "name": "Sitong Zhao"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Shuaiting Chen"
                    },
                    {
                        "name": "Yiping Peng"
                    },
                    {
                        "name": "Yunjie Ji"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Xiangang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangang Li"
                },
                "author": "Xiangang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17565v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17565v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14587v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14587v2",
                "updated": "2025-04-25T15:04:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    4,
                    43,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-20T12:28:49Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    12,
                    28,
                    49,
                    6,
                    110,
                    0
                ],
                "title": "Generative Auto-Bidding with Value-Guided Explorations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Auto-Bidding with Value-Guided Explorations"
                },
                "summary": "Auto-bidding, with its strong capability to optimize bidding decisions within\ndynamic and competitive online environments, has become a pivotal strategy for\nadvertising platforms. Existing approaches typically employ rule-based\nstrategies or Reinforcement Learning (RL) techniques. However, rule-based\nstrategies lack the flexibility to adapt to time-varying market conditions, and\nRL-based methods struggle to capture essential historical dependencies and\nobservations within Markov Decision Process (MDP) frameworks. Furthermore,\nthese approaches often face challenges in ensuring strategy adaptability across\ndiverse advertising objectives. Additionally, as offline training methods are\nincreasingly adopted to facilitate the deployment and maintenance of stable\nonline strategies, the issues of documented behavioral patterns and behavioral\ncollapse resulting from training on fixed offline datasets become increasingly\nsignificant. To address these limitations, this paper introduces a novel\noffline Generative Auto-bidding framework with Value-Guided Explorations\n(GAVE). GAVE accommodates various advertising objectives through a score-based\nReturn-To-Go (RTG) module. Moreover, GAVE integrates an action exploration\nmechanism with an RTG-based evaluation method to explore novel actions while\nensuring stability-preserving updates. A learnable value function is also\ndesigned to guide the direction of action exploration and mitigate\nOut-of-Distribution (OOD) problems. Experimental results on two offline\ndatasets and real-world deployments demonstrate that GAVE outperforms\nstate-of-the-art baselines in both offline evaluations and online A/B tests. By\napplying the core methods of this framework, we proudly secured first place in\nthe NeurIPS 2024 competition, 'AIGB Track: Learning Auto-Bidding Agents with\nGenerative Models'.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-bidding, with its strong capability to optimize bidding decisions within\ndynamic and competitive online environments, has become a pivotal strategy for\nadvertising platforms. Existing approaches typically employ rule-based\nstrategies or Reinforcement Learning (RL) techniques. However, rule-based\nstrategies lack the flexibility to adapt to time-varying market conditions, and\nRL-based methods struggle to capture essential historical dependencies and\nobservations within Markov Decision Process (MDP) frameworks. Furthermore,\nthese approaches often face challenges in ensuring strategy adaptability across\ndiverse advertising objectives. Additionally, as offline training methods are\nincreasingly adopted to facilitate the deployment and maintenance of stable\nonline strategies, the issues of documented behavioral patterns and behavioral\ncollapse resulting from training on fixed offline datasets become increasingly\nsignificant. To address these limitations, this paper introduces a novel\noffline Generative Auto-bidding framework with Value-Guided Explorations\n(GAVE). GAVE accommodates various advertising objectives through a score-based\nReturn-To-Go (RTG) module. Moreover, GAVE integrates an action exploration\nmechanism with an RTG-based evaluation method to explore novel actions while\nensuring stability-preserving updates. A learnable value function is also\ndesigned to guide the direction of action exploration and mitigate\nOut-of-Distribution (OOD) problems. Experimental results on two offline\ndatasets and real-world deployments demonstrate that GAVE outperforms\nstate-of-the-art baselines in both offline evaluations and online A/B tests. By\napplying the core methods of this framework, we proudly secured first place in\nthe NeurIPS 2024 competition, 'AIGB Track: Learning Auto-Bidding Agents with\nGenerative Models'."
                },
                "authors": [
                    {
                        "name": "Jingtong Gao"
                    },
                    {
                        "name": "Yewen Li"
                    },
                    {
                        "name": "Shuai Mao"
                    },
                    {
                        "name": "Peng Jiang"
                    },
                    {
                        "name": "Nan Jiang"
                    },
                    {
                        "name": "Yejing Wang"
                    },
                    {
                        "name": "Qingpeng Cai"
                    },
                    {
                        "name": "Fei Pan"
                    },
                    {
                        "name": "Peng Jiang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Bo An"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhao"
                },
                "author": "Xiangyu Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14587v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14587v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20135v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20135v2",
                "updated": "2025-04-25T14:52:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    14,
                    52,
                    38,
                    4,
                    115,
                    0
                ],
                "published": "2024-07-29T16:02:23Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    16,
                    2,
                    23,
                    0,
                    211,
                    0
                ],
                "title": "Trade-offs in Reliability and Performance Using Selective Beamforming\n  for Ultra-Massive MIMO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trade-offs in Reliability and Performance Using Selective Beamforming\n  for Ultra-Massive MIMO"
                },
                "summary": "This paper addresses the optimization challenges in Ultra-Massive MIMO\ncommunication systems, focusing on array selection and beamforming in dynamic\nand diverse operational contexts. We introduce a novel array selection\ncriterion that incorporates antenna health information into the optimization\nprocess, distinguishing our approach from traditional methods. Our methodology\nemploys dual proximal-gradient ascent to effectively tackle the constrained\nnon-convex and non-smooth nature of sparse array selection problems. A central\nfeature of our strategy is the implementation of proportional fairness among\ncommunication users, aligning with system resource limitations while ensuring\nminimum rate requirements for all users. This approach not only enhances system\nefficiency and responsiveness but also ensures equitable resource distribution.\nExtensive simulations validate the effectiveness of the proposed solutions in\noptimizing Ultra-Massive MIMO system performance, demonstrating their\napplicability in complex communication scenarios. Our findings reveal key\ntrade-offs influenced by the sparsity promotion weight (\\(\\gamma\\)). As\n\\(\\gamma\\) increases, spectral efficiency (SE) and communication rate (Ri)\ndecrease, while beamforming matrix density (BMD) reduces and antenna\nreliability (RL) significantly improves. These results highlight the critical\nbalance between performance and reliability, essential for the practical\ndeployment of Ultra-Massive MIMO systems. This work advances the field by\nproviding innovative solutions and new insights into array selection and\nbeamforming optimization, setting a foundation for future research in\nUltra-Massive MIMO communication systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the optimization challenges in Ultra-Massive MIMO\ncommunication systems, focusing on array selection and beamforming in dynamic\nand diverse operational contexts. We introduce a novel array selection\ncriterion that incorporates antenna health information into the optimization\nprocess, distinguishing our approach from traditional methods. Our methodology\nemploys dual proximal-gradient ascent to effectively tackle the constrained\nnon-convex and non-smooth nature of sparse array selection problems. A central\nfeature of our strategy is the implementation of proportional fairness among\ncommunication users, aligning with system resource limitations while ensuring\nminimum rate requirements for all users. This approach not only enhances system\nefficiency and responsiveness but also ensures equitable resource distribution.\nExtensive simulations validate the effectiveness of the proposed solutions in\noptimizing Ultra-Massive MIMO system performance, demonstrating their\napplicability in complex communication scenarios. Our findings reveal key\ntrade-offs influenced by the sparsity promotion weight (\\(\\gamma\\)). As\n\\(\\gamma\\) increases, spectral efficiency (SE) and communication rate (Ri)\ndecrease, while beamforming matrix density (BMD) reduces and antenna\nreliability (RL) significantly improves. These results highlight the critical\nbalance between performance and reliability, essential for the practical\ndeployment of Ultra-Massive MIMO systems. This work advances the field by\nproviding innovative solutions and new insights into array selection and\nbeamforming optimization, setting a foundation for future research in\nUltra-Massive MIMO communication systems."
                },
                "authors": [
                    {
                        "name": "Anis Hamadouche"
                    },
                    {
                        "name": "Mathini Sellathurai"
                    }
                ],
                "author_detail": {
                    "name": "Mathini Sellathurai"
                },
                "author": "Mathini Sellathurai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20135v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20135v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10060v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10060v3",
                "updated": "2025-04-25T14:44:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    14,
                    44,
                    51,
                    4,
                    115,
                    0
                ],
                "published": "2024-06-14T14:16:39Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    14,
                    16,
                    39,
                    4,
                    166,
                    0
                ],
                "title": "PRIMER: Perception-Aware Robust Learning-based Multiagent Trajectory\n  Planner",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRIMER: Perception-Aware Robust Learning-based Multiagent Trajectory\n  Planner"
                },
                "summary": "In decentralized multiagent trajectory planners, agents need to communicate\nand exchange their positions to generate collision-free trajectories. However,\ndue to localization errors/uncertainties, trajectory deconfliction can fail\neven if trajectories are perfectly shared between agents. To address this\nissue, we first present PARM and PARM*, perception-aware, decentralized,\nasynchronous multiagent trajectory planners that enable a team of agents to\nnavigate uncertain environments while deconflicting trajectories and avoiding\nobstacles using perception information. PARM* differs from PARM as it is less\nconservative, using more computation to find closer-to-optimal solutions. While\nthese methods achieve state-of-the-art performance, they suffer from high\ncomputational costs as they need to solve large optimization problems onboard,\nmaking it difficult for agents to replan at high rates. To overcome this\nchallenge, we present our second key contribution, PRIMER, a learning-based\nplanner trained with imitation learning (IL) using PARM* as the expert\ndemonstrator. PRIMER leverages the low computational requirements at deployment\nof neural networks and achieves a computation speed up to 5500 times faster\nthan optimization-based approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In decentralized multiagent trajectory planners, agents need to communicate\nand exchange their positions to generate collision-free trajectories. However,\ndue to localization errors/uncertainties, trajectory deconfliction can fail\neven if trajectories are perfectly shared between agents. To address this\nissue, we first present PARM and PARM*, perception-aware, decentralized,\nasynchronous multiagent trajectory planners that enable a team of agents to\nnavigate uncertain environments while deconflicting trajectories and avoiding\nobstacles using perception information. PARM* differs from PARM as it is less\nconservative, using more computation to find closer-to-optimal solutions. While\nthese methods achieve state-of-the-art performance, they suffer from high\ncomputational costs as they need to solve large optimization problems onboard,\nmaking it difficult for agents to replan at high rates. To overcome this\nchallenge, we present our second key contribution, PRIMER, a learning-based\nplanner trained with imitation learning (IL) using PARM* as the expert\ndemonstrator. PRIMER leverages the low computational requirements at deployment\nof neural networks and achieves a computation speed up to 5500 times faster\nthan optimization-based approaches."
                },
                "authors": [
                    {
                        "name": "Kota Kondo"
                    },
                    {
                        "name": "Claudius T. Tewari"
                    },
                    {
                        "name": "Andrea Tagliabue"
                    },
                    {
                        "name": "Jesus Tordesillas"
                    },
                    {
                        "name": "Parker C. Lusk"
                    },
                    {
                        "name": "Mason B. Peterson"
                    },
                    {
                        "name": "Jonathan P. How"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan P. How"
                },
                "author": "Jonathan P. How",
                "arxiv_doi": "10.13140/RG.2.2.14435.57124",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.13140/RG.2.2.14435.57124",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.10060v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10060v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "7 pages, 3 figures",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18383v1",
                "updated": "2025-04-25T14:30:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    14,
                    30,
                    25,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T14:30:25Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    14,
                    30,
                    25,
                    4,
                    115,
                    0
                ],
                "title": "Bridge the Domains: Large Language Models Enhanced Cross-domain\n  Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridge the Domains: Large Language Models Enhanced Cross-domain\n  Sequential Recommendation"
                },
                "summary": "Cross-domain Sequential Recommendation (CDSR) aims to extract the preference\nfrom the user's historical interactions across various domains. Despite some\nprogress in CDSR, two problems set the barrier for further advancements, i.e.,\noverlap dilemma and transition complexity. The former means existing CDSR\nmethods severely rely on users who own interactions on all domains to learn\ncross-domain item relationships, compromising the practicability. The latter\nrefers to the difficulties in learning the complex transition patterns from the\nmixed behavior sequences. With powerful representation and reasoning abilities,\nLarge Language Models (LLMs) are promising to address these two problems by\nbridging the items and capturing the user's preferences from a semantic view.\nTherefore, we propose an LLMs Enhanced Cross-domain Sequential Recommendation\nmodel (LLM4CDSR). To obtain the semantic item relationships, we first propose\nan LLM-based unified representation module to represent items. Then, a\ntrainable adapter with contrastive regularization is designed to adapt the CDSR\ntask. Besides, a hierarchical LLMs profiling module is designed to summarize\nuser cross-domain preferences. Finally, these two modules are integrated into\nthe proposed tri-thread framework to derive recommendations. We have conducted\nextensive experiments on three public cross-domain datasets, validating the\neffectiveness of LLM4CDSR. We have released the code online.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-domain Sequential Recommendation (CDSR) aims to extract the preference\nfrom the user's historical interactions across various domains. Despite some\nprogress in CDSR, two problems set the barrier for further advancements, i.e.,\noverlap dilemma and transition complexity. The former means existing CDSR\nmethods severely rely on users who own interactions on all domains to learn\ncross-domain item relationships, compromising the practicability. The latter\nrefers to the difficulties in learning the complex transition patterns from the\nmixed behavior sequences. With powerful representation and reasoning abilities,\nLarge Language Models (LLMs) are promising to address these two problems by\nbridging the items and capturing the user's preferences from a semantic view.\nTherefore, we propose an LLMs Enhanced Cross-domain Sequential Recommendation\nmodel (LLM4CDSR). To obtain the semantic item relationships, we first propose\nan LLM-based unified representation module to represent items. Then, a\ntrainable adapter with contrastive regularization is designed to adapt the CDSR\ntask. Besides, a hierarchical LLMs profiling module is designed to summarize\nuser cross-domain preferences. Finally, these two modules are integrated into\nthe proposed tri-thread framework to derive recommendations. We have conducted\nextensive experiments on three public cross-domain datasets, validating the\neffectiveness of LLM4CDSR. We have released the code online."
                },
                "authors": [
                    {
                        "name": "Qidong Liu"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Yejing Wang"
                    },
                    {
                        "name": "Zijian Zhang"
                    },
                    {
                        "name": "Howard Zhong"
                    },
                    {
                        "name": "Chong Chen"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Feng Tian"
                    }
                ],
                "author_detail": {
                    "name": "Feng Tian"
                },
                "author": "Feng Tian",
                "arxiv_comment": "accepted by SIGIR'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18373v1",
                "updated": "2025-04-25T14:17:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    14,
                    17,
                    47,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T14:17:47Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    14,
                    17,
                    47,
                    4,
                    115,
                    0
                ],
                "title": "Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in\n  Smart Personal Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in\n  Smart Personal Assistant"
                },
                "summary": "In recent years, multi-agent frameworks powered by large language models\n(LLMs) have advanced rapidly. Despite this progress, there is still a notable\nabsence of benchmark datasets specifically tailored to evaluate their\nperformance. To bridge this gap, we introduce Auto-SLURP, a benchmark dataset\naimed at evaluating LLM-based multi-agent frameworks in the context of\nintelligent personal assistants. Auto-SLURP extends the original SLURP dataset\n-- initially developed for natural language understanding tasks -- by\nrelabeling the data and integrating simulated servers and external services.\nThis enhancement enables a comprehensive end-to-end evaluation pipeline,\ncovering language understanding, task execution, and response generation. Our\nexperiments demonstrate that Auto-SLURP presents a significant challenge for\ncurrent state-of-the-art frameworks, highlighting that truly reliable and\nintelligent multi-agent personal assistants remain a work in progress. The\ndataset and related code are available at\nhttps://github.com/lorashen/Auto-SLURP/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, multi-agent frameworks powered by large language models\n(LLMs) have advanced rapidly. Despite this progress, there is still a notable\nabsence of benchmark datasets specifically tailored to evaluate their\nperformance. To bridge this gap, we introduce Auto-SLURP, a benchmark dataset\naimed at evaluating LLM-based multi-agent frameworks in the context of\nintelligent personal assistants. Auto-SLURP extends the original SLURP dataset\n-- initially developed for natural language understanding tasks -- by\nrelabeling the data and integrating simulated servers and external services.\nThis enhancement enables a comprehensive end-to-end evaluation pipeline,\ncovering language understanding, task execution, and response generation. Our\nexperiments demonstrate that Auto-SLURP presents a significant challenge for\ncurrent state-of-the-art frameworks, highlighting that truly reliable and\nintelligent multi-agent personal assistants remain a work in progress. The\ndataset and related code are available at\nhttps://github.com/lorashen/Auto-SLURP/."
                },
                "authors": [
                    {
                        "name": "Lei Shen"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18369v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18369v1",
                "updated": "2025-04-25T14:11:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    14,
                    11,
                    42,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T14:11:42Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    14,
                    11,
                    42,
                    4,
                    115,
                    0
                ],
                "title": "ThreMoLIA: Threat Modeling of Large Language Model-Integrated\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThreMoLIA: Threat Modeling of Large Language Model-Integrated\n  Applications"
                },
                "summary": "Large Language Models (LLMs) are currently being integrated into industrial\nsoftware applications to help users perform more complex tasks in less time.\nHowever, these LLM-Integrated Applications (LIA) expand the attack surface and\nintroduce new kinds of threats. Threat modeling is commonly used to identify\nthese threats and suggest mitigations. However, it is a time-consuming practice\nthat requires the involvement of a security practitioner. Our goals are to 1)\nprovide a method for performing threat modeling for LIAs early in their\nlifecycle, (2) develop a threat modeling tool that integrates existing threat\nmodels, and (3) ensure high-quality threat modeling. To achieve the goals, we\nwork in collaboration with our industry partner. Our proposed way of performing\nthreat modeling will benefit industry by requiring fewer security experts'\nparticipation and reducing the time spent on this activity. Our proposed tool\ncombines LLMs and Retrieval Augmented Generation (RAG) and uses sources such as\nexisting threat models and application architecture repositories to\ncontinuously create and update threat models. We propose to evaluate the tool\noffline -- i.e., using benchmarking -- and online with practitioners in the\nfield. We conducted an early evaluation using ChatGPT on a simple LIA and\nobtained results that encouraged us to proceed with our research efforts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are currently being integrated into industrial\nsoftware applications to help users perform more complex tasks in less time.\nHowever, these LLM-Integrated Applications (LIA) expand the attack surface and\nintroduce new kinds of threats. Threat modeling is commonly used to identify\nthese threats and suggest mitigations. However, it is a time-consuming practice\nthat requires the involvement of a security practitioner. Our goals are to 1)\nprovide a method for performing threat modeling for LIAs early in their\nlifecycle, (2) develop a threat modeling tool that integrates existing threat\nmodels, and (3) ensure high-quality threat modeling. To achieve the goals, we\nwork in collaboration with our industry partner. Our proposed way of performing\nthreat modeling will benefit industry by requiring fewer security experts'\nparticipation and reducing the time spent on this activity. Our proposed tool\ncombines LLMs and Retrieval Augmented Generation (RAG) and uses sources such as\nexisting threat models and application architecture repositories to\ncontinuously create and update threat models. We propose to evaluate the tool\noffline -- i.e., using benchmarking -- and online with practitioners in the\nfield. We conducted an early evaluation using ChatGPT on a simple LIA and\nobtained results that encouraged us to proceed with our research efforts."
                },
                "authors": [
                    {
                        "name": "Felix Viktor Jedrzejewski"
                    },
                    {
                        "name": "Davide Fucci"
                    },
                    {
                        "name": "Oleksandr Adamov"
                    }
                ],
                "author_detail": {
                    "name": "Oleksandr Adamov"
                },
                "author": "Oleksandr Adamov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18369v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18369v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18359v1",
                "updated": "2025-04-25T14:01:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    14,
                    1,
                    0,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T14:01:00Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    14,
                    1,
                    0,
                    4,
                    115,
                    0
                ],
                "title": "Predicting sampling advantage of stochastic Ising Machines for Quantum\n  Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting sampling advantage of stochastic Ising Machines for Quantum\n  Simulations"
                },
                "summary": "Stochastic Ising machines, sIMs, are highly promising accelerators for\noptimization and sampling of computational problems that can be formulated as\nan Ising model. Here we investigate the computational advantage of sIM for\nsimulations of quantum magnets with neural-network quantum states (NQS), in\nwhich the quantum many-body wave function is mapped onto an Ising model. We\nstudy the sampling performance of sIM for NQS by comparing sampling on a\nsoftware-emulated sIM with standard Metropolis-Hastings sampling for NQS. We\nquantify the sampling efficiency by the number of steps required to reach\niso-accurate stochastic estimation of the variational energy and show that this\nis entirely determined by the autocorrelation time of the sampling. This\nenables predications of sampling advantage without direct deployment on\nhardware. For the quantum Heisenberg models studied and experimental results on\nthe runtime of sIMs, we project a possible speed-up of 100 to 10000, suggesting\ngreat opportunities for studying complex quantum systems at larger scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Ising machines, sIMs, are highly promising accelerators for\noptimization and sampling of computational problems that can be formulated as\nan Ising model. Here we investigate the computational advantage of sIM for\nsimulations of quantum magnets with neural-network quantum states (NQS), in\nwhich the quantum many-body wave function is mapped onto an Ising model. We\nstudy the sampling performance of sIM for NQS by comparing sampling on a\nsoftware-emulated sIM with standard Metropolis-Hastings sampling for NQS. We\nquantify the sampling efficiency by the number of steps required to reach\niso-accurate stochastic estimation of the variational energy and show that this\nis entirely determined by the autocorrelation time of the sampling. This\nenables predications of sampling advantage without direct deployment on\nhardware. For the quantum Heisenberg models studied and experimental results on\nthe runtime of sIMs, we project a possible speed-up of 100 to 10000, suggesting\ngreat opportunities for studying complex quantum systems at larger scales."
                },
                "authors": [
                    {
                        "name": "Rutger J. L. F. Berns"
                    },
                    {
                        "name": "Davi R. Rodrigues"
                    },
                    {
                        "name": "Giovanni Finocchio"
                    },
                    {
                        "name": "Johan H. Mentink"
                    }
                ],
                "author_detail": {
                    "name": "Johan H. Mentink"
                },
                "author": "Johan H. Mentink",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17119v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17119v2",
                "updated": "2025-04-25T13:42:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    13,
                    42,
                    19,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-23T22:02:25Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    22,
                    2,
                    25,
                    2,
                    113,
                    0
                ],
                "title": "The Rise of Small Language Models in Healthcare: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Rise of Small Language Models in Healthcare: A Comprehensive Survey"
                },
                "summary": "Despite substantial progress in healthcare applications driven by large\nlanguage models (LLMs), growing concerns around data privacy, and limited\nresources; the small language models (SLMs) offer a scalable and clinically\nviable solution for efficient performance in resource-constrained environments\nfor next-generation healthcare informatics. Our comprehensive survey presents a\ntaxonomic framework to identify and categorize them for healthcare\nprofessionals and informaticians. The timeline of healthcare SLM contributions\nestablishes a foundational framework for analyzing models across three\ndimensions: NLP tasks, stakeholder roles, and the continuum of care. We present\na taxonomic framework to identify the architectural foundations for building\nmodels from scratch; adapting SLMs to clinical precision through prompting,\ninstruction fine-tuning, and reasoning; and accessibility and sustainability\nthrough compression techniques. Our primary objective is to offer a\ncomprehensive survey for healthcare professionals, introducing recent\ninnovations in model optimization and equipping them with curated resources to\nsupport future research and development in the field. Aiming to showcase the\ngroundbreaking advancements in SLMs for healthcare, we present a comprehensive\ncompilation of experimental results across widely studied NLP tasks in\nhealthcare to highlight the transformative potential of SLMs in healthcare. The\nupdated repository is available at Github",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite substantial progress in healthcare applications driven by large\nlanguage models (LLMs), growing concerns around data privacy, and limited\nresources; the small language models (SLMs) offer a scalable and clinically\nviable solution for efficient performance in resource-constrained environments\nfor next-generation healthcare informatics. Our comprehensive survey presents a\ntaxonomic framework to identify and categorize them for healthcare\nprofessionals and informaticians. The timeline of healthcare SLM contributions\nestablishes a foundational framework for analyzing models across three\ndimensions: NLP tasks, stakeholder roles, and the continuum of care. We present\na taxonomic framework to identify the architectural foundations for building\nmodels from scratch; adapting SLMs to clinical precision through prompting,\ninstruction fine-tuning, and reasoning; and accessibility and sustainability\nthrough compression techniques. Our primary objective is to offer a\ncomprehensive survey for healthcare professionals, introducing recent\ninnovations in model optimization and equipping them with curated resources to\nsupport future research and development in the field. Aiming to showcase the\ngroundbreaking advancements in SLMs for healthcare, we present a comprehensive\ncompilation of experimental results across widely studied NLP tasks in\nhealthcare to highlight the transformative potential of SLMs in healthcare. The\nupdated repository is available at Github"
                },
                "authors": [
                    {
                        "name": "Muskan Garg"
                    },
                    {
                        "name": "Shaina Raza"
                    },
                    {
                        "name": "Shebuti Rayana"
                    },
                    {
                        "name": "Xingyi Liu"
                    },
                    {
                        "name": "Sunghwan Sohn"
                    }
                ],
                "author_detail": {
                    "name": "Sunghwan Sohn"
                },
                "author": "Sunghwan Sohn",
                "arxiv_comment": "35 pages, 7 tables, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17119v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17119v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18349v1",
                "updated": "2025-04-25T13:38:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    13,
                    38,
                    23,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T13:38:23Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    13,
                    38,
                    23,
                    4,
                    115,
                    0
                ],
                "title": "Revisiting Data Auditing in Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Data Auditing in Large Vision-Language Models"
                },
                "summary": "With the surge of large language models (LLMs), Large Vision-Language Models\n(VLMs)--which integrate vision encoders with LLMs for accurate visual\ngrounding--have shown great potential in tasks like generalist agents and\nrobotic control. However, VLMs are typically trained on massive web-scraped\nimages, raising concerns over copyright infringement and privacy violations,\nand making data auditing increasingly urgent. Membership inference (MI), which\ndetermines whether a sample was used in training, has emerged as a key auditing\ntechnique, with promising results on open-source VLMs like LLaVA (AUC > 80%).\nIn this work, we revisit these advances and uncover a critical issue: current\nMI benchmarks suffer from distribution shifts between member and non-member\nimages, introducing shortcut cues that inflate MI performance. We further\nanalyze the nature of these shifts and propose a principled metric based on\noptimal transport to quantify the distribution discrepancy. To evaluate MI in\nrealistic settings, we construct new benchmarks with i.i.d. member and\nnon-member images. Existing MI methods fail under these unbiased conditions,\nperforming only marginally better than chance. Further, we explore the\ntheoretical upper bound of MI by probing the Bayes Optimality within the VLM's\nembedding space and find the irreducible error rate remains high. Despite this\npessimistic outlook, we analyze why MI for VLMs is particularly challenging and\nidentify three practical scenarios--fine-tuning, access to ground-truth texts,\nand set-based inference--where auditing becomes feasible. Our study presents a\nsystematic view of the limits and opportunities of MI for VLMs, providing\nguidance for future efforts in trustworthy data auditing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the surge of large language models (LLMs), Large Vision-Language Models\n(VLMs)--which integrate vision encoders with LLMs for accurate visual\ngrounding--have shown great potential in tasks like generalist agents and\nrobotic control. However, VLMs are typically trained on massive web-scraped\nimages, raising concerns over copyright infringement and privacy violations,\nand making data auditing increasingly urgent. Membership inference (MI), which\ndetermines whether a sample was used in training, has emerged as a key auditing\ntechnique, with promising results on open-source VLMs like LLaVA (AUC > 80%).\nIn this work, we revisit these advances and uncover a critical issue: current\nMI benchmarks suffer from distribution shifts between member and non-member\nimages, introducing shortcut cues that inflate MI performance. We further\nanalyze the nature of these shifts and propose a principled metric based on\noptimal transport to quantify the distribution discrepancy. To evaluate MI in\nrealistic settings, we construct new benchmarks with i.i.d. member and\nnon-member images. Existing MI methods fail under these unbiased conditions,\nperforming only marginally better than chance. Further, we explore the\ntheoretical upper bound of MI by probing the Bayes Optimality within the VLM's\nembedding space and find the irreducible error rate remains high. Despite this\npessimistic outlook, we analyze why MI for VLMs is particularly challenging and\nidentify three practical scenarios--fine-tuning, access to ground-truth texts,\nand set-based inference--where auditing becomes feasible. Our study presents a\nsystematic view of the limits and opportunities of MI for VLMs, providing\nguidance for future efforts in trustworthy data auditing."
                },
                "authors": [
                    {
                        "name": "Hongyu Zhu"
                    },
                    {
                        "name": "Sichu Liang"
                    },
                    {
                        "name": "Wenwen Wang"
                    },
                    {
                        "name": "Boheng Li"
                    },
                    {
                        "name": "Tongxin Yuan"
                    },
                    {
                        "name": "Fangqi Li"
                    },
                    {
                        "name": "ShiLin Wang"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhuosheng Zhang"
                },
                "author": "Zhuosheng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18346v1",
                "updated": "2025-04-25T13:34:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    13,
                    34,
                    40,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T13:34:40Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    13,
                    34,
                    40,
                    4,
                    115,
                    0
                ],
                "title": "Comparing Uncertainty Measurement and Mitigation Methods for Large\n  Language Models: A Systematic Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing Uncertainty Measurement and Mitigation Methods for Large\n  Language Models: A Systematic Review"
                },
                "summary": "Large Language Models (LLMs) have been transformative across many domains.\nHowever, hallucination -- confidently outputting incorrect information --\nremains one of the leading challenges for LLMs. This raises the question of how\nto accurately assess and quantify the uncertainty of LLMs. Extensive literature\non traditional models has explored Uncertainty Quantification (UQ) to measure\nuncertainty and employed calibration techniques to address the misalignment\nbetween uncertainty and accuracy. While some of these methods have been adapted\nfor LLMs, the literature lacks an in-depth analysis of their effectiveness and\ndoes not offer a comprehensive benchmark to enable insightful comparison among\nexisting solutions. In this work, we fill this gap via a systematic survey of\nrepresentative prior works on UQ and calibration for LLMs and introduce a\nrigorous benchmark. Using two widely used reliability datasets, we empirically\nevaluate six related methods, which justify the significant findings of our\nreview. Finally, we provide outlooks for key future directions and outline open\nchallenges. To the best of our knowledge, this survey is the first dedicated\nstudy to review the calibration methods and relevant metrics for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been transformative across many domains.\nHowever, hallucination -- confidently outputting incorrect information --\nremains one of the leading challenges for LLMs. This raises the question of how\nto accurately assess and quantify the uncertainty of LLMs. Extensive literature\non traditional models has explored Uncertainty Quantification (UQ) to measure\nuncertainty and employed calibration techniques to address the misalignment\nbetween uncertainty and accuracy. While some of these methods have been adapted\nfor LLMs, the literature lacks an in-depth analysis of their effectiveness and\ndoes not offer a comprehensive benchmark to enable insightful comparison among\nexisting solutions. In this work, we fill this gap via a systematic survey of\nrepresentative prior works on UQ and calibration for LLMs and introduce a\nrigorous benchmark. Using two widely used reliability datasets, we empirically\nevaluate six related methods, which justify the significant findings of our\nreview. Finally, we provide outlooks for key future directions and outline open\nchallenges. To the best of our knowledge, this survey is the first dedicated\nstudy to review the calibration methods and relevant metrics for LLMs."
                },
                "authors": [
                    {
                        "name": "Toghrul Abbasli"
                    },
                    {
                        "name": "Kentaroh Toyoda"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Leon Witt"
                    },
                    {
                        "name": "Muhammad Asif Ali"
                    },
                    {
                        "name": "Yukai Miao"
                    },
                    {
                        "name": "Dan Li"
                    },
                    {
                        "name": "Qingsong Wei"
                    }
                ],
                "author_detail": {
                    "name": "Qingsong Wei"
                },
                "author": "Qingsong Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18340v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18340v1",
                "updated": "2025-04-25T13:30:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    13,
                    30,
                    7,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T13:30:07Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    13,
                    30,
                    7,
                    4,
                    115,
                    0
                ],
                "title": "Large Language Models to Accelerate Organic Chemistry Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models to Accelerate Organic Chemistry Synthesis"
                },
                "summary": "Chemical synthesis, as a foundational methodology in the creation of\ntransformative molecules, exerts substantial influence across diverse sectors\nfrom life sciences to materials and energy. Current chemical synthesis\npractices emphasize laborious and costly trial-and-error workflows,\nunderscoring the urgent need for advanced AI assistants. Nowadays, large\nlanguage models (LLMs), typified by GPT-4, have been introduced as an efficient\ntool to facilitate scientific research. Here, we present Chemma, a fully\nfine-tuned LLM with 1.28 million pairs of Q&A about reactions, as an assistant\nto accelerate organic chemistry synthesis. Chemma surpasses the best-known\nresults in multiple chemical tasks, e.g., single-step retrosynthesis and yield\nprediction, which highlights the potential of general AI for organic chemistry.\nVia predicting yields across the experimental reaction space, Chemma\nsignificantly improves the reaction exploration capability of Bayesian\noptimization. More importantly, integrated in an active learning framework,\nChemma exhibits advanced potential for autonomous experimental exploration and\noptimization in open reaction spaces. For an unreported Suzuki-Miyaura\ncross-coupling reaction of cyclic aminoboronates and aryl halides for the\nsynthesis of $\\alpha$-Aryl N-heterocycles, the human-AI collaboration\nsuccessfully explored suitable ligand and solvent (1,4-dioxane) within only 15\nruns, achieving an isolated yield of 67%. These results reveal that, without\nquantum-chemical calculations, Chemma can comprehend and extract chemical\ninsights from reaction data, in a manner akin to human experts. This work opens\navenues for accelerating organic chemistry synthesis with adapted large\nlanguage models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chemical synthesis, as a foundational methodology in the creation of\ntransformative molecules, exerts substantial influence across diverse sectors\nfrom life sciences to materials and energy. Current chemical synthesis\npractices emphasize laborious and costly trial-and-error workflows,\nunderscoring the urgent need for advanced AI assistants. Nowadays, large\nlanguage models (LLMs), typified by GPT-4, have been introduced as an efficient\ntool to facilitate scientific research. Here, we present Chemma, a fully\nfine-tuned LLM with 1.28 million pairs of Q&A about reactions, as an assistant\nto accelerate organic chemistry synthesis. Chemma surpasses the best-known\nresults in multiple chemical tasks, e.g., single-step retrosynthesis and yield\nprediction, which highlights the potential of general AI for organic chemistry.\nVia predicting yields across the experimental reaction space, Chemma\nsignificantly improves the reaction exploration capability of Bayesian\noptimization. More importantly, integrated in an active learning framework,\nChemma exhibits advanced potential for autonomous experimental exploration and\noptimization in open reaction spaces. For an unreported Suzuki-Miyaura\ncross-coupling reaction of cyclic aminoboronates and aryl halides for the\nsynthesis of $\\alpha$-Aryl N-heterocycles, the human-AI collaboration\nsuccessfully explored suitable ligand and solvent (1,4-dioxane) within only 15\nruns, achieving an isolated yield of 67%. These results reveal that, without\nquantum-chemical calculations, Chemma can comprehend and extract chemical\ninsights from reaction data, in a manner akin to human experts. This work opens\navenues for accelerating organic chemistry synthesis with adapted large\nlanguage models."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Yang Han"
                    },
                    {
                        "name": "Shuai Chen"
                    },
                    {
                        "name": "Ruijie Yu"
                    },
                    {
                        "name": "Xin Zhao"
                    },
                    {
                        "name": "Xianbin Liu"
                    },
                    {
                        "name": "Kaipeng Zeng"
                    },
                    {
                        "name": "Mengdi Yu"
                    },
                    {
                        "name": "Jidong Tian"
                    },
                    {
                        "name": "Feng Zhu"
                    },
                    {
                        "name": "Xiaokang Yang"
                    },
                    {
                        "name": "Yaohui Jin"
                    },
                    {
                        "name": "Yanyan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yanyan Xu"
                },
                "author": "Yanyan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18340v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18340v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18333v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18333v1",
                "updated": "2025-04-25T13:18:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    13,
                    18,
                    42,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T13:18:42Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    13,
                    18,
                    42,
                    4,
                    115,
                    0
                ],
                "title": "Adversarial Attacks on LLM-as-a-Judge Systems: Insights from Prompt\n  Injections",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Attacks on LLM-as-a-Judge Systems: Insights from Prompt\n  Injections"
                },
                "summary": "LLM as judge systems used to assess text quality code correctness and\nargument strength are vulnerable to prompt injection attacks. We introduce a\nframework that separates content author attacks from system prompt attacks and\nevaluate five models Gemma 3.27B Gemma 3.4B Llama 3.2 3B GPT 4 and Claude 3\nOpus on four tasks with various defenses using fifty prompts per condition.\nAttacks achieved up to seventy three point eight percent success smaller models\nproved more vulnerable and transferability ranged from fifty point five to\nsixty two point six percent. Our results contrast with Universal Prompt\nInjection and AdvPrompter We recommend multi model committees and comparative\nscoring and release all code and datasets",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM as judge systems used to assess text quality code correctness and\nargument strength are vulnerable to prompt injection attacks. We introduce a\nframework that separates content author attacks from system prompt attacks and\nevaluate five models Gemma 3.27B Gemma 3.4B Llama 3.2 3B GPT 4 and Claude 3\nOpus on four tasks with various defenses using fifty prompts per condition.\nAttacks achieved up to seventy three point eight percent success smaller models\nproved more vulnerable and transferability ranged from fifty point five to\nsixty two point six percent. Our results contrast with Universal Prompt\nInjection and AdvPrompter We recommend multi model committees and comparative\nscoring and release all code and datasets"
                },
                "authors": [
                    {
                        "name": "Narek Maloyan"
                    },
                    {
                        "name": "Dmitry Namiot"
                    }
                ],
                "author_detail": {
                    "name": "Dmitry Namiot"
                },
                "author": "Dmitry Namiot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18333v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18333v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18326v1",
                "updated": "2025-04-25T13:08:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    13,
                    8,
                    59,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T13:08:59Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    13,
                    8,
                    59,
                    4,
                    115,
                    0
                ],
                "title": "Exhaled Breath Analysis Through the Lens of Molecular Communication: A\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exhaled Breath Analysis Through the Lens of Molecular Communication: A\n  Survey"
                },
                "summary": "Molecular Communication (MC) has long been envisioned to enable an Internet\nof Bio-Nano Things (IoBNT) with medical applications, where nanomachines within\nthe human body conduct monitoring, diagnosis, and therapy at micro- and\nnanoscale levels. MC involves information transfer via molecules and is\nsupported by well-established theoretical models. However, practically\nachieving reliable, energy-efficient, and bio-compatible communication at these\nscales still remains a challenge. Air-Based Molecular Communication (ABMC) is a\ntype of MC that operates over larger, meter-scale distances and extends even\noutside the human body. Therefore, devices and techniques to realize ABMC are\nreadily accessible, and associated use cases can be very promising in the near\nfuture. Exhaled breath analysis has previously been proposed. It provides a\nnon-invasive approach for health monitoring, leveraging existing commercial\nsensor technologies and reducing deployment barriers. The breath contains a\ndiverse range of molecules and particles that serve as biomarkers linked to\nvarious physiological and pathological conditions. The plethora of proven\nmethods, models, and optimization approaches in MC enable macroscale breath\nanalysis, treating human as the transmitter, the breath as the information\ncarrier, and macroscale sensors as the receiver. Using ABMC to interface with\nthe inherent dynamic networks of cells, tissues, and organs could create a\nnovel Internet of Bio Things (IoBT), a preliminary macroscale stage of the\nIoBNT. This survey extensively reviews exhaled breath modeling and analysis\nthrough the lens of MC, offering insights into theoretical frameworks and\npractical implementations from ABMC, bringing the IoBT a step closer to\nreal-world use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecular Communication (MC) has long been envisioned to enable an Internet\nof Bio-Nano Things (IoBNT) with medical applications, where nanomachines within\nthe human body conduct monitoring, diagnosis, and therapy at micro- and\nnanoscale levels. MC involves information transfer via molecules and is\nsupported by well-established theoretical models. However, practically\nachieving reliable, energy-efficient, and bio-compatible communication at these\nscales still remains a challenge. Air-Based Molecular Communication (ABMC) is a\ntype of MC that operates over larger, meter-scale distances and extends even\noutside the human body. Therefore, devices and techniques to realize ABMC are\nreadily accessible, and associated use cases can be very promising in the near\nfuture. Exhaled breath analysis has previously been proposed. It provides a\nnon-invasive approach for health monitoring, leveraging existing commercial\nsensor technologies and reducing deployment barriers. The breath contains a\ndiverse range of molecules and particles that serve as biomarkers linked to\nvarious physiological and pathological conditions. The plethora of proven\nmethods, models, and optimization approaches in MC enable macroscale breath\nanalysis, treating human as the transmitter, the breath as the information\ncarrier, and macroscale sensors as the receiver. Using ABMC to interface with\nthe inherent dynamic networks of cells, tissues, and organs could create a\nnovel Internet of Bio Things (IoBT), a preliminary macroscale stage of the\nIoBNT. This survey extensively reviews exhaled breath modeling and analysis\nthrough the lens of MC, offering insights into theoretical frameworks and\npractical implementations from ABMC, bringing the IoBT a step closer to\nreal-world use."
                },
                "authors": [
                    {
                        "name": "Sunasheer Bhattacharjee"
                    },
                    {
                        "name": "Dadi Bi"
                    },
                    {
                        "name": "Pit Hofmann"
                    },
                    {
                        "name": "Alexander Wietfeld"
                    },
                    {
                        "name": "Sophie Becke"
                    },
                    {
                        "name": "Michael Lommel"
                    },
                    {
                        "name": "Pengjie Zhou"
                    },
                    {
                        "name": "Ruifeng Zheng"
                    },
                    {
                        "name": "Ulrich Kertzscher"
                    },
                    {
                        "name": "Yansha Deng"
                    },
                    {
                        "name": "Wolfgang Kellerer"
                    },
                    {
                        "name": "Frank H. P. Fitzek"
                    },
                    {
                        "name": "Falko Dressler"
                    }
                ],
                "author_detail": {
                    "name": "Falko Dressler"
                },
                "author": "Falko Dressler",
                "arxiv_comment": "27 pages, 2 tables, 12 figures. Submitted to IEEE Communications\n  Surveys & Tutorials for review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12486v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12486v5",
                "updated": "2025-04-25T13:03:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    13,
                    3,
                    37,
                    4,
                    115,
                    0
                ],
                "published": "2025-02-18T03:15:55Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    3,
                    15,
                    55,
                    1,
                    49,
                    0
                ],
                "title": "EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via\n  Reinforcement Learning"
                },
                "summary": "Large Language Models (LLMs) have shown impressive reasoning capabilities in\nwell-defined problems with clear solutions, such as mathematics and coding.\nHowever, they still struggle with complex real-world scenarios like business\nnegotiations, which require strategic reasoning-an ability to navigate dynamic\nenvironments and align long-term goals amidst uncertainty. Existing methods for\nstrategic reasoning face challenges in adaptability, scalability, and\ntransferring strategies to new contexts. To address these issues, we propose\nexplicit policy optimization (EPO) for strategic reasoning, featuring an LLM\nthat provides strategies in open-ended action space and can be plugged into\narbitrary LLM agents to motivate goal-directed behavior. To improve\nadaptability and policy transferability, we train the strategic reasoning model\nvia multi-turn reinforcement learning (RL) using process rewards and iterative\nself-play, without supervised fine-tuning (SFT) as a preliminary step.\nExperiments across social and physical domains demonstrate EPO's ability of\nlong-term goal alignment through enhanced strategic reasoning, achieving\nstate-of-the-art performance on social dialogue and web navigation tasks. Our\nfindings reveal various collaborative reasoning mechanisms emergent in EPO and\nits effectiveness in generating novel strategies, underscoring its potential\nfor strategic reasoning in real-world applications. Code and data are available\nat https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/EPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive reasoning capabilities in\nwell-defined problems with clear solutions, such as mathematics and coding.\nHowever, they still struggle with complex real-world scenarios like business\nnegotiations, which require strategic reasoning-an ability to navigate dynamic\nenvironments and align long-term goals amidst uncertainty. Existing methods for\nstrategic reasoning face challenges in adaptability, scalability, and\ntransferring strategies to new contexts. To address these issues, we propose\nexplicit policy optimization (EPO) for strategic reasoning, featuring an LLM\nthat provides strategies in open-ended action space and can be plugged into\narbitrary LLM agents to motivate goal-directed behavior. To improve\nadaptability and policy transferability, we train the strategic reasoning model\nvia multi-turn reinforcement learning (RL) using process rewards and iterative\nself-play, without supervised fine-tuning (SFT) as a preliminary step.\nExperiments across social and physical domains demonstrate EPO's ability of\nlong-term goal alignment through enhanced strategic reasoning, achieving\nstate-of-the-art performance on social dialogue and web navigation tasks. Our\nfindings reveal various collaborative reasoning mechanisms emergent in EPO and\nits effectiveness in generating novel strategies, underscoring its potential\nfor strategic reasoning in real-world applications. Code and data are available\nat https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/EPO."
                },
                "authors": [
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Yongbin Li"
                    },
                    {
                        "name": "Yuchuan Wu"
                    },
                    {
                        "name": "Wentao Ma"
                    },
                    {
                        "name": "Aobo Kong"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Jianbin Jiao"
                    },
                    {
                        "name": "Junge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Junge Zhang"
                },
                "author": "Junge Zhang",
                "arxiv_comment": "22 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12486v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12486v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18316v1",
                "updated": "2025-04-25T12:48:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    12,
                    48,
                    8,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T12:48:08Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    12,
                    48,
                    8,
                    4,
                    115,
                    0
                ],
                "title": "Towards Adaptive Software Agents for Debugging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Adaptive Software Agents for Debugging"
                },
                "summary": "Using multiple agents was found to improve the debugging capabilities of\nLarge Language Models. However, increasing the number of LLM-agents has several\ndrawbacks such as increasing the running costs and rising the risk for the\nagents to lose focus. In this work, we propose an adaptive agentic design,\nwhere the number of agents and their roles are determined dynamically based on\nthe characteristics of the task to be achieved. In this design, the agents\nroles are not predefined, but are generated after analyzing the problem to be\nsolved. Our initial evaluation shows that, with the adaptive design, the number\nof agents that are generated depends on the complexity of the buggy code. In\nfact, for simple code with mere syntax issues, the problem was usually fixed\nusing one agent only. However, for more complex problems, we noticed the\ncreation of a higher number of agents. Regarding the effectiveness of the fix,\nwe noticed an average improvement of 11% compared to the one-shot prompting.\nGiven these promising results, we outline future research directions to improve\nour design for adaptive software agents that can autonomously plan and conduct\ntheir software goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using multiple agents was found to improve the debugging capabilities of\nLarge Language Models. However, increasing the number of LLM-agents has several\ndrawbacks such as increasing the running costs and rising the risk for the\nagents to lose focus. In this work, we propose an adaptive agentic design,\nwhere the number of agents and their roles are determined dynamically based on\nthe characteristics of the task to be achieved. In this design, the agents\nroles are not predefined, but are generated after analyzing the problem to be\nsolved. Our initial evaluation shows that, with the adaptive design, the number\nof agents that are generated depends on the complexity of the buggy code. In\nfact, for simple code with mere syntax issues, the problem was usually fixed\nusing one agent only. However, for more complex problems, we noticed the\ncreation of a higher number of agents. Regarding the effectiveness of the fix,\nwe noticed an average improvement of 11% compared to the one-shot prompting.\nGiven these promising results, we outline future research directions to improve\nour design for adaptive software agents that can autonomously plan and conduct\ntheir software goals."
                },
                "authors": [
                    {
                        "name": "Yacine Majdoub"
                    },
                    {
                        "name": "Eya Ben Charrada"
                    },
                    {
                        "name": "Haifa Touati"
                    }
                ],
                "author_detail": {
                    "name": "Haifa Touati"
                },
                "author": "Haifa Touati",
                "arxiv_comment": "5 pages, 3 figures, FSE2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.02252v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.02252v3",
                "updated": "2025-04-25T12:47:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    12,
                    47,
                    21,
                    4,
                    115,
                    0
                ],
                "published": "2023-12-04T18:14:29Z",
                "published_parsed": [
                    2023,
                    12,
                    4,
                    18,
                    14,
                    29,
                    0,
                    338,
                    0
                ],
                "title": "StoryGPT-V: Large Language Models as Consistent Story Visualizers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StoryGPT-V: Large Language Models as Consistent Story Visualizers"
                },
                "summary": "Recent generative models have demonstrated impressive capabilities in\ngenerating realistic and visually pleasing images grounded on textual prompts.\nNevertheless, a significant challenge remains in applying these models for the\nmore intricate task of story visualization. Since it requires resolving\npronouns (he, she, they) in the frame descriptions, i.e., anaphora resolution,\nand ensuring consistent characters and background synthesis across frames. Yet,\nthe emerging Large Language Model (LLM) showcases robust reasoning abilities to\nnavigate through ambiguous references and process extensive sequences.\nTherefore, we introduce \\emph{StoryGPT-V}, which leverages the merits of the\nlatent diffusion (LDM) and LLM to produce images with consistent and\nhigh-quality characters grounded on given story descriptions. First, we train a\ncharacter-aware LDM, which takes character-augmented semantic embedding as\ninput and includes the supervision of the cross-attention map using character\nsegmentation masks, aiming to enhance character generation accuracy and\nfaithfulness. In the second stage, we enable an alignment between the output of\nLLM and the character-augmented embedding residing in the input space of the\nfirst-stage model. This harnesses the reasoning ability of LLM to address\nambiguous references and the comprehension capability to memorize the context.\nWe conduct comprehensive experiments on two visual story visualization\nbenchmarks. Our model reports superior quantitative results and consistently\ngenerates accurate characters of remarkable quality with low memory\nconsumption. Our code is publicly available at:\n\\href{https://xiaoqian-shen.github.io/StoryGPT-V}{https://xiaoqian-shen.github.io/StoryGPT-V}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent generative models have demonstrated impressive capabilities in\ngenerating realistic and visually pleasing images grounded on textual prompts.\nNevertheless, a significant challenge remains in applying these models for the\nmore intricate task of story visualization. Since it requires resolving\npronouns (he, she, they) in the frame descriptions, i.e., anaphora resolution,\nand ensuring consistent characters and background synthesis across frames. Yet,\nthe emerging Large Language Model (LLM) showcases robust reasoning abilities to\nnavigate through ambiguous references and process extensive sequences.\nTherefore, we introduce \\emph{StoryGPT-V}, which leverages the merits of the\nlatent diffusion (LDM) and LLM to produce images with consistent and\nhigh-quality characters grounded on given story descriptions. First, we train a\ncharacter-aware LDM, which takes character-augmented semantic embedding as\ninput and includes the supervision of the cross-attention map using character\nsegmentation masks, aiming to enhance character generation accuracy and\nfaithfulness. In the second stage, we enable an alignment between the output of\nLLM and the character-augmented embedding residing in the input space of the\nfirst-stage model. This harnesses the reasoning ability of LLM to address\nambiguous references and the comprehension capability to memorize the context.\nWe conduct comprehensive experiments on two visual story visualization\nbenchmarks. Our model reports superior quantitative results and consistently\ngenerates accurate characters of remarkable quality with low memory\nconsumption. Our code is publicly available at:\n\\href{https://xiaoqian-shen.github.io/StoryGPT-V}{https://xiaoqian-shen.github.io/StoryGPT-V}."
                },
                "authors": [
                    {
                        "name": "Xiaoqian Shen"
                    },
                    {
                        "name": "Mohamed Elhoseiny"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Elhoseiny"
                },
                "author": "Mohamed Elhoseiny",
                "arxiv_comment": "Accepted to CVPR 2025; Project page:\n  https://xiaoqian-shen.github.io/StoryGPT-V",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.02252v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.02252v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02810v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02810v2",
                "updated": "2025-04-25T12:02:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    12,
                    2,
                    19,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-03T17:54:18Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    54,
                    18,
                    3,
                    93,
                    0
                ],
                "title": "Generative Evaluation of Complex Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Evaluation of Complex Reasoning in Large Language Models"
                },
                "summary": "With powerful large language models (LLMs) demonstrating superhuman reasoning\ncapabilities, a critical question arises: Do LLMs genuinely reason, or do they\nmerely recall answers from their extensive, web-scraped training datasets?\nPublicly released benchmarks inevitably become contaminated once incorporated\ninto subsequent LLM training sets, undermining their reliability as faithful\nassessments. To address this, we introduce KUMO, a generative evaluation\nframework designed specifically for assessing reasoning in LLMs. KUMO\nsynergistically combines LLMs with symbolic engines to dynamically produce\ndiverse, multi-turn reasoning tasks that are partially observable and\nadjustable in difficulty. Through an automated pipeline, KUMO continuously\ngenerates novel tasks across open-ended domains, compelling models to\ndemonstrate genuine generalization rather than memorization. We evaluated 23\nstate-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO,\nbenchmarking their reasoning abilities against university students. Our\nfindings reveal that many LLMs have outperformed university-level performance\non easy reasoning tasks, and reasoning-scaled LLMs reach university-level\nperformance on complex reasoning challenges. Moreover, LLM performance on KUMO\ntasks correlates strongly with results on newly released real-world reasoning\nbenchmarks, underscoring KUMO's value as a robust, enduring assessment tool for\ngenuine LLM reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With powerful large language models (LLMs) demonstrating superhuman reasoning\ncapabilities, a critical question arises: Do LLMs genuinely reason, or do they\nmerely recall answers from their extensive, web-scraped training datasets?\nPublicly released benchmarks inevitably become contaminated once incorporated\ninto subsequent LLM training sets, undermining their reliability as faithful\nassessments. To address this, we introduce KUMO, a generative evaluation\nframework designed specifically for assessing reasoning in LLMs. KUMO\nsynergistically combines LLMs with symbolic engines to dynamically produce\ndiverse, multi-turn reasoning tasks that are partially observable and\nadjustable in difficulty. Through an automated pipeline, KUMO continuously\ngenerates novel tasks across open-ended domains, compelling models to\ndemonstrate genuine generalization rather than memorization. We evaluated 23\nstate-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO,\nbenchmarking their reasoning abilities against university students. Our\nfindings reveal that many LLMs have outperformed university-level performance\non easy reasoning tasks, and reasoning-scaled LLMs reach university-level\nperformance on complex reasoning challenges. Moreover, LLM performance on KUMO\ntasks correlates strongly with results on newly released real-world reasoning\nbenchmarks, underscoring KUMO's value as a robust, enduring assessment tool for\ngenuine LLM reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Haowei Lin"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Ruilin Yan"
                    },
                    {
                        "name": "Baizhou Huang"
                    },
                    {
                        "name": "Haotian Ye"
                    },
                    {
                        "name": "Jianhua Zhu"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Jianzhu Ma"
                    },
                    {
                        "name": "Yitao Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yitao Liang"
                },
                "author": "Yitao Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02810v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02810v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18271v1",
                "updated": "2025-04-25T11:29:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    11,
                    29,
                    30,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T11:29:30Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    11,
                    29,
                    30,
                    4,
                    115,
                    0
                ],
                "title": "LEAM: A Prompt-only Large Language Model-enabled Antenna Modeling Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEAM: A Prompt-only Large Language Model-enabled Antenna Modeling Method"
                },
                "summary": "Antenna modeling is a time-consuming and complex process, decreasing the\nspeed of antenna analysis and design. In this paper, a large language model\n(LLM)- enabled antenna modeling method, called LEAM, is presented to address\nthis challenge. LEAM enables automatic antenna model generation based on\nlanguage descriptions via prompt input, images, descriptions from academic\npapers, patents, and technical reports (either one or multiple). The\neffectiveness of LEAM is demonstrated by three examples: a Vivaldi antenna\ngenerated from a complete user description, a slotted patch antenna generated\nfrom an incomplete user description and the operating frequency, and a monopole\nslotted antenna generated from images and descriptions scanned from the\nliterature. For all the examples, correct antenna models are generated in a few\nminutes. The code can be accessed via https://github.com/TaoWu974/LEAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Antenna modeling is a time-consuming and complex process, decreasing the\nspeed of antenna analysis and design. In this paper, a large language model\n(LLM)- enabled antenna modeling method, called LEAM, is presented to address\nthis challenge. LEAM enables automatic antenna model generation based on\nlanguage descriptions via prompt input, images, descriptions from academic\npapers, patents, and technical reports (either one or multiple). The\neffectiveness of LEAM is demonstrated by three examples: a Vivaldi antenna\ngenerated from a complete user description, a slotted patch antenna generated\nfrom an incomplete user description and the operating frequency, and a monopole\nslotted antenna generated from images and descriptions scanned from the\nliterature. For all the examples, correct antenna models are generated in a few\nminutes. The code can be accessed via https://github.com/TaoWu974/LEAM."
                },
                "authors": [
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Kexue Fu"
                    },
                    {
                        "name": "Qiang Hua"
                    },
                    {
                        "name": "Xinxin Liu"
                    },
                    {
                        "name": "Muhammad Ali Imran"
                    },
                    {
                        "name": "Bo Liu"
                    }
                ],
                "author_detail": {
                    "name": "Bo Liu"
                },
                "author": "Bo Liu",
                "arxiv_comment": "Code are available: https://github.com/TaoWu974/LEAM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18269v1",
                "updated": "2025-04-25T11:27:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    11,
                    27,
                    44,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T11:27:44Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    11,
                    27,
                    44,
                    4,
                    115,
                    0
                ],
                "title": "TextTIGER: Text-based Intelligent Generation with Entity Prompt\n  Refinement for Text-to-Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TextTIGER: Text-based Intelligent Generation with Entity Prompt\n  Refinement for Text-to-Image Generation"
                },
                "summary": "Generating images from prompts containing specific entities requires models\nto retain as much entity-specific knowledge as possible. However, fully\nmemorizing such knowledge is impractical due to the vast number of entities and\ntheir continuous emergence. To address this, we propose Text-based Intelligent\nGeneration with Entity prompt Refinement (TextTIGER), which augments knowledge\non entities included in the prompts and then summarizes the augmented\ndescriptions using Large Language Models (LLMs) to mitigate performance\ndegradation from longer inputs. To evaluate our method, we introduce WiT-Cub\n(WiT with Captions and Uncomplicated Background-explanations), a dataset\ncomprising captions, images, and an entity list. Experiments on four image\ngeneration models and five LLMs show that TextTIGER improves image generation\nperformance in standard metrics (IS, FID, and CLIPScore) compared to\ncaption-only prompts. Additionally, multiple annotators' evaluation confirms\nthat the summarized descriptions are more informative, validating LLMs' ability\nto generate concise yet rich descriptions. These findings demonstrate that\nrefining prompts with augmented and summarized entity-related descriptions\nenhances image generation capabilities. The code and dataset will be available\nupon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating images from prompts containing specific entities requires models\nto retain as much entity-specific knowledge as possible. However, fully\nmemorizing such knowledge is impractical due to the vast number of entities and\ntheir continuous emergence. To address this, we propose Text-based Intelligent\nGeneration with Entity prompt Refinement (TextTIGER), which augments knowledge\non entities included in the prompts and then summarizes the augmented\ndescriptions using Large Language Models (LLMs) to mitigate performance\ndegradation from longer inputs. To evaluate our method, we introduce WiT-Cub\n(WiT with Captions and Uncomplicated Background-explanations), a dataset\ncomprising captions, images, and an entity list. Experiments on four image\ngeneration models and five LLMs show that TextTIGER improves image generation\nperformance in standard metrics (IS, FID, and CLIPScore) compared to\ncaption-only prompts. Additionally, multiple annotators' evaluation confirms\nthat the summarized descriptions are more informative, validating LLMs' ability\nto generate concise yet rich descriptions. These findings demonstrate that\nrefining prompts with augmented and summarized entity-related descriptions\nenhances image generation capabilities. The code and dataset will be available\nupon acceptance."
                },
                "authors": [
                    {
                        "name": "Shintaro Ozaki"
                    },
                    {
                        "name": "Kazuki Hayashi"
                    },
                    {
                        "name": "Yusuke Sakai"
                    },
                    {
                        "name": "Jingun Kwon"
                    },
                    {
                        "name": "Hidetaka Kamigaito"
                    },
                    {
                        "name": "Katsuhiko Hayashi"
                    },
                    {
                        "name": "Manabu Okumura"
                    },
                    {
                        "name": "Taro Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Taro Watanabe"
                },
                "author": "Taro Watanabe",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06276v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06276v5",
                "updated": "2025-04-25T11:10:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    11,
                    10,
                    43,
                    4,
                    115,
                    0
                ],
                "published": "2024-08-12T16:39:03Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    39,
                    3,
                    0,
                    225,
                    0
                ],
                "title": "Review-driven Personalized Preference Reasoning with Large Language\n  Models for Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Review-driven Personalized Preference Reasoning with Large Language\n  Models for Recommendation"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nexceptional performance across a wide range of tasks, generating significant\ninterest in their application to recommendation systems. However, existing\nmethods have not fully capitalized on the potential of LLMs, often constrained\nby limited input information or failing to fully utilize their advanced\nreasoning capabilities. To address these limitations, we introduce EXP3RT, a\nnovel LLM-based recommender designed to leverage rich preference information\ncontained in user and item reviews. EXP3RT is basically fine-tuned through\ndistillation from a teacher LLM to perform three key tasks in order: EXP3RT\nfirst extracts and encapsulates essential subjective preferences from raw\nreviews, aggregates and summarizes them according to specific criteria to\ncreate user and item profiles. It then generates detailed step-by-step\nreasoning followed by predicted rating, i.e., reasoning-enhanced rating\nprediction, by considering both subjective and objective information from\nuser/item profiles and item descriptions. This personalized preference\nreasoning from EXP3RT enhances rating prediction accuracy and also provides\nfaithful and reasonable explanations for recommendation. Extensive experiments\nshow that EXP3RT outperforms existing methods on both rating prediction and\ncandidate item reranking for top-k recommendation, while significantly\nenhancing the explainability of recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have demonstrated\nexceptional performance across a wide range of tasks, generating significant\ninterest in their application to recommendation systems. However, existing\nmethods have not fully capitalized on the potential of LLMs, often constrained\nby limited input information or failing to fully utilize their advanced\nreasoning capabilities. To address these limitations, we introduce EXP3RT, a\nnovel LLM-based recommender designed to leverage rich preference information\ncontained in user and item reviews. EXP3RT is basically fine-tuned through\ndistillation from a teacher LLM to perform three key tasks in order: EXP3RT\nfirst extracts and encapsulates essential subjective preferences from raw\nreviews, aggregates and summarizes them according to specific criteria to\ncreate user and item profiles. It then generates detailed step-by-step\nreasoning followed by predicted rating, i.e., reasoning-enhanced rating\nprediction, by considering both subjective and objective information from\nuser/item profiles and item descriptions. This personalized preference\nreasoning from EXP3RT enhances rating prediction accuracy and also provides\nfaithful and reasonable explanations for recommendation. Extensive experiments\nshow that EXP3RT outperforms existing methods on both rating prediction and\ncandidate item reranking for top-k recommendation, while significantly\nenhancing the explainability of recommendation systems."
                },
                "authors": [
                    {
                        "name": "Jieyong Kim"
                    },
                    {
                        "name": "Hyunseo Kim"
                    },
                    {
                        "name": "Hyunjin Cho"
                    },
                    {
                        "name": "SeongKu Kang"
                    },
                    {
                        "name": "Buru Chang"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "arxiv_doi": "10.1145/3726302.3730055",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730055",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.06276v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06276v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to SIGIR 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18260v1",
                "updated": "2025-04-25T11:08:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    11,
                    8,
                    27,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T11:08:27Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    11,
                    8,
                    27,
                    4,
                    115,
                    0
                ],
                "title": "MAGI: Multi-Agent Guided Interview for Psychiatric Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAGI: Multi-Agent Guided Interview for Psychiatric Assessment"
                },
                "summary": "Automating structured clinical interviews could revolutionize mental\nhealthcare accessibility, yet existing large language models (LLMs) approaches\nfail to align with psychiatric diagnostic protocols. We present MAGI, the first\nframework that transforms the gold-standard Mini International Neuropsychiatric\nInterview (MINI) into automatic computational workflows through coordinated\nmulti-agent collaboration. MAGI dynamically navigates clinical logic via four\nspecialized agents: 1) an interview tree guided navigation agent adhering to\nthe MINI's branching structure, 2) an adaptive question agent blending\ndiagnostic probing, explaining, and empathy, 3) a judgment agent validating\nwhether the response from participants meet the node, and 4) a diagnosis Agent\ngenerating Psychometric Chain-of- Thought (PsyCoT) traces that explicitly map\nsymptoms to clinical criteria. Experimental results on 1,002 real-world\nparticipants covering depression, generalized anxiety, social anxiety and\nsuicide shows that MAGI advances LLM- assisted mental health assessment by\ncombining clinical rigor, conversational adaptability, and explainable\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating structured clinical interviews could revolutionize mental\nhealthcare accessibility, yet existing large language models (LLMs) approaches\nfail to align with psychiatric diagnostic protocols. We present MAGI, the first\nframework that transforms the gold-standard Mini International Neuropsychiatric\nInterview (MINI) into automatic computational workflows through coordinated\nmulti-agent collaboration. MAGI dynamically navigates clinical logic via four\nspecialized agents: 1) an interview tree guided navigation agent adhering to\nthe MINI's branching structure, 2) an adaptive question agent blending\ndiagnostic probing, explaining, and empathy, 3) a judgment agent validating\nwhether the response from participants meet the node, and 4) a diagnosis Agent\ngenerating Psychometric Chain-of- Thought (PsyCoT) traces that explicitly map\nsymptoms to clinical criteria. Experimental results on 1,002 real-world\nparticipants covering depression, generalized anxiety, social anxiety and\nsuicide shows that MAGI advances LLM- assisted mental health assessment by\ncombining clinical rigor, conversational adaptability, and explainable\nreasoning."
                },
                "authors": [
                    {
                        "name": "Guanqun Bi"
                    },
                    {
                        "name": "Zhuang Chen"
                    },
                    {
                        "name": "Zhoufu Liu"
                    },
                    {
                        "name": "Hongkai Wang"
                    },
                    {
                        "name": "Xiyao Xiao"
                    },
                    {
                        "name": "Yuqiang Xie"
                    },
                    {
                        "name": "Wen Zhang"
                    },
                    {
                        "name": "Yongkang Huang"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Libiao Peng"
                    },
                    {
                        "name": "Yi Feng"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "arxiv_comment": "In progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10602v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10602v2",
                "updated": "2025-04-25T10:53:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    53,
                    27,
                    4,
                    115,
                    0
                ],
                "published": "2024-06-15T11:31:39Z",
                "published_parsed": [
                    2024,
                    6,
                    15,
                    11,
                    31,
                    39,
                    5,
                    167,
                    0
                ],
                "title": "Multilingual Large Language Models and Curse of Multilinguality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Large Language Models and Curse of Multilinguality"
                },
                "summary": "Multilingual Large Language Models (LLMs) have gained large popularity among\nNatural Language Processing (NLP) researchers and practitioners. These models,\ntrained on huge datasets, show proficiency across various languages and\ndemonstrate effectiveness in numerous downstream tasks. This paper navigates\nthe landscape of multilingual LLMs, providing an introductory overview of their\ntechnical aspects. It explains underlying architectures, objective functions,\npre-training data sources, and tokenization methods. This work explores the\nunique features of different model types: encoder-only (mBERT, XLM-R),\ndecoder-only (XGLM, PALM, BLOOM, GPT-3), and encoder-decoder models (mT5,\nmBART). Additionally, it addresses one of the significant limitations of\nmultilingual LLMs - the curse of multilinguality - and discusses current\nattempts to overcome it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Large Language Models (LLMs) have gained large popularity among\nNatural Language Processing (NLP) researchers and practitioners. These models,\ntrained on huge datasets, show proficiency across various languages and\ndemonstrate effectiveness in numerous downstream tasks. This paper navigates\nthe landscape of multilingual LLMs, providing an introductory overview of their\ntechnical aspects. It explains underlying architectures, objective functions,\npre-training data sources, and tokenization methods. This work explores the\nunique features of different model types: encoder-only (mBERT, XLM-R),\ndecoder-only (XGLM, PALM, BLOOM, GPT-3), and encoder-decoder models (mT5,\nmBART). Additionally, it addresses one of the significant limitations of\nmultilingual LLMs - the curse of multilinguality - and discusses current\nattempts to overcome it."
                },
                "authors": [
                    {
                        "name": "Daniil Gurgurov"
                    },
                    {
                        "name": "Tanja Bäumel"
                    },
                    {
                        "name": "Tatiana Anikina"
                    }
                ],
                "author_detail": {
                    "name": "Tatiana Anikina"
                },
                "author": "Tatiana Anikina",
                "arxiv_doi": "10.48550/arXiv.2406.10602",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.48550/arXiv.2406.10602",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.10602v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10602v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18246v1",
                "updated": "2025-04-25T10:46:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    46,
                    56,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T10:46:56Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    46,
                    56,
                    4,
                    115,
                    0
                ],
                "title": "Efficient Single-Pass Training for Multi-Turn Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Single-Pass Training for Multi-Turn Reasoning"
                },
                "summary": "Training Large Language Models ( LLMs) to generate explicit reasoning before\nthey produce an answer has been shown to improve their performance across\nvarious tasks such as mathematics and coding. However, fine-tuning LLMs on\nmulti-turn reasoning datasets presents a unique challenge: LLMs must generate\nreasoning tokens that are excluded from subsequent inputs to the LLM. This\ndiscrepancy prevents us from processing an entire conversation in a single\nforward pass-an optimization readily available when we fine-tune on a\nmulti-turn non-reasoning dataset. This paper proposes a novel approach that\novercomes this limitation through response token duplication and a custom\nattention mask that enforces appropriate visibility constraints. Our approach\nsignificantly reduces the training time and allows efficient fine-tuning on\nmulti-turn reasoning datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Large Language Models ( LLMs) to generate explicit reasoning before\nthey produce an answer has been shown to improve their performance across\nvarious tasks such as mathematics and coding. However, fine-tuning LLMs on\nmulti-turn reasoning datasets presents a unique challenge: LLMs must generate\nreasoning tokens that are excluded from subsequent inputs to the LLM. This\ndiscrepancy prevents us from processing an entire conversation in a single\nforward pass-an optimization readily available when we fine-tune on a\nmulti-turn non-reasoning dataset. This paper proposes a novel approach that\novercomes this limitation through response token duplication and a custom\nattention mask that enforces appropriate visibility constraints. Our approach\nsignificantly reduces the training time and allows efficient fine-tuning on\nmulti-turn reasoning datasets."
                },
                "authors": [
                    {
                        "name": "Ritesh Goru"
                    },
                    {
                        "name": "Shanay Mehta"
                    },
                    {
                        "name": "Prateek Jain"
                    }
                ],
                "author_detail": {
                    "name": "Prateek Jain"
                },
                "author": "Prateek Jain",
                "arxiv_comment": "9 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18241v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18241v1",
                "updated": "2025-04-25T10:39:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    39,
                    42,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T10:39:42Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    39,
                    42,
                    4,
                    115,
                    0
                ],
                "title": "Switch-Based Multi-Part Neural Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Switch-Based Multi-Part Neural Network"
                },
                "summary": "This paper introduces decentralized and modular neural network framework\ndesigned to enhance the scalability, interpretability, and performance of\nartificial intelligence (AI) systems. At the heart of this framework is a\ndynamic switch mechanism that governs the selective activation and training of\nindividual neurons based on input characteristics, allowing neurons to\nspecialize in distinct segments of the data domain. This approach enables\nneurons to learn from disjoint subsets of data, mimicking biological brain\nfunction by promoting task specialization and improving the interpretability of\nneural network behavior. Furthermore, the paper explores the application of\nfederated learning and decentralized training for real-world AI deployments,\nparticularly in edge computing and distributed environments. By simulating\nlocalized training on non-overlapping data subsets, we demonstrate how modular\nnetworks can be efficiently trained and evaluated. The proposed framework also\naddresses scalability, enabling AI systems to handle large datasets and\ndistributed processing while preserving model transparency and\ninterpretability. Finally, we discuss the potential of this approach in\nadvancing the design of scalable, privacy-preserving, and efficient AI systems\nfor diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces decentralized and modular neural network framework\ndesigned to enhance the scalability, interpretability, and performance of\nartificial intelligence (AI) systems. At the heart of this framework is a\ndynamic switch mechanism that governs the selective activation and training of\nindividual neurons based on input characteristics, allowing neurons to\nspecialize in distinct segments of the data domain. This approach enables\nneurons to learn from disjoint subsets of data, mimicking biological brain\nfunction by promoting task specialization and improving the interpretability of\nneural network behavior. Furthermore, the paper explores the application of\nfederated learning and decentralized training for real-world AI deployments,\nparticularly in edge computing and distributed environments. By simulating\nlocalized training on non-overlapping data subsets, we demonstrate how modular\nnetworks can be efficiently trained and evaluated. The proposed framework also\naddresses scalability, enabling AI systems to handle large datasets and\ndistributed processing while preserving model transparency and\ninterpretability. Finally, we discuss the potential of this approach in\nadvancing the design of scalable, privacy-preserving, and efficient AI systems\nfor diverse applications."
                },
                "authors": [
                    {
                        "name": "Surajit Majumder"
                    },
                    {
                        "name": "Paritosh Ranjan"
                    },
                    {
                        "name": "Prodip Roy"
                    },
                    {
                        "name": "Bhuban Padhan"
                    }
                ],
                "author_detail": {
                    "name": "Bhuban Padhan"
                },
                "author": "Bhuban Padhan",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18241v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18241v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18236v1",
                "updated": "2025-04-25T10:34:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    34,
                    0,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T10:34:00Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    34,
                    0,
                    4,
                    115,
                    0
                ],
                "title": "\"Two Means to an End Goal\": Connecting Explainability and Contestability\n  in the Regulation of Public Sector AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Two Means to an End Goal\": Connecting Explainability and Contestability\n  in the Regulation of Public Sector AI"
                },
                "summary": "Explainability and its emerging counterpart contestability have become\nimportant normative and design principles for the trustworthy use of AI as they\nenable users and subjects to understand and challenge AI decisions. However,\nthe regulation of AI systems spans technical, legal, and organizational\ndimensions, producing a multiplicity in meaning that complicates the\nimplementation of explainability and contestability. Resolving this conceptual\nambiguity requires specifying and comparing the meaning of both principles\nacross regulation dimensions, disciplines, and actors. This process, here\ndefined as translation, is essential to provide guidance on the principles'\nrealization. We present the findings of a semi-structured interview study with\n14 interdisciplinary AI regulation experts. We report on the experts'\nunderstanding of the intersection between explainability and contestability in\npublic AI regulation, their advice for a decision subject and a public agency\nin a welfare allocation AI use case, and their perspectives on the connections\nand gaps within the research landscape. We provide differentiations between\ndescriptive and normative explainability, judicial and non-judicial channels of\ncontestation, and individual and collective contestation action. We further\noutline three translation processes in the alignment of top-down and bottom-up\nregulation, the assignment of responsibility for interpreting regulations, and\nthe establishment of interdisciplinary collaboration. Our contributions include\nan empirically grounded conceptualization of the intersection between\nexplainability and contestability and recommendations on implementing these\nprinciples in public institutions. We believe our contributions can inform\npolicy-making and regulation of these core principles and enable more effective\nand equitable design, development, and deployment of trustworthy public AI\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainability and its emerging counterpart contestability have become\nimportant normative and design principles for the trustworthy use of AI as they\nenable users and subjects to understand and challenge AI decisions. However,\nthe regulation of AI systems spans technical, legal, and organizational\ndimensions, producing a multiplicity in meaning that complicates the\nimplementation of explainability and contestability. Resolving this conceptual\nambiguity requires specifying and comparing the meaning of both principles\nacross regulation dimensions, disciplines, and actors. This process, here\ndefined as translation, is essential to provide guidance on the principles'\nrealization. We present the findings of a semi-structured interview study with\n14 interdisciplinary AI regulation experts. We report on the experts'\nunderstanding of the intersection between explainability and contestability in\npublic AI regulation, their advice for a decision subject and a public agency\nin a welfare allocation AI use case, and their perspectives on the connections\nand gaps within the research landscape. We provide differentiations between\ndescriptive and normative explainability, judicial and non-judicial channels of\ncontestation, and individual and collective contestation action. We further\noutline three translation processes in the alignment of top-down and bottom-up\nregulation, the assignment of responsibility for interpreting regulations, and\nthe establishment of interdisciplinary collaboration. Our contributions include\nan empirically grounded conceptualization of the intersection between\nexplainability and contestability and recommendations on implementing these\nprinciples in public institutions. We believe our contributions can inform\npolicy-making and regulation of these core principles and enable more effective\nand equitable design, development, and deployment of trustworthy public AI\nsystems."
                },
                "authors": [
                    {
                        "name": "Timothée Schmude"
                    },
                    {
                        "name": "Mireia Yurrita"
                    },
                    {
                        "name": "Kars Alfrink"
                    },
                    {
                        "name": "Thomas Le Goff"
                    },
                    {
                        "name": "Tiphaine Viard"
                    }
                ],
                "author_detail": {
                    "name": "Tiphaine Viard"
                },
                "author": "Tiphaine Viard",
                "arxiv_comment": "15 pages main text, 3 figures. Supplementary material is provided",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18225v1",
                "updated": "2025-04-25T10:17:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    17,
                    4,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T10:17:04Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    17,
                    4,
                    4,
                    115,
                    0
                ],
                "title": "Even Small Reasoners Should Quote Their Sources: Introducing the\n  Pleias-RAG Model Family",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Even Small Reasoners Should Quote Their Sources: Introducing the\n  Pleias-RAG Model Family"
                },
                "summary": "We introduce a new generation of small reasoning models for RAG, search, and\nsource summarization. Pleias-RAG-350m and Pleias-RAG-1B are mid-trained on a\nlarge synthetic dataset emulating the retrieval of a wide variety of\nmultilingual open sources from the Common Corpus. They provide native support\nfor citation and grounding with literal quotes and reintegrate multiple\nfeatures associated with RAG workflows, such as query routing, query\nreformulation, and source reranking. Pleias-RAG-350m and Pleias-RAG-1B\noutperform SLMs below 4 billion parameters on standardized RAG benchmarks\n(HotPotQA, 2wiki) and are competitive with popular larger models, including\nQwen-2.5-7B, Llama-3.1-8B, and Gemma-3-4B. They are the only SLMs to date\nmaintaining consistent RAG performance across leading European languages and\nensuring systematic reference grounding for statements. Due to their size and\nease of deployment on constrained infrastructure and higher factuality by\ndesign, the models unlock a range of new use cases for generative AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new generation of small reasoning models for RAG, search, and\nsource summarization. Pleias-RAG-350m and Pleias-RAG-1B are mid-trained on a\nlarge synthetic dataset emulating the retrieval of a wide variety of\nmultilingual open sources from the Common Corpus. They provide native support\nfor citation and grounding with literal quotes and reintegrate multiple\nfeatures associated with RAG workflows, such as query routing, query\nreformulation, and source reranking. Pleias-RAG-350m and Pleias-RAG-1B\noutperform SLMs below 4 billion parameters on standardized RAG benchmarks\n(HotPotQA, 2wiki) and are competitive with popular larger models, including\nQwen-2.5-7B, Llama-3.1-8B, and Gemma-3-4B. They are the only SLMs to date\nmaintaining consistent RAG performance across leading European languages and\nensuring systematic reference grounding for statements. Due to their size and\nease of deployment on constrained infrastructure and higher factuality by\ndesign, the models unlock a range of new use cases for generative AI."
                },
                "authors": [
                    {
                        "name": "Pierre-Carl Langlais"
                    },
                    {
                        "name": "Pavel Chizhov"
                    },
                    {
                        "name": "Mattia Nee"
                    },
                    {
                        "name": "Carlos Rosas Hinostroza"
                    },
                    {
                        "name": "Matthieu Delsart"
                    },
                    {
                        "name": "Irène Girard"
                    },
                    {
                        "name": "Othman Hicheur"
                    },
                    {
                        "name": "Anastasia Stasenko"
                    },
                    {
                        "name": "Ivan P. Yamshchikov"
                    }
                ],
                "author_detail": {
                    "name": "Ivan P. Yamshchikov"
                },
                "author": "Ivan P. Yamshchikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16048v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16048v2",
                "updated": "2025-04-25T09:59:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    9,
                    59,
                    56,
                    4,
                    115,
                    0
                ],
                "published": "2024-09-24T12:51:32Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    12,
                    51,
                    32,
                    1,
                    268,
                    0
                ],
                "title": "Whole-body End-Effector Pose Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whole-body End-Effector Pose Tracking"
                },
                "summary": "Combining manipulation with the mobility of legged robots is essential for a\nwide range of robotic applications. However, integrating an arm with a mobile\nbase significantly increases the system's complexity, making precise\nend-effector control challenging. Existing model-based approaches are often\nconstrained by their modeling assumptions, leading to limited robustness.\nMeanwhile, recent Reinforcement Learning (RL) implementations restrict the\narm's workspace to be in front of the robot or track only the position to\nobtain decent tracking accuracy. In this work, we address these limitations by\nintroducing a whole-body RL formulation for end-effector pose tracking in a\nlarge workspace on rough, unstructured terrains. Our proposed method involves a\nterrain-aware sampling strategy for the robot's initial configuration and\nend-effector pose commands, as well as a game-based curriculum to extend the\nrobot's operating range. We validate our approach on the ANYmal quadrupedal\nrobot with a six DoF robotic arm. Through our experiments, we show that the\nlearned controller achieves precise command tracking over a large workspace and\nadapts across varying terrains such as stairs and slopes. On deployment, it\nachieves a pose-tracking error of 2.64 cm and 3.64 degrees, outperforming\nexisting competitive baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining manipulation with the mobility of legged robots is essential for a\nwide range of robotic applications. However, integrating an arm with a mobile\nbase significantly increases the system's complexity, making precise\nend-effector control challenging. Existing model-based approaches are often\nconstrained by their modeling assumptions, leading to limited robustness.\nMeanwhile, recent Reinforcement Learning (RL) implementations restrict the\narm's workspace to be in front of the robot or track only the position to\nobtain decent tracking accuracy. In this work, we address these limitations by\nintroducing a whole-body RL formulation for end-effector pose tracking in a\nlarge workspace on rough, unstructured terrains. Our proposed method involves a\nterrain-aware sampling strategy for the robot's initial configuration and\nend-effector pose commands, as well as a game-based curriculum to extend the\nrobot's operating range. We validate our approach on the ANYmal quadrupedal\nrobot with a six DoF robotic arm. Through our experiments, we show that the\nlearned controller achieves precise command tracking over a large workspace and\nadapts across varying terrains such as stairs and slopes. On deployment, it\nachieves a pose-tracking error of 2.64 cm and 3.64 degrees, outperforming\nexisting competitive baselines."
                },
                "authors": [
                    {
                        "name": "Tifanny Portela"
                    },
                    {
                        "name": "Andrei Cramariuc"
                    },
                    {
                        "name": "Mayank Mittal"
                    },
                    {
                        "name": "Marco Hutter"
                    }
                ],
                "author_detail": {
                    "name": "Marco Hutter"
                },
                "author": "Marco Hutter",
                "arxiv_journal_ref": "ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16048v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16048v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19662v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19662v2",
                "updated": "2025-04-25T09:54:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    9,
                    54,
                    59,
                    4,
                    115,
                    0
                ],
                "published": "2025-02-27T01:08:33Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    1,
                    8,
                    33,
                    3,
                    58,
                    0
                ],
                "title": "HALO: Hardware-aware quantization with low critical-path-delay weights\n  for LLM acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HALO: Hardware-aware quantization with low critical-path-delay weights\n  for LLM acceleration"
                },
                "summary": "Quantization is critical for efficiently deploying large language models\n(LLMs). Yet conventional methods remain hardware-agnostic, limited to bit-width\nconstraints, and do not account for intrinsic circuit characteristics such as\nthe timing behaviors and energy profiles of Multiply-Accumulate (MAC) units.\nThis disconnect from circuit-level behavior limits the ability to exploit\navailable timing margins and energy-saving opportunities, reducing the overall\nefficiency of deployment on modern accelerators.\n  To address these limitations, we propose HALO, a versatile framework for\nHardware-Aware Post-Training Quantization (PTQ). Unlike traditional methods,\nHALO explicitly incorporates detailed hardware characteristics, including\ncritical-path timing and power consumption, into its quantization approach.\nHALO strategically selects weights with low critical-path-delays enabling\nhigher operational frequencies and dynamic frequency scaling without disrupting\nthe architecture's dataflow. Remarkably, HALO achieves these improvements with\nonly a few dynamic voltage and frequency scaling (DVFS) adjustments, ensuring\nsimplicity and practicality in deployment. Additionally, by reducing switching\nactivity within the MAC units, HALO effectively lowers energy consumption.\nEvaluations on accelerators such as Tensor Processing Units (TPUs) and Graphics\nProcessing Units (GPUs) demonstrate that HALO significantly enhances inference\nefficiency, achieving average performance improvements of 270% and energy\nsavings of 51% over baseline quantization methods, all with minimal impact on\naccuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is critical for efficiently deploying large language models\n(LLMs). Yet conventional methods remain hardware-agnostic, limited to bit-width\nconstraints, and do not account for intrinsic circuit characteristics such as\nthe timing behaviors and energy profiles of Multiply-Accumulate (MAC) units.\nThis disconnect from circuit-level behavior limits the ability to exploit\navailable timing margins and energy-saving opportunities, reducing the overall\nefficiency of deployment on modern accelerators.\n  To address these limitations, we propose HALO, a versatile framework for\nHardware-Aware Post-Training Quantization (PTQ). Unlike traditional methods,\nHALO explicitly incorporates detailed hardware characteristics, including\ncritical-path timing and power consumption, into its quantization approach.\nHALO strategically selects weights with low critical-path-delays enabling\nhigher operational frequencies and dynamic frequency scaling without disrupting\nthe architecture's dataflow. Remarkably, HALO achieves these improvements with\nonly a few dynamic voltage and frequency scaling (DVFS) adjustments, ensuring\nsimplicity and practicality in deployment. Additionally, by reducing switching\nactivity within the MAC units, HALO effectively lowers energy consumption.\nEvaluations on accelerators such as Tensor Processing Units (TPUs) and Graphics\nProcessing Units (GPUs) demonstrate that HALO significantly enhances inference\nefficiency, achieving average performance improvements of 270% and energy\nsavings of 51% over baseline quantization methods, all with minimal impact on\naccuracy."
                },
                "authors": [
                    {
                        "name": "Rohan Juneja"
                    },
                    {
                        "name": "Shivam Aggarwal"
                    },
                    {
                        "name": "Safeen Huda"
                    },
                    {
                        "name": "Tulika Mitra"
                    },
                    {
                        "name": "Li-Shiuan Peh"
                    }
                ],
                "author_detail": {
                    "name": "Li-Shiuan Peh"
                },
                "author": "Li-Shiuan Peh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19662v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19662v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17671v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17671v2",
                "updated": "2025-04-25T09:34:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    9,
                    34,
                    59,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-24T15:39:46Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    39,
                    46,
                    3,
                    114,
                    0
                ],
                "title": "Data-Driven Calibration of Prediction Sets in Large Vision-Language\n  Models Based on Inductive Conformal Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Driven Calibration of Prediction Sets in Large Vision-Language\n  Models Based on Inductive Conformal Prediction"
                },
                "summary": "This study addresses the critical challenge of hallucination mitigation in\nLarge Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks\nthrough a Split Conformal Prediction (SCP) framework. While LVLMs excel in\nmulti-modal reasoning, their outputs often exhibit hallucinated content with\nhigh confidence, posing risks in safety-critical applications. We propose a\nmodel-agnostic uncertainty quantification method that integrates dynamic\nthreshold calibration and cross-modal consistency verification. By partitioning\ndata into calibration and test sets, the framework computes nonconformity\nscores to construct prediction sets with statistical guarantees under\nuser-defined risk levels ($\\alpha$). Key innovations include: (1) rigorous\ncontrol of \\textbf{marginal coverage} to ensure empirical error rates remain\nstrictly below $\\alpha$; (2) dynamic adjustment of prediction set sizes\ninversely with $\\alpha$, filtering low-confidence outputs; (3) elimination of\nprior distribution assumptions and retraining requirements. Evaluations on\nbenchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces\ntheoretical guarantees across all $\\alpha$ values. The framework achieves\nstable performance across varying calibration-to-test split ratios,\nunderscoring its robustness for real-world deployment in healthcare, autonomous\nsystems, and other safety-sensitive domains. This work bridges the gap between\ntheoretical reliability and practical applicability in multi-modal AI systems,\noffering a scalable solution for hallucination detection and uncertainty-aware\ndecision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study addresses the critical challenge of hallucination mitigation in\nLarge Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks\nthrough a Split Conformal Prediction (SCP) framework. While LVLMs excel in\nmulti-modal reasoning, their outputs often exhibit hallucinated content with\nhigh confidence, posing risks in safety-critical applications. We propose a\nmodel-agnostic uncertainty quantification method that integrates dynamic\nthreshold calibration and cross-modal consistency verification. By partitioning\ndata into calibration and test sets, the framework computes nonconformity\nscores to construct prediction sets with statistical guarantees under\nuser-defined risk levels ($\\alpha$). Key innovations include: (1) rigorous\ncontrol of \\textbf{marginal coverage} to ensure empirical error rates remain\nstrictly below $\\alpha$; (2) dynamic adjustment of prediction set sizes\ninversely with $\\alpha$, filtering low-confidence outputs; (3) elimination of\nprior distribution assumptions and retraining requirements. Evaluations on\nbenchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces\ntheoretical guarantees across all $\\alpha$ values. The framework achieves\nstable performance across varying calibration-to-test split ratios,\nunderscoring its robustness for real-world deployment in healthcare, autonomous\nsystems, and other safety-sensitive domains. This work bridges the gap between\ntheoretical reliability and practical applicability in multi-modal AI systems,\noffering a scalable solution for hallucination detection and uncertainty-aware\ndecision-making."
                },
                "authors": [
                    {
                        "name": "Yuanchang Ye"
                    },
                    {
                        "name": "Weiyan Wen"
                    }
                ],
                "author_detail": {
                    "name": "Weiyan Wen"
                },
                "author": "Weiyan Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17671v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17671v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18200v1",
                "updated": "2025-04-25T09:30:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    9,
                    30,
                    43,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T09:30:43Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    9,
                    30,
                    43,
                    4,
                    115,
                    0
                ],
                "title": "Implementation Analysis of Collaborative Robot Digital Twins in Physics\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implementation Analysis of Collaborative Robot Digital Twins in Physics\n  Engines"
                },
                "summary": "This paper presents a Digital Twin (DT) of a 6G communications system testbed\nthat integrates two robotic manipulators with a high-precision optical infrared\ntracking system in Unreal Engine 5. Practical details of the setup and\nimplementation insights provide valuable guidance for users aiming to replicate\nsuch systems, an endeavor that is crucial to advancing DT applications within\nthe scientific community. Key topics discussed include video streaming,\nintegration within the Robot Operating System 2 (ROS 2), and bidirectional\ncommunication. The insights provided are intended to support the development\nand deployment of DTs in robotics and automation research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a Digital Twin (DT) of a 6G communications system testbed\nthat integrates two robotic manipulators with a high-precision optical infrared\ntracking system in Unreal Engine 5. Practical details of the setup and\nimplementation insights provide valuable guidance for users aiming to replicate\nsuch systems, an endeavor that is crucial to advancing DT applications within\nthe scientific community. Key topics discussed include video streaming,\nintegration within the Robot Operating System 2 (ROS 2), and bidirectional\ncommunication. The insights provided are intended to support the development\nand deployment of DTs in robotics and automation research."
                },
                "authors": [
                    {
                        "name": "Christian König"
                    },
                    {
                        "name": "Jan Petershans"
                    },
                    {
                        "name": "Jan Herbst"
                    },
                    {
                        "name": "Matthias Rüb"
                    },
                    {
                        "name": "Dennis Krummacker"
                    },
                    {
                        "name": "Eric Mittag"
                    },
                    {
                        "name": "Hand D. Schooten"
                    }
                ],
                "author_detail": {
                    "name": "Hand D. Schooten"
                },
                "author": "Hand D. Schooten",
                "arxiv_comment": "15 Pages, 3 figures, 7th International Congress on Human-Computer\n  Interaction, Optimization and Robotic Applications (ICHORA 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18195v1",
                "updated": "2025-04-25T09:18:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    9,
                    18,
                    52,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T09:18:52Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    9,
                    18,
                    52,
                    4,
                    115,
                    0
                ],
                "title": "Copper-impurity-free photonic integrated circuits enable deterministic\n  soliton microcombs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Copper-impurity-free photonic integrated circuits enable deterministic\n  soliton microcombs"
                },
                "summary": "Chip-scale optical frequency combs based on microresonators (microcombs)\nenable GHz-THz repetition rates, broad bandwidth, compactness, and\ncompatibility with wafer-scale manufacturing. Silicon nitride photonic\nintegrated circuits have become a leading platform due to their low loss, broad\ntransparency, lithographic dispersion control, and commercial 200-mm-wafer\nfoundry access. They have enabled system-level applications in optical\ncommunications, LiDAR, frequency synthesis, low-noise microwave generation, and\nconvolutional processing. However, real-world deployment is hindered by the\nchallenge of deterministic soliton microcomb generation, primarily due to\nthermal instabilities. Although techniques like pulsed pumping, fast scanning,\nand auxiliary lasers help mitigate these effects, they often add complexity or\nreduce soliton stability. In this work, we overcome thermal limitations and\ndemonstrate deterministic soliton generation in silicon nitride photonic\ncircuits. We trace the thermal effects to copper impurities within waveguides,\noriginating from residual contaminants in CMOS-grade silicon wafers that are\ngettered into silicon nitride during fabrication. By developing effective\ncopper removal techniques, we significantly reduce thermal instabilities. This\nenables soliton generation with arbitrary or slow laser scanning, removing a\nkey barrier to microcomb deployment. Our approach is compatible with\nfront-end-of-line foundry processing, paving the way for broader adoption of\nsoliton microcomb technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chip-scale optical frequency combs based on microresonators (microcombs)\nenable GHz-THz repetition rates, broad bandwidth, compactness, and\ncompatibility with wafer-scale manufacturing. Silicon nitride photonic\nintegrated circuits have become a leading platform due to their low loss, broad\ntransparency, lithographic dispersion control, and commercial 200-mm-wafer\nfoundry access. They have enabled system-level applications in optical\ncommunications, LiDAR, frequency synthesis, low-noise microwave generation, and\nconvolutional processing. However, real-world deployment is hindered by the\nchallenge of deterministic soliton microcomb generation, primarily due to\nthermal instabilities. Although techniques like pulsed pumping, fast scanning,\nand auxiliary lasers help mitigate these effects, they often add complexity or\nreduce soliton stability. In this work, we overcome thermal limitations and\ndemonstrate deterministic soliton generation in silicon nitride photonic\ncircuits. We trace the thermal effects to copper impurities within waveguides,\noriginating from residual contaminants in CMOS-grade silicon wafers that are\ngettered into silicon nitride during fabrication. By developing effective\ncopper removal techniques, we significantly reduce thermal instabilities. This\nenables soliton generation with arbitrary or slow laser scanning, removing a\nkey barrier to microcomb deployment. Our approach is compatible with\nfront-end-of-line foundry processing, paving the way for broader adoption of\nsoliton microcomb technologies."
                },
                "authors": [
                    {
                        "name": "Xinru Ji"
                    },
                    {
                        "name": "Xurong Li"
                    },
                    {
                        "name": "Zheru Qiu"
                    },
                    {
                        "name": "Rui Ning Wang"
                    },
                    {
                        "name": "Marta Divall"
                    },
                    {
                        "name": "Andrey Gelash"
                    },
                    {
                        "name": "Grigory Lihachev"
                    },
                    {
                        "name": "Tobias J. Kippenberg"
                    }
                ],
                "author_detail": {
                    "name": "Tobias J. Kippenberg"
                },
                "author": "Tobias J. Kippenberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19912v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19912v3",
                "updated": "2025-04-25T09:16:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    9,
                    16,
                    54,
                    4,
                    115,
                    0
                ],
                "published": "2024-05-30T10:23:16Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    10,
                    23,
                    16,
                    3,
                    151,
                    0
                ],
                "title": "Robust Kernel Hypothesis Testing under Data Corruption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Kernel Hypothesis Testing under Data Corruption"
                },
                "summary": "We propose a general method for constructing robust permutation tests under\ndata corruption. The proposed tests effectively control the non-asymptotic type\nI error under data corruption, and we prove their consistency in power under\nminimal conditions. This contributes to the practical deployment of hypothesis\ntests for real-world applications with potential adversarial attacks. For the\ntwo-sample and independence settings, we show that our kernel robust tests are\nminimax optimal, in the sense that they are guaranteed to be non-asymptotically\npowerful against alternatives uniformly separated from the null in the kernel\nMMD and HSIC metrics at some optimal rate (tight with matching lower bound). We\npoint out that existing differentially private tests can be adapted to be\nrobust to data corruption, and we demonstrate in experiments that our proposed\ntests achieve much higher power than these private tests. Finally, we provide\npublicly available implementations and empirically illustrate the practicality\nof our robust tests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a general method for constructing robust permutation tests under\ndata corruption. The proposed tests effectively control the non-asymptotic type\nI error under data corruption, and we prove their consistency in power under\nminimal conditions. This contributes to the practical deployment of hypothesis\ntests for real-world applications with potential adversarial attacks. For the\ntwo-sample and independence settings, we show that our kernel robust tests are\nminimax optimal, in the sense that they are guaranteed to be non-asymptotically\npowerful against alternatives uniformly separated from the null in the kernel\nMMD and HSIC metrics at some optimal rate (tight with matching lower bound). We\npoint out that existing differentially private tests can be adapted to be\nrobust to data corruption, and we demonstrate in experiments that our proposed\ntests achieve much higher power than these private tests. Finally, we provide\npublicly available implementations and empirically illustrate the practicality\nof our robust tests."
                },
                "authors": [
                    {
                        "name": "Antonin Schrab"
                    },
                    {
                        "name": "Ilmun Kim"
                    }
                ],
                "author_detail": {
                    "name": "Ilmun Kim"
                },
                "author": "Ilmun Kim",
                "arxiv_comment": "22 pages, 2 figures, 2 algorithms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19912v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19912v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16834v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16834v2",
                "updated": "2025-04-25T08:51:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    8,
                    51,
                    9,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-23T15:56:28Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    56,
                    28,
                    2,
                    113,
                    0
                ],
                "title": "Improving Significant Wave Height Prediction Using Chronos Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Significant Wave Height Prediction Using Chronos Models"
                },
                "summary": "Accurate wave height prediction is critical for maritime safety and coastal\nresilience, yet conventional physics-based models and traditional machine\nlearning methods face challenges in computational efficiency and nonlinear\ndynamics modeling. This study introduces Chronos, the first implementation of a\nlarge language model (LLM)-powered temporal architecture (Chronos) optimized\nfor wave forecasting. Through advanced temporal pattern recognition applied to\nhistorical wave data from three strategically chosen marine zones in the\nNorthwest Pacific basin, our framework achieves multimodal improvements: (1)\n14.3% reduction in training time with 2.5x faster inference speed compared to\nPatchTST baselines, achieving 0.575 mean absolute scaled error (MASE) units;\n(2) superior short-term forecasting (1-24h) across comprehensive metrics; (3)\nsustained predictive leadership in extended-range forecasts (1-120h); and (4)\ndemonstrated zero-shot capability maintaining median performance (rank 4/12)\nagainst specialized operational models. This LLM-enhanced temporal modeling\nparadigm establishes a new standard in wave prediction, offering both\ncomputationally efficient solutions and a transferable framework for complex\ngeophysical systems modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate wave height prediction is critical for maritime safety and coastal\nresilience, yet conventional physics-based models and traditional machine\nlearning methods face challenges in computational efficiency and nonlinear\ndynamics modeling. This study introduces Chronos, the first implementation of a\nlarge language model (LLM)-powered temporal architecture (Chronos) optimized\nfor wave forecasting. Through advanced temporal pattern recognition applied to\nhistorical wave data from three strategically chosen marine zones in the\nNorthwest Pacific basin, our framework achieves multimodal improvements: (1)\n14.3% reduction in training time with 2.5x faster inference speed compared to\nPatchTST baselines, achieving 0.575 mean absolute scaled error (MASE) units;\n(2) superior short-term forecasting (1-24h) across comprehensive metrics; (3)\nsustained predictive leadership in extended-range forecasts (1-120h); and (4)\ndemonstrated zero-shot capability maintaining median performance (rank 4/12)\nagainst specialized operational models. This LLM-enhanced temporal modeling\nparadigm establishes a new standard in wave prediction, offering both\ncomputationally efficient solutions and a transferable framework for complex\ngeophysical systems modeling."
                },
                "authors": [
                    {
                        "name": "Yilin Zhai"
                    },
                    {
                        "name": "Hongyuan Shi"
                    },
                    {
                        "name": "Chao Zhan"
                    },
                    {
                        "name": "Qing Wang"
                    },
                    {
                        "name": "Zaijin You"
                    },
                    {
                        "name": "Nan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Nan Wang"
                },
                "author": "Nan Wang",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2403.07815 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16834v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16834v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18165v1",
                "updated": "2025-04-25T08:29:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    8,
                    29,
                    0,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T08:29:00Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    8,
                    29,
                    0,
                    4,
                    115,
                    0
                ],
                "title": "PerfCam: Digital Twinning for Production Lines Using 3D Gaussian\n  Splatting and Vision Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PerfCam: Digital Twinning for Production Lines Using 3D Gaussian\n  Splatting and Vision Models"
                },
                "summary": "We introduce PerfCam, an open source Proof-of-Concept (PoC) digital twinning\nframework that combines camera and sensory data with 3D Gaussian Splatting and\ncomputer vision models for digital twinning, object tracking, and Key\nPerformance Indicators (KPIs) extraction in industrial production lines. By\nutilizing 3D reconstruction and Convolutional Neural Networks (CNNs), PerfCam\noffers a semi-automated approach to object tracking and spatial mapping,\nenabling digital twins that capture real-time KPIs such as availability,\nperformance, Overall Equipment Effectiveness (OEE), and rate of conveyor belts\nin the production line. We validate the effectiveness of PerfCam through a\npractical deployment within realistic test production lines in the\npharmaceutical industry and contribute an openly published dataset to support\nfurther research and development in the field. The results demonstrate\nPerfCam's ability to deliver actionable insights through its precise digital\ntwin capabilities, underscoring its value as an effective tool for developing\nusable digital twins in smart manufacturing environments and extracting\noperational analytics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce PerfCam, an open source Proof-of-Concept (PoC) digital twinning\nframework that combines camera and sensory data with 3D Gaussian Splatting and\ncomputer vision models for digital twinning, object tracking, and Key\nPerformance Indicators (KPIs) extraction in industrial production lines. By\nutilizing 3D reconstruction and Convolutional Neural Networks (CNNs), PerfCam\noffers a semi-automated approach to object tracking and spatial mapping,\nenabling digital twins that capture real-time KPIs such as availability,\nperformance, Overall Equipment Effectiveness (OEE), and rate of conveyor belts\nin the production line. We validate the effectiveness of PerfCam through a\npractical deployment within realistic test production lines in the\npharmaceutical industry and contribute an openly published dataset to support\nfurther research and development in the field. The results demonstrate\nPerfCam's ability to deliver actionable insights through its precise digital\ntwin capabilities, underscoring its value as an effective tool for developing\nusable digital twins in smart manufacturing environments and extracting\noperational analytics."
                },
                "authors": [
                    {
                        "name": "Michel Gokan Khan"
                    },
                    {
                        "name": "Renan Guarese"
                    },
                    {
                        "name": "Fabian Johnson"
                    },
                    {
                        "name": "Xi Vincent Wang"
                    },
                    {
                        "name": "Anders Bergman"
                    },
                    {
                        "name": "Benjamin Edvinsson"
                    },
                    {
                        "name": "Mario Romero"
                    },
                    {
                        "name": "Jérémy Vachier"
                    },
                    {
                        "name": "Jan Kronqvist"
                    }
                ],
                "author_detail": {
                    "name": "Jan Kronqvist"
                },
                "author": "Jan Kronqvist",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16968v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16968v2",
                "updated": "2025-04-25T08:26:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    8,
                    26,
                    21,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-23T12:28:27Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    12,
                    28,
                    27,
                    2,
                    113,
                    0
                ],
                "title": "BackSlash: Rate Constrained Optimized Training of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BackSlash: Rate Constrained Optimized Training of Large Language Models"
                },
                "summary": "The rapid advancement of large-language models (LLMs) has driven extensive\nresearch into parameter compression after training has been completed, yet\ncompression during the training phase remains largely unexplored. In this work,\nwe introduce Rate-Constrained Training (BackSlash), a novel training-time\ncompression approach based on rate-distortion optimization (RDO). BackSlash\nenables a flexible trade-off between model accuracy and complexity,\nsignificantly reducing parameter redundancy while preserving performance.\nExperiments in various architectures and tasks demonstrate that BackSlash can\nreduce memory usage by 60% - 90% without accuracy loss and provides significant\ncompression gain compared to compression after training. Moreover, BackSlash\nproves to be highly versatile: it enhances generalization with small Lagrange\nmultipliers, improves model robustness to pruning (maintaining accuracy even at\n80% pruning rates), and enables network simplification for accelerated\ninference on edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large-language models (LLMs) has driven extensive\nresearch into parameter compression after training has been completed, yet\ncompression during the training phase remains largely unexplored. In this work,\nwe introduce Rate-Constrained Training (BackSlash), a novel training-time\ncompression approach based on rate-distortion optimization (RDO). BackSlash\nenables a flexible trade-off between model accuracy and complexity,\nsignificantly reducing parameter redundancy while preserving performance.\nExperiments in various architectures and tasks demonstrate that BackSlash can\nreduce memory usage by 60% - 90% without accuracy loss and provides significant\ncompression gain compared to compression after training. Moreover, BackSlash\nproves to be highly versatile: it enhances generalization with small Lagrange\nmultipliers, improves model robustness to pruning (maintaining accuracy even at\n80% pruning rates), and enables network simplification for accelerated\ninference on edge devices."
                },
                "authors": [
                    {
                        "name": "Jun Wu"
                    },
                    {
                        "name": "Jiangtao Wen"
                    },
                    {
                        "name": "Yuxing Han"
                    }
                ],
                "author_detail": {
                    "name": "Yuxing Han"
                },
                "author": "Yuxing Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16968v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16968v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17419v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17419v3",
                "updated": "2025-04-25T08:15:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    8,
                    15,
                    51,
                    4,
                    115,
                    0
                ],
                "published": "2025-02-24T18:50:52Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    50,
                    52,
                    0,
                    55,
                    0
                ],
                "title": "From System 1 to System 2: A Survey of Reasoning Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From System 1 to System 2: A Survey of Reasoning Large Language Models"
                },
                "summary": "Achieving human-level intelligence requires refining the transition from the\nfast, intuitive System 1 to the slower, more deliberate System 2 reasoning.\nWhile System 1 excels in quick, heuristic decisions, System 2 relies on logical\nreasoning for more accurate judgments and reduced biases. Foundational Large\nLanguage Models (LLMs) excel at fast decision-making but lack the depth for\ncomplex reasoning, as they have not yet fully embraced the step-by-step\nanalysis characteristic of true System 2 thinking. Recently, reasoning LLMs\nlike OpenAI's o1/o3 and DeepSeek's R1 have demonstrated expert-level\nperformance in fields such as mathematics and coding, closely mimicking the\ndeliberate reasoning of System 2 and showcasing human-like cognitive abilities.\nThis survey begins with a brief overview of the progress in foundational LLMs\nand the early development of System 2 technologies, exploring how their\ncombination has paved the way for reasoning LLMs. Next, we discuss how to\nconstruct reasoning LLMs, analyzing their features, the core methods enabling\nadvanced reasoning, and the evolution of various reasoning LLMs. Additionally,\nwe provide an overview of reasoning benchmarks, offering an in-depth comparison\nof the performance of representative reasoning LLMs. Finally, we explore\npromising directions for advancing reasoning LLMs and maintain a real-time\n\\href{https://github.com/zzli2022/Awesome-Slow-Reason-System}{GitHub\nRepository} to track the latest developments. We hope this survey will serve as\na valuable resource to inspire innovation and drive progress in this rapidly\nevolving field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving human-level intelligence requires refining the transition from the\nfast, intuitive System 1 to the slower, more deliberate System 2 reasoning.\nWhile System 1 excels in quick, heuristic decisions, System 2 relies on logical\nreasoning for more accurate judgments and reduced biases. Foundational Large\nLanguage Models (LLMs) excel at fast decision-making but lack the depth for\ncomplex reasoning, as they have not yet fully embraced the step-by-step\nanalysis characteristic of true System 2 thinking. Recently, reasoning LLMs\nlike OpenAI's o1/o3 and DeepSeek's R1 have demonstrated expert-level\nperformance in fields such as mathematics and coding, closely mimicking the\ndeliberate reasoning of System 2 and showcasing human-like cognitive abilities.\nThis survey begins with a brief overview of the progress in foundational LLMs\nand the early development of System 2 technologies, exploring how their\ncombination has paved the way for reasoning LLMs. Next, we discuss how to\nconstruct reasoning LLMs, analyzing their features, the core methods enabling\nadvanced reasoning, and the evolution of various reasoning LLMs. Additionally,\nwe provide an overview of reasoning benchmarks, offering an in-depth comparison\nof the performance of representative reasoning LLMs. Finally, we explore\npromising directions for advancing reasoning LLMs and maintain a real-time\n\\href{https://github.com/zzli2022/Awesome-Slow-Reason-System}{GitHub\nRepository} to track the latest developments. We hope this survey will serve as\na valuable resource to inspire innovation and drive progress in this rapidly\nevolving field."
                },
                "authors": [
                    {
                        "name": "Zhong-Zhi Li"
                    },
                    {
                        "name": "Duzhen Zhang"
                    },
                    {
                        "name": "Ming-Liang Zhang"
                    },
                    {
                        "name": "Jiaxin Zhang"
                    },
                    {
                        "name": "Zengyan Liu"
                    },
                    {
                        "name": "Yuxuan Yao"
                    },
                    {
                        "name": "Haotian Xu"
                    },
                    {
                        "name": "Junhao Zheng"
                    },
                    {
                        "name": "Pei-Jie Wang"
                    },
                    {
                        "name": "Xiuyi Chen"
                    },
                    {
                        "name": "Yingying Zhang"
                    },
                    {
                        "name": "Fei Yin"
                    },
                    {
                        "name": "Jiahua Dong"
                    },
                    {
                        "name": "Zhiwei Li"
                    },
                    {
                        "name": "Bao-Long Bi"
                    },
                    {
                        "name": "Ling-Rui Mei"
                    },
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Le Song"
                    },
                    {
                        "name": "Cheng-Lin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Cheng-Lin Liu"
                },
                "author": "Cheng-Lin Liu",
                "arxiv_comment": "Slow-thinking, Large Language Models, Human-like Reasoning, Decision\n  Making in AI, AGI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17419v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17419v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18155v1",
                "updated": "2025-04-25T08:07:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    8,
                    7,
                    42,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T08:07:42Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    8,
                    7,
                    42,
                    4,
                    115,
                    0
                ],
                "title": "Hierarchical Cell-Free Massive MIMO: A Simplified Design for Uniform\n  Service Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Cell-Free Massive MIMO: A Simplified Design for Uniform\n  Service Quality"
                },
                "summary": "In traditional cellular networks, users at the cell edge often suffer from\npoor quality of service (QoS) due to large distance-dependent path loss and\nsevere inter-cell interference. While cell-free (CF) massive multi-input\nmulti-out (MIMO) mitigates this issue by distributing access points (APs) to\nensure uniform QoS, the deployment of numerous distributed APs and a fronthaul\nnetwork incurs high infrastructure costs. To balance performance and cost\nefficiency, this article proposes a simplified design called hierarchical\ncell-free (HCF) massive MIMO. The key idea is to reduce the number of APs, thus\nminimizing the scale of the fronthaul network. The antennas from the\ndecommissioned APs are aggregated at a central base station (cBS), which also\nserves as the coordinator for distributed APs. We derive closed-form\nexpressions for uplink and downlink spectral efficiency (SE) for HCF, CF, and\ncellular massive MIMO under pilot contamination and correlated fading channels,\nconsidering the use of multi-antenna APs. Numerical results confirm that the\nhierarchical architecture achieves $95\\%$-likely per-user SE comparable to CF,\nenhancing cell-edge user rates in cellular systems by over 100 times, while\nsignificantly reducing the complexity and cost of the fronthaul network in CF.\nWe develop max-min fairness algorithms for joint power control of the cBS and\nAPs in the downlink, and the users in the uplink. These algorithms not only\nboost fairness and system capacity but also dramatically lower transmission\npower, e.g., achieving over $70\\%$ savings in uplink, particularly beneficial\nfor battery-powered mobile devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In traditional cellular networks, users at the cell edge often suffer from\npoor quality of service (QoS) due to large distance-dependent path loss and\nsevere inter-cell interference. While cell-free (CF) massive multi-input\nmulti-out (MIMO) mitigates this issue by distributing access points (APs) to\nensure uniform QoS, the deployment of numerous distributed APs and a fronthaul\nnetwork incurs high infrastructure costs. To balance performance and cost\nefficiency, this article proposes a simplified design called hierarchical\ncell-free (HCF) massive MIMO. The key idea is to reduce the number of APs, thus\nminimizing the scale of the fronthaul network. The antennas from the\ndecommissioned APs are aggregated at a central base station (cBS), which also\nserves as the coordinator for distributed APs. We derive closed-form\nexpressions for uplink and downlink spectral efficiency (SE) for HCF, CF, and\ncellular massive MIMO under pilot contamination and correlated fading channels,\nconsidering the use of multi-antenna APs. Numerical results confirm that the\nhierarchical architecture achieves $95\\%$-likely per-user SE comparable to CF,\nenhancing cell-edge user rates in cellular systems by over 100 times, while\nsignificantly reducing the complexity and cost of the fronthaul network in CF.\nWe develop max-min fairness algorithms for joint power control of the cBS and\nAPs in the downlink, and the users in the uplink. These algorithms not only\nboost fairness and system capacity but also dramatically lower transmission\npower, e.g., achieving over $70\\%$ savings in uplink, particularly beneficial\nfor battery-powered mobile devices."
                },
                "authors": [
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Hans Dieter Schotten"
                    }
                ],
                "author_detail": {
                    "name": "Hans Dieter Schotten"
                },
                "author": "Hans Dieter Schotten",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18154v1",
                "updated": "2025-04-25T08:06:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    8,
                    6,
                    22,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T08:06:22Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    8,
                    6,
                    22,
                    4,
                    115,
                    0
                ],
                "title": "EcoServe: Enabling Cost-effective LLM Serving with Proactive Intra- and\n  Inter-Instance Orchestration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EcoServe: Enabling Cost-effective LLM Serving with Proactive Intra- and\n  Inter-Instance Orchestration"
                },
                "summary": "Existing LLM serving strategies can be categorized based on whether prefill\nand decode phases are disaggregated: non-disaggregated (NoDG) or fully\ndisaggregated (FuDG). However, the NoDG strategy leads to strong prefill-decode\ninterference and the FuDG strategy highly relies on high-performance\ninterconnects, making them less cost-effective.\n  We introduce EcoServe, a system that enables cost-effective LLM serving on\nclusters with commodity interconnects. EcoServe is built on the partially\ndisaggregated (PaDG) strategy, applying temporal disaggregation and rolling\nactivation for proactive intra- and inter-instance scheduling. It first\ndisaggregates the prefill and decode phases along the time dimension within a\nsingle instance to mitigate inter-phase interference and enhance throughput.\nNext, it coordinates multiple instances and cyclically activates them to ensure\nthe continuous availability of prefill processing, thereby improving latency.\nThus, EcoServe's basic serving unit is the macro instance, within which\nmultiple instances collaborate. It further integrates an adaptive scheduling\nalgorithm to route requests in a macro instance and a mitosis scaling approach\nto enable fine-grained capacity scaling. Beyond delivering high goodput,\nEcoServe excels in load balancing, hardware cost, parallelism compatibility,\nand even engineering simplicity compared to existing solutions.\n  When serving 30B- and 70B-scale models on a production-level cluster with 32\nNVIDIA L20 GPUs using commodity Ethernet, EcoServe averagely improves goodput\nby 82.49%, 86.17%, 122.76%, and 126.96% over four representative NoDG and FuDG\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM serving strategies can be categorized based on whether prefill\nand decode phases are disaggregated: non-disaggregated (NoDG) or fully\ndisaggregated (FuDG). However, the NoDG strategy leads to strong prefill-decode\ninterference and the FuDG strategy highly relies on high-performance\ninterconnects, making them less cost-effective.\n  We introduce EcoServe, a system that enables cost-effective LLM serving on\nclusters with commodity interconnects. EcoServe is built on the partially\ndisaggregated (PaDG) strategy, applying temporal disaggregation and rolling\nactivation for proactive intra- and inter-instance scheduling. It first\ndisaggregates the prefill and decode phases along the time dimension within a\nsingle instance to mitigate inter-phase interference and enhance throughput.\nNext, it coordinates multiple instances and cyclically activates them to ensure\nthe continuous availability of prefill processing, thereby improving latency.\nThus, EcoServe's basic serving unit is the macro instance, within which\nmultiple instances collaborate. It further integrates an adaptive scheduling\nalgorithm to route requests in a macro instance and a mitosis scaling approach\nto enable fine-grained capacity scaling. Beyond delivering high goodput,\nEcoServe excels in load balancing, hardware cost, parallelism compatibility,\nand even engineering simplicity compared to existing solutions.\n  When serving 30B- and 70B-scale models on a production-level cluster with 32\nNVIDIA L20 GPUs using commodity Ethernet, EcoServe averagely improves goodput\nby 82.49%, 86.17%, 122.76%, and 126.96% over four representative NoDG and FuDG\nsystems."
                },
                "authors": [
                    {
                        "name": "Jiangsu Du"
                    },
                    {
                        "name": "Hongbin Zhang"
                    },
                    {
                        "name": "Taosheng Wei"
                    },
                    {
                        "name": "Zhenyi Zheng"
                    },
                    {
                        "name": "Kaiyi Wu"
                    },
                    {
                        "name": "Zhiguang Chen"
                    },
                    {
                        "name": "Yutong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yutong Lu"
                },
                "author": "Yutong Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18147v1",
                "updated": "2025-04-25T07:56:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    7,
                    56,
                    24,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T07:56:24Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    7,
                    56,
                    24,
                    4,
                    115,
                    0
                ],
                "title": "NoEsis: Differentially Private Knowledge Transfer in Modular LLM\n  Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NoEsis: Differentially Private Knowledge Transfer in Modular LLM\n  Adaptation"
                },
                "summary": "Large Language Models (LLM) are typically trained on vast amounts of data\nfrom various sources. Even when designed modularly (e.g., Mixture-of-Experts),\nLLMs can leak privacy on their sources. Conversely, training such models in\nisolation arguably prohibits generalization. To this end, we propose a\nframework, NoEsis, which builds upon the desired properties of modularity,\nprivacy, and knowledge transfer. NoEsis integrates differential privacy with a\nhybrid two-staged parameter-efficient fine-tuning that combines domain-specific\nlow-rank adapters, acting as experts, with common prompt tokens, acting as a\nknowledge-sharing backbone. Results from our evaluation on CodeXGLUE showcase\nthat NoEsis can achieve provable privacy guarantees with tangible knowledge\ntransfer across domains, and empirically show protection against Membership\nInference Attacks. Finally, on code completion tasks, NoEsis bridges at least\n77% of the accuracy gap between the non-shared and the non-private baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) are typically trained on vast amounts of data\nfrom various sources. Even when designed modularly (e.g., Mixture-of-Experts),\nLLMs can leak privacy on their sources. Conversely, training such models in\nisolation arguably prohibits generalization. To this end, we propose a\nframework, NoEsis, which builds upon the desired properties of modularity,\nprivacy, and knowledge transfer. NoEsis integrates differential privacy with a\nhybrid two-staged parameter-efficient fine-tuning that combines domain-specific\nlow-rank adapters, acting as experts, with common prompt tokens, acting as a\nknowledge-sharing backbone. Results from our evaluation on CodeXGLUE showcase\nthat NoEsis can achieve provable privacy guarantees with tangible knowledge\ntransfer across domains, and empirically show protection against Membership\nInference Attacks. Finally, on code completion tasks, NoEsis bridges at least\n77% of the accuracy gap between the non-shared and the non-private baseline."
                },
                "authors": [
                    {
                        "name": "Rob Romijnders"
                    },
                    {
                        "name": "Stefanos Laskaridis"
                    },
                    {
                        "name": "Ali Shahin Shamsabadi"
                    },
                    {
                        "name": "Hamed Haddadi"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Haddadi"
                },
                "author": "Hamed Haddadi",
                "arxiv_comment": "ICLR 2025 MCDC workshop",
                "arxiv_journal_ref": "ICLR 2025 Workshop on Modularity for Collaborative, Decentralized,\n  and Continual Deep Learning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15843v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15843v2",
                "updated": "2025-04-25T07:47:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    7,
                    47,
                    16,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-22T12:39:30Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    12,
                    39,
                    30,
                    1,
                    112,
                    0
                ],
                "title": "Pre-DPO: Improving Data Utilization in Direct Preference Optimization\n  Using a Guiding Reference Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-DPO: Improving Data Utilization in Direct Preference Optimization\n  Using a Guiding Reference Model"
                },
                "summary": "Direct Preference Optimization (DPO) simplifies reinforcement learning from\nhuman feedback (RLHF) for large language models (LLMs) by directly optimizing\nhuman preferences without an explicit reward model. We find that during DPO\ntraining, the reference model plays the role of a data weight adjuster.\nHowever, the common practice of initializing the policy and reference models\nidentically in DPO can lead to inefficient data utilization and impose a\nperformance ceiling. Meanwhile, the lack of a reference model in Simple\nPreference Optimization (SimPO) reduces training robustness and necessitates\nstricter conditions to prevent catastrophic forgetting. In this work, we\npropose Pre-DPO, a simple yet effective DPO-based training paradigm that\nenhances preference optimization performance by leveraging a guiding reference\nmodel. This reference model provides foresight into the optimal policy state\nachievable through the training preference data, serving as a guiding mechanism\nthat adaptively assigns higher weights to samples more suitable for the model\nand lower weights to those less suitable. Extensive experiments on AlpacaEval\n2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently\nimproves the performance of both DPO and SimPO, without relying on external\nmodels or additional data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) simplifies reinforcement learning from\nhuman feedback (RLHF) for large language models (LLMs) by directly optimizing\nhuman preferences without an explicit reward model. We find that during DPO\ntraining, the reference model plays the role of a data weight adjuster.\nHowever, the common practice of initializing the policy and reference models\nidentically in DPO can lead to inefficient data utilization and impose a\nperformance ceiling. Meanwhile, the lack of a reference model in Simple\nPreference Optimization (SimPO) reduces training robustness and necessitates\nstricter conditions to prevent catastrophic forgetting. In this work, we\npropose Pre-DPO, a simple yet effective DPO-based training paradigm that\nenhances preference optimization performance by leveraging a guiding reference\nmodel. This reference model provides foresight into the optimal policy state\nachievable through the training preference data, serving as a guiding mechanism\nthat adaptively assigns higher weights to samples more suitable for the model\nand lower weights to those less suitable. Extensive experiments on AlpacaEval\n2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently\nimproves the performance of both DPO and SimPO, without relying on external\nmodels or additional data."
                },
                "authors": [
                    {
                        "name": "Junshu Pan"
                    },
                    {
                        "name": "Wei Shen"
                    },
                    {
                        "name": "Shulin Huang"
                    },
                    {
                        "name": "Qiji Zhou"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15843v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15843v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01482v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01482v2",
                "updated": "2025-04-25T07:40:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    7,
                    40,
                    9,
                    4,
                    115,
                    0
                ],
                "published": "2025-03-03T12:41:01Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    12,
                    41,
                    1,
                    0,
                    62,
                    0
                ],
                "title": "Revisiting Locally Differentially Private Protocols: Towards Better\n  Trade-offs in Privacy, Utility, and Attack Resistance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Locally Differentially Private Protocols: Towards Better\n  Trade-offs in Privacy, Utility, and Attack Resistance"
                },
                "summary": "Local Differential Privacy (LDP) offers strong privacy protection, especially\nin settings in which the server collecting the data is untrusted. However,\ndesigning LDP mechanisms that achieve an optimal trade-off between privacy,\nutility and robustness to adversarial inference attacks remains challenging. In\nthis work, we introduce a general multi-objective optimization framework for\nrefining LDP protocols, enabling the joint optimization of privacy and utility\nunder various adversarial settings. While our framework is flexible to\naccommodate multiple privacy and security attacks as well as utility metrics,\nin this paper, we specifically optimize for Attacker Success Rate (ASR) under\n\\emph{data reconstruction attack} as a concrete measure of privacy leakage and\nMean Squared Error (MSE) as a measure of utility. More precisely, we\nsystematically revisit these trade-offs by analyzing eight state-of-the-art LDP\nprotocols and proposing refined counterparts that leverage tailored\noptimization techniques. Experimental results demonstrate that our proposed\nadaptive mechanisms consistently outperform their non-adaptive counterparts,\nachieving substantial reductions in ASR while preserving utility, and pushing\ncloser to the ASR-MSE Pareto frontier. By bridging the gap between theoretical\nguarantees and real-world vulnerabilities, our framework enables modular and\ncontext-aware deployment of LDP mechanisms with tunable privacy-utility\ntrade-offs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local Differential Privacy (LDP) offers strong privacy protection, especially\nin settings in which the server collecting the data is untrusted. However,\ndesigning LDP mechanisms that achieve an optimal trade-off between privacy,\nutility and robustness to adversarial inference attacks remains challenging. In\nthis work, we introduce a general multi-objective optimization framework for\nrefining LDP protocols, enabling the joint optimization of privacy and utility\nunder various adversarial settings. While our framework is flexible to\naccommodate multiple privacy and security attacks as well as utility metrics,\nin this paper, we specifically optimize for Attacker Success Rate (ASR) under\n\\emph{data reconstruction attack} as a concrete measure of privacy leakage and\nMean Squared Error (MSE) as a measure of utility. More precisely, we\nsystematically revisit these trade-offs by analyzing eight state-of-the-art LDP\nprotocols and proposing refined counterparts that leverage tailored\noptimization techniques. Experimental results demonstrate that our proposed\nadaptive mechanisms consistently outperform their non-adaptive counterparts,\nachieving substantial reductions in ASR while preserving utility, and pushing\ncloser to the ASR-MSE Pareto frontier. By bridging the gap between theoretical\nguarantees and real-world vulnerabilities, our framework enables modular and\ncontext-aware deployment of LDP mechanisms with tunable privacy-utility\ntrade-offs."
                },
                "authors": [
                    {
                        "name": "Héber H. Arcolezi"
                    },
                    {
                        "name": "Sébastien Gambs"
                    }
                ],
                "author_detail": {
                    "name": "Sébastien Gambs"
                },
                "author": "Sébastien Gambs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01482v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01482v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17384v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17384v2",
                "updated": "2025-04-25T07:35:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    7,
                    35,
                    21,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-24T09:08:24Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    9,
                    8,
                    24,
                    3,
                    114,
                    0
                ],
                "title": "On the workflow, opportunities and challenges of developing foundation\n  model in geophysics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the workflow, opportunities and challenges of developing foundation\n  model in geophysics"
                },
                "summary": "Foundation models, as a mainstream technology in artificial intelligence,\nhave demonstrated immense potential across various domains in recent years,\nparticularly in handling complex tasks and multimodal data. In the field of\ngeophysics, although the application of foundation models is gradually\nexpanding, there is currently a lack of comprehensive reviews discussing the\nfull workflow of integrating foundation models with geophysical data. To\naddress this gap, this paper presents a complete framework that systematically\nexplores the entire process of developing foundation models in conjunction with\ngeophysical data. From data collection and preprocessing to model architecture\nselection, pre-training strategies, and model deployment, we provide a detailed\nanalysis of the key techniques and methodologies at each stage. In particular,\nconsidering the diversity, complexity, and physical consistency constraints of\ngeophysical data, we discuss targeted solutions to address these challenges.\nFurthermore, we discuss how to leverage the transfer learning capabilities of\nfoundation models to reduce reliance on labeled data, enhance computational\nefficiency, and incorporate physical constraints into model training, thereby\nimproving physical consistency and interpretability. Through a comprehensive\nsummary and analysis of the current technological landscape, this paper not\nonly fills the gap in the geophysics domain regarding a full-process review of\nfoundation models but also offers valuable practical guidance for their\napplication in geophysical data analysis, driving innovation and advancement in\nthe field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models, as a mainstream technology in artificial intelligence,\nhave demonstrated immense potential across various domains in recent years,\nparticularly in handling complex tasks and multimodal data. In the field of\ngeophysics, although the application of foundation models is gradually\nexpanding, there is currently a lack of comprehensive reviews discussing the\nfull workflow of integrating foundation models with geophysical data. To\naddress this gap, this paper presents a complete framework that systematically\nexplores the entire process of developing foundation models in conjunction with\ngeophysical data. From data collection and preprocessing to model architecture\nselection, pre-training strategies, and model deployment, we provide a detailed\nanalysis of the key techniques and methodologies at each stage. In particular,\nconsidering the diversity, complexity, and physical consistency constraints of\ngeophysical data, we discuss targeted solutions to address these challenges.\nFurthermore, we discuss how to leverage the transfer learning capabilities of\nfoundation models to reduce reliance on labeled data, enhance computational\nefficiency, and incorporate physical constraints into model training, thereby\nimproving physical consistency and interpretability. Through a comprehensive\nsummary and analysis of the current technological landscape, this paper not\nonly fills the gap in the geophysics domain regarding a full-process review of\nfoundation models but also offers valuable practical guidance for their\napplication in geophysical data analysis, driving innovation and advancement in\nthe field."
                },
                "authors": [
                    {
                        "name": "Hanlin Sheng"
                    },
                    {
                        "name": "Xinming Wu"
                    },
                    {
                        "name": "Hang Gao"
                    },
                    {
                        "name": "Haibin Di"
                    },
                    {
                        "name": "Sergey Fomel"
                    },
                    {
                        "name": "Jintao Li"
                    },
                    {
                        "name": "Xu Si"
                    }
                ],
                "author_detail": {
                    "name": "Xu Si"
                },
                "author": "Xu Si",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17384v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17384v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08558v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08558v2",
                "updated": "2025-04-25T07:12:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    7,
                    12,
                    28,
                    4,
                    115,
                    0
                ],
                "published": "2025-03-11T15:47:12Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    47,
                    12,
                    1,
                    70,
                    0
                ],
                "title": "Can We Detect Failures Without Failure Data? Uncertainty-Aware Runtime\n  Failure Detection for Imitation Learning Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can We Detect Failures Without Failure Data? Uncertainty-Aware Runtime\n  Failure Detection for Imitation Learning Policies"
                },
                "summary": "Recent years have witnessed impressive robotic manipulation systems driven by\nadvances in imitation learning and generative modeling, such as diffusion- and\nflow-based approaches. As robot policy performance increases, so does the\ncomplexity and time horizon of achievable tasks, inducing unexpected and\ndiverse failure modes that are difficult to predict a priori. To enable\ntrustworthy policy deployment in safety-critical human environments, reliable\nruntime failure detection becomes important during policy inference. However,\nmost existing failure detection approaches rely on prior knowledge of failure\nmodes and require failure data during training, which imposes a significant\nchallenge in practicality and scalability. In response to these limitations, we\npresent FAIL-Detect, a modular two-stage approach for failure detection in\nimitation learning-based robotic manipulation. To accurately identify failures\nfrom successful training data alone, we frame the problem as sequential\nout-of-distribution (OOD) detection. We first distill policy inputs and outputs\ninto scalar signals that correlate with policy failures and capture epistemic\nuncertainty. FAIL-Detect then employs conformal prediction (CP) as a versatile\nframework for uncertainty quantification with statistical guarantees.\nEmpirically, we thoroughly investigate both learned and post-hoc scalar signal\ncandidates on diverse robotic manipulation tasks. Our experiments show learned\nsignals to be mostly consistently effective, particularly when using our novel\nflow-based density estimator. Furthermore, our method detects failures more\naccurately and faster than state-of-the-art (SOTA) failure detection baselines.\nThese results highlight the potential of FAIL-Detect to enhance the safety and\nreliability of imitation learning-based robotic systems as they progress toward\nreal-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed impressive robotic manipulation systems driven by\nadvances in imitation learning and generative modeling, such as diffusion- and\nflow-based approaches. As robot policy performance increases, so does the\ncomplexity and time horizon of achievable tasks, inducing unexpected and\ndiverse failure modes that are difficult to predict a priori. To enable\ntrustworthy policy deployment in safety-critical human environments, reliable\nruntime failure detection becomes important during policy inference. However,\nmost existing failure detection approaches rely on prior knowledge of failure\nmodes and require failure data during training, which imposes a significant\nchallenge in practicality and scalability. In response to these limitations, we\npresent FAIL-Detect, a modular two-stage approach for failure detection in\nimitation learning-based robotic manipulation. To accurately identify failures\nfrom successful training data alone, we frame the problem as sequential\nout-of-distribution (OOD) detection. We first distill policy inputs and outputs\ninto scalar signals that correlate with policy failures and capture epistemic\nuncertainty. FAIL-Detect then employs conformal prediction (CP) as a versatile\nframework for uncertainty quantification with statistical guarantees.\nEmpirically, we thoroughly investigate both learned and post-hoc scalar signal\ncandidates on diverse robotic manipulation tasks. Our experiments show learned\nsignals to be mostly consistently effective, particularly when using our novel\nflow-based density estimator. Furthermore, our method detects failures more\naccurately and faster than state-of-the-art (SOTA) failure detection baselines.\nThese results highlight the potential of FAIL-Detect to enhance the safety and\nreliability of imitation learning-based robotic systems as they progress toward\nreal-world deployment."
                },
                "authors": [
                    {
                        "name": "Chen Xu"
                    },
                    {
                        "name": "Tony Khuong Nguyen"
                    },
                    {
                        "name": "Emma Dixon"
                    },
                    {
                        "name": "Christopher Rodriguez"
                    },
                    {
                        "name": "Patrick Miller"
                    },
                    {
                        "name": "Robert Lee"
                    },
                    {
                        "name": "Paarth Shah"
                    },
                    {
                        "name": "Rares Ambrus"
                    },
                    {
                        "name": "Haruki Nishimura"
                    },
                    {
                        "name": "Masha Itkina"
                    }
                ],
                "author_detail": {
                    "name": "Masha Itkina"
                },
                "author": "Masha Itkina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08558v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08558v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18116v1",
                "updated": "2025-04-25T06:48:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    6,
                    48,
                    55,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T06:48:55Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    6,
                    48,
                    55,
                    4,
                    115,
                    0
                ],
                "title": "Think, Prune, Train, Improve: Scaling Reasoning without Scaling Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think, Prune, Train, Improve: Scaling Reasoning without Scaling Models"
                },
                "summary": "Large language models (LLMs) have demonstrated strong capabilities in\nprogramming and mathematical reasoning tasks, but are constrained by limited\nhigh-quality training data. Synthetic data can be leveraged to enhance\nfine-tuning outcomes, but several factors influence this process, including\nmodel size, synthetic data volume, pruning strategy, and number of fine-tuning\nrounds. We explore these axes and investigate which conditions enable model\nself-improvement. We introduce the Think, Prune, Train process, a scalable\nframework that iteratively fine-tunes models on their own reasoning traces,\nusing ground-truth pruning to ensure high-quality training data. This approach\nyields improved performance: on GSM8K, Gemma2-2B achieves a Pass@1 of 57.6%\n(from 41.9%), Gemma2-9B reaches 82%, matching LLaMA-3.1-70B, and LLaMA-3.1-70B\nattains 91%, even surpassing GPT-4o, demonstrating the effectiveness of\nself-generated reasoning and systematic data selection for improving LLM\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong capabilities in\nprogramming and mathematical reasoning tasks, but are constrained by limited\nhigh-quality training data. Synthetic data can be leveraged to enhance\nfine-tuning outcomes, but several factors influence this process, including\nmodel size, synthetic data volume, pruning strategy, and number of fine-tuning\nrounds. We explore these axes and investigate which conditions enable model\nself-improvement. We introduce the Think, Prune, Train process, a scalable\nframework that iteratively fine-tunes models on their own reasoning traces,\nusing ground-truth pruning to ensure high-quality training data. This approach\nyields improved performance: on GSM8K, Gemma2-2B achieves a Pass@1 of 57.6%\n(from 41.9%), Gemma2-9B reaches 82%, matching LLaMA-3.1-70B, and LLaMA-3.1-70B\nattains 91%, even surpassing GPT-4o, demonstrating the effectiveness of\nself-generated reasoning and systematic data selection for improving LLM\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Caia Costello"
                    },
                    {
                        "name": "Simon Guo"
                    },
                    {
                        "name": "Anna Goldie"
                    },
                    {
                        "name": "Azalia Mirhoseini"
                    }
                ],
                "author_detail": {
                    "name": "Azalia Mirhoseini"
                },
                "author": "Azalia Mirhoseini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18114v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18114v1",
                "updated": "2025-04-25T06:37:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    6,
                    37,
                    29,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T06:37:29Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    6,
                    37,
                    29,
                    4,
                    115,
                    0
                ],
                "title": "Evaluating Evaluation Metrics -- The Mirage of Hallucination Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Evaluation Metrics -- The Mirage of Hallucination Detection"
                },
                "summary": "Hallucinations pose a significant obstacle to the reliability and widespread\nadoption of language models, yet their accurate measurement remains a\npersistent challenge. While many task- and domain-specific metrics have been\nproposed to assess faithfulness and factuality concerns, the robustness and\ngeneralization of these metrics are still untested. In this paper, we conduct a\nlarge-scale empirical evaluation of 6 diverse sets of hallucination detection\nmetrics across 4 datasets, 37 language models from 5 families, and 5 decoding\nmethods. Our extensive investigation reveals concerning gaps in current\nhallucination evaluation: metrics often fail to align with human judgments,\ntake an overtly myopic view of the problem, and show inconsistent gains with\nparameter scaling. Encouragingly, LLM-based evaluation, particularly with\nGPT-4, yields the best overall results, and mode-seeking decoding methods seem\nto reduce hallucinations, especially in knowledge-grounded settings. These\nfindings underscore the need for more robust metrics to understand and quantify\nhallucinations, and better strategies to mitigate them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations pose a significant obstacle to the reliability and widespread\nadoption of language models, yet their accurate measurement remains a\npersistent challenge. While many task- and domain-specific metrics have been\nproposed to assess faithfulness and factuality concerns, the robustness and\ngeneralization of these metrics are still untested. In this paper, we conduct a\nlarge-scale empirical evaluation of 6 diverse sets of hallucination detection\nmetrics across 4 datasets, 37 language models from 5 families, and 5 decoding\nmethods. Our extensive investigation reveals concerning gaps in current\nhallucination evaluation: metrics often fail to align with human judgments,\ntake an overtly myopic view of the problem, and show inconsistent gains with\nparameter scaling. Encouragingly, LLM-based evaluation, particularly with\nGPT-4, yields the best overall results, and mode-seeking decoding methods seem\nto reduce hallucinations, especially in knowledge-grounded settings. These\nfindings underscore the need for more robust metrics to understand and quantify\nhallucinations, and better strategies to mitigate them."
                },
                "authors": [
                    {
                        "name": "Atharva Kulkarni"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Joel Ruben Antony Moniz"
                    },
                    {
                        "name": "Xiou Ge"
                    },
                    {
                        "name": "Bo-Hsiang Tseng"
                    },
                    {
                        "name": "Dhivya Piraviperumal"
                    },
                    {
                        "name": "Swabha Swayamdipta"
                    },
                    {
                        "name": "Hong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Yu"
                },
                "author": "Hong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18114v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18114v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14657v2",
                "updated": "2025-04-25T06:34:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    6,
                    34,
                    43,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-20T15:37:05Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    15,
                    37,
                    5,
                    6,
                    110,
                    0
                ],
                "title": "A Case Study Exploring the Current Landscape of Synthetic Medical Record\n  Generation with Commercial LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Case Study Exploring the Current Landscape of Synthetic Medical Record\n  Generation with Commercial LLMs"
                },
                "summary": "Synthetic Electronic Health Records (EHRs) offer a valuable opportunity to\ncreate privacy preserving and harmonized structured data, supporting numerous\napplications in healthcare. Key benefits of synthetic data include precise\ncontrol over the data schema, improved fairness and representation of patient\npopulations, and the ability to share datasets without concerns about\ncompromising real individuals privacy. Consequently, the AI community has\nincreasingly turned to Large Language Models (LLMs) to generate synthetic data\nacross various domains. However, a significant challenge in healthcare is\nensuring that synthetic health records reliably generalize across different\nhospitals, a long standing issue in the field. In this work, we evaluate the\ncurrent state of commercial LLMs for generating synthetic data and investigate\nmultiple aspects of the generation process to identify areas where these models\nexcel and where they fall short. Our main finding from this work is that while\nLLMs can reliably generate synthetic health records for smaller subsets of\nfeatures, they struggle to preserve realistic distributions and correlations as\nthe dimensionality of the data increases, ultimately limiting their ability to\ngeneralize across diverse hospital settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Electronic Health Records (EHRs) offer a valuable opportunity to\ncreate privacy preserving and harmonized structured data, supporting numerous\napplications in healthcare. Key benefits of synthetic data include precise\ncontrol over the data schema, improved fairness and representation of patient\npopulations, and the ability to share datasets without concerns about\ncompromising real individuals privacy. Consequently, the AI community has\nincreasingly turned to Large Language Models (LLMs) to generate synthetic data\nacross various domains. However, a significant challenge in healthcare is\nensuring that synthetic health records reliably generalize across different\nhospitals, a long standing issue in the field. In this work, we evaluate the\ncurrent state of commercial LLMs for generating synthetic data and investigate\nmultiple aspects of the generation process to identify areas where these models\nexcel and where they fall short. Our main finding from this work is that while\nLLMs can reliably generate synthetic health records for smaller subsets of\nfeatures, they struggle to preserve realistic distributions and correlations as\nthe dimensionality of the data increases, ultimately limiting their ability to\ngeneralize across diverse hospital settings."
                },
                "authors": [
                    {
                        "name": "Yihan Lin"
                    },
                    {
                        "name": "Zhirong Bella Yu"
                    },
                    {
                        "name": "Simon Lee"
                    }
                ],
                "author_detail": {
                    "name": "Simon Lee"
                },
                "author": "Simon Lee",
                "arxiv_comment": "Accepted at the Conference of Health, Inference, Learning (CHIL 2025)\n  in Berkeley, CA. To appear in PMLR later in 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18106v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18106v1",
                "updated": "2025-04-25T06:23:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    6,
                    23,
                    55,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T06:23:55Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    6,
                    23,
                    55,
                    4,
                    115,
                    0
                ],
                "title": "Comparative Study on the Discourse Meaning of Chinese and English Media\n  in the Paris Olympics Based on LDA Topic Modeling Technology and LLM Prompt\n  Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Study on the Discourse Meaning of Chinese and English Media\n  in the Paris Olympics Based on LDA Topic Modeling Technology and LLM Prompt\n  Engineering"
                },
                "summary": "This study analyzes Chinese and English media reports on the Paris Olympics\nusing topic modeling, Large Language Model (LLM) prompt engineering, and corpus\nphraseology methods to explore similarities and differences in discourse\nconstruction and attitudinal meanings. Common topics include the opening\nceremony, athlete performance, and sponsorship brands. Chinese media focus on\nspecific sports, sports spirit, doping controversies, and new technologies,\nwhile English media focus on female athletes, medal wins, and eligibility\ncontroversies. Chinese reports show more frequent prepositional co-occurrences\nand positive semantic prosody in describing the opening ceremony and sports\nspirit. English reports exhibit positive semantic prosody when covering female\nathletes but negative prosody in predicting opening ceremony reactions and\ndiscussing women's boxing controversies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study analyzes Chinese and English media reports on the Paris Olympics\nusing topic modeling, Large Language Model (LLM) prompt engineering, and corpus\nphraseology methods to explore similarities and differences in discourse\nconstruction and attitudinal meanings. Common topics include the opening\nceremony, athlete performance, and sponsorship brands. Chinese media focus on\nspecific sports, sports spirit, doping controversies, and new technologies,\nwhile English media focus on female athletes, medal wins, and eligibility\ncontroversies. Chinese reports show more frequent prepositional co-occurrences\nand positive semantic prosody in describing the opening ceremony and sports\nspirit. English reports exhibit positive semantic prosody when covering female\nathletes but negative prosody in predicting opening ceremony reactions and\ndiscussing women's boxing controversies."
                },
                "authors": [
                    {
                        "name": "Yinglong Yu"
                    },
                    {
                        "name": "Zhaopu Yao"
                    },
                    {
                        "name": "Fang Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Fang Yuan"
                },
                "author": "Fang Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18106v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18106v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01841v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01841v3",
                "updated": "2025-04-25T05:36:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    36,
                    19,
                    4,
                    115,
                    0
                ],
                "published": "2024-11-04T06:27:14Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    6,
                    27,
                    14,
                    0,
                    309,
                    0
                ],
                "title": "Leveraging Label Semantics and Meta-Label Refinement for Multi-Label\n  Question Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Label Semantics and Meta-Label Refinement for Multi-Label\n  Question Classification"
                },
                "summary": "Accurate annotation of educational resources is crucial for effective\npersonalized learning and resource recommendation in online education. However,\nfine-grained knowledge labels often overlap or share similarities, making it\ndifficult for existing multi-label classification methods to differentiate\nthem. The label distribution imbalance due to sparsity of human annotations\nfurther intensifies these challenges. To address these issues, this paper\nintroduces RR2QC, a novel Retrieval Reranking method to multi-label Question\nClassification by leveraging label semantics and meta-label refinement. First,\nRR2QC improves the pre-training strategy by utilizing semantic relationships\nwithin and across label groups. Second, it introduces a class center learning\ntask to align questions with label semantics during downstream training.\nFinally, this method decomposes labels into meta-labels and uses a meta-label\nclassifier to rerank the retrieved label sequences. In doing so, RR2QC enhances\nthe understanding and prediction capability of long-tail labels by learning\nfrom meta-labels that frequently appear in other labels. Additionally, a\nmathematical LLM is used to generate solutions for questions, extracting latent\ninformation to further refine the model's insights. Experimental results show\nthat RR2QC outperforms existing methods in Precision@K and F1 scores across\nmultiple educational datasets, demonstrating its effectiveness for online\neducation applications. The code and datasets are available at\nhttps://github.com/78Erii/RR2QC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate annotation of educational resources is crucial for effective\npersonalized learning and resource recommendation in online education. However,\nfine-grained knowledge labels often overlap or share similarities, making it\ndifficult for existing multi-label classification methods to differentiate\nthem. The label distribution imbalance due to sparsity of human annotations\nfurther intensifies these challenges. To address these issues, this paper\nintroduces RR2QC, a novel Retrieval Reranking method to multi-label Question\nClassification by leveraging label semantics and meta-label refinement. First,\nRR2QC improves the pre-training strategy by utilizing semantic relationships\nwithin and across label groups. Second, it introduces a class center learning\ntask to align questions with label semantics during downstream training.\nFinally, this method decomposes labels into meta-labels and uses a meta-label\nclassifier to rerank the retrieved label sequences. In doing so, RR2QC enhances\nthe understanding and prediction capability of long-tail labels by learning\nfrom meta-labels that frequently appear in other labels. Additionally, a\nmathematical LLM is used to generate solutions for questions, extracting latent\ninformation to further refine the model's insights. Experimental results show\nthat RR2QC outperforms existing methods in Precision@K and F1 scores across\nmultiple educational datasets, demonstrating its effectiveness for online\neducation applications. The code and datasets are available at\nhttps://github.com/78Erii/RR2QC."
                },
                "authors": [
                    {
                        "name": "Shi Dong"
                    },
                    {
                        "name": "Xiaobei Niu"
                    },
                    {
                        "name": "Rui Zhong"
                    },
                    {
                        "name": "Zhifeng Wang"
                    },
                    {
                        "name": "Mingzhang Zuo"
                    }
                ],
                "author_detail": {
                    "name": "Mingzhang Zuo"
                },
                "author": "Mingzhang Zuo",
                "arxiv_doi": "10.1016/j.knosys.2025.113412",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.knosys.2025.113412",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.01841v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01841v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Knowledge-Based Systems, 2025: 113412",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08813v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08813v2",
                "updated": "2025-04-25T05:34:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    34,
                    47,
                    4,
                    115,
                    0
                ],
                "published": "2024-09-13T13:24:52Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    13,
                    24,
                    52,
                    4,
                    257,
                    0
                ],
                "title": "Your Weak LLM is Secretly a Strong Teacher for Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Your Weak LLM is Secretly a Strong Teacher for Alignment"
                },
                "summary": "The burgeoning capabilities of large language models (LLMs) have underscored\nthe need for alignment to ensure these models act in accordance with human\nvalues and intentions. Existing alignment frameworks present constraints either\nin the form of expensive human effort or high computational costs. This paper\nexplores a promising middle ground, where we employ a weak LLM that is\nsignificantly less resource-intensive than top-tier models, yet offers more\nautomation than purely human feedback. We present a systematic study to\nevaluate and understand weak LLM's ability to generate feedback for alignment.\nOur empirical findings demonstrate that weak LLMs can provide feedback that\nrivals or even exceeds that of fully human-annotated data. Our study indicates\na minimized impact of model size on feedback efficacy, shedding light on a\nscalable and sustainable alignment strategy. To deepen our understanding of\nalignment under weak LLM feedback, we conduct a series of qualitative and\nquantitative analyses, offering novel insights into the quality discrepancies\nbetween human feedback vs. weak LLM feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The burgeoning capabilities of large language models (LLMs) have underscored\nthe need for alignment to ensure these models act in accordance with human\nvalues and intentions. Existing alignment frameworks present constraints either\nin the form of expensive human effort or high computational costs. This paper\nexplores a promising middle ground, where we employ a weak LLM that is\nsignificantly less resource-intensive than top-tier models, yet offers more\nautomation than purely human feedback. We present a systematic study to\nevaluate and understand weak LLM's ability to generate feedback for alignment.\nOur empirical findings demonstrate that weak LLMs can provide feedback that\nrivals or even exceeds that of fully human-annotated data. Our study indicates\na minimized impact of model size on feedback efficacy, shedding light on a\nscalable and sustainable alignment strategy. To deepen our understanding of\nalignment under weak LLM feedback, we conduct a series of qualitative and\nquantitative analyses, offering novel insights into the quality discrepancies\nbetween human feedback vs. weak LLM feedback."
                },
                "authors": [
                    {
                        "name": "Leitian Tao"
                    },
                    {
                        "name": "Yixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Yixuan Li"
                },
                "author": "Yixuan Li",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08813v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08813v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18085v1",
                "updated": "2025-04-25T05:25:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    25,
                    27,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T05:25:27Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    25,
                    27,
                    4,
                    115,
                    0
                ],
                "title": "Random-Set Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random-Set Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are known to produce very high-quality tests and\nresponses to our queries. But how much can we trust this generated text? In\nthis paper, we study the problem of uncertainty quantification in LLMs. We\npropose a novel Random-Set Large Language Model (RSLLM) approach which predicts\nfinite random sets (belief functions) over the token space, rather than\nprobability vectors as in classical LLMs. In order to allow so efficiently, we\nalso present a methodology based on hierarchical clustering to extract and use\na budget of \"focal\" subsets of tokens upon which the belief prediction is\ndefined, rather than using all possible collections of tokens, making the\nmethod scalable yet effective. RS-LLMs encode the epistemic uncertainty induced\nin their generation process by the size and diversity of its training set via\nthe size of the credal sets associated with the predicted belief functions. The\nproposed approach is evaluated on CoQA and OBQA datasets using Llama2-7b,\nMistral-7b and Phi-2 models and is shown to outperform the standard model in\nboth datasets in terms of correctness of answer while also showing potential in\nestimating the second level uncertainty in its predictions and providing the\ncapability to detect when its hallucinating.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are known to produce very high-quality tests and\nresponses to our queries. But how much can we trust this generated text? In\nthis paper, we study the problem of uncertainty quantification in LLMs. We\npropose a novel Random-Set Large Language Model (RSLLM) approach which predicts\nfinite random sets (belief functions) over the token space, rather than\nprobability vectors as in classical LLMs. In order to allow so efficiently, we\nalso present a methodology based on hierarchical clustering to extract and use\na budget of \"focal\" subsets of tokens upon which the belief prediction is\ndefined, rather than using all possible collections of tokens, making the\nmethod scalable yet effective. RS-LLMs encode the epistemic uncertainty induced\nin their generation process by the size and diversity of its training set via\nthe size of the credal sets associated with the predicted belief functions. The\nproposed approach is evaluated on CoQA and OBQA datasets using Llama2-7b,\nMistral-7b and Phi-2 models and is shown to outperform the standard model in\nboth datasets in terms of correctness of answer while also showing potential in\nestimating the second level uncertainty in its predictions and providing the\ncapability to detect when its hallucinating."
                },
                "authors": [
                    {
                        "name": "Muhammad Mubashar"
                    },
                    {
                        "name": "Shireen Kudukkil Manchingal"
                    },
                    {
                        "name": "Fabio Cuzzolin"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Cuzzolin"
                },
                "author": "Fabio Cuzzolin",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18083v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18083v1",
                "updated": "2025-04-25T05:19:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    19,
                    2,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T05:19:02Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    19,
                    2,
                    4,
                    115,
                    0
                ],
                "title": "Automating Function-Level TARA for Automotive Full-Lifecycle Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Function-Level TARA for Automotive Full-Lifecycle Security"
                },
                "summary": "As modern vehicles evolve into intelligent and connected systems, their\ngrowing complexity introduces significant cybersecurity risks. Threat Analysis\nand Risk Assessment (TARA) has therefore become essential for managing these\nrisks under mandatory regulations. However, existing TARA automation methods\nrely on static threat libraries, limiting their utility in the detailed,\nfunction-level analyses demanded by industry. This paper introduces\nDefenseWeaver, the first system that automates function-level TARA using\ncomponent-specific details and large language models (LLMs). DefenseWeaver\ndynamically generates attack trees and risk evaluations from system\nconfigurations described in an extended OpenXSAM++ format, then employs a\nmulti-agent framework to coordinate specialized LLM roles for more robust\nanalysis. To further adapt to evolving threats and diverse standards,\nDefenseWeaver incorporates Low-Rank Adaptation (LoRA) fine-tuning and\nRetrieval-Augmented Generation (RAG) with expert-curated TARA reports. We\nvalidated DefenseWeaver through deployment in four automotive security\nprojects, where it identified 11 critical attack paths, verified through\npenetration testing, and subsequently reported and remediated by the relevant\nautomakers and suppliers. Additionally, DefenseWeaver demonstrated cross-domain\nadaptability, successfully applying to unmanned aerial vehicles (UAVs) and\nmarine navigation systems. In comparison to human experts, DefenseWeaver\noutperformed manual attack tree generation across six assessment scenarios.\nIntegrated into commercial cybersecurity platforms such as UAES and Xiaomi,\nDefenseWeaver has generated over 8,200 attack trees. These results highlight\nits ability to significantly reduce processing time, and its scalability and\ntransformative impact on cybersecurity across industries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As modern vehicles evolve into intelligent and connected systems, their\ngrowing complexity introduces significant cybersecurity risks. Threat Analysis\nand Risk Assessment (TARA) has therefore become essential for managing these\nrisks under mandatory regulations. However, existing TARA automation methods\nrely on static threat libraries, limiting their utility in the detailed,\nfunction-level analyses demanded by industry. This paper introduces\nDefenseWeaver, the first system that automates function-level TARA using\ncomponent-specific details and large language models (LLMs). DefenseWeaver\ndynamically generates attack trees and risk evaluations from system\nconfigurations described in an extended OpenXSAM++ format, then employs a\nmulti-agent framework to coordinate specialized LLM roles for more robust\nanalysis. To further adapt to evolving threats and diverse standards,\nDefenseWeaver incorporates Low-Rank Adaptation (LoRA) fine-tuning and\nRetrieval-Augmented Generation (RAG) with expert-curated TARA reports. We\nvalidated DefenseWeaver through deployment in four automotive security\nprojects, where it identified 11 critical attack paths, verified through\npenetration testing, and subsequently reported and remediated by the relevant\nautomakers and suppliers. Additionally, DefenseWeaver demonstrated cross-domain\nadaptability, successfully applying to unmanned aerial vehicles (UAVs) and\nmarine navigation systems. In comparison to human experts, DefenseWeaver\noutperformed manual attack tree generation across six assessment scenarios.\nIntegrated into commercial cybersecurity platforms such as UAES and Xiaomi,\nDefenseWeaver has generated over 8,200 attack trees. These results highlight\nits ability to significantly reduce processing time, and its scalability and\ntransformative impact on cybersecurity across industries."
                },
                "authors": [
                    {
                        "name": "Yuqiao Yang"
                    },
                    {
                        "name": "Yongzhao Zhang"
                    },
                    {
                        "name": "Wenhao Liu"
                    },
                    {
                        "name": "Jun Li"
                    },
                    {
                        "name": "Pengtao Shi"
                    },
                    {
                        "name": "DingYu Zhong"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Ting Chen"
                    },
                    {
                        "name": "Sheng Cao"
                    },
                    {
                        "name": "Yuntao Ren"
                    },
                    {
                        "name": "Yongyue Wu"
                    },
                    {
                        "name": "Xiaosong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaosong Zhang"
                },
                "author": "Xiaosong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18083v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18080v1",
                "updated": "2025-04-25T05:15:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    15,
                    31,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T05:15:31Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    15,
                    31,
                    4,
                    115,
                    0
                ],
                "title": "Stabilizing Reasoning in Medical LLMs with Continued Pretraining and\n  Reasoning Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stabilizing Reasoning in Medical LLMs with Continued Pretraining and\n  Reasoning Preference Optimization"
                },
                "summary": "Large Language Models (LLMs) show potential in medicine, yet clinical\nadoption is hindered by concerns over factual accuracy, language-specific\nlimitations (e.g., Japanese), and critically, their reliability when required\nto generate reasoning explanations -- a prerequisite for trust. This paper\nintroduces Preferred-MedLLM-Qwen-72B, a 72B-parameter model optimized for the\nJapanese medical domain to achieve both high accuracy and stable reasoning. We\nemploy a two-stage fine-tuning process on the Qwen2.5-72B base model: first,\nContinued Pretraining (CPT) on a comprehensive Japanese medical corpus instills\ndeep domain knowledge. Second, Reasoning Preference Optimization (RPO), a\npreference-based method, enhances the generation of reliable reasoning pathways\nwhile preserving high answer accuracy. Evaluations on the Japanese Medical\nLicensing Exam benchmark (IgakuQA) show Preferred-MedLLM-Qwen-72B achieves\nstate-of-the-art performance (0.868 accuracy), surpassing strong proprietary\nmodels like GPT-4o (0.866). Crucially, unlike baseline or CPT-only models which\nexhibit significant accuracy degradation (up to 11.5\\% and 3.8\\% respectively\non IgakuQA) when prompted for explanations, our model maintains its high\naccuracy (0.868) under such conditions. This highlights RPO's effectiveness in\nstabilizing reasoning generation. This work underscores the importance of\noptimizing for reliable explanations alongside accuracy. We release the\nPreferred-MedLLM-Qwen-72B model weights to foster research into trustworthy\nLLMs for specialized, high-stakes applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show potential in medicine, yet clinical\nadoption is hindered by concerns over factual accuracy, language-specific\nlimitations (e.g., Japanese), and critically, their reliability when required\nto generate reasoning explanations -- a prerequisite for trust. This paper\nintroduces Preferred-MedLLM-Qwen-72B, a 72B-parameter model optimized for the\nJapanese medical domain to achieve both high accuracy and stable reasoning. We\nemploy a two-stage fine-tuning process on the Qwen2.5-72B base model: first,\nContinued Pretraining (CPT) on a comprehensive Japanese medical corpus instills\ndeep domain knowledge. Second, Reasoning Preference Optimization (RPO), a\npreference-based method, enhances the generation of reliable reasoning pathways\nwhile preserving high answer accuracy. Evaluations on the Japanese Medical\nLicensing Exam benchmark (IgakuQA) show Preferred-MedLLM-Qwen-72B achieves\nstate-of-the-art performance (0.868 accuracy), surpassing strong proprietary\nmodels like GPT-4o (0.866). Crucially, unlike baseline or CPT-only models which\nexhibit significant accuracy degradation (up to 11.5\\% and 3.8\\% respectively\non IgakuQA) when prompted for explanations, our model maintains its high\naccuracy (0.868) under such conditions. This highlights RPO's effectiveness in\nstabilizing reasoning generation. This work underscores the importance of\noptimizing for reliable explanations alongside accuracy. We release the\nPreferred-MedLLM-Qwen-72B model weights to foster research into trustworthy\nLLMs for specialized, high-stakes applications."
                },
                "authors": [
                    {
                        "name": "Wataru Kawakami"
                    },
                    {
                        "name": "Keita Suzuki"
                    },
                    {
                        "name": "Junichiro Iwasawa"
                    }
                ],
                "author_detail": {
                    "name": "Junichiro Iwasawa"
                },
                "author": "Junichiro Iwasawa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14603v2",
                "updated": "2025-04-25T05:14:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    14,
                    14,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-20T13:04:43Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    13,
                    4,
                    43,
                    6,
                    110,
                    0
                ],
                "title": "UFO2: The Desktop AgentOS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UFO2: The Desktop AgentOS"
                },
                "summary": "Recent Computer-Using Agents (CUAs), powered by multimodal large language\nmodels (LLMs), offer a promising direction for automating complex desktop\nworkflows through natural language. However, most existing CUAs remain\nconceptual prototypes, hindered by shallow OS integration, fragile\nscreenshot-based interaction, and disruptive execution.\n  We present UFO2, a multiagent AgentOS for Windows desktops that elevates CUAs\ninto practical, system-level automation. UFO2 features a centralized HostAgent\nfor task decomposition and coordination, alongside a collection of\napplication-specialized AppAgent equipped with native APIs, domain-specific\nknowledge, and a unified GUI--API action layer. This architecture enables\nrobust task execution while preserving modularity and extensibility. A hybrid\ncontrol detection pipeline fuses Windows UI Automation (UIA) with vision-based\nparsing to support diverse interface styles. Runtime efficiency is further\nenhanced through speculative multi-action planning, reducing per-step LLM\noverhead. Finally, a Picture-in-Picture (PiP) interface enables automation\nwithin an isolated virtual desktop, allowing agents and users to operate\nconcurrently without interference.\n  We evaluate UFO2 across over 20 real-world Windows applications,\ndemonstrating substantial improvements in robustness and execution accuracy\nover prior CUAs. Our results show that deep OS integration unlocks a scalable\npath toward reliable, user-aligned desktop automation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Computer-Using Agents (CUAs), powered by multimodal large language\nmodels (LLMs), offer a promising direction for automating complex desktop\nworkflows through natural language. However, most existing CUAs remain\nconceptual prototypes, hindered by shallow OS integration, fragile\nscreenshot-based interaction, and disruptive execution.\n  We present UFO2, a multiagent AgentOS for Windows desktops that elevates CUAs\ninto practical, system-level automation. UFO2 features a centralized HostAgent\nfor task decomposition and coordination, alongside a collection of\napplication-specialized AppAgent equipped with native APIs, domain-specific\nknowledge, and a unified GUI--API action layer. This architecture enables\nrobust task execution while preserving modularity and extensibility. A hybrid\ncontrol detection pipeline fuses Windows UI Automation (UIA) with vision-based\nparsing to support diverse interface styles. Runtime efficiency is further\nenhanced through speculative multi-action planning, reducing per-step LLM\noverhead. Finally, a Picture-in-Picture (PiP) interface enables automation\nwithin an isolated virtual desktop, allowing agents and users to operate\nconcurrently without interference.\n  We evaluate UFO2 across over 20 real-world Windows applications,\ndemonstrating substantial improvements in robustness and execution accuracy\nover prior CUAs. Our results show that deep OS integration unlocks a scalable\npath toward reliable, user-aligned desktop automation."
                },
                "authors": [
                    {
                        "name": "Chaoyun Zhang"
                    },
                    {
                        "name": "He Huang"
                    },
                    {
                        "name": "Chiming Ni"
                    },
                    {
                        "name": "Jian Mu"
                    },
                    {
                        "name": "Si Qin"
                    },
                    {
                        "name": "Shilin He"
                    },
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Fangkai Yang"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Liqun Li"
                    },
                    {
                        "name": "Yu Kang"
                    },
                    {
                        "name": "Zhao Jiang"
                    },
                    {
                        "name": "Suzhen Zheng"
                    },
                    {
                        "name": "Rujia Wang"
                    },
                    {
                        "name": "Jiaxu Qian"
                    },
                    {
                        "name": "Minghua Ma"
                    },
                    {
                        "name": "Jian-Guang Lou"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dongmei Zhang"
                },
                "author": "Dongmei Zhang",
                "arxiv_comment": "The source code of UFO2 is publicly available at\n  https://github.com/microsoft/UFO/, with comprehensive documentation provided\n  at https://microsoft.github.io/UFO/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13939v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13939v4",
                "updated": "2025-04-25T05:11:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    11,
                    19,
                    4,
                    115,
                    0
                ],
                "published": "2025-03-18T06:12:38Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    6,
                    12,
                    38,
                    1,
                    77,
                    0
                ],
                "title": "Med-R1: Reinforcement Learning for Generalizable Medical Reasoning in\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Med-R1: Reinforcement Learning for Generalizable Medical Reasoning in\n  Vision-Language Models"
                },
                "summary": "Vision-language models (VLMs) have achieved impressive progress in natural\nimage reasoning, yet their potential in medical imaging remains underexplored.\nMedical vision-language tasks demand precise understanding and clinically\ncoherent answers, which are difficult to achieve due to the complexity of\nmedical data and the scarcity of high-quality expert annotations. These\nchallenges limit the effectiveness of conventional supervised fine-tuning (SFT)\nand Chain-of-Thought (CoT) strategies that work well in general domains. To\naddress these challenges, we propose Med-R1, a reinforcement learning\n(RL)-enhanced vision-language model designed to improve generalization and\nreliability in medical reasoning. Built on the DeepSeek strategy, Med-R1 adopts\nGroup Relative Policy Optimization (GRPO) to encourage reward-guided learning\nbeyond static annotations. We comprehensively evaluate Med-R1 across eight\ndistinct medical imaging modalities. Med-R1 achieves a 29.94% improvement in\naverage accuracy over its base model Qwen2-VL-2B, and even outperforms\nQwen2-VL-72B-a model with 36x more parameters. To assess cross-task\ngeneralization, we further evaluate Med-R1 on five question types. Med-R1\noutperforms Qwen2-VL-2B by 32.06% in question-type generalization, also\nsurpassing Qwen2-VL-72B. We further explore the thinking process in Med-R1, a\ncrucial component for the success of Deepseek-R1. Our results show that\nomitting intermediate rationales (No-Thinking-Med-R1) not only improves\nin-domain and cross-domain generalization with less training, but also\nchallenges the assumption that more reasoning always helps. These findings\nsuggest that in medical VQA, it is not reasoning itself, but its quality and\ndomain alignment, that determine effectiveness. Together, these results\nhighlight that RL improves medical reasoning and generalization, enabling\nefficient and reliable VLMs for real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) have achieved impressive progress in natural\nimage reasoning, yet their potential in medical imaging remains underexplored.\nMedical vision-language tasks demand precise understanding and clinically\ncoherent answers, which are difficult to achieve due to the complexity of\nmedical data and the scarcity of high-quality expert annotations. These\nchallenges limit the effectiveness of conventional supervised fine-tuning (SFT)\nand Chain-of-Thought (CoT) strategies that work well in general domains. To\naddress these challenges, we propose Med-R1, a reinforcement learning\n(RL)-enhanced vision-language model designed to improve generalization and\nreliability in medical reasoning. Built on the DeepSeek strategy, Med-R1 adopts\nGroup Relative Policy Optimization (GRPO) to encourage reward-guided learning\nbeyond static annotations. We comprehensively evaluate Med-R1 across eight\ndistinct medical imaging modalities. Med-R1 achieves a 29.94% improvement in\naverage accuracy over its base model Qwen2-VL-2B, and even outperforms\nQwen2-VL-72B-a model with 36x more parameters. To assess cross-task\ngeneralization, we further evaluate Med-R1 on five question types. Med-R1\noutperforms Qwen2-VL-2B by 32.06% in question-type generalization, also\nsurpassing Qwen2-VL-72B. We further explore the thinking process in Med-R1, a\ncrucial component for the success of Deepseek-R1. Our results show that\nomitting intermediate rationales (No-Thinking-Med-R1) not only improves\nin-domain and cross-domain generalization with less training, but also\nchallenges the assumption that more reasoning always helps. These findings\nsuggest that in medical VQA, it is not reasoning itself, but its quality and\ndomain alignment, that determine effectiveness. Together, these results\nhighlight that RL improves medical reasoning and generalization, enabling\nefficient and reliable VLMs for real-world deployment."
                },
                "authors": [
                    {
                        "name": "Yuxiang Lai"
                    },
                    {
                        "name": "Jike Zhong"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Shitian Zhao"
                    },
                    {
                        "name": "Xiaofeng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofeng Yang"
                },
                "author": "Xiaofeng Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13939v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13939v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18070v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18070v1",
                "updated": "2025-04-25T04:47:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    4,
                    47,
                    34,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T04:47:34Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    4,
                    47,
                    34,
                    4,
                    115,
                    0
                ],
                "title": "PropRAG: Guiding Retrieval with Beam Search over Proposition Paths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PropRAG: Guiding Retrieval with Beam Search over Proposition Paths"
                },
                "summary": "Retrieval Augmented Generation (RAG) has become the standard non-parametric\napproach for equipping Large Language Models (LLMs) with up-to-date knowledge\nand mitigating catastrophic forgetting common in continual learning. However,\nstandard RAG, relying on independent passage retrieval, fails to capture the\ninterconnected nature of human memory crucial for complex reasoning\n(associativity) and contextual understanding (sense-making). While structured\nRAG methods like HippoRAG utilize knowledge graphs (KGs) built from triples,\nthe inherent context loss limits fidelity. We introduce PropRAG, a framework\nleveraging contextually rich propositions and a novel beam search algorithm\nover proposition paths to explicitly discover multi-step reasoning chains.\nCrucially, PropRAG's online retrieval process operates entirely without\ninvoking generative LLMs, relying instead on efficient graph traversal and\npre-computed embeddings. This avoids online LLM inference costs and potential\ninconsistencies during evidence gathering. LLMs are used effectively offline\nfor high-quality proposition extraction and post-retrieval for answer\ngeneration. PropRAG achieves state-of-the-art zero-shot Recall@5 results on\nPopQA (55.3%), 2Wiki (93.7%), HotpotQA (97.0%), and MuSiQue (77.3%), alongside\ntop F1 scores (e.g., 52.4% on MuSiQue). By improving evidence retrieval through\nricher representation and explicit, LLM-free online path finding, PropRAG\nadvances non-parametric continual learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) has become the standard non-parametric\napproach for equipping Large Language Models (LLMs) with up-to-date knowledge\nand mitigating catastrophic forgetting common in continual learning. However,\nstandard RAG, relying on independent passage retrieval, fails to capture the\ninterconnected nature of human memory crucial for complex reasoning\n(associativity) and contextual understanding (sense-making). While structured\nRAG methods like HippoRAG utilize knowledge graphs (KGs) built from triples,\nthe inherent context loss limits fidelity. We introduce PropRAG, a framework\nleveraging contextually rich propositions and a novel beam search algorithm\nover proposition paths to explicitly discover multi-step reasoning chains.\nCrucially, PropRAG's online retrieval process operates entirely without\ninvoking generative LLMs, relying instead on efficient graph traversal and\npre-computed embeddings. This avoids online LLM inference costs and potential\ninconsistencies during evidence gathering. LLMs are used effectively offline\nfor high-quality proposition extraction and post-retrieval for answer\ngeneration. PropRAG achieves state-of-the-art zero-shot Recall@5 results on\nPopQA (55.3%), 2Wiki (93.7%), HotpotQA (97.0%), and MuSiQue (77.3%), alongside\ntop F1 scores (e.g., 52.4% on MuSiQue). By improving evidence retrieval through\nricher representation and explicit, LLM-free online path finding, PropRAG\nadvances non-parametric continual learning."
                },
                "authors": [
                    {
                        "name": "Jingjin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jingjin Wang"
                },
                "author": "Jingjin Wang",
                "arxiv_comment": "Code and data to be released at:\n  https://github.com/ReLink-Inc/PropRAG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18070v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15638v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15638v2",
                "updated": "2025-04-25T04:37:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    4,
                    37,
                    45,
                    4,
                    115,
                    0
                ],
                "published": "2024-05-24T15:25:28Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    15,
                    25,
                    28,
                    4,
                    145,
                    0
                ],
                "title": "M4U: Evaluating Multilingual Understanding and Reasoning for Large\n  Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M4U: Evaluating Multilingual Understanding and Reasoning for Large\n  Multimodal Models"
                },
                "summary": "Multilingual capability is an essential aspect for large multimodal models,\nsince they are usually deployed across various countries and languages.\nHowever, most existing benchmarks for multilingual multimodal reasoning\nstruggle to differentiate between models of varying performance; even language\nmodels without visual capabilities can easily achieve high scores. This leaves\na comprehensive evaluation of leading multilingual multimodal models largely\nunexplored. In this work, we introduce M4U, a novel and challenging benchmark\nfor assessing the capability of multi-discipline multilingual multimodal\nunderstanding and reasoning. M4U contains 10k samples covering 64 disciplines\nacross 16 subfields in Science, Engineering, and Healthcare in six languages.\nUsing M4U, we conduct extensive evaluations of leading Large Multimodal Models\n(LMMs) and Large Language Models (LLMs) with external tools. The evaluation\nresults demonstrate that the state-of-the-art model, GPT-4o, achieves only\n47.6% average accuracy on M4U. Additionally, we observe that the leading LMMs\nexhibit significant language preferences. Our in-depth analysis indicates that\nleading LMMs, including GPT-4o, struggle to perform reasoning using\nmultilingual information present in both visual and textual context.\nSpecifically, they suffer performance degradation when prompted with\ncross-lingual multimodal questions. Our code and dataset is public available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual capability is an essential aspect for large multimodal models,\nsince they are usually deployed across various countries and languages.\nHowever, most existing benchmarks for multilingual multimodal reasoning\nstruggle to differentiate between models of varying performance; even language\nmodels without visual capabilities can easily achieve high scores. This leaves\na comprehensive evaluation of leading multilingual multimodal models largely\nunexplored. In this work, we introduce M4U, a novel and challenging benchmark\nfor assessing the capability of multi-discipline multilingual multimodal\nunderstanding and reasoning. M4U contains 10k samples covering 64 disciplines\nacross 16 subfields in Science, Engineering, and Healthcare in six languages.\nUsing M4U, we conduct extensive evaluations of leading Large Multimodal Models\n(LMMs) and Large Language Models (LLMs) with external tools. The evaluation\nresults demonstrate that the state-of-the-art model, GPT-4o, achieves only\n47.6% average accuracy on M4U. Additionally, we observe that the leading LMMs\nexhibit significant language preferences. Our in-depth analysis indicates that\nleading LMMs, including GPT-4o, struggle to perform reasoning using\nmultilingual information present in both visual and textual context.\nSpecifically, they suffer performance degradation when prompted with\ncross-lingual multimodal questions. Our code and dataset is public available."
                },
                "authors": [
                    {
                        "name": "Hongyu Wang"
                    },
                    {
                        "name": "Jiayu Xu"
                    },
                    {
                        "name": "Senwei Xie"
                    },
                    {
                        "name": "Ruiping Wang"
                    },
                    {
                        "name": "Jialin Li"
                    },
                    {
                        "name": "Zhaojie Xie"
                    },
                    {
                        "name": "Bin Zhang"
                    },
                    {
                        "name": "Chuyan Xiong"
                    },
                    {
                        "name": "Xilin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xilin Chen"
                },
                "author": "Xilin Chen",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15638v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15638v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18062v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18062v1",
                "updated": "2025-04-25T04:18:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    4,
                    18,
                    23,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T04:18:23Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    4,
                    18,
                    23,
                    4,
                    115,
                    0
                ],
                "title": "LLM-Guided Open RAN: Empowering Hierarchical RAN Intelligent Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Guided Open RAN: Empowering Hierarchical RAN Intelligent Control"
                },
                "summary": "Recent advancements in large language models (LLMs) have led to a significant\ninterest in deploying LLMempowered algorithms for wireless communication\nnetworks. Meanwhile, open radio access network (O-RAN) techniques offer\nunprecedented flexibility, with the non-real-time (non-RT) radio access network\n(RAN) intelligent controller (RIC) (non-RT RIC) and near-real-time (near-RT)\nRIC (near-RT RIC) components enabling intelligent resource management across\ndifferent time scales. In this paper, we propose the LLM empowered hierarchical\nRIC (LLM-hRIC) framework to improve the collaboration between RICs. This\nframework integrates LLMs with reinforcement learning (RL) for efficient\nnetwork resource management. In this framework, LLMs-empowered non-RT RICs\nprovide strategic guidance and high-level policies based on environmental\ncontext. Concurrently, RL-empowered near-RT RICs perform low-latency tasks\nbased on strategic guidance and local near-RT observation. We evaluate the\nLLM-hRIC framework in an integrated access and backhaul (IAB) network setting.\nSimulation results demonstrate that the proposed framework achieves superior\nperformance. Finally, we discuss the key future challenges in applying LLMs to\nO-RAN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have led to a significant\ninterest in deploying LLMempowered algorithms for wireless communication\nnetworks. Meanwhile, open radio access network (O-RAN) techniques offer\nunprecedented flexibility, with the non-real-time (non-RT) radio access network\n(RAN) intelligent controller (RIC) (non-RT RIC) and near-real-time (near-RT)\nRIC (near-RT RIC) components enabling intelligent resource management across\ndifferent time scales. In this paper, we propose the LLM empowered hierarchical\nRIC (LLM-hRIC) framework to improve the collaboration between RICs. This\nframework integrates LLMs with reinforcement learning (RL) for efficient\nnetwork resource management. In this framework, LLMs-empowered non-RT RICs\nprovide strategic guidance and high-level policies based on environmental\ncontext. Concurrently, RL-empowered near-RT RICs perform low-latency tasks\nbased on strategic guidance and local near-RT observation. We evaluate the\nLLM-hRIC framework in an integrated access and backhaul (IAB) network setting.\nSimulation results demonstrate that the proposed framework achieves superior\nperformance. Finally, we discuss the key future challenges in applying LLMs to\nO-RAN."
                },
                "authors": [
                    {
                        "name": "Lingyan Bao"
                    },
                    {
                        "name": "Sinwoong Yun"
                    },
                    {
                        "name": "Jemin Lee"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18062v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18062v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07611v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07611v2",
                "updated": "2025-04-25T03:53:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    3,
                    53,
                    34,
                    4,
                    115,
                    0
                ],
                "published": "2024-11-12T07:34:56Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    7,
                    34,
                    56,
                    1,
                    317,
                    0
                ],
                "title": "Knowledge-Augmented Multimodal Clinical Rationale Generation for Disease\n  Diagnosis with Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Augmented Multimodal Clinical Rationale Generation for Disease\n  Diagnosis with Small Language Models"
                },
                "summary": "Interpretation is critical for disease diagnosis, but existing models\nstruggle to balance predictive accuracy with human-understandable rationales.\nWhile large language models (LLMs) offer strong reasoning abilities, their\nclinical use is limited by high computational costs and restricted multimodal\nreasoning ability. Small language models (SLMs) are efficient but lack advanced\nreasoning for integrating multimodal medical data. In addition, both LLMs and\nSLMs lack of domain knowledge for trustworthy reasoning. Therefore, we propose\nClinRaGen, enhancing SLMs by leveraging LLM-derived reasoning ability via\nrationale distillation and domain knowledge injection for trustworthy\nmultimodal rationale generation. Key innovations include a sequential rationale\ndistillation framework that equips SLMs with LLM-comparable mutlimodal\nreasoning abilities, and a knowledge-augmented attention mechanism that jointly\nunifies multimodal representation from time series and textual data in a same\nencoding space, enabling it naturally interpreted by SLMs while incorporating\ndomain knowledge for reliable rationale generation. Experiments on real-world\nmedical datasets show that ClinRaGen achieves state-of-the-art performance in\ndisease diagnosis and rationale generation, demonstrating the effectiveness of\ncombining LLM-driven reasoning with knowledge augmentation for improved\ninterpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretation is critical for disease diagnosis, but existing models\nstruggle to balance predictive accuracy with human-understandable rationales.\nWhile large language models (LLMs) offer strong reasoning abilities, their\nclinical use is limited by high computational costs and restricted multimodal\nreasoning ability. Small language models (SLMs) are efficient but lack advanced\nreasoning for integrating multimodal medical data. In addition, both LLMs and\nSLMs lack of domain knowledge for trustworthy reasoning. Therefore, we propose\nClinRaGen, enhancing SLMs by leveraging LLM-derived reasoning ability via\nrationale distillation and domain knowledge injection for trustworthy\nmultimodal rationale generation. Key innovations include a sequential rationale\ndistillation framework that equips SLMs with LLM-comparable mutlimodal\nreasoning abilities, and a knowledge-augmented attention mechanism that jointly\nunifies multimodal representation from time series and textual data in a same\nencoding space, enabling it naturally interpreted by SLMs while incorporating\ndomain knowledge for reliable rationale generation. Experiments on real-world\nmedical datasets show that ClinRaGen achieves state-of-the-art performance in\ndisease diagnosis and rationale generation, demonstrating the effectiveness of\ncombining LLM-driven reasoning with knowledge augmentation for improved\ninterpretability."
                },
                "authors": [
                    {
                        "name": "Shuai Niu"
                    },
                    {
                        "name": "Jing Ma"
                    },
                    {
                        "name": "Hongzhan Lin"
                    },
                    {
                        "name": "Liang Bai"
                    },
                    {
                        "name": "Zhihua Wang"
                    },
                    {
                        "name": "Yida Xu"
                    },
                    {
                        "name": "Yunya Song"
                    },
                    {
                        "name": "Xian Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xian Yang"
                },
                "author": "Xian Yang",
                "arxiv_comment": "13 pages. 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07611v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07611v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18050v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18050v1",
                "updated": "2025-04-25T03:39:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    3,
                    39,
                    19,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T03:39:19Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    3,
                    39,
                    19,
                    4,
                    115,
                    0
                ],
                "title": "Validating Network Protocol Parsers with Traceable RFC Document\n  Interpretation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Validating Network Protocol Parsers with Traceable RFC Document\n  Interpretation"
                },
                "summary": "Validating the correctness of network protocol implementations is highly\nchallenging due to the oracle and traceability problems. The former determines\nwhen a protocol implementation can be considered buggy, especially when the\nbugs do not cause any observable symptoms. The latter allows developers to\nunderstand how an implementation violates the protocol specification, thereby\nfacilitating bug fixes. Unlike existing works that rarely take both problems\ninto account, this work considers both and provides an effective solution using\nrecent advances in large language models (LLMs). Our key observation is that\nnetwork protocols are often released with structured specification documents,\na.k.a. RFC documents, which can be systematically translated to formal protocol\nmessage specifications via LLMs. Such specifications, which may contain errors\ndue to the hallucination of LLMs, are used as a quasi-oracle to validate\nprotocol parsers, while the validation results in return gradually refine the\noracle. Since the oracle is derived from the document, any bugs we find in a\nprotocol implementation can be traced back to the document, thus addressing the\ntraceability problem. We have extensively evaluated our approach using nine\nnetwork protocols and their implementations written in C, Python, and Go. The\nresults show that our approach outperforms the state-of-the-art and has\ndetected 69 bugs, with 36 confirmed. The project also demonstrates the\npotential for fully automating software validation based on natural language\nspecifications, a process previously considered predominantly manual due to the\nneed to understand specification documents and derive expected outputs for test\ninputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Validating the correctness of network protocol implementations is highly\nchallenging due to the oracle and traceability problems. The former determines\nwhen a protocol implementation can be considered buggy, especially when the\nbugs do not cause any observable symptoms. The latter allows developers to\nunderstand how an implementation violates the protocol specification, thereby\nfacilitating bug fixes. Unlike existing works that rarely take both problems\ninto account, this work considers both and provides an effective solution using\nrecent advances in large language models (LLMs). Our key observation is that\nnetwork protocols are often released with structured specification documents,\na.k.a. RFC documents, which can be systematically translated to formal protocol\nmessage specifications via LLMs. Such specifications, which may contain errors\ndue to the hallucination of LLMs, are used as a quasi-oracle to validate\nprotocol parsers, while the validation results in return gradually refine the\noracle. Since the oracle is derived from the document, any bugs we find in a\nprotocol implementation can be traced back to the document, thus addressing the\ntraceability problem. We have extensively evaluated our approach using nine\nnetwork protocols and their implementations written in C, Python, and Go. The\nresults show that our approach outperforms the state-of-the-art and has\ndetected 69 bugs, with 36 confirmed. The project also demonstrates the\npotential for fully automating software validation based on natural language\nspecifications, a process previously considered predominantly manual due to the\nneed to understand specification documents and derive expected outputs for test\ninputs."
                },
                "authors": [
                    {
                        "name": "Mingwei Zheng"
                    },
                    {
                        "name": "Danning Xie"
                    },
                    {
                        "name": "Qingkai Shi"
                    },
                    {
                        "name": "Chengpeng Wang"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhang"
                },
                "author": "Xiangyu Zhang",
                "arxiv_doi": "10.1145/3728955",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3728955",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.18050v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18050v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18044v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18044v1",
                "updated": "2025-04-25T03:26:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    3,
                    26,
                    30,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T03:26:30Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    3,
                    26,
                    30,
                    4,
                    115,
                    0
                ],
                "title": "AI Ethics and Social Norms: Exploring ChatGPT's Capabilities From What\n  to How",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Ethics and Social Norms: Exploring ChatGPT's Capabilities From What\n  to How"
                },
                "summary": "Using LLMs in healthcare, Computer-Supported Cooperative Work, and Social\nComputing requires the examination of ethical and social norms to ensure safe\nincorporation into human life. We conducted a mixed-method study, including an\nonline survey with 111 participants and an interview study with 38 experts, to\ninvestigate the AI ethics and social norms in ChatGPT as everyday life tools.\nThis study aims to evaluate whether ChatGPT in an empirical context operates\nfollowing ethics and social norms, which is critical for understanding actions\nin industrial and academic research and achieving machine ethics. The findings\nof this study provide initial insights into six important aspects of AI ethics,\nincluding bias, trustworthiness, security, toxicology, social norms, and\nethical data. Significant obstacles related to transparency and bias in\nunsupervised data collection methods are identified as ChatGPT's ethical\nconcerns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using LLMs in healthcare, Computer-Supported Cooperative Work, and Social\nComputing requires the examination of ethical and social norms to ensure safe\nincorporation into human life. We conducted a mixed-method study, including an\nonline survey with 111 participants and an interview study with 38 experts, to\ninvestigate the AI ethics and social norms in ChatGPT as everyday life tools.\nThis study aims to evaluate whether ChatGPT in an empirical context operates\nfollowing ethics and social norms, which is critical for understanding actions\nin industrial and academic research and achieving machine ethics. The findings\nof this study provide initial insights into six important aspects of AI ethics,\nincluding bias, trustworthiness, security, toxicology, social norms, and\nethical data. Significant obstacles related to transparency and bias in\nunsupervised data collection methods are identified as ChatGPT's ethical\nconcerns."
                },
                "authors": [
                    {
                        "name": "Omid Veisi"
                    },
                    {
                        "name": "Sasan Bahrami"
                    },
                    {
                        "name": "Roman Englert"
                    },
                    {
                        "name": "Claudia Müller"
                    }
                ],
                "author_detail": {
                    "name": "Claudia Müller"
                },
                "author": "Claudia Müller",
                "arxiv_comment": "Accepted for presentation at the ACM Conference on Computer-Supported\n  Cooperative Work and Social Computing (CSCW) 2025. To appear in Proceedings\n  of the ACM on Human-Computer Interaction (PACM HCI)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18044v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18044v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18041v1",
                "updated": "2025-04-25T03:25:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    3,
                    25,
                    18,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T03:25:18Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    3,
                    25,
                    18,
                    4,
                    115,
                    0
                ],
                "title": "RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented\n  Generation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented\n  Generation for Large Language Models"
                },
                "summary": "Efforts to ensure the safety of large language models (LLMs) include safety\nfine-tuning, evaluation, and red teaming. However, despite the widespread use\nof the Retrieval-Augmented Generation (RAG) framework, AI safety work focuses\non standard LLMs, which means we know little about how RAG use cases change a\nmodel's safety profile. We conduct a detailed comparative analysis of RAG and\nnon-RAG frameworks with eleven LLMs. We find that RAG can make models less safe\nand change their safety profile. We explore the causes of this change and find\nthat even combinations of safe models with safe documents can cause unsafe\ngenerations. In addition, we evaluate some existing red teaming methods for RAG\nsettings and show that they are less effective than when used for non-RAG\nsettings. Our work highlights the need for safety research and red-teaming\nmethods specifically tailored for RAG LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efforts to ensure the safety of large language models (LLMs) include safety\nfine-tuning, evaluation, and red teaming. However, despite the widespread use\nof the Retrieval-Augmented Generation (RAG) framework, AI safety work focuses\non standard LLMs, which means we know little about how RAG use cases change a\nmodel's safety profile. We conduct a detailed comparative analysis of RAG and\nnon-RAG frameworks with eleven LLMs. We find that RAG can make models less safe\nand change their safety profile. We explore the causes of this change and find\nthat even combinations of safe models with safe documents can cause unsafe\ngenerations. In addition, we evaluate some existing red teaming methods for RAG\nsettings and show that they are less effective than when used for non-RAG\nsettings. Our work highlights the need for safety research and red-teaming\nmethods specifically tailored for RAG LLMs."
                },
                "authors": [
                    {
                        "name": "Bang An"
                    },
                    {
                        "name": "Shiyue Zhang"
                    },
                    {
                        "name": "Mark Dredze"
                    }
                ],
                "author_detail": {
                    "name": "Mark Dredze"
                },
                "author": "Mark Dredze",
                "arxiv_comment": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12881v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12881v2",
                "updated": "2025-04-25T03:14:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    3,
                    14,
                    33,
                    4,
                    115,
                    0
                ],
                "published": "2024-10-15T18:25:53Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    18,
                    25,
                    53,
                    1,
                    289,
                    0
                ],
                "title": "MIND: Math Informed syNthetic Dialogues for Pretraining LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIND: Math Informed syNthetic Dialogues for Pretraining LLMs"
                },
                "summary": "The utility of synthetic data to enhance pretraining data quality and hence\nto improve downstream task accuracy has been widely explored in recent large\nlanguage models (LLMs). Yet, these approaches fall inadequate in complex,\nmulti-hop and mathematical reasoning tasks as the synthetic data typically\nfails to add complementary knowledge to the existing raw corpus. In this work,\nwe propose a novel large-scale and diverse Math Informed syNthetic Dialogue\n(MIND) generation method that improves the mathematical reasoning ability of\nLLMs. Specifically, using MIND, we generate synthetic conversations based on\nOpenWebMath (OWM), resulting in a new math corpus, MIND-OWM. Our experiments\nwith different conversational settings reveal that incorporating knowledge gaps\nbetween dialog participants is essential for generating high-quality math data.\nWe further identify an effective way to format and integrate synthetic and raw\ndata during pretraining to maximize the gain in mathematical reasoning,\nemphasizing the need to restructure raw data rather than use it as-is. Compared\nto pretraining just on raw data, a model pretrained on MIND-OWM shows\nsignificant boost in mathematical reasoning (GSM8K: +13.42%, MATH: +2.30%),\nincluding superior performance in specialized knowledge (MMLU: +4.55%,\nMMLU-STEM: +4.28%) and general purpose reasoning tasks (GENERAL REASONING:\n+2.51%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The utility of synthetic data to enhance pretraining data quality and hence\nto improve downstream task accuracy has been widely explored in recent large\nlanguage models (LLMs). Yet, these approaches fall inadequate in complex,\nmulti-hop and mathematical reasoning tasks as the synthetic data typically\nfails to add complementary knowledge to the existing raw corpus. In this work,\nwe propose a novel large-scale and diverse Math Informed syNthetic Dialogue\n(MIND) generation method that improves the mathematical reasoning ability of\nLLMs. Specifically, using MIND, we generate synthetic conversations based on\nOpenWebMath (OWM), resulting in a new math corpus, MIND-OWM. Our experiments\nwith different conversational settings reveal that incorporating knowledge gaps\nbetween dialog participants is essential for generating high-quality math data.\nWe further identify an effective way to format and integrate synthetic and raw\ndata during pretraining to maximize the gain in mathematical reasoning,\nemphasizing the need to restructure raw data rather than use it as-is. Compared\nto pretraining just on raw data, a model pretrained on MIND-OWM shows\nsignificant boost in mathematical reasoning (GSM8K: +13.42%, MATH: +2.30%),\nincluding superior performance in specialized knowledge (MMLU: +4.55%,\nMMLU-STEM: +4.28%) and general purpose reasoning tasks (GENERAL REASONING:\n+2.51%)."
                },
                "authors": [
                    {
                        "name": "Syeda Nahida Akter"
                    },
                    {
                        "name": "Shrimai Prabhumoye"
                    },
                    {
                        "name": "John Kamalu"
                    },
                    {
                        "name": "Sanjeev Satheesh"
                    },
                    {
                        "name": "Eric Nyberg"
                    },
                    {
                        "name": "Mostofa Patwary"
                    },
                    {
                        "name": "Mohammad Shoeybi"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Catanzaro"
                },
                "author": "Bryan Catanzaro",
                "arxiv_comment": "31 pages, 5 figures, 14 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12881v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12881v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18039v1",
                "updated": "2025-04-25T03:12:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    3,
                    12,
                    43,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T03:12:43Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    3,
                    12,
                    43,
                    4,
                    115,
                    0
                ],
                "title": "MultiMind: Enhancing Werewolf Agents with Multimodal Reasoning and\n  Theory of Mind",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiMind: Enhancing Werewolf Agents with Multimodal Reasoning and\n  Theory of Mind"
                },
                "summary": "Large Language Model (LLM) agents have demonstrated impressive capabilities\nin social deduction games (SDGs) like Werewolf, where strategic reasoning and\nsocial deception are essential. However, current approaches remain limited to\ntextual information, ignoring crucial multimodal cues such as facial\nexpressions and tone of voice that humans naturally use to communicate.\nMoreover, existing SDG agents primarily focus on inferring other players'\nidentities without modeling how others perceive themselves or fellow players.\nTo address these limitations, we use One Night Ultimate Werewolf (ONUW) as a\ntestbed and present MultiMind, the first framework integrating multimodal\ninformation into SDG agents. MultiMind processes facial expressions and vocal\ntones alongside verbal content, while employing a Theory of Mind (ToM) model to\nrepresent each player's suspicion levels toward others. By combining this ToM\nmodel with Monte Carlo Tree Search (MCTS), our agent identifies communication\nstrategies that minimize suspicion directed at itself. Through comprehensive\nevaluation in both agent-versus-agent simulations and studies with human\nplayers, we demonstrate MultiMind's superior performance in gameplay. Our work\npresents a significant advancement toward LLM agents capable of human-like\nsocial reasoning across multimodal domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents have demonstrated impressive capabilities\nin social deduction games (SDGs) like Werewolf, where strategic reasoning and\nsocial deception are essential. However, current approaches remain limited to\ntextual information, ignoring crucial multimodal cues such as facial\nexpressions and tone of voice that humans naturally use to communicate.\nMoreover, existing SDG agents primarily focus on inferring other players'\nidentities without modeling how others perceive themselves or fellow players.\nTo address these limitations, we use One Night Ultimate Werewolf (ONUW) as a\ntestbed and present MultiMind, the first framework integrating multimodal\ninformation into SDG agents. MultiMind processes facial expressions and vocal\ntones alongside verbal content, while employing a Theory of Mind (ToM) model to\nrepresent each player's suspicion levels toward others. By combining this ToM\nmodel with Monte Carlo Tree Search (MCTS), our agent identifies communication\nstrategies that minimize suspicion directed at itself. Through comprehensive\nevaluation in both agent-versus-agent simulations and studies with human\nplayers, we demonstrate MultiMind's superior performance in gameplay. Our work\npresents a significant advancement toward LLM agents capable of human-like\nsocial reasoning across multimodal domains."
                },
                "authors": [
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Nuoqian Xiao"
                    },
                    {
                        "name": "Qi Chai"
                    },
                    {
                        "name": "Deheng Ye"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12285v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12285v2",
                "updated": "2025-04-25T03:07:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    3,
                    7,
                    55,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-16T17:51:43Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    17,
                    51,
                    43,
                    2,
                    106,
                    0
                ],
                "title": "BitNet b1.58 2B4T Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitNet b1.58 2B4T Technical Report"
                },
                "summary": "We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large\nLanguage Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4\ntrillion tokens, the model has been rigorously evaluated across benchmarks\ncovering language understanding, mathematical reasoning, coding proficiency,\nand conversational ability. Our results demonstrate that BitNet b1.58 2B4T\nachieves performance on par with leading open-weight, full-precision LLMs of\nsimilar size, while offering significant advantages in computational\nefficiency, including substantially reduced memory footprint, energy\nconsumption, and decoding latency. To facilitate further research and adoption,\nthe model weights are released via Hugging Face along with open-source\ninference implementations for both GPU and CPU architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large\nLanguage Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4\ntrillion tokens, the model has been rigorously evaluated across benchmarks\ncovering language understanding, mathematical reasoning, coding proficiency,\nand conversational ability. Our results demonstrate that BitNet b1.58 2B4T\nachieves performance on par with leading open-weight, full-precision LLMs of\nsimilar size, while offering significant advantages in computational\nefficiency, including substantially reduced memory footprint, energy\nconsumption, and decoding latency. To facilitate further research and adoption,\nthe model weights are released via Hugging Face along with open-source\ninference implementations for both GPU and CPU architectures."
                },
                "authors": [
                    {
                        "name": "Shuming Ma"
                    },
                    {
                        "name": "Hongyu Wang"
                    },
                    {
                        "name": "Shaohan Huang"
                    },
                    {
                        "name": "Xingxing Zhang"
                    },
                    {
                        "name": "Ying Hu"
                    },
                    {
                        "name": "Ting Song"
                    },
                    {
                        "name": "Yan Xia"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12285v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12285v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18024v1",
                "updated": "2025-04-25T02:29:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    2,
                    29,
                    56,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T02:29:56Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    2,
                    29,
                    56,
                    4,
                    115,
                    0
                ],
                "title": "SMARTFinRAG: Interactive Modularized Financial RAG Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMARTFinRAG: Interactive Modularized Financial RAG Benchmark"
                },
                "summary": "Financial sectors are rapidly adopting language model technologies, yet\nevaluating specialized RAG systems in this domain remains challenging. This\npaper introduces SMARTFinRAG, addressing three critical gaps in financial RAG\nassessment: (1) a fully modular architecture where components can be\ndynamically interchanged during runtime; (2) a document-centric evaluation\nparadigm generating domain-specific QA pairs from newly ingested financial\ndocuments; and (3) an intuitive interface bridging research-implementation\ndivides. Our evaluation quantifies both retrieval efficacy and response\nquality, revealing significant performance variations across configurations.\nThe platform's open-source architecture supports transparent, reproducible\nresearch while addressing practical deployment challenges faced by financial\ninstitutions implementing RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Financial sectors are rapidly adopting language model technologies, yet\nevaluating specialized RAG systems in this domain remains challenging. This\npaper introduces SMARTFinRAG, addressing three critical gaps in financial RAG\nassessment: (1) a fully modular architecture where components can be\ndynamically interchanged during runtime; (2) a document-centric evaluation\nparadigm generating domain-specific QA pairs from newly ingested financial\ndocuments; and (3) an intuitive interface bridging research-implementation\ndivides. Our evaluation quantifies both retrieval efficacy and response\nquality, revealing significant performance variations across configurations.\nThe platform's open-source architecture supports transparent, reproducible\nresearch while addressing practical deployment challenges faced by financial\ninstitutions implementing RAG systems."
                },
                "authors": [
                    {
                        "name": "Yiwei Zha"
                    }
                ],
                "author_detail": {
                    "name": "Yiwei Zha"
                },
                "author": "Yiwei Zha",
                "arxiv_comment": "For open source github repo, see\n  https://github.com/JonathanZha47/SMARTFinRAG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01840v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01840v2",
                "updated": "2025-04-25T01:57:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    1,
                    57,
                    31,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-02T15:45:03Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    45,
                    3,
                    2,
                    92,
                    0
                ],
                "title": "LRAGE: Legal Retrieval Augmented Generation Evaluation Tool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LRAGE: Legal Retrieval Augmented Generation Evaluation Tool"
                },
                "summary": "Recently, building retrieval-augmented generation (RAG) systems to enhance\nthe capability of large language models (LLMs) has become a common practice.\nEspecially in the legal domain, previous judicial decisions play a significant\nrole under the doctrine of stare decisis which emphasizes the importance of\nmaking decisions based on (retrieved) prior documents. However, the overall\nperformance of RAG system depends on many components: (1) retrieval corpora,\n(2) retrieval algorithms, (3) rerankers, (4) LLM backbones, and (5) evaluation\nmetrics. Here we propose LRAGE, an open-source tool for holistic evaluation of\nRAG systems focusing on the legal domain. LRAGE provides GUI and CLI interfaces\nto facilitate seamless experiments and investigate how changes in the\naforementioned five components affect the overall accuracy. We validated LRAGE\nusing multilingual legal benches including Korean (KBL), English (LegalBench),\nand Chinese (LawBench) by demonstrating how the overall accuracy changes when\nvarying the five components mentioned above. The source code is available at\nhttps://github.com/hoorangyee/LRAGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, building retrieval-augmented generation (RAG) systems to enhance\nthe capability of large language models (LLMs) has become a common practice.\nEspecially in the legal domain, previous judicial decisions play a significant\nrole under the doctrine of stare decisis which emphasizes the importance of\nmaking decisions based on (retrieved) prior documents. However, the overall\nperformance of RAG system depends on many components: (1) retrieval corpora,\n(2) retrieval algorithms, (3) rerankers, (4) LLM backbones, and (5) evaluation\nmetrics. Here we propose LRAGE, an open-source tool for holistic evaluation of\nRAG systems focusing on the legal domain. LRAGE provides GUI and CLI interfaces\nto facilitate seamless experiments and investigate how changes in the\naforementioned five components affect the overall accuracy. We validated LRAGE\nusing multilingual legal benches including Korean (KBL), English (LegalBench),\nand Chinese (LawBench) by demonstrating how the overall accuracy changes when\nvarying the five components mentioned above. The source code is available at\nhttps://github.com/hoorangyee/LRAGE."
                },
                "authors": [
                    {
                        "name": "Minhu Park"
                    },
                    {
                        "name": "Hongseok Oh"
                    },
                    {
                        "name": "Eunkyung Choi"
                    },
                    {
                        "name": "Wonseok Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Wonseok Hwang"
                },
                "author": "Wonseok Hwang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01840v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01840v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00709v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00709v3",
                "updated": "2025-04-25T01:33:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    1,
                    33,
                    35,
                    4,
                    115,
                    0
                ],
                "published": "2025-02-02T07:49:56Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    7,
                    49,
                    56,
                    6,
                    33,
                    0
                ],
                "title": "RankFlow: A Multi-Role Collaborative Reranking Workflow Utilizing Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RankFlow: A Multi-Role Collaborative Reranking Workflow Utilizing Large\n  Language Models"
                },
                "summary": "In an Information Retrieval (IR) system, reranking plays a critical role by\nsorting candidate passages according to their relevance to a specific query.\nThis process demands a nuanced understanding of the variations among passages\nlinked to the query. In this work, we introduce RankFlow, a multi-role\nreranking workflow that leverages the capabilities of Large Language Models\n(LLMs) and role specializations to improve reranking performance. RankFlow\nenlists LLMs to fulfill four distinct roles: the query Rewriter, the pseudo\nAnswerer, the passage Summarizer, and the Reranker. This orchestrated approach\nenables RankFlow to: (1) accurately interpret queries, (2) draw upon LLMs'\nextensive pre-existing knowledge, (3) distill passages into concise versions,\nand (4) assess passages in a comprehensive manner, resulting in notably better\nreranking results. Our experimental results reveal that RankFlow outperforms\nexisting leading approaches on widely recognized IR benchmarks, such as\nTREC-DL, BEIR, and NovelEval. Additionally, we investigate the individual\ncontributions of each role in RankFlow. Code is available at\nhttps://github.com/jincan333/RankFlow",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In an Information Retrieval (IR) system, reranking plays a critical role by\nsorting candidate passages according to their relevance to a specific query.\nThis process demands a nuanced understanding of the variations among passages\nlinked to the query. In this work, we introduce RankFlow, a multi-role\nreranking workflow that leverages the capabilities of Large Language Models\n(LLMs) and role specializations to improve reranking performance. RankFlow\nenlists LLMs to fulfill four distinct roles: the query Rewriter, the pseudo\nAnswerer, the passage Summarizer, and the Reranker. This orchestrated approach\nenables RankFlow to: (1) accurately interpret queries, (2) draw upon LLMs'\nextensive pre-existing knowledge, (3) distill passages into concise versions,\nand (4) assess passages in a comprehensive manner, resulting in notably better\nreranking results. Our experimental results reveal that RankFlow outperforms\nexisting leading approaches on widely recognized IR benchmarks, such as\nTREC-DL, BEIR, and NovelEval. Additionally, we investigate the individual\ncontributions of each role in RankFlow. Code is available at\nhttps://github.com/jincan333/RankFlow"
                },
                "authors": [
                    {
                        "name": "Can Jin"
                    },
                    {
                        "name": "Hongwu Peng"
                    },
                    {
                        "name": "Anxiang Zhang"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Jiahui Zhao"
                    },
                    {
                        "name": "Xi Xie"
                    },
                    {
                        "name": "Kuangzheng Li"
                    },
                    {
                        "name": "Shuya Feng"
                    },
                    {
                        "name": "Kai Zhong"
                    },
                    {
                        "name": "Caiwen Ding"
                    },
                    {
                        "name": "Dimitris N. Metaxas"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris N. Metaxas"
                },
                "author": "Dimitris N. Metaxas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00709v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00709v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18008v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18008v1",
                "updated": "2025-04-25T01:28:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    1,
                    28,
                    32,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T01:28:32Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    1,
                    28,
                    32,
                    4,
                    115,
                    0
                ],
                "title": "TGDT: A Temporal Graph-based Digital Twin for Urban Traffic Corridors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TGDT: A Temporal Graph-based Digital Twin for Urban Traffic Corridors"
                },
                "summary": "Urban congestion at signalized intersections leads to significant delays,\neconomic losses, and increased emissions. Existing deep learning models often\nlack spatial generalizability, rely on complex architectures, and struggle with\nreal-time deployment. To address these limitations, we propose the Temporal\nGraph-based Digital Twin (TGDT), a scalable framework that integrates Temporal\nConvolutional Networks and Attentional Graph Neural Networks for dynamic,\ndirection-aware traffic modeling and assessment at urban corridors. TGDT\nestimates key Measures of Effectiveness (MOEs) for traffic flow optimization at\nboth the intersection level (e.g., queue length, waiting time) and the corridor\nlevel (e.g., traffic volume, travel time). Its modular architecture and\nsequential optimization scheme enable easy extension to any number of\nintersections and MOEs. The model outperforms state-of-the-art baselines by\naccurately producing high-dimensional, concurrent multi-output estimates. It\nalso demonstrates high robustness and accuracy across diverse traffic\nconditions, including extreme scenarios, while relying on only a minimal set of\ntraffic features. Fully parallelized, TGDT can simulate over a thousand\nscenarios within a matter of seconds, offering a cost-effective, interpretable,\nand real-time solution for traffic signal optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Urban congestion at signalized intersections leads to significant delays,\neconomic losses, and increased emissions. Existing deep learning models often\nlack spatial generalizability, rely on complex architectures, and struggle with\nreal-time deployment. To address these limitations, we propose the Temporal\nGraph-based Digital Twin (TGDT), a scalable framework that integrates Temporal\nConvolutional Networks and Attentional Graph Neural Networks for dynamic,\ndirection-aware traffic modeling and assessment at urban corridors. TGDT\nestimates key Measures of Effectiveness (MOEs) for traffic flow optimization at\nboth the intersection level (e.g., queue length, waiting time) and the corridor\nlevel (e.g., traffic volume, travel time). Its modular architecture and\nsequential optimization scheme enable easy extension to any number of\nintersections and MOEs. The model outperforms state-of-the-art baselines by\naccurately producing high-dimensional, concurrent multi-output estimates. It\nalso demonstrates high robustness and accuracy across diverse traffic\nconditions, including extreme scenarios, while relying on only a minimal set of\ntraffic features. Fully parallelized, TGDT can simulate over a thousand\nscenarios within a matter of seconds, offering a cost-effective, interpretable,\nand real-time solution for traffic signal optimization."
                },
                "authors": [
                    {
                        "name": "Nooshin Yousefzadeh"
                    },
                    {
                        "name": "Rahul Sengupta"
                    },
                    {
                        "name": "Sanjay Ranka"
                    }
                ],
                "author_detail": {
                    "name": "Sanjay Ranka"
                },
                "author": "Sanjay Ranka",
                "arxiv_comment": "8 pages, 4 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18008v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18008v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14625v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14625v2",
                "updated": "2025-04-25T01:27:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    1,
                    27,
                    48,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-20T14:05:17Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    14,
                    5,
                    17,
                    6,
                    110,
                    0
                ],
                "title": "Towards Optimal Circuit Generation: Multi-Agent Collaboration Meets\n  Collective Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Optimal Circuit Generation: Multi-Agent Collaboration Meets\n  Collective Intelligence"
                },
                "summary": "Large language models (LLMs) have transformed code generation, yet their\napplication in hardware design produces gate counts 38\\%--1075\\% higher than\nhuman designs. We present CircuitMind, a multi-agent framework that achieves\nhuman-competitive efficiency through three key innovations: syntax locking\n(constraining generation to basic logic gates), retrieval-augmented generation\n(enabling knowledge-driven design), and dual-reward optimization (balancing\ncorrectness with efficiency). To evaluate our approach, we introduce TC-Bench,\nthe first gate-level benchmark harnessing collective intelligence from the\nTuringComplete ecosystem -- a competitive circuit design platform with hundreds\nof thousands of players. Experiments show CircuitMind enables 55.6\\% of model\nimplementations to match or exceed top-tier human experts in composite\nefficiency metrics. Most remarkably, our framework elevates the 14B Phi-4 model\nto outperform both GPT-4o mini and Gemini 2.0 Flash, achieving efficiency\ncomparable to the top 25\\% of human experts without requiring specialized\ntraining. These innovations establish a new paradigm for hardware optimization\nwhere collaborative AI systems leverage collective human expertise to achieve\noptimal circuit designs. Our model, data, and code are open-source at\nhttps://github.com/BUAA-CLab/CircuitMind.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have transformed code generation, yet their\napplication in hardware design produces gate counts 38\\%--1075\\% higher than\nhuman designs. We present CircuitMind, a multi-agent framework that achieves\nhuman-competitive efficiency through three key innovations: syntax locking\n(constraining generation to basic logic gates), retrieval-augmented generation\n(enabling knowledge-driven design), and dual-reward optimization (balancing\ncorrectness with efficiency). To evaluate our approach, we introduce TC-Bench,\nthe first gate-level benchmark harnessing collective intelligence from the\nTuringComplete ecosystem -- a competitive circuit design platform with hundreds\nof thousands of players. Experiments show CircuitMind enables 55.6\\% of model\nimplementations to match or exceed top-tier human experts in composite\nefficiency metrics. Most remarkably, our framework elevates the 14B Phi-4 model\nto outperform both GPT-4o mini and Gemini 2.0 Flash, achieving efficiency\ncomparable to the top 25\\% of human experts without requiring specialized\ntraining. These innovations establish a new paradigm for hardware optimization\nwhere collaborative AI systems leverage collective human expertise to achieve\noptimal circuit designs. Our model, data, and code are open-source at\nhttps://github.com/BUAA-CLab/CircuitMind."
                },
                "authors": [
                    {
                        "name": "Haiyan Qin"
                    },
                    {
                        "name": "Jiahao Feng"
                    },
                    {
                        "name": "Xiaotong Feng"
                    },
                    {
                        "name": "Wei W. Xing"
                    },
                    {
                        "name": "Wang Kang"
                    }
                ],
                "author_detail": {
                    "name": "Wang Kang"
                },
                "author": "Wang Kang",
                "arxiv_comment": "9 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14625v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14625v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14560v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14560v2",
                "updated": "2025-04-25T01:27:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    1,
                    27,
                    29,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-20T10:16:59Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    10,
                    16,
                    59,
                    6,
                    110,
                    0
                ],
                "title": "ReasoningV: Efficient Verilog Code Generation with Adaptive Hybrid\n  Reasoning Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReasoningV: Efficient Verilog Code Generation with Adaptive Hybrid\n  Reasoning Model"
                },
                "summary": "Large Language Models (LLMs) have advanced Verilog code generation\nsignificantly, yet face challenges in data quality, reasoning capabilities, and\ncomputational efficiency. This paper presents ReasoningV, a novel model\nemploying a hybrid reasoning strategy that integrates trained intrinsic\ncapabilities with dynamic inference adaptation for Verilog code generation. Our\nframework introduces three complementary innovations: (1) ReasoningV-5K, a\nhigh-quality dataset of 5,000 functionally verified instances with reasoning\npaths created through multi-dimensional filtering of PyraNet samples; (2) a\ntwo-stage training approach combining parameter-efficient fine-tuning for\nfoundational knowledge with full-parameter optimization for enhanced reasoning;\nand (3) an adaptive reasoning mechanism that dynamically adjusts reasoning\ndepth based on problem complexity, reducing token consumption by up to 75\\%\nwhile preserving performance. Experimental results demonstrate ReasoningV's\neffectiveness with a pass@1 accuracy of 57.8\\% on VerilogEval-human, achieving\nperformance competitive with leading commercial models like Gemini-2.0-flash\n(59.5\\%) and exceeding the previous best open-source model by 10.4 percentage\npoints. ReasoningV offers a more reliable and accessible pathway for advancing\nAI-driven hardware design automation, with our model, data, and code available\nat https://github.com/BUAA-CLab/ReasoningV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have advanced Verilog code generation\nsignificantly, yet face challenges in data quality, reasoning capabilities, and\ncomputational efficiency. This paper presents ReasoningV, a novel model\nemploying a hybrid reasoning strategy that integrates trained intrinsic\ncapabilities with dynamic inference adaptation for Verilog code generation. Our\nframework introduces three complementary innovations: (1) ReasoningV-5K, a\nhigh-quality dataset of 5,000 functionally verified instances with reasoning\npaths created through multi-dimensional filtering of PyraNet samples; (2) a\ntwo-stage training approach combining parameter-efficient fine-tuning for\nfoundational knowledge with full-parameter optimization for enhanced reasoning;\nand (3) an adaptive reasoning mechanism that dynamically adjusts reasoning\ndepth based on problem complexity, reducing token consumption by up to 75\\%\nwhile preserving performance. Experimental results demonstrate ReasoningV's\neffectiveness with a pass@1 accuracy of 57.8\\% on VerilogEval-human, achieving\nperformance competitive with leading commercial models like Gemini-2.0-flash\n(59.5\\%) and exceeding the previous best open-source model by 10.4 percentage\npoints. ReasoningV offers a more reliable and accessible pathway for advancing\nAI-driven hardware design automation, with our model, data, and code available\nat https://github.com/BUAA-CLab/ReasoningV."
                },
                "authors": [
                    {
                        "name": "Haiyan Qin"
                    },
                    {
                        "name": "Zhiwei Xie"
                    },
                    {
                        "name": "Jingjing Li"
                    },
                    {
                        "name": "Liangchen Li"
                    },
                    {
                        "name": "Xiaotong Feng"
                    },
                    {
                        "name": "Junzhan Liu"
                    },
                    {
                        "name": "Wang Kang"
                    }
                ],
                "author_detail": {
                    "name": "Wang Kang"
                },
                "author": "Wang Kang",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14560v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14560v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19325v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19325v3",
                "updated": "2025-04-25T01:08:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    1,
                    8,
                    35,
                    4,
                    115,
                    0
                ],
                "published": "2024-05-29T17:55:03Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    17,
                    55,
                    3,
                    2,
                    150,
                    0
                ],
                "title": "Nearest Neighbor Speculative Decoding for LLM Generation and Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nearest Neighbor Speculative Decoding for LLM Generation and Attribution"
                },
                "summary": "Large language models (LLMs) often hallucinate and lack the ability to\nprovide attribution for their generations. Semi-parametric LMs, such as kNN-LM,\napproach these limitations by refining the output of an LM for a given prompt\nusing its nearest neighbor matches in a non-parametric data store. However,\nthese models often exhibit slow inference speeds and produce non-fluent texts.\nIn this paper, we introduce Nearest Neighbor Speculative Decoding (NEST), a\nnovel semi-parametric language modeling approach that is capable of\nincorporating real-world text spans of arbitrary length into the LM generations\nand providing attribution to their sources. NEST performs token-level retrieval\nat each inference step to compute a semi-parametric mixture distribution and\nidentify promising span continuations in a corpus. It then uses an approximate\nspeculative decoding procedure that accepts a prefix of the retrieved span or\ngenerates a new token. NEST significantly enhances the generation quality and\nattribution rate of the base LM across a variety of knowledge-intensive tasks,\nsurpassing the conventional kNN-LM method and performing competitively with\nin-context retrieval augmentation. In addition, NEST substantially improves the\ngeneration speed, achieving a 1.8x speedup in inference time when applied to\nLlama-2-Chat 70B. Code will be released at\nhttps://github.com/facebookresearch/NEST/tree/main.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often hallucinate and lack the ability to\nprovide attribution for their generations. Semi-parametric LMs, such as kNN-LM,\napproach these limitations by refining the output of an LM for a given prompt\nusing its nearest neighbor matches in a non-parametric data store. However,\nthese models often exhibit slow inference speeds and produce non-fluent texts.\nIn this paper, we introduce Nearest Neighbor Speculative Decoding (NEST), a\nnovel semi-parametric language modeling approach that is capable of\nincorporating real-world text spans of arbitrary length into the LM generations\nand providing attribution to their sources. NEST performs token-level retrieval\nat each inference step to compute a semi-parametric mixture distribution and\nidentify promising span continuations in a corpus. It then uses an approximate\nspeculative decoding procedure that accepts a prefix of the retrieved span or\ngenerates a new token. NEST significantly enhances the generation quality and\nattribution rate of the base LM across a variety of knowledge-intensive tasks,\nsurpassing the conventional kNN-LM method and performing competitively with\nin-context retrieval augmentation. In addition, NEST substantially improves the\ngeneration speed, achieving a 1.8x speedup in inference time when applied to\nLlama-2-Chat 70B. Code will be released at\nhttps://github.com/facebookresearch/NEST/tree/main."
                },
                "authors": [
                    {
                        "name": "Minghan Li"
                    },
                    {
                        "name": "Xilun Chen"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Jimmy Lin"
                    },
                    {
                        "name": "Wen-tau Yih"
                    },
                    {
                        "name": "Xi Victoria Lin"
                    }
                ],
                "author_detail": {
                    "name": "Xi Victoria Lin"
                },
                "author": "Xi Victoria Lin",
                "arxiv_journal_ref": "Advances in Neural Information Processing Systems (2024), vol. 37,\n  page 80987-81015",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19325v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19325v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17999v1",
                "updated": "2025-04-25T00:58:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    58,
                    37,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T00:58:37Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    58,
                    37,
                    4,
                    115,
                    0
                ],
                "title": "Streaming, Fast and Slow: Cognitive Load-Aware Streaming for Efficient\n  LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming, Fast and Slow: Cognitive Load-Aware Streaming for Efficient\n  LLM Serving"
                },
                "summary": "Generative conversational interfaces powered by large language models (LLMs)\ntypically stream output token-by-token at a rate determined by computational\nbudget, often neglecting actual human reading speeds and the cognitive load\nassociated with the content. This mismatch frequently leads to inefficient use\nof computational resources. For example, in cloud-based services, streaming\ncontent faster than users can read appears unnecessary, resulting in wasted\ncomputational resources and potential delays for other users, particularly\nduring peak usage periods. To address this issue, we propose an adaptive\nstreaming method that dynamically adjusts the pacing of LLM streaming output in\nreal-time based on inferred cognitive load. Our approach estimates the\ncognitive load associated with streaming content and strategically slows down\nthe stream during complex or information-rich segments, thereby freeing\ncomputational resources for other users. Our statistical analysis of\ncomputational savings, combined with crowdsourced user studies, provides\ninsights into the trade-offs between service efficiency and user satisfaction,\ndemonstrating that our method can significantly reduce computational\nconsumption up to 16.8\\%. This context-aware computational resource management\nstrategy presents a practical framework for enhancing system efficiency in\ncloud-based conversational AI interfaces without compromising user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative conversational interfaces powered by large language models (LLMs)\ntypically stream output token-by-token at a rate determined by computational\nbudget, often neglecting actual human reading speeds and the cognitive load\nassociated with the content. This mismatch frequently leads to inefficient use\nof computational resources. For example, in cloud-based services, streaming\ncontent faster than users can read appears unnecessary, resulting in wasted\ncomputational resources and potential delays for other users, particularly\nduring peak usage periods. To address this issue, we propose an adaptive\nstreaming method that dynamically adjusts the pacing of LLM streaming output in\nreal-time based on inferred cognitive load. Our approach estimates the\ncognitive load associated with streaming content and strategically slows down\nthe stream during complex or information-rich segments, thereby freeing\ncomputational resources for other users. Our statistical analysis of\ncomputational savings, combined with crowdsourced user studies, provides\ninsights into the trade-offs between service efficiency and user satisfaction,\ndemonstrating that our method can significantly reduce computational\nconsumption up to 16.8\\%. This context-aware computational resource management\nstrategy presents a practical framework for enhancing system efficiency in\ncloud-based conversational AI interfaces without compromising user experience."
                },
                "authors": [
                    {
                        "name": "Chang Xiao"
                    },
                    {
                        "name": "Brenda Yang"
                    }
                ],
                "author_detail": {
                    "name": "Brenda Yang"
                },
                "author": "Brenda Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17997v1",
                "updated": "2025-04-25T00:45:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    45,
                    41,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T00:45:41Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    45,
                    41,
                    4,
                    115,
                    0
                ],
                "title": "Chatperone: An LLM-Based Negotiable Scaffolding System for Mediating\n  Adolescent Mobile Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chatperone: An LLM-Based Negotiable Scaffolding System for Mediating\n  Adolescent Mobile Interactions"
                },
                "summary": "Adolescents' uncontrolled exposure to digital content can negatively impact\ntheir development. Traditional regulatory methods, such as time limits or app\nrestrictions, often take a rigid approach, ignoring adolescents'\ndecision-making abilities. Another issue is the lack of content and services\ntailored for adolescents. To address this, we propose Chatperone, a concept of\na system that provides adaptive scaffolding to support adolescents. Chatperone\nfosters healthy mobile interactions through three key modules: Perception,\nNegotiation, and Moderation. This paper outlines these modules' functionalities\nand discusses considerations for real-world implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adolescents' uncontrolled exposure to digital content can negatively impact\ntheir development. Traditional regulatory methods, such as time limits or app\nrestrictions, often take a rigid approach, ignoring adolescents'\ndecision-making abilities. Another issue is the lack of content and services\ntailored for adolescents. To address this, we propose Chatperone, a concept of\na system that provides adaptive scaffolding to support adolescents. Chatperone\nfosters healthy mobile interactions through three key modules: Perception,\nNegotiation, and Moderation. This paper outlines these modules' functionalities\nand discusses considerations for real-world implementation."
                },
                "authors": [
                    {
                        "name": "Suwon Yoon"
                    },
                    {
                        "name": "Seungwon Yang"
                    },
                    {
                        "name": "Jeongwon Choi"
                    },
                    {
                        "name": "Wonjeong Park"
                    },
                    {
                        "name": "Inseok Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Inseok Hwang"
                },
                "author": "Inseok Hwang",
                "arxiv_comment": "5 pages, Workshop Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17996v1",
                "updated": "2025-04-25T00:43:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    43,
                    20,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T00:43:20Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    43,
                    20,
                    4,
                    115,
                    0
                ],
                "title": "Back to Fundamentals: Low-Level Visual Features Guided Progressive Token\n  Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Back to Fundamentals: Low-Level Visual Features Guided Progressive Token\n  Pruning"
                },
                "summary": "Vision Transformers (ViTs) excel in semantic segmentation but demand\nsignificant computation, posing challenges for deployment on\nresource-constrained devices. Existing token pruning methods often overlook\nfundamental visual data characteristics. This study introduces 'LVTP', a\nprogressive token pruning framework guided by multi-scale Tsallis entropy and\nlow-level visual features with twice clustering. It integrates high-level\nsemantics and basic visual attributes for precise segmentation. A novel dynamic\nscoring mechanism using multi-scale Tsallis entropy weighting overcomes\nlimitations of traditional single-parameter entropy. The framework also\nincorporates low-level feature analysis to preserve critical edge information\nwhile optimizing computational cost. As a plug-and-play module, it requires no\narchitectural changes or additional training. Evaluations across multiple\ndatasets show 20%-45% computational reductions with negligible performance\nloss, outperforming existing methods in balancing cost and accuracy, especially\nin complex edge regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Transformers (ViTs) excel in semantic segmentation but demand\nsignificant computation, posing challenges for deployment on\nresource-constrained devices. Existing token pruning methods often overlook\nfundamental visual data characteristics. This study introduces 'LVTP', a\nprogressive token pruning framework guided by multi-scale Tsallis entropy and\nlow-level visual features with twice clustering. It integrates high-level\nsemantics and basic visual attributes for precise segmentation. A novel dynamic\nscoring mechanism using multi-scale Tsallis entropy weighting overcomes\nlimitations of traditional single-parameter entropy. The framework also\nincorporates low-level feature analysis to preserve critical edge information\nwhile optimizing computational cost. As a plug-and-play module, it requires no\narchitectural changes or additional training. Evaluations across multiple\ndatasets show 20%-45% computational reductions with negligible performance\nloss, outperforming existing methods in balancing cost and accuracy, especially\nin complex edge regions."
                },
                "authors": [
                    {
                        "name": "Yuanbing Ouyang"
                    },
                    {
                        "name": "Yizhuo Liang"
                    },
                    {
                        "name": "Qingpeng Li"
                    },
                    {
                        "name": "Xinfei Guo"
                    },
                    {
                        "name": "Yiming Luo"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Yushan Pan"
                    }
                ],
                "author_detail": {
                    "name": "Yushan Pan"
                },
                "author": "Yushan Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17993v1",
                "updated": "2025-04-25T00:36:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    36,
                    39,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T00:36:39Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    36,
                    39,
                    4,
                    115,
                    0
                ],
                "title": "Improving LLM Personas via Rationalization with Psychological Scaffolds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving LLM Personas via Rationalization with Psychological Scaffolds"
                },
                "summary": "Language models prompted with a user description or persona can predict a\nuser's preferences and opinions, but existing approaches to building personas\n-- based solely on a user's demographic attributes and/or prior judgments --\nfail to capture the underlying reasoning behind said user judgments. We\nintroduce PB&J (Psychology of Behavior and Judgments), a framework that\nimproves LLM personas by incorporating rationales of why a user might make\nspecific judgments. These rationales are LLM-generated, and aim to reason about\na user's behavior on the basis of their experiences, personality traits or\nbeliefs. This is done using psychological scaffolds -- structured frameworks\ngrounded in theories such as the Big 5 Personality Traits and Primal World\nBeliefs -- that help provide structure to the generated rationales. Experiments\non public opinion and movie preference prediction tasks demonstrate that LLM\npersonas augmented with PB&J rationales consistently outperform methods using\nonly a user's demographics and/or judgments. Additionally, LLM personas\nconstructed using scaffolds describing user beliefs perform competitively with\nthose using human-written rationales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models prompted with a user description or persona can predict a\nuser's preferences and opinions, but existing approaches to building personas\n-- based solely on a user's demographic attributes and/or prior judgments --\nfail to capture the underlying reasoning behind said user judgments. We\nintroduce PB&J (Psychology of Behavior and Judgments), a framework that\nimproves LLM personas by incorporating rationales of why a user might make\nspecific judgments. These rationales are LLM-generated, and aim to reason about\na user's behavior on the basis of their experiences, personality traits or\nbeliefs. This is done using psychological scaffolds -- structured frameworks\ngrounded in theories such as the Big 5 Personality Traits and Primal World\nBeliefs -- that help provide structure to the generated rationales. Experiments\non public opinion and movie preference prediction tasks demonstrate that LLM\npersonas augmented with PB&J rationales consistently outperform methods using\nonly a user's demographics and/or judgments. Additionally, LLM personas\nconstructed using scaffolds describing user beliefs perform competitively with\nthose using human-written rationales."
                },
                "authors": [
                    {
                        "name": "Brihi Joshi"
                    },
                    {
                        "name": "Xiang Ren"
                    },
                    {
                        "name": "Swabha Swayamdipta"
                    },
                    {
                        "name": "Rik Koncel-Kedziorski"
                    },
                    {
                        "name": "Tim Paek"
                    }
                ],
                "author_detail": {
                    "name": "Tim Paek"
                },
                "author": "Tim Paek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10370v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10370v3",
                "updated": "2025-04-25T00:10:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    10,
                    42,
                    4,
                    115,
                    0
                ],
                "published": "2024-06-14T18:56:40Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    18,
                    56,
                    40,
                    4,
                    166,
                    0
                ],
                "title": "Papers-to-Posts: Supporting Detailed Long-Document Summarization with an\n  Interactive LLM-Powered Source Outline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Papers-to-Posts: Supporting Detailed Long-Document Summarization with an\n  Interactive LLM-Powered Source Outline"
                },
                "summary": "Compressing long and technical documents (e.g., >10 pages) into shorter-form\narticles (e.g., <2 pages) is critical for communicating information to\ndifferent audiences, for example, blog posts of scientific research paper or\nlegal briefs of dense court proceedings. While large language models (LLMs) are\npowerful tools for condensing large amounts of text, current interfaces to\nthese models lack support for understanding and controlling what content is\nincluded in a detailed summarizing article. Such capability is especially\nimportant for detail- and technical-oriented domains, in which tactical\nselection and coherent synthesis of key details is critical for effective\ncommunication to the target audience. For this, we present interactive reverse\nsource outlines, a novel mechanism for controllable long-form summarization\nfeaturing outline bullet points with automatic point selections that the user\ncan iteratively adjust to obtain an article with the desired content coverage.\nWe implement this mechanism in Papers-to-Posts, a new LLM-powered system for\nauthoring research-paper blog posts. Through a within-subjects lab study (n=20)\nand a between-subjects deployment study (n=37 blog posts, 26 participants), we\ncompare Papers-to-Posts to a strong baseline tool that provides an\nLLM-generated draft and access to free-form prompting. Under time constraints,\nPapers-to-Posts significantly increases writer satisfaction with blog post\nquality, particularly with respect to content coverage. Furthermore,\nquantitative results showed an increase in editing power (change in text for an\namount of time or writing actions) while using Papers-to-Posts, and qualitative\nresults showed that participants found incorporating key research-paper\ninsights in their blog posts easier while using Papers-to-Posts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressing long and technical documents (e.g., >10 pages) into shorter-form\narticles (e.g., <2 pages) is critical for communicating information to\ndifferent audiences, for example, blog posts of scientific research paper or\nlegal briefs of dense court proceedings. While large language models (LLMs) are\npowerful tools for condensing large amounts of text, current interfaces to\nthese models lack support for understanding and controlling what content is\nincluded in a detailed summarizing article. Such capability is especially\nimportant for detail- and technical-oriented domains, in which tactical\nselection and coherent synthesis of key details is critical for effective\ncommunication to the target audience. For this, we present interactive reverse\nsource outlines, a novel mechanism for controllable long-form summarization\nfeaturing outline bullet points with automatic point selections that the user\ncan iteratively adjust to obtain an article with the desired content coverage.\nWe implement this mechanism in Papers-to-Posts, a new LLM-powered system for\nauthoring research-paper blog posts. Through a within-subjects lab study (n=20)\nand a between-subjects deployment study (n=37 blog posts, 26 participants), we\ncompare Papers-to-Posts to a strong baseline tool that provides an\nLLM-generated draft and access to free-form prompting. Under time constraints,\nPapers-to-Posts significantly increases writer satisfaction with blog post\nquality, particularly with respect to content coverage. Furthermore,\nquantitative results showed an increase in editing power (change in text for an\namount of time or writing actions) while using Papers-to-Posts, and qualitative\nresults showed that participants found incorporating key research-paper\ninsights in their blog posts easier while using Papers-to-Posts."
                },
                "authors": [
                    {
                        "name": "Marissa Radensky"
                    },
                    {
                        "name": "Daniel S. Weld"
                    },
                    {
                        "name": "Joseph Chee Chang"
                    },
                    {
                        "name": "Pao Siangliulue"
                    },
                    {
                        "name": "Jonathan Bragg"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Bragg"
                },
                "author": "Jonathan Bragg",
                "arxiv_comment": "Revised for clearer message",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10370v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10370v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17974v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17974v1",
                "updated": "2025-04-24T23:00:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    23,
                    0,
                    46,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T23:00:46Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    23,
                    0,
                    46,
                    3,
                    114,
                    0
                ],
                "title": "Optimism, Expectation, or Sarcasm? Multi-Class Hope Speech Detection in\n  Spanish and English",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimism, Expectation, or Sarcasm? Multi-Class Hope Speech Detection in\n  Spanish and English"
                },
                "summary": "Hope is a complex and underexplored emotional state that plays a significant\nrole in education, mental health, and social interaction. Unlike basic\nemotions, hope manifests in nuanced forms ranging from grounded optimism to\nexaggerated wishfulness or sarcasm, making it difficult for Natural Language\nProcessing systems to detect accurately. This study introduces PolyHope V2, a\nmultilingual, fine-grained hope speech dataset comprising over 30,000 annotated\ntweets in English and Spanish. This resource distinguishes between four hope\nsubtypes Generalized, Realistic, Unrealistic, and Sarcastic and enhances\nexisting datasets by explicitly labeling sarcastic instances. We benchmark\nmultiple pretrained transformer models and compare them with large language\nmodels (LLMs) such as GPT 4 and Llama 3 under zero-shot and few-shot regimes.\nOur findings show that fine-tuned transformers outperform prompt-based LLMs,\nespecially in distinguishing nuanced hope categories and sarcasm. Through\nqualitative analysis and confusion matrices, we highlight systematic challenges\nin separating closely related hope subtypes. The dataset and results provide a\nrobust foundation for future emotion recognition tasks that demand greater\nsemantic and contextual sensitivity across languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hope is a complex and underexplored emotional state that plays a significant\nrole in education, mental health, and social interaction. Unlike basic\nemotions, hope manifests in nuanced forms ranging from grounded optimism to\nexaggerated wishfulness or sarcasm, making it difficult for Natural Language\nProcessing systems to detect accurately. This study introduces PolyHope V2, a\nmultilingual, fine-grained hope speech dataset comprising over 30,000 annotated\ntweets in English and Spanish. This resource distinguishes between four hope\nsubtypes Generalized, Realistic, Unrealistic, and Sarcastic and enhances\nexisting datasets by explicitly labeling sarcastic instances. We benchmark\nmultiple pretrained transformer models and compare them with large language\nmodels (LLMs) such as GPT 4 and Llama 3 under zero-shot and few-shot regimes.\nOur findings show that fine-tuned transformers outperform prompt-based LLMs,\nespecially in distinguishing nuanced hope categories and sarcasm. Through\nqualitative analysis and confusion matrices, we highlight systematic challenges\nin separating closely related hope subtypes. The dataset and results provide a\nrobust foundation for future emotion recognition tasks that demand greater\nsemantic and contextual sensitivity across languages."
                },
                "authors": [
                    {
                        "name": "Sabur Butt"
                    },
                    {
                        "name": "Fazlourrahman Balouchzahi"
                    },
                    {
                        "name": "Ahmad Imam Amjad"
                    },
                    {
                        "name": "Maaz Amjad"
                    },
                    {
                        "name": "Hector G. Ceballos"
                    },
                    {
                        "name": "Salud Maria Jimenez-Zafra"
                    }
                ],
                "author_detail": {
                    "name": "Salud Maria Jimenez-Zafra"
                },
                "author": "Salud Maria Jimenez-Zafra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17974v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17974v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03727v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03727v3",
                "updated": "2025-04-24T22:33:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    22,
                    33,
                    2,
                    3,
                    114,
                    0
                ],
                "published": "2024-09-30T06:27:53Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    27,
                    53,
                    0,
                    274,
                    0
                ],
                "title": "FaithEval: Can Your Language Model Stay Faithful to Context, Even If\n  \"The Moon is Made of Marshmallows\"",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FaithEval: Can Your Language Model Stay Faithful to Context, Even If\n  \"The Moon is Made of Marshmallows\""
                },
                "summary": "Ensuring faithfulness to context in large language models (LLMs) and\nretrieval-augmented generation (RAG) systems is crucial for reliable deployment\nin real-world applications, as incorrect or unsupported information can erode\nuser trust. Despite advancements on standard benchmarks, faithfulness\nhallucination-where models generate responses misaligned with the provided\ncontext-remains a significant challenge. In this work, we introduce FaithEval,\na novel and comprehensive benchmark tailored to evaluate the faithfulness of\nLLMs in contextual scenarios across three diverse tasks: unanswerable,\ninconsistent, and counterfactual contexts. These tasks simulate real-world\nchallenges where retrieval mechanisms may surface incomplete, contradictory, or\nfabricated information. FaithEval comprises 4.9K high-quality problems in\ntotal, validated through a rigorous four-stage context construction and\nvalidation framework, employing both LLM-based auto-evaluation and human\nvalidation. Our extensive study across a wide range of open-source and\nproprietary models reveals that even state-of-the-art models often struggle to\nremain faithful to the given context, and that larger models do not necessarily\nexhibit improved faithfulness.Project is available at:\nhttps://github.com/SalesforceAIResearch/FaithEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring faithfulness to context in large language models (LLMs) and\nretrieval-augmented generation (RAG) systems is crucial for reliable deployment\nin real-world applications, as incorrect or unsupported information can erode\nuser trust. Despite advancements on standard benchmarks, faithfulness\nhallucination-where models generate responses misaligned with the provided\ncontext-remains a significant challenge. In this work, we introduce FaithEval,\na novel and comprehensive benchmark tailored to evaluate the faithfulness of\nLLMs in contextual scenarios across three diverse tasks: unanswerable,\ninconsistent, and counterfactual contexts. These tasks simulate real-world\nchallenges where retrieval mechanisms may surface incomplete, contradictory, or\nfabricated information. FaithEval comprises 4.9K high-quality problems in\ntotal, validated through a rigorous four-stage context construction and\nvalidation framework, employing both LLM-based auto-evaluation and human\nvalidation. Our extensive study across a wide range of open-source and\nproprietary models reveals that even state-of-the-art models often struggle to\nremain faithful to the given context, and that larger models do not necessarily\nexhibit improved faithfulness.Project is available at:\nhttps://github.com/SalesforceAIResearch/FaithEval."
                },
                "authors": [
                    {
                        "name": "Yifei Ming"
                    },
                    {
                        "name": "Senthil Purushwalkam"
                    },
                    {
                        "name": "Shrey Pandit"
                    },
                    {
                        "name": "Zixuan Ke"
                    },
                    {
                        "name": "Xuan-Phi Nguyen"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Shafiq Joty"
                    }
                ],
                "author_detail": {
                    "name": "Shafiq Joty"
                },
                "author": "Shafiq Joty",
                "arxiv_comment": "The conference version of this paper is published at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03727v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03727v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17967v1",
                "updated": "2025-04-24T22:27:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    22,
                    27,
                    50,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T22:27:50Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    22,
                    27,
                    50,
                    3,
                    114,
                    0
                ],
                "title": "LLM Agent Swarm for Hypothesis-Driven Drug Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Agent Swarm for Hypothesis-Driven Drug Discovery"
                },
                "summary": "Drug discovery remains a formidable challenge: more than 90 percent of\ncandidate molecules fail in clinical evaluation, and development costs often\nexceed one billion dollars per approved therapy. Disparate data streams, from\ngenomics and transcriptomics to chemical libraries and clinical records, hinder\ncoherent mechanistic insight and slow progress. Meanwhile, large language\nmodels excel at reasoning and tool integration but lack the modular\nspecialization and iterative memory required for regulated, hypothesis-driven\nworkflows. We introduce PharmaSwarm, a unified multi-agent framework that\norchestrates specialized LLM \"agents\" to propose, validate, and refine\nhypotheses for novel drug targets and lead compounds. Each agent accesses\ndedicated functionality--automated genomic and expression analysis; a curated\nbiomedical knowledge graph; pathway enrichment and network simulation;\ninterpretable binding affinity prediction--while a central Evaluator LLM\ncontinuously ranks proposals by biological plausibility, novelty, in silico\nefficacy, and safety. A shared memory layer captures validated insights and\nfine-tunes underlying submodels over time, yielding a self-improving system.\nDeployable on low-code platforms or Kubernetes-based microservices, PharmaSwarm\nsupports literature-driven discovery, omics-guided target identification, and\nmarket-informed repurposing. We also describe a rigorous four-tier validation\npipeline spanning retrospective benchmarking, independent computational assays,\nexperimental testing, and expert user studies to ensure transparency,\nreproducibility, and real-world impact. By acting as an AI copilot, PharmaSwarm\ncan accelerate translational research and deliver high-confidence hypotheses\nmore efficiently than traditional pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drug discovery remains a formidable challenge: more than 90 percent of\ncandidate molecules fail in clinical evaluation, and development costs often\nexceed one billion dollars per approved therapy. Disparate data streams, from\ngenomics and transcriptomics to chemical libraries and clinical records, hinder\ncoherent mechanistic insight and slow progress. Meanwhile, large language\nmodels excel at reasoning and tool integration but lack the modular\nspecialization and iterative memory required for regulated, hypothesis-driven\nworkflows. We introduce PharmaSwarm, a unified multi-agent framework that\norchestrates specialized LLM \"agents\" to propose, validate, and refine\nhypotheses for novel drug targets and lead compounds. Each agent accesses\ndedicated functionality--automated genomic and expression analysis; a curated\nbiomedical knowledge graph; pathway enrichment and network simulation;\ninterpretable binding affinity prediction--while a central Evaluator LLM\ncontinuously ranks proposals by biological plausibility, novelty, in silico\nefficacy, and safety. A shared memory layer captures validated insights and\nfine-tunes underlying submodels over time, yielding a self-improving system.\nDeployable on low-code platforms or Kubernetes-based microservices, PharmaSwarm\nsupports literature-driven discovery, omics-guided target identification, and\nmarket-informed repurposing. We also describe a rigorous four-tier validation\npipeline spanning retrospective benchmarking, independent computational assays,\nexperimental testing, and expert user studies to ensure transparency,\nreproducibility, and real-world impact. By acting as an AI copilot, PharmaSwarm\ncan accelerate translational research and deliver high-confidence hypotheses\nmore efficiently than traditional pipelines."
                },
                "authors": [
                    {
                        "name": "Kevin Song"
                    },
                    {
                        "name": "Andrew Trotter"
                    },
                    {
                        "name": "Jake Y. Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jake Y. Chen"
                },
                "author": "Jake Y. Chen",
                "arxiv_comment": "15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]