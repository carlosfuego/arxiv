[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.04394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04394v1",
                "updated": "2025-01-08T10:14:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    14,
                    19,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T10:14:19Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    14,
                    19,
                    2,
                    8,
                    0
                ],
                "title": "Modern Hardware Security: A Review of Attacks and Countermeasures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Hardware Security: A Review of Attacks and Countermeasures"
                },
                "summary": "With the exponential rise in the use of cloud services, smart devices, and\nIoT devices, advanced cyber attacks have become increasingly sophisticated and\nubiquitous. Furthermore, the rapid evolution of computing architectures and\nmemory technologies has created an urgent need to understand and address\nhardware security vulnerabilities. In this paper, we review the current state\nof vulnerabilities and mitigation strategies in contemporary computing systems.\nWe discuss cache side-channel attacks (including Spectre and Meltdown), power\nside-channel attacks (such as Simple Power Analysis, Differential Power\nAnalysis, Correlation Power Analysis, and Template Attacks), and advanced\ntechniques like Voltage Glitching and Electromagnetic Analysis to help\nunderstand and build robust cybersecurity defense systems and guide further\nresearch. We also examine memory encryption, focusing on confidentiality,\ngranularity, key management, masking, and re-keying strategies. Additionally,\nwe cover Cryptographic Instruction Set Architectures, Secure Boot, Root of\nTrust mechanisms, Physical Unclonable Functions, and hardware fault injection\ntechniques. The paper concludes with an analysis of the RISC-V architecture's\nunique security challenges. The comprehensive analysis presented in this paper\nis essential for building resilient hardware security solutions that can\nprotect against both current and emerging threats in an increasingly\nchallenging security landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the exponential rise in the use of cloud services, smart devices, and\nIoT devices, advanced cyber attacks have become increasingly sophisticated and\nubiquitous. Furthermore, the rapid evolution of computing architectures and\nmemory technologies has created an urgent need to understand and address\nhardware security vulnerabilities. In this paper, we review the current state\nof vulnerabilities and mitigation strategies in contemporary computing systems.\nWe discuss cache side-channel attacks (including Spectre and Meltdown), power\nside-channel attacks (such as Simple Power Analysis, Differential Power\nAnalysis, Correlation Power Analysis, and Template Attacks), and advanced\ntechniques like Voltage Glitching and Electromagnetic Analysis to help\nunderstand and build robust cybersecurity defense systems and guide further\nresearch. We also examine memory encryption, focusing on confidentiality,\ngranularity, key management, masking, and re-keying strategies. Additionally,\nwe cover Cryptographic Instruction Set Architectures, Secure Boot, Root of\nTrust mechanisms, Physical Unclonable Functions, and hardware fault injection\ntechniques. The paper concludes with an analysis of the RISC-V architecture's\nunique security challenges. The comprehensive analysis presented in this paper\nis essential for building resilient hardware security solutions that can\nprotect against both current and emerging threats in an increasingly\nchallenging security landscape."
                },
                "authors": [
                    {
                        "name": "Jyotiprakash Mishra"
                    },
                    {
                        "name": "Sanjay K. Sahay"
                    }
                ],
                "author_detail": {
                    "name": "Sanjay K. Sahay"
                },
                "author": "Sanjay K. Sahay",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04216v1",
                "updated": "2025-01-08T01:23:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    1,
                    23,
                    29,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T01:23:29Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    1,
                    23,
                    29,
                    2,
                    8,
                    0
                ],
                "title": "Optimal Oblivious Algorithms for Multi-way Joins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Oblivious Algorithms for Multi-way Joins"
                },
                "summary": "In cloud databases, cloud computation over sensitive data uploaded by clients\ninevitably causes concern about data security and privacy. Even when encryption\nprimitives and trusted computing environments are integrated into query\nprocessing to safeguard the actual contents of the data, access patterns of\nalgorithms can still leak private information about the data. Oblivious Random\nAccess Memory (ORAM) and circuits are two generic approaches to address this\nissue, ensuring that access patterns of algorithms remain oblivious to the\ndata. However, deploying these methods on insecure algorithms, particularly for\nmulti-way join processing, is computationally expensive and inherently\nchallenging.\n  In this paper, we propose a novel sorting-based algorithm for multi-way join\nprocessing that operates without relying on ORAM simulations or other security\nassumptions. Our algorithm is a non-trivial, provably oblivious composition of\nbasic primitives, with time complexity matching the insecure worst-case optimal\njoin algorithm, up to a logarithmic factor. Furthermore, it is cache-agnostic,\nwith cache complexity matching the insecure lower bound, also up to a\nlogarithmic factor. This clean and straightforward approach has the potential\nto be extended to other security settings and implemented in practical database\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In cloud databases, cloud computation over sensitive data uploaded by clients\ninevitably causes concern about data security and privacy. Even when encryption\nprimitives and trusted computing environments are integrated into query\nprocessing to safeguard the actual contents of the data, access patterns of\nalgorithms can still leak private information about the data. Oblivious Random\nAccess Memory (ORAM) and circuits are two generic approaches to address this\nissue, ensuring that access patterns of algorithms remain oblivious to the\ndata. However, deploying these methods on insecure algorithms, particularly for\nmulti-way join processing, is computationally expensive and inherently\nchallenging.\n  In this paper, we propose a novel sorting-based algorithm for multi-way join\nprocessing that operates without relying on ORAM simulations or other security\nassumptions. Our algorithm is a non-trivial, provably oblivious composition of\nbasic primitives, with time complexity matching the insecure worst-case optimal\njoin algorithm, up to a logarithmic factor. Furthermore, it is cache-agnostic,\nwith cache complexity matching the insecure lower bound, also up to a\nlogarithmic factor. This clean and straightforward approach has the potential\nto be extended to other security settings and implemented in practical database\nsystems."
                },
                "authors": [
                    {
                        "name": "Xiao Hu"
                    },
                    {
                        "name": "Zhiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiang Wu"
                },
                "author": "Zhiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00799v2",
                "updated": "2025-01-07T17:32:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    32,
                    19,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-01T10:50:35Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    10,
                    50,
                    35,
                    2,
                    1,
                    0
                ],
                "title": "Follow The Approximate Sparse Leader for No-Regret Online Sparse Linear\n  Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow The Approximate Sparse Leader for No-Regret Online Sparse Linear\n  Approximation"
                },
                "summary": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy."
                },
                "authors": [
                    {
                        "name": "Samrat Mukhopadhyay"
                    },
                    {
                        "name": "Debasmita Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Debasmita Mukherjee"
                },
                "author": "Debasmita Mukherjee",
                "arxiv_comment": "12 pages, 5 figures, corrected title, added proof of a lemma in\n  appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v1",
                "updated": "2025-01-07T17:00:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-González"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martín"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09275v2",
                "updated": "2025-01-06T23:16:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    23,
                    16,
                    22,
                    0,
                    6,
                    0
                ],
                "published": "2024-11-14T08:25:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Parallel $k$d-tree with Batch Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel $k$d-tree with Batch Updates"
                },
                "summary": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code."
                },
                "authors": [
                    {
                        "name": "Ziyang Men"
                    },
                    {
                        "name": "Zheqi Shen"
                    },
                    {
                        "name": "Yan Gu"
                    },
                    {
                        "name": "Yihan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yihan Sun"
                },
                "author": "Yihan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04052v1",
                "updated": "2025-01-06T22:40:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    22,
                    40,
                    40,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T22:40:40Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    22,
                    40,
                    40,
                    0,
                    6,
                    0
                ],
                "title": "The Power of Negative Zero: Datatype Customization for Quantized Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Power of Negative Zero: Datatype Customization for Quantized Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious machine learning tasks, quickly becoming one of the most prevalent AI\nworkloads. Yet the substantial memory requirement of LLMs significantly hinders\ntheir deployment for end users. Post-training quantization (PTQ) serves as one\nof the most hardware-efficient methods to mitigate the memory and computational\ndemands of LLMs. Although the traditional integer (INT) datatype has received\nwidespread adoption in PTQ methods, floating-point (FP) quantization has\nemerged as a viable alternative thanks to its effectiveness in fitting LLM\nnumerical distributions. However, the FP datatype in sign-magnitude binary\nrepresentation contains both positive and negative zero, which constrains its\nrepresentation capability, particularly under low precision (3 and 4 bits). In\nthis paper, we extend the basic FP datatype to perform Redundant Zero Remapping\n(RaZeR), which remaps the negative zero FP encoding to a set of pre-defined\nspecial values to maximally utilize FP quantization encodings and to better fit\nLLM numerical distributions. Through careful selection of special values, RaZeR\noutperforms conventional asymmetric INT quantization while achieving high\ncomputational efficiency. We demonstrate that RaZeR can be seamlessly\nintegrated with quantization algorithms for both weights and KV-cache,\nincluding advanced methods with clipping and transformations, and consistently\nachieve better model accuracy. Additionally, we implement a fast GEMV kernel\nwith fused dequantization that efficiently converts the 4-bit RaZeR value to\nFP16 through novel bit-level manipulation. On modern GPUs, our evaluation shows\nthat RaZeR improves the GEMV speed by up to 7.56$\\times$ compared to the FP16\nimplementation, while achieving up to 2.72$\\times$ speedup in the LLM decoding\nthroughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious machine learning tasks, quickly becoming one of the most prevalent AI\nworkloads. Yet the substantial memory requirement of LLMs significantly hinders\ntheir deployment for end users. Post-training quantization (PTQ) serves as one\nof the most hardware-efficient methods to mitigate the memory and computational\ndemands of LLMs. Although the traditional integer (INT) datatype has received\nwidespread adoption in PTQ methods, floating-point (FP) quantization has\nemerged as a viable alternative thanks to its effectiveness in fitting LLM\nnumerical distributions. However, the FP datatype in sign-magnitude binary\nrepresentation contains both positive and negative zero, which constrains its\nrepresentation capability, particularly under low precision (3 and 4 bits). In\nthis paper, we extend the basic FP datatype to perform Redundant Zero Remapping\n(RaZeR), which remaps the negative zero FP encoding to a set of pre-defined\nspecial values to maximally utilize FP quantization encodings and to better fit\nLLM numerical distributions. Through careful selection of special values, RaZeR\noutperforms conventional asymmetric INT quantization while achieving high\ncomputational efficiency. We demonstrate that RaZeR can be seamlessly\nintegrated with quantization algorithms for both weights and KV-cache,\nincluding advanced methods with clipping and transformations, and consistently\nachieve better model accuracy. Additionally, we implement a fast GEMV kernel\nwith fused dequantization that efficiently converts the 4-bit RaZeR value to\nFP16 through novel bit-level manipulation. On modern GPUs, our evaluation shows\nthat RaZeR improves the GEMV speed by up to 7.56$\\times$ compared to the FP16\nimplementation, while achieving up to 2.72$\\times$ speedup in the LLM decoding\nthroughput."
                },
                "authors": [
                    {
                        "name": "Yuzong Chen"
                    },
                    {
                        "name": "Xilai Dai"
                    },
                    {
                        "name": "Chi-chih Chang"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "arxiv_comment": "under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03322v1",
                "updated": "2025-01-06T19:00:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    19,
                    0,
                    3,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T19:00:03Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    19,
                    0,
                    3,
                    0,
                    6,
                    0
                ],
                "title": "Twinkle: A GPU-based binary-lens microlensing code with contour\n  integration method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twinkle: A GPU-based binary-lens microlensing code with contour\n  integration method"
                },
                "summary": "With the rapidly increasing rate of microlensing planet detections,\nmicrolensing modeling software faces significant challenges in computation\nefficiency. Here, we develop the Twinkle code, an efficient and robust\nbinary-lens modeling software suite optimized for heterogeneous computing\ndevices, especially GPUs. Existing microlensing codes have the issue of\ncatastrophic cancellation that undermines the numerical stability and\nprecision, and Twinkle resolves them by refining the coefficients of the\nbinary-lens equation. We also devise an improved method for robustly\nidentifying ghost images, thereby enhancing computational reliability. We have\nadvanced the state of the art by optimizing Twinkle specifically for\nheterogeneous computing devices by taking into account the unique task and\ncache memory dispatching patterns of GPUs, while the compatibility with the\ntraditional computing architectures of CPUs is still maintained. Twinkle has\ndemonstrated an acceleration of approximately 2 orders of magnitude (>~100\ntimes) on contemporary GPUs. The enhancement in computational speed of Twinkle\nwill translate to the delivery of accurate and highly efficient data analysis\nfor ongoing and upcoming microlensing projects. Both GPU and CPU versions of\nTwinkle are open-source and publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapidly increasing rate of microlensing planet detections,\nmicrolensing modeling software faces significant challenges in computation\nefficiency. Here, we develop the Twinkle code, an efficient and robust\nbinary-lens modeling software suite optimized for heterogeneous computing\ndevices, especially GPUs. Existing microlensing codes have the issue of\ncatastrophic cancellation that undermines the numerical stability and\nprecision, and Twinkle resolves them by refining the coefficients of the\nbinary-lens equation. We also devise an improved method for robustly\nidentifying ghost images, thereby enhancing computational reliability. We have\nadvanced the state of the art by optimizing Twinkle specifically for\nheterogeneous computing devices by taking into account the unique task and\ncache memory dispatching patterns of GPUs, while the compatibility with the\ntraditional computing architectures of CPUs is still maintained. Twinkle has\ndemonstrated an acceleration of approximately 2 orders of magnitude (>~100\ntimes) on contemporary GPUs. The enhancement in computational speed of Twinkle\nwill translate to the delivery of accurate and highly efficient data analysis\nfor ongoing and upcoming microlensing projects. Both GPU and CPU versions of\nTwinkle are open-source and publicly available."
                },
                "authors": [
                    {
                        "name": "Suwei Wang"
                    },
                    {
                        "name": "Lile Wang"
                    },
                    {
                        "name": "Subo Dong"
                    }
                ],
                "author_detail": {
                    "name": "Subo Dong"
                },
                "author": "Subo Dong",
                "arxiv_comment": "Accepted by ApJS, GitHub link:\n  https://github.com/AsterLight0626/Twinkle",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19919v2",
                "updated": "2025-01-06T15:59:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    15,
                    59,
                    23,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-27T20:47:23Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    20,
                    47,
                    23,
                    4,
                    362,
                    0
                ],
                "title": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)"
                },
                "summary": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family."
                },
                "authors": [
                    {
                        "name": "Austin Kaczmarek"
                    },
                    {
                        "name": "Andrea Capa Salinas"
                    },
                    {
                        "name": "Stephen D. Wilson"
                    },
                    {
                        "name": "Katja C. Nowack"
                    }
                ],
                "author_detail": {
                    "name": "Katja C. Nowack"
                },
                "author": "Katja C. Nowack",
                "arxiv_comment": "5 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02803v1",
                "updated": "2025-01-06T06:44:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    44,
                    13,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T06:44:13Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    44,
                    13,
                    0,
                    6,
                    0
                ],
                "title": "Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism"
                },
                "summary": "Multi-Agent Path Finding (MAPF), which focuses on finding collision-free\npaths for multiple robots, is crucial in autonomous warehouse operations.\nLifelong MAPF (L-MAPF), where agents are continuously reassigned new targets\nupon completing their current tasks, offers a more realistic approximation of\nreal-world warehouse scenarios. While cache storage systems can enhance\nefficiency and reduce operational costs, existing approaches primarily rely on\nexpectations and mathematical models, often without adequately addressing the\nchallenges of multi-robot planning and execution. In this paper, we introduce a\nnovel mechanism called Lifelong MAPF with Cache Mechanism (L-MAPF-CM), which\nintegrates high-level cache storage with low-level path planning. We have\ninvolved a new type of map grid called cache for temporary item storage.\nAdditionally, we involved a task assigner (TA) with a locking mechanism to\nbridge the gap between the new cache grid and L-MAPF algorithm. The TA\ndynamically allocates target locations to agents based on their status in\nvarious scenarios. We evaluated L-MAPF-CM using different cache replacement\npolicies and task distributions. L-MAPF-CM has demonstrated performance\nimprovements particularly with high cache hit rates and smooth traffic\nconditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Path Finding (MAPF), which focuses on finding collision-free\npaths for multiple robots, is crucial in autonomous warehouse operations.\nLifelong MAPF (L-MAPF), where agents are continuously reassigned new targets\nupon completing their current tasks, offers a more realistic approximation of\nreal-world warehouse scenarios. While cache storage systems can enhance\nefficiency and reduce operational costs, existing approaches primarily rely on\nexpectations and mathematical models, often without adequately addressing the\nchallenges of multi-robot planning and execution. In this paper, we introduce a\nnovel mechanism called Lifelong MAPF with Cache Mechanism (L-MAPF-CM), which\nintegrates high-level cache storage with low-level path planning. We have\ninvolved a new type of map grid called cache for temporary item storage.\nAdditionally, we involved a task assigner (TA) with a locking mechanism to\nbridge the gap between the new cache grid and L-MAPF algorithm. The TA\ndynamically allocates target locations to agents based on their status in\nvarious scenarios. We evaluated L-MAPF-CM using different cache replacement\npolicies and task distributions. L-MAPF-CM has demonstrated performance\nimprovements particularly with high cache hit rates and smooth traffic\nconditions."
                },
                "authors": [
                    {
                        "name": "Yimin Tang"
                    },
                    {
                        "name": "Zhenghong Yu"
                    },
                    {
                        "name": "Yi Zheng"
                    },
                    {
                        "name": "T. K. Satish Kumar"
                    },
                    {
                        "name": "Jiaoyang Li"
                    },
                    {
                        "name": "Sven Koenig"
                    }
                ],
                "author_detail": {
                    "name": "Sven Koenig"
                },
                "author": "Sven Koenig",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2403.13421",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v2",
                "updated": "2025-01-06T01:26:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    1,
                    26,
                    42,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner. We will release the code based on an\nopen-source model in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner. We will release the code based on an\nopen-source model in the future."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v2",
                "updated": "2025-01-05T14:11:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    14,
                    11,
                    48,
                    6,
                    5,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe"
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Update performance in MLVU-dev and LVBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02524v1",
                "updated": "2025-01-05T12:51:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    12,
                    51,
                    8,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T12:51:08Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    12,
                    51,
                    8,
                    6,
                    5,
                    0
                ],
                "title": "A Full-System Simulation Framework for CXL-Based SSD Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Full-System Simulation Framework for CXL-Based SSD Memory System"
                },
                "summary": "Compute eXpress Link (CXL) is a promising technology for memory\ndisaggregation and expansion. Especially, CXL makes it more effectively for\nlarge-capacity storage devices such as Solid State Drive (SSD) to be deployed\nin the memory pool. However, CXL-based SSDs are still in early stages,\nnecessitating the development of reliable simulation tools. In this paper, we\npropose CXL-SSD-Sim, the first open-source full-system simulator designed to\nsimulate CXL-based SSD memory system. Constructed on the foundation of gem5 and\nSimpleSSD, CXL-SSD-Sim extends an high fidelity SSD memory expander model along\nwith the corresponding device driver. In addition, CXL-SSD-Sim models a DRAM\nlayer as a caching mechanism for the SSD, meticulously engineered to counteract\nlatency issues inherent to CXL-based SSD memory access. Experiments are\nperformed among five different memory devices with CXL-SSD-Sim in aspect of\nlatency, bandwidth and real-world benchmark performance. These experiments\nserve to underscore the efficacy of our simulation tool in providing a\ncomprehensive analysis of CXL-based SSD memory systems. The CXL-SSD-Sim\nsimulator is available at https://github.com/WangYaohuii/CXL-SSD-Sim.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute eXpress Link (CXL) is a promising technology for memory\ndisaggregation and expansion. Especially, CXL makes it more effectively for\nlarge-capacity storage devices such as Solid State Drive (SSD) to be deployed\nin the memory pool. However, CXL-based SSDs are still in early stages,\nnecessitating the development of reliable simulation tools. In this paper, we\npropose CXL-SSD-Sim, the first open-source full-system simulator designed to\nsimulate CXL-based SSD memory system. Constructed on the foundation of gem5 and\nSimpleSSD, CXL-SSD-Sim extends an high fidelity SSD memory expander model along\nwith the corresponding device driver. In addition, CXL-SSD-Sim models a DRAM\nlayer as a caching mechanism for the SSD, meticulously engineered to counteract\nlatency issues inherent to CXL-based SSD memory access. Experiments are\nperformed among five different memory devices with CXL-SSD-Sim in aspect of\nlatency, bandwidth and real-world benchmark performance. These experiments\nserve to underscore the efficacy of our simulation tool in providing a\ncomprehensive analysis of CXL-based SSD memory systems. The CXL-SSD-Sim\nsimulator is available at https://github.com/WangYaohuii/CXL-SSD-Sim."
                },
                "authors": [
                    {
                        "name": "Yaohui Wang"
                    },
                    {
                        "name": "Zicong Wang"
                    },
                    {
                        "name": "Fanfeng Meng"
                    },
                    {
                        "name": "Yanjing Wang"
                    },
                    {
                        "name": "Yang Ou"
                    },
                    {
                        "name": "Lizhou Wu"
                    },
                    {
                        "name": "Wentao Hong"
                    },
                    {
                        "name": "Xuran Ge"
                    },
                    {
                        "name": "Jijun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jijun Cao"
                },
                "author": "Jijun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02469v1",
                "updated": "2025-01-05T07:41:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T07:41:53Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "title": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks"
                },
                "summary": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page."
                },
                "authors": [
                    {
                        "name": "Atonu Ghosh"
                    },
                    {
                        "name": "Sudip Misra"
                    }
                ],
                "author_detail": {
                    "name": "Sudip Misra"
                },
                "author": "Sudip Misra",
                "arxiv_comment": "11 pages, 15 figures, and 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v1",
                "updated": "2025-01-04T20:59:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01805v1",
                "updated": "2025-01-03T13:32:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T13:32:57Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "title": "End-to-End Long Document Summarization using Gradient Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End Long Document Summarization using Gradient Caching"
                },
                "summary": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters."
                },
                "authors": [
                    {
                        "name": "Rohit Saxena"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Frank Keller"
                    }
                ],
                "author_detail": {
                    "name": "Frank Keller"
                },
                "author": "Frank Keller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01792v1",
                "updated": "2025-01-03T12:51:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    51,
                    37,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T12:51:37Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    51,
                    37,
                    4,
                    3,
                    0
                ],
                "title": "Efficient LLM Inference with Activation Checkpointing and Hybrid Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference with Activation Checkpointing and Hybrid Caching"
                },
                "summary": "Recent large language models (LLMs) with enormous model sizes use many GPUs\nto meet memory capacity requirements incurring substantial costs for token\ngeneration. To provide cost-effective LLM inference with relaxed latency\nconstraints, extensive research has focused on expanding GPU memory by\nleveraging the host memory. However, LLM inference engines that utilize the\nhost memory often face underutilization of GPU compute units, as a considerable\nportion of inference time is spent in loading the model onto the GPU via\nhost-GPU interconnect. To tackle these challenges of the host memory offloading\nfor LLM, we introduce HybridServe, an LLM inference system with activation\ncheckpointing based on activation caching. The activation cache stores\nactivation checkpoints generated during intermediate inference stages, allowing\nthe fast recomputation of KV cache while model parameters are transferred to\nGPU from host memory. Unlike conventional methods that recompute the KV cache\nfrom scratch using token IDs, the activation cache allows bypassing projection\nand FFN operations. To balance between the activation recomputation and\nparameter loading overhead, this study proposes a KV-activation hybrid caching\nscheme which finds the best ratio of the key-value and activation caches to\nadjust the recomputation time. Our system achieves 2.19x throughput improvement\nover the state-of-the-art prior work for offloading both model weights and KV\ncache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) with enormous model sizes use many GPUs\nto meet memory capacity requirements incurring substantial costs for token\ngeneration. To provide cost-effective LLM inference with relaxed latency\nconstraints, extensive research has focused on expanding GPU memory by\nleveraging the host memory. However, LLM inference engines that utilize the\nhost memory often face underutilization of GPU compute units, as a considerable\nportion of inference time is spent in loading the model onto the GPU via\nhost-GPU interconnect. To tackle these challenges of the host memory offloading\nfor LLM, we introduce HybridServe, an LLM inference system with activation\ncheckpointing based on activation caching. The activation cache stores\nactivation checkpoints generated during intermediate inference stages, allowing\nthe fast recomputation of KV cache while model parameters are transferred to\nGPU from host memory. Unlike conventional methods that recompute the KV cache\nfrom scratch using token IDs, the activation cache allows bypassing projection\nand FFN operations. To balance between the activation recomputation and\nparameter loading overhead, this study proposes a KV-activation hybrid caching\nscheme which finds the best ratio of the key-value and activation caches to\nadjust the recomputation time. Our system achieves 2.19x throughput improvement\nover the state-of-the-art prior work for offloading both model weights and KV\ncache."
                },
                "authors": [
                    {
                        "name": "Sanghyeon Lee"
                    },
                    {
                        "name": "Hongbeen Kim"
                    },
                    {
                        "name": "Soojin Hwang"
                    },
                    {
                        "name": "Guseul Heo"
                    },
                    {
                        "name": "Minwoo Noh"
                    },
                    {
                        "name": "Jaehyuk Huh"
                    }
                ],
                "author_detail": {
                    "name": "Jaehyuk Huh"
                },
                "author": "Jaehyuk Huh",
                "arxiv_comment": "14 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01424v1",
                "updated": "2025-01-02T18:59:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    44,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T18:59:44Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    44,
                    3,
                    2,
                    0
                ],
                "title": "Object-level Visual Prompts for Compositional Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object-level Visual Prompts for Compositional Image Generation"
                },
                "summary": "We introduce a method for composing object-level visual prompts within a\ntext-to-image diffusion model. Our approach addresses the task of generating\nsemantically coherent compositions across diverse scenes and styles, similar to\nthe versatility and expressiveness offered by text prompts. A key challenge in\nthis task is to preserve the identity of the objects depicted in the input\nvisual prompts, while also generating diverse compositions across different\nimages. To address this challenge, we introduce a new KV-mixed cross-attention\nmechanism, in which keys and values are learned from distinct visual\nrepresentations. The keys are derived from an encoder with a small bottleneck\nfor layout control, whereas the values come from a larger bottleneck encoder\nthat captures fine-grained appearance details. By mixing keys and values from\nthese complementary sources, our model preserves the identity of the visual\nprompts while supporting flexible variations in object arrangement, pose, and\ncomposition. During inference, we further propose object-level compositional\nguidance to improve the method's identity preservation and layout correctness.\nResults show that our technique produces diverse scene compositions that\npreserve the unique characteristics of each visual prompt, expanding the\ncreative potential of text-to-image generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a method for composing object-level visual prompts within a\ntext-to-image diffusion model. Our approach addresses the task of generating\nsemantically coherent compositions across diverse scenes and styles, similar to\nthe versatility and expressiveness offered by text prompts. A key challenge in\nthis task is to preserve the identity of the objects depicted in the input\nvisual prompts, while also generating diverse compositions across different\nimages. To address this challenge, we introduce a new KV-mixed cross-attention\nmechanism, in which keys and values are learned from distinct visual\nrepresentations. The keys are derived from an encoder with a small bottleneck\nfor layout control, whereas the values come from a larger bottleneck encoder\nthat captures fine-grained appearance details. By mixing keys and values from\nthese complementary sources, our model preserves the identity of the visual\nprompts while supporting flexible variations in object arrangement, pose, and\ncomposition. During inference, we further propose object-level compositional\nguidance to improve the method's identity preservation and layout correctness.\nResults show that our technique produces diverse scene compositions that\npreserve the unique characteristics of each visual prompt, expanding the\ncreative potential of text-to-image generation."
                },
                "authors": [
                    {
                        "name": "Gaurav Parmar"
                    },
                    {
                        "name": "Or Patashnik"
                    },
                    {
                        "name": "Kuan-Chieh Wang"
                    },
                    {
                        "name": "Daniil Ostashev"
                    },
                    {
                        "name": "Srinivasa Narasimhan"
                    },
                    {
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "name": "Daniel Cohen-Or"
                    },
                    {
                        "name": "Kfir Aberman"
                    }
                ],
                "author_detail": {
                    "name": "Kfir Aberman"
                },
                "author": "Kfir Aberman",
                "arxiv_comment": "Project: https://snap-research.github.io/visual-composer/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01039v1",
                "updated": "2025-01-02T03:41:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    41,
                    32,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T03:41:32Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    41,
                    32,
                    3,
                    2,
                    0
                ],
                "title": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention"
                },
                "summary": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shivank Nag"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Lu Tian"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19442v2",
                "updated": "2025-01-02T03:40:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    40,
                    15,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-27T04:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "title": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
                },
                "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Tianhao Tang"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01005v1",
                "updated": "2025-01-02T02:02:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T02:02:20Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving"
                },
                "summary": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation."
                },
                "authors": [
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Lequn Chen"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Wuwei Lin"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Stephanie Wang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Vinod Grover"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Luis Ceze"
                    }
                ],
                "author_detail": {
                    "name": "Luis Ceze"
                },
                "author": "Luis Ceze",
                "arxiv_comment": "code available at http://github.com/flashinfer-ai/flashinfer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00946v1",
                "updated": "2025-01-01T20:16:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    16,
                    27,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T20:16:27Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    16,
                    27,
                    2,
                    1,
                    0
                ],
                "title": "Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant\n  Computation Elimination in Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant\n  Computation Elimination in Diffusion Model"
                },
                "summary": "Diffusion models have emerged as a promising approach for generating\nhigh-quality, high-dimensional images. Nevertheless, these models are hindered\nby their high computational cost and slow inference, partly due to the\nquadratic computational complexity of the self-attention mechanisms with\nrespect to input size. Various approaches have been proposed to address this\ndrawback. One such approach focuses on reducing the number of tokens fed into\nthe self-attention, known as token merging (ToMe). In our method, which is\ncalled cached adaptive token merging(CA-ToMe), we calculate the similarity\nbetween tokens and then merge the r proportion of the most similar tokens.\nHowever, due to the repetitive patterns observed in adjacent steps and the\nvariation in the frequency of similarities, we aim to enhance this approach by\nimplementing an adaptive threshold for merging tokens and adding a caching\nmechanism that stores similar pairs across several adjacent steps. Empirical\nresults demonstrate that our method operates as a training-free acceleration\nmethod, achieving a speedup factor of 1.24 in the denoising process while\nmaintaining the same FID scores compared to existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as a promising approach for generating\nhigh-quality, high-dimensional images. Nevertheless, these models are hindered\nby their high computational cost and slow inference, partly due to the\nquadratic computational complexity of the self-attention mechanisms with\nrespect to input size. Various approaches have been proposed to address this\ndrawback. One such approach focuses on reducing the number of tokens fed into\nthe self-attention, known as token merging (ToMe). In our method, which is\ncalled cached adaptive token merging(CA-ToMe), we calculate the similarity\nbetween tokens and then merge the r proportion of the most similar tokens.\nHowever, due to the repetitive patterns observed in adjacent steps and the\nvariation in the frequency of similarities, we aim to enhance this approach by\nimplementing an adaptive threshold for merging tokens and adding a caching\nmechanism that stores similar pairs across several adjacent steps. Empirical\nresults demonstrate that our method operates as a training-free acceleration\nmethod, achieving a speedup factor of 1.24 in the denoising process while\nmaintaining the same FID scores compared to existing approaches."
                },
                "authors": [
                    {
                        "name": "Omid Saghatchian"
                    },
                    {
                        "name": "Atiyeh Gh. Moghadam"
                    },
                    {
                        "name": "Ahmad Nickabadi"
                    }
                ],
                "author_detail": {
                    "name": "Ahmad Nickabadi"
                },
                "author": "Ahmad Nickabadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21023v2",
                "updated": "2024-12-31T20:40:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    20,
                    40,
                    43,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-30T15:46:53Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    46,
                    53,
                    0,
                    365,
                    0
                ],
                "title": "EdgeRAG: Online-Indexed RAG for Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeRAG: Online-Indexed RAG for Edge Devices"
                },
                "summary": "Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory."
                },
                "authors": [
                    {
                        "name": "Korakit Seemakhupt"
                    },
                    {
                        "name": "Sihang Liu"
                    },
                    {
                        "name": "Samira Khan"
                    }
                ],
                "author_detail": {
                    "name": "Samira Khan"
                },
                "author": "Samira Khan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00375v1",
                "updated": "2024-12-31T09:56:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    56,
                    40,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T09:56:40Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    56,
                    40,
                    1,
                    366,
                    0
                ],
                "title": "Token Pruning for Caching Better: 9 Times Acceleration on Stable\n  Diffusion for Free",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Pruning for Caching Better: 9 Times Acceleration on Stable\n  Diffusion for Free"
                },
                "summary": "Stable Diffusion has achieved remarkable success in the field of\ntext-to-image generation, with its powerful generative capabilities and diverse\ngeneration results making a lasting impact. However, its iterative denoising\nintroduces high computational costs and slows generation speed, limiting\nbroader adoption. The community has made numerous efforts to reduce this\ncomputational burden, with methods like feature caching attracting attention\ndue to their effectiveness and simplicity. Nonetheless, simply reusing features\ncomputed at previous timesteps causes the features across adjacent timesteps to\nbecome similar, reducing the dynamics of features over time and ultimately\ncompromising the quality of generated images. In this paper, we introduce a\ndynamics-aware token pruning (DaTo) approach that addresses the limitations of\nfeature caching. DaTo selectively prunes tokens with lower dynamics, allowing\nonly high-dynamic tokens to participate in self-attention layers, thereby\nextending feature dynamics across timesteps. DaTo combines feature caching with\ntoken pruning in a training-free manner, achieving both temporal and token-wise\ninformation reuse. Applied to Stable Diffusion on the ImageNet, our approach\ndelivered a 9$\\times$ speedup while reducing FID by 0.33, indicating enhanced\nimage quality. On the COCO-30k, we observed a 7$\\times$ acceleration coupled\nwith a notable FID reduction of 2.17.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable Diffusion has achieved remarkable success in the field of\ntext-to-image generation, with its powerful generative capabilities and diverse\ngeneration results making a lasting impact. However, its iterative denoising\nintroduces high computational costs and slows generation speed, limiting\nbroader adoption. The community has made numerous efforts to reduce this\ncomputational burden, with methods like feature caching attracting attention\ndue to their effectiveness and simplicity. Nonetheless, simply reusing features\ncomputed at previous timesteps causes the features across adjacent timesteps to\nbecome similar, reducing the dynamics of features over time and ultimately\ncompromising the quality of generated images. In this paper, we introduce a\ndynamics-aware token pruning (DaTo) approach that addresses the limitations of\nfeature caching. DaTo selectively prunes tokens with lower dynamics, allowing\nonly high-dynamic tokens to participate in self-attention layers, thereby\nextending feature dynamics across timesteps. DaTo combines feature caching with\ntoken pruning in a training-free manner, achieving both temporal and token-wise\ninformation reuse. Applied to Stable Diffusion on the ImageNet, our approach\ndelivered a 9$\\times$ speedup while reducing FID by 0.33, indicating enhanced\nimage quality. On the COCO-30k, we observed a 7$\\times$ acceleration coupled\nwith a notable FID reduction of 2.17."
                },
                "authors": [
                    {
                        "name": "Evelyn Zhang"
                    },
                    {
                        "name": "Bang Xiao"
                    },
                    {
                        "name": "Jiayi Tang"
                    },
                    {
                        "name": "Qianli Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10516v3",
                "updated": "2024-12-31T07:11:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    7,
                    11,
                    0,
                    1,
                    366,
                    0
                ],
                "published": "2024-09-16T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval"
                },
                "summary": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds."
                },
                "authors": [
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v1",
                "updated": "2024-12-31T05:24:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00243v1",
                "updated": "2024-12-31T03:19:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    3,
                    19,
                    38,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T03:19:38Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    3,
                    19,
                    38,
                    1,
                    366,
                    0
                ],
                "title": "Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained\n  Image Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained\n  Image Recognition"
                },
                "summary": "Ultra-fine-grained image recognition (UFGIR) is a challenging task that\ninvolves classifying images within a macro-category. While traditional FGIR\ndeals with classifying different species, UFGIR goes beyond by classifying\nsub-categories within a species such as cultivars of a plant. In recent times\nthe usage of Vision Transformer-based backbones has allowed methods to obtain\noutstanding recognition performances in this task but this comes at a\nsignificant cost in terms of computation specially since this task\nsignificantly benefits from incorporating higher resolution images. Therefore,\ntechniques such as token reduction have emerged to reduce the computational\ncost. However, dropping tokens leads to loss of essential information for\nfine-grained categories, specially as the token keep rate is reduced.\nTherefore, to counteract the loss of information brought by the usage of token\nreduction we propose a novel Cross-Layer Aggregation Classification Head and a\nCross-Layer Cache mechanism to recover and access information from previous\nlayers in later locations. Extensive experiments covering more than 2000 runs\nacross diverse settings including 5 datasets, 9 backbones, 7 token reduction\nmethods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the\nproposed plug-and-play modules and allow us to push the boundaries of accuracy\nvs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to\n10\\% while maintaining a competitive accuracy to state-of-the-art models. Code\nis available at: \\url{https://github.com/arkel23/CLCA}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-fine-grained image recognition (UFGIR) is a challenging task that\ninvolves classifying images within a macro-category. While traditional FGIR\ndeals with classifying different species, UFGIR goes beyond by classifying\nsub-categories within a species such as cultivars of a plant. In recent times\nthe usage of Vision Transformer-based backbones has allowed methods to obtain\noutstanding recognition performances in this task but this comes at a\nsignificant cost in terms of computation specially since this task\nsignificantly benefits from incorporating higher resolution images. Therefore,\ntechniques such as token reduction have emerged to reduce the computational\ncost. However, dropping tokens leads to loss of essential information for\nfine-grained categories, specially as the token keep rate is reduced.\nTherefore, to counteract the loss of information brought by the usage of token\nreduction we propose a novel Cross-Layer Aggregation Classification Head and a\nCross-Layer Cache mechanism to recover and access information from previous\nlayers in later locations. Extensive experiments covering more than 2000 runs\nacross diverse settings including 5 datasets, 9 backbones, 7 token reduction\nmethods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the\nproposed plug-and-play modules and allow us to push the boundaries of accuracy\nvs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to\n10\\% while maintaining a competitive accuracy to state-of-the-art models. Code\nis available at: \\url{https://github.com/arkel23/CLCA}"
                },
                "authors": [
                    {
                        "name": "Edwin Arkel Rios"
                    },
                    {
                        "name": "Jansen Christopher Yuanda"
                    },
                    {
                        "name": "Vincent Leon Ghanz"
                    },
                    {
                        "name": "Cheng-Wei Yu"
                    },
                    {
                        "name": "Bo-Cheng Lai"
                    },
                    {
                        "name": "Min-Chun Hu"
                    }
                ],
                "author_detail": {
                    "name": "Min-Chun Hu"
                },
                "author": "Min-Chun Hu",
                "arxiv_comment": "Accepted to ICASSP 2025. Main: 5 pages, 4 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21015v1",
                "updated": "2024-12-30T15:33:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T15:33:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "MapQaTor: A System for Efficient Annotation of Map Query Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MapQaTor: A System for Efficient Annotation of Map Query Datasets"
                },
                "summary": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q."
                },
                "authors": [
                    {
                        "name": "Mahir Labib Dihan"
                    },
                    {
                        "name": "Mohammed Eunus Ali"
                    },
                    {
                        "name": "Md Rizwan Parvez"
                    }
                ],
                "author_detail": {
                    "name": "Md Rizwan Parvez"
                },
                "author": "Md Rizwan Parvez",
                "arxiv_comment": "13 pages, 35 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v3",
                "updated": "2024-12-30T14:54:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    54,
                    29,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20887v1",
                "updated": "2024-12-30T11:54:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    54,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T11:54:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    54,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field"
                },
                "summary": "Coherent manipulation of lattice vibrations using ultrafast light pulses\nenables access to nonequilibrium 'hidden' phases with designed functionalities\nin quantum materials. However, expanding the understanding of nonlinear\nlight-phonon interaction mechanisms remains crucial for developing new\nstrategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3\ndriven by intense terahertz excitation. As the terahertz field increases, the\nsystem transitions from the quantum paraelectric (QPE) ground state to an\nintermediate ferroelectric phase, and then unexpectedly reverts to a QPE state\nabove ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice\ndynamics compared to the initial phases, highlighting activated\nantiferrodistortive phonon modes. Aided by first-principles dynamical\ncalculations, we identify the mechanism for these complex behaviors as a\nsuperposition of multiple coherently excited eigenstates of the polar soft\nmode. Our results reveal a previously uncharted quantum facet of SrTiO3 and\nopen pathways for harnessing high-order excitations to engineer quantum\nmaterials in the ultrafast regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherent manipulation of lattice vibrations using ultrafast light pulses\nenables access to nonequilibrium 'hidden' phases with designed functionalities\nin quantum materials. However, expanding the understanding of nonlinear\nlight-phonon interaction mechanisms remains crucial for developing new\nstrategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3\ndriven by intense terahertz excitation. As the terahertz field increases, the\nsystem transitions from the quantum paraelectric (QPE) ground state to an\nintermediate ferroelectric phase, and then unexpectedly reverts to a QPE state\nabove ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice\ndynamics compared to the initial phases, highlighting activated\nantiferrodistortive phonon modes. Aided by first-principles dynamical\ncalculations, we identify the mechanism for these complex behaviors as a\nsuperposition of multiple coherently excited eigenstates of the polar soft\nmode. Our results reveal a previously uncharted quantum facet of SrTiO3 and\nopen pathways for harnessing high-order excitations to engineer quantum\nmaterials in the ultrafast regime."
                },
                "authors": [
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Hanbyul Kim"
                    },
                    {
                        "name": "Xinbo Wang"
                    },
                    {
                        "name": "Jianlin Luo"
                    },
                    {
                        "name": "Simone Latini"
                    },
                    {
                        "name": "Dongbin Shin"
                    },
                    {
                        "name": "Jun-Ming Liu"
                    },
                    {
                        "name": "Jing-Feng Li"
                    },
                    {
                        "name": "Angel Rubio"
                    },
                    {
                        "name": "Ce-Wen Nan"
                    },
                    {
                        "name": "Qian Li"
                    }
                ],
                "author_detail": {
                    "name": "Qian Li"
                },
                "author": "Qian Li",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v2",
                "updated": "2024-12-30T05:01:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    5,
                    1,
                    44,
                    0,
                    365,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20677v1",
                "updated": "2024-12-30T03:05:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T03:05:45Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "title": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA"
                },
                "summary": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning."
                },
                "authors": [
                    {
                        "name": "Qingyun Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Zengchang Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zengchang Qin"
                },
                "author": "Zengchang Qin",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00068v1",
                "updated": "2024-12-29T17:41:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    41,
                    40,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T17:41:40Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    41,
                    40,
                    6,
                    364,
                    0
                ],
                "title": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques"
                },
                "summary": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1]. The proposed framework operates within the storage\nkernel, ensuring minimal latency and low computational overhead. Through an\nadaptive feedback mechanism, RL-Storage dynamically adjusts critical\nparameters, achieving efficient resource utilization across a wide range of\nworkloads. Experimental evaluations conducted on a range of benchmarks,\nincluding RocksDB and PostgreSQL, demonstrate significant improvements, with\nthroughput gains of up to 2.6x and latency reductions of 43% compared to\nbaseline heuristics. Additionally, RL-Storage achieves these performance\nenhancements with a negligible CPU overhead of 0.11% and a memory footprint of\nonly 5 KB, making it suitable for seamless deployment in production\nenvironments. This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1]. The proposed framework operates within the storage\nkernel, ensuring minimal latency and low computational overhead. Through an\nadaptive feedback mechanism, RL-Storage dynamically adjusts critical\nparameters, achieving efficient resource utilization across a wide range of\nworkloads. Experimental evaluations conducted on a range of benchmarks,\nincluding RocksDB and PostgreSQL, demonstrate significant improvements, with\nthroughput gains of up to 2.6x and latency reductions of 43% compared to\nbaseline heuristics. Additionally, RL-Storage achieves these performance\nenhancements with a negligible CPU overhead of 0.11% and a memory footprint of\nonly 5 KB, making it suitable for seamless deployment in production\nenvironments. This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Cao"
                },
                "author": "Jin Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20524v1",
                "updated": "2024-12-29T17:18:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    18,
                    21,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T17:18:21Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    18,
                    21,
                    6,
                    364,
                    0
                ],
                "title": "Ns3 meets Sionna: Using Realistic Channels in Network Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ns3 meets Sionna: Using Realistic Channels in Network Simulation"
                },
                "summary": "Network simulators are indispensable tools for the advancement of wireless\nnetwork technologies, offering a cost-effective and controlled environment to\nsimulate real-world network behavior. However, traditional simulators, such as\nthe widely used ns-3, exhibit limitations in accurately modeling indoor and\noutdoor scenarios due to their reliance on simplified statistical and\nstochastic channel propagation models, which often fail to accurately capture\nphysical phenomena like multipath signal propagation and shadowing by obstacles\nin the line-of-sight path. We present Ns3Sionna, which integrates a ray\ntracing-based channel model, implemented using the Sionna RT framework, within\nthe ns-3 network simulator. It allows to simulate environment-specific and\nphysically accurate channel realizations for a given 3D scene and wireless\ndevice positions. Additionally, a mobility model based on ray tracing was\ndeveloped to accurately represent device movements within the simulated 3D\nspace. Ns3Sionna provides more realistic path and delay loss estimates for both\nindoor and outdoor environments than existing ns-3 propagation models,\nparticularly in terms of spatial and temporal correlation. Moreover,\nfine-grained channel state information is provided, which could be used for the\ndevelopment of sensing applications. Due to the significant computational\ndemands of ray tracing, Ns3Sionna takes advantage of the parallel execution\ncapabilities of modern GPUs and multi-core CPUs by incorporating intelligent\npre-caching mechanisms that leverage the channel's coherence time to optimize\nruntime performance. This enables the efficient simulation of scenarios with a\nsmall to medium number of mobile nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network simulators are indispensable tools for the advancement of wireless\nnetwork technologies, offering a cost-effective and controlled environment to\nsimulate real-world network behavior. However, traditional simulators, such as\nthe widely used ns-3, exhibit limitations in accurately modeling indoor and\noutdoor scenarios due to their reliance on simplified statistical and\nstochastic channel propagation models, which often fail to accurately capture\nphysical phenomena like multipath signal propagation and shadowing by obstacles\nin the line-of-sight path. We present Ns3Sionna, which integrates a ray\ntracing-based channel model, implemented using the Sionna RT framework, within\nthe ns-3 network simulator. It allows to simulate environment-specific and\nphysically accurate channel realizations for a given 3D scene and wireless\ndevice positions. Additionally, a mobility model based on ray tracing was\ndeveloped to accurately represent device movements within the simulated 3D\nspace. Ns3Sionna provides more realistic path and delay loss estimates for both\nindoor and outdoor environments than existing ns-3 propagation models,\nparticularly in terms of spatial and temporal correlation. Moreover,\nfine-grained channel state information is provided, which could be used for the\ndevelopment of sensing applications. Due to the significant computational\ndemands of ray tracing, Ns3Sionna takes advantage of the parallel execution\ncapabilities of modern GPUs and multi-core CPUs by incorporating intelligent\npre-caching mechanisms that leverage the channel's coherence time to optimize\nruntime performance. This enables the efficient simulation of scenarios with a\nsmall to medium number of mobile nodes."
                },
                "authors": [
                    {
                        "name": "Anatolij Zubow"
                    },
                    {
                        "name": "Yannik Pilz"
                    },
                    {
                        "name": "Sascha Rösler"
                    },
                    {
                        "name": "Falko Dressler"
                    }
                ],
                "author_detail": {
                    "name": "Falko Dressler"
                },
                "author": "Falko Dressler",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20221v1",
                "updated": "2024-12-28T17:17:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    17,
                    17,
                    3,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T17:17:03Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    17,
                    17,
                    3,
                    5,
                    363,
                    0
                ],
                "title": "Revisiting Cache Freshness for Emerging Real-Time Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Cache Freshness for Emerging Real-Time Applications"
                },
                "summary": "Caching is widely used in industry to improve application performance by\nreducing data-access latency and taking the load off the backend\ninfrastructure. TTLs have become the de-facto mechanism used to keep cached\ndata reasonably fresh (i.e., not too out of date with the backend). However,\nthe emergence of real-time applications requires tighter data freshness, which\nis impractical to achieve with TTLs. We discuss why this is the case, and\npropose a simple yet effective adaptive policy to achieve the desired\nfreshness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is widely used in industry to improve application performance by\nreducing data-access latency and taking the load off the backend\ninfrastructure. TTLs have become the de-facto mechanism used to keep cached\ndata reasonably fresh (i.e., not too out of date with the backend). However,\nthe emergence of real-time applications requires tighter data freshness, which\nis impractical to achieve with TTLs. We discuss why this is the case, and\npropose a simple yet effective adaptive policy to achieve the desired\nfreshness."
                },
                "authors": [
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Rishabh Iyer"
                    },
                    {
                        "name": "Scott Shenker"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "arxiv_doi": "10.1145/3696348.3696858",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696348.3696858",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.20221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "HotNets '24",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20166v1",
                "updated": "2024-12-28T14:38:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T14:38:16Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "title": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System"
                },
                "summary": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications."
                },
                "authors": [
                    {
                        "name": "Hyucksung Kwon"
                    },
                    {
                        "name": "Kyungmo Koo"
                    },
                    {
                        "name": "Janghyeon Kim"
                    },
                    {
                        "name": "Woongkyu Lee"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyungdeok Lee"
                    },
                    {
                        "name": "Yousub Jung"
                    },
                    {
                        "name": "Jaehan Park"
                    },
                    {
                        "name": "Yosub Song"
                    },
                    {
                        "name": "Byeongsu Yang"
                    },
                    {
                        "name": "Haerang Choi"
                    },
                    {
                        "name": "Guhyun Kim"
                    },
                    {
                        "name": "Jongsoon Won"
                    },
                    {
                        "name": "Woojae Shin"
                    },
                    {
                        "name": "Changhyun Kim"
                    },
                    {
                        "name": "Gyeongcheol Shin"
                    },
                    {
                        "name": "Yongkee Kwon"
                    },
                    {
                        "name": "Ilkon Kim"
                    },
                    {
                        "name": "Euicheol Lim"
                    },
                    {
                        "name": "John Kim"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20105v1",
                "updated": "2024-12-28T10:17:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    10,
                    17,
                    29,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T10:17:29Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    10,
                    17,
                    29,
                    5,
                    363,
                    0
                ],
                "title": "ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal\n  Visual Token Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal\n  Visual Token Trimming"
                },
                "summary": "Multimodal large language models (MLLMs) enhance their perceptual\ncapabilities by integrating visual and textual information. However, processing\nthe massive number of visual tokens incurs a significant computational cost.\nExisting analysis of the MLLM attention mechanisms remains shallow, leading to\ncoarse-grain token pruning strategies that fail to effectively balance speed\nand accuracy. In this paper, we conduct a comprehensive investigation of MLLM\nattention mechanisms with LLaVA. We find that numerous visual tokens and\npartial attention computations are redundant during the decoding process. Based\non this insight, we propose Spatial-Temporal Visual Token Trimming\n($\\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without\nretraining. $\\textbf{ST}^{3}$ consists of two primary components: 1)\nProgressive Visual Token Pruning (\\textbf{PVTP}), which eliminates inattentive\nvisual tokens across layers, and 2) Visual Token Annealing (\\textbf{VTA}),\nwhich dynamically reduces the number of visual tokens in each layer as the\ngenerated tokens grow. Together, these techniques deliver around\n$\\mathbf{2\\times}$ faster inference with only about $\\mathbf{30\\%}$ KV cache\nmemory compared to the original LLaVA, while maintaining consistent performance\nacross various datasets. Crucially, $\\textbf{ST}^{3}$ can be seamlessly\nintegrated into existing pre-trained MLLMs, providing a plug-and-play solution\nfor efficient inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) enhance their perceptual\ncapabilities by integrating visual and textual information. However, processing\nthe massive number of visual tokens incurs a significant computational cost.\nExisting analysis of the MLLM attention mechanisms remains shallow, leading to\ncoarse-grain token pruning strategies that fail to effectively balance speed\nand accuracy. In this paper, we conduct a comprehensive investigation of MLLM\nattention mechanisms with LLaVA. We find that numerous visual tokens and\npartial attention computations are redundant during the decoding process. Based\non this insight, we propose Spatial-Temporal Visual Token Trimming\n($\\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without\nretraining. $\\textbf{ST}^{3}$ consists of two primary components: 1)\nProgressive Visual Token Pruning (\\textbf{PVTP}), which eliminates inattentive\nvisual tokens across layers, and 2) Visual Token Annealing (\\textbf{VTA}),\nwhich dynamically reduces the number of visual tokens in each layer as the\ngenerated tokens grow. Together, these techniques deliver around\n$\\mathbf{2\\times}$ faster inference with only about $\\mathbf{30\\%}$ KV cache\nmemory compared to the original LLaVA, while maintaining consistent performance\nacross various datasets. Crucially, $\\textbf{ST}^{3}$ can be seamlessly\nintegrated into existing pre-trained MLLMs, providing a plug-and-play solution\nfor efficient inference."
                },
                "authors": [
                    {
                        "name": "Jiedong Zhuang"
                    },
                    {
                        "name": "Lu Lu"
                    },
                    {
                        "name": "Ming Dai"
                    },
                    {
                        "name": "Rui Hu"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Haoji Hu"
                    }
                ],
                "author_detail": {
                    "name": "Haoji Hu"
                },
                "author": "Haoji Hu",
                "arxiv_comment": "Accepted to AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19991v1",
                "updated": "2024-12-28T03:28:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    3,
                    28,
                    52,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T03:28:52Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    3,
                    28,
                    52,
                    5,
                    363,
                    0
                ],
                "title": "A Robust Federated Learning Framework for Undependable Devices at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Robust Federated Learning Framework for Undependable Devices at Scale"
                },
                "summary": "In a federated learning (FL) system, many devices, such as smartphones, are\noften undependable (e.g., frequently disconnected from WiFi) during training.\nExisting FL frameworks always assume a dependable environment and exclude\nundependable devices from training, leading to poor model performance and\nresource wastage. In this paper, we propose FLUDE to effectively deal with\nundependable environments. First, FLUDE assesses the dependability of devices\nbased on the probability distribution of their historical behaviors (e.g., the\nlikelihood of successfully completing training). Based on this assessment,\nFLUDE adaptively selects devices with high dependability for training. To\nmitigate resource wastage during the training phase, FLUDE maintains a model\ncache on each device, aiming to preserve the latest training state for later\nuse in case local training on an undependable device is interrupted. Moreover,\nFLUDE proposes a staleness-aware strategy to judiciously distribute the global\nmodel to a subset of devices, thus significantly reducing resource wastage\nwhile maintaining model performance. We have implemented FLUDE on two physical\nplatforms with 120 smartphones and NVIDIA Jetson devices. Extensive\nexperimental results demonstrate that FLUDE can effectively improve model\nperformance and resource efficiency of FL training in undependable\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a federated learning (FL) system, many devices, such as smartphones, are\noften undependable (e.g., frequently disconnected from WiFi) during training.\nExisting FL frameworks always assume a dependable environment and exclude\nundependable devices from training, leading to poor model performance and\nresource wastage. In this paper, we propose FLUDE to effectively deal with\nundependable environments. First, FLUDE assesses the dependability of devices\nbased on the probability distribution of their historical behaviors (e.g., the\nlikelihood of successfully completing training). Based on this assessment,\nFLUDE adaptively selects devices with high dependability for training. To\nmitigate resource wastage during the training phase, FLUDE maintains a model\ncache on each device, aiming to preserve the latest training state for later\nuse in case local training on an undependable device is interrupted. Moreover,\nFLUDE proposes a staleness-aware strategy to judiciously distribute the global\nmodel to a subset of devices, thus significantly reducing resource wastage\nwhile maintaining model performance. We have implemented FLUDE on two physical\nplatforms with 120 smartphones and NVIDIA Jetson devices. Extensive\nexperimental results demonstrate that FLUDE can effectively improve model\nperformance and resource efficiency of FL training in undependable\nenvironments."
                },
                "authors": [
                    {
                        "name": "Shilong Wang"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Chunming Qiao"
                    },
                    {
                        "name": "Huarong Deng"
                    },
                    {
                        "name": "Qiuye Zheng"
                    },
                    {
                        "name": "Jiantao Gong"
                    }
                ],
                "author_detail": {
                    "name": "Jiantao Gong"
                },
                "author": "Jiantao Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19255v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19255v1",
                "updated": "2024-12-26T15:45:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    45,
                    45,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T15:45:45Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    45,
                    45,
                    3,
                    361,
                    0
                ],
                "title": "Multi-matrix Factorization Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-matrix Factorization Attention"
                },
                "summary": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Shuigeng Zhou"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    }
                ],
                "author_detail": {
                    "name": "Heung-Yeung Shum"
                },
                "author": "Heung-Yeung Shum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19255v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19051v1",
                "updated": "2024-12-26T04:13:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    13,
                    52,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T04:13:52Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    13,
                    52,
                    3,
                    361,
                    0
                ],
                "title": "Performance Characterization and Optimizations of Traditional ML\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Characterization and Optimizations of Traditional ML\n  Applications"
                },
                "summary": "Even in the era of Deep Learning based methods, traditional machine learning\nmethods with large data sets continue to attract significant attention.\nHowever, we find an apparent lack of a detailed performance characterization of\nthese methods in the context of large training datasets. In this work, we study\nthe system's behavior of a number of traditional ML methods as implemented in\npopular free software libraries/modules to identify critical performance\nbottlenecks experienced by these applications. The performance characterization\nstudy reveals several interesting insights on the performance of these\napplications. Then we evaluate the performance benefits of applying some\nwell-known optimizations at the levels of caches and the main memory. More\nspecifically, we test the usefulness of optimizations such as (i) software\nprefetching to improve cache performance and (ii) data layout and computation\nreordering optimizations to improve locality in DRAM accesses. These\noptimizations are implemented as modifications to the well-known scikit-learn\nlibrary, and hence can be easily leveraged by application programmers. We\nevaluate the impact of the proposed optimizations using a combination of\nsimulation and execution on a real system. The software prefetching\noptimization results in performance benefits varying from 5.2%-27.1% on\ndifferent ML applications while the data layout and computation reordering\napproaches yield 6.16%-28.0% performance improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Even in the era of Deep Learning based methods, traditional machine learning\nmethods with large data sets continue to attract significant attention.\nHowever, we find an apparent lack of a detailed performance characterization of\nthese methods in the context of large training datasets. In this work, we study\nthe system's behavior of a number of traditional ML methods as implemented in\npopular free software libraries/modules to identify critical performance\nbottlenecks experienced by these applications. The performance characterization\nstudy reveals several interesting insights on the performance of these\napplications. Then we evaluate the performance benefits of applying some\nwell-known optimizations at the levels of caches and the main memory. More\nspecifically, we test the usefulness of optimizations such as (i) software\nprefetching to improve cache performance and (ii) data layout and computation\nreordering optimizations to improve locality in DRAM accesses. These\noptimizations are implemented as modifications to the well-known scikit-learn\nlibrary, and hence can be easily leveraged by application programmers. We\nevaluate the impact of the proposed optimizations using a combination of\nsimulation and execution on a real system. The software prefetching\noptimization results in performance benefits varying from 5.2%-27.1% on\ndifferent ML applications while the data layout and computation reordering\napproaches yield 6.16%-28.0% performance improvement."
                },
                "authors": [
                    {
                        "name": "Harsh Kumar"
                    },
                    {
                        "name": "R. Govindarajan"
                    }
                ],
                "author_detail": {
                    "name": "R. Govindarajan"
                },
                "author": "R. Govindarajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18960v1",
                "updated": "2024-12-25T18:36:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    18,
                    36,
                    21,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T18:36:21Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    18,
                    36,
                    21,
                    2,
                    360,
                    0
                ],
                "title": "XRFlux: Virtual Reality Benchmark for Edge Caching Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XRFlux: Virtual Reality Benchmark for Edge Caching Systems"
                },
                "summary": "We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality\n(VR) delivery systems using edge-cloud caching. As VR applications and systems\nprogress, the need to meet strict latency and Quality of Experience (QoE)\nrequirements is increasingly evident. In the context of VR, traditional cloud\narchitectures (e.g., remote AWS S3 for content delivery) often struggle to meet\nthese demands, especially for users of the same application in different\nlocations. With edge computing, resources are brought closer to users in\nefforts to reduce latency and improve QoEs. However, VR's dynamic nature, with\nchanging fields of view (FoVs) and user synchronization requirements, creates\nvarious challenges for edge caching. We address the lack of suitable benchmarks\nand propose a framework that simulates multiuser VR scenarios while logging\nusers' interaction with objects within their actual and predicted FoVs. The\nbenchmark's activity log can then be played back through an edge cache to\nassess the resulting QoEs. This tool fills a gap by supporting research in the\noptimization of edge caching (and other edge-cloud functions) for VR streaming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality\n(VR) delivery systems using edge-cloud caching. As VR applications and systems\nprogress, the need to meet strict latency and Quality of Experience (QoE)\nrequirements is increasingly evident. In the context of VR, traditional cloud\narchitectures (e.g., remote AWS S3 for content delivery) often struggle to meet\nthese demands, especially for users of the same application in different\nlocations. With edge computing, resources are brought closer to users in\nefforts to reduce latency and improve QoEs. However, VR's dynamic nature, with\nchanging fields of view (FoVs) and user synchronization requirements, creates\nvarious challenges for edge caching. We address the lack of suitable benchmarks\nand propose a framework that simulates multiuser VR scenarios while logging\nusers' interaction with objects within their actual and predicted FoVs. The\nbenchmark's activity log can then be played back through an edge cache to\nassess the resulting QoEs. This tool fills a gap by supporting research in the\noptimization of edge caching (and other edge-cloud functions) for VR streaming."
                },
                "authors": [
                    {
                        "name": "Nader Alfares"
                    },
                    {
                        "name": "George Kesidis"
                    }
                ],
                "author_detail": {
                    "name": "George Kesidis"
                },
                "author": "George Kesidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v1",
                "updated": "2024-12-25T14:14:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With\n  Structured Memories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With\n  Structured Memories"
                },
                "summary": "Long-range tasks require reasoning over long inputs. Existing solutions\neither need large compute budgets, training data, access to model weights, or\nuse complex, task-specific approaches. We present PRISM, which alleviates these\nconcerns by processing information as a stream of chunks, maintaining a\nstructured in-context memory specified by a typed hierarchy schema. This\napproach demonstrates superior performance to baselines on diverse tasks while\nusing at least 4x smaller contexts than long-context models. Moreover, PRISM is\ntoken-efficient. By producing short outputs and efficiently leveraging\nkey-value (KV) caches, it achieves up to 54% cost reduction when compared to\nalternative short-context approaches. The method also scales down to tiny\ninformation chunks (e.g., 500 tokens) without increasing the number of tokens\nencoded or sacrificing quality. Furthermore, we show that it is possible to\ngenerate schemas to generalize our approach to new tasks with minimal effort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks require reasoning over long inputs. Existing solutions\neither need large compute budgets, training data, access to model weights, or\nuse complex, task-specific approaches. We present PRISM, which alleviates these\nconcerns by processing information as a stream of chunks, maintaining a\nstructured in-context memory specified by a typed hierarchy schema. This\napproach demonstrates superior performance to baselines on diverse tasks while\nusing at least 4x smaller contexts than long-context models. Moreover, PRISM is\ntoken-efficient. By producing short outputs and efficiently leveraging\nkey-value (KV) caches, it achieves up to 54% cost reduction when compared to\nalternative short-context approaches. The method also scales down to tiny\ninformation chunks (e.g., 500 tokens) without increasing the number of tokens\nencoded or sacrificing quality. Furthermore, we show that it is possible to\ngenerate schemas to generalize our approach to new tasks with minimal effort."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "23 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18911v1",
                "updated": "2024-12-25T14:00:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    0,
                    14,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T14:00:14Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    0,
                    14,
                    2,
                    360,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Dual Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Dual Feature Caching"
                },
                "summary": "Diffusion Transformers (DiT) have become the dominant methods in image and\nvideo generation yet still suffer substantial computational costs. As an\neffective approach for DiT acceleration, feature caching methods are designed\nto cache the features of DiT in previous timesteps and reuse them in the next\ntimesteps, allowing us to skip the computation in the next timesteps. However,\non the one hand, aggressively reusing all the features cached in previous\ntimesteps leads to a severe drop in generation quality. On the other hand,\nconservatively caching only the features in the redundant layers or tokens but\nstill computing the important ones successfully preserves the generation\nquality but results in reductions in acceleration ratios. Observing such a\ntradeoff between generation quality and acceleration performance, this paper\nbegins by quantitatively studying the accumulated error from cached features.\nSurprisingly, we find that aggressive caching does not introduce significantly\nmore caching errors in the caching step, and the conservative feature caching\ncan fix the error introduced by aggressive caching. Thereby, we propose a dual\ncaching strategy that adopts aggressive and conservative caching iteratively,\nleading to significant acceleration and high generation quality at the same\ntime. Besides, we further introduce a V-caching strategy for token-wise\nconservative caching, which is compatible with flash attention and requires no\ntraining and calibration data.\n  Our codes have been released in Github: \\textbf{Code:\n\\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have become the dominant methods in image and\nvideo generation yet still suffer substantial computational costs. As an\neffective approach for DiT acceleration, feature caching methods are designed\nto cache the features of DiT in previous timesteps and reuse them in the next\ntimesteps, allowing us to skip the computation in the next timesteps. However,\non the one hand, aggressively reusing all the features cached in previous\ntimesteps leads to a severe drop in generation quality. On the other hand,\nconservatively caching only the features in the redundant layers or tokens but\nstill computing the important ones successfully preserves the generation\nquality but results in reductions in acceleration ratios. Observing such a\ntradeoff between generation quality and acceleration performance, this paper\nbegins by quantitatively studying the accumulated error from cached features.\nSurprisingly, we find that aggressive caching does not introduce significantly\nmore caching errors in the caching step, and the conservative feature caching\ncan fix the error introduced by aggressive caching. Thereby, we propose a dual\ncaching strategy that adopts aggressive and conservative caching iteratively,\nleading to significant acceleration and high generation quality at the same\ntime. Besides, we further introduce a V-caching strategy for token-wise\nconservative caching, which is compatible with flash attention and requires no\ntraining and calibration data.\n  Our codes have been released in Github: \\textbf{Code:\n\\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}"
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Evelyn Zhang"
                    },
                    {
                        "name": "Runlin Guo"
                    },
                    {
                        "name": "Haohang Xu"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18885v1",
                "updated": "2024-12-25T11:59:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    11,
                    59,
                    17,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T11:59:17Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    11,
                    59,
                    17,
                    2,
                    360,
                    0
                ],
                "title": "Aspect-oriented Programming with Julia",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-oriented Programming with Julia"
                },
                "summary": "This paper proposes integrating Aspect-oriented Programming (AOP) into Julia,\na language widely used in scientific and High-Performance Computing (HPC). AOP\nenhances software modularity by encapsulating cross-cutting concerns, such as\nlogging, caching, and parallelizing, into separate, reusable aspects.\nLeveraging Julia's powerful metaprogramming and abstract syntax tree (AST)\nmanipulation capabilities, we introduce AspectJulia, an AOP framework designed\nto operate within Julia's runtime environment as a package. AspectJulia enables\ndevelopers to define and apply aspects seamlessly, leading to more modular,\nmaintainable, and adaptable code. We detail the implementation of AspectJulia\nand present diverse use cases, ranging from HPC and scientific computing to\nbusiness applications, demonstrating its effectiveness in managing\ncross-cutting concerns. This integration simplifies application development and\nimproves the adaptability of existing Julia modules and packages, paving the\nway for more efficient and maintainable software systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes integrating Aspect-oriented Programming (AOP) into Julia,\na language widely used in scientific and High-Performance Computing (HPC). AOP\nenhances software modularity by encapsulating cross-cutting concerns, such as\nlogging, caching, and parallelizing, into separate, reusable aspects.\nLeveraging Julia's powerful metaprogramming and abstract syntax tree (AST)\nmanipulation capabilities, we introduce AspectJulia, an AOP framework designed\nto operate within Julia's runtime environment as a package. AspectJulia enables\ndevelopers to define and apply aspects seamlessly, leading to more modular,\nmaintainable, and adaptable code. We detail the implementation of AspectJulia\nand present diverse use cases, ranging from HPC and scientific computing to\nbusiness applications, demonstrating its effectiveness in managing\ncross-cutting concerns. This integration simplifies application development and\nimproves the adaptability of existing Julia modules and packages, paving the\nway for more efficient and maintainable software systems."
                },
                "authors": [
                    {
                        "name": "Osamu Ishimura"
                    },
                    {
                        "name": "Yoshihide Yoshimoto"
                    }
                ],
                "author_detail": {
                    "name": "Yoshihide Yoshimoto"
                },
                "author": "Yoshihide Yoshimoto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16187v2",
                "updated": "2024-12-24T13:04:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    4,
                    45,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-13T06:00:27Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    6,
                    0,
                    27,
                    4,
                    348,
                    0
                ],
                "title": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing"
                },
                "summary": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks."
                },
                "authors": [
                    {
                        "name": "Minghui Liu"
                    },
                    {
                        "name": "Tahseen Rabbani"
                    },
                    {
                        "name": "Tony O'Halloran"
                    },
                    {
                        "name": "Ananth Sankaralingam"
                    },
                    {
                        "name": "Mary-Anne Hartley"
                    },
                    {
                        "name": "Brian Gravelle"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Cornelia Fermüller"
                    },
                    {
                        "name": "Yiannis Aloimonos"
                    }
                ],
                "author_detail": {
                    "name": "Yiannis Aloimonos"
                },
                "author": "Yiannis Aloimonos",
                "arxiv_comment": "10 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01959v2",
                "updated": "2024-12-24T00:46:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    0,
                    46,
                    0,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-02T20:39:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Development and Application of a Decentralized Domain Name Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Application of a Decentralized Domain Name Service"
                },
                "summary": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guang Yang"
                },
                "author": "Guang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17747v1",
                "updated": "2024-12-23T18:02:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T18:02:25Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "title": "Deliberation in Latent Space via Differentiable Cache Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deliberation in Latent Space via Differentiable Cache Augmentation"
                },
                "summary": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks."
                },
                "authors": [
                    {
                        "name": "Luyang Liu"
                    },
                    {
                        "name": "Jonas Pfeiffer"
                    },
                    {
                        "name": "Jiaxing Wu"
                    },
                    {
                        "name": "Jun Xie"
                    },
                    {
                        "name": "Arthur Szlam"
                    }
                ],
                "author_detail": {
                    "name": "Arthur Szlam"
                },
                "author": "Arthur Szlam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17685v1",
                "updated": "2024-12-23T16:11:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T16:11:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "A Reproducible Method for Mapping Electricity Transmission\n  Infrastructure for Space Weather Risk Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Reproducible Method for Mapping Electricity Transmission\n  Infrastructure for Space Weather Risk Assessment"
                },
                "summary": "Space weather impact assessment is constrained by the lack of available asset\ninformation to undertake modeling of Geomagnetically Induced Currents (GICs) in\nExtra High Voltage electricity infrastructure networks. The U.S. National Space\nWeather Strategy and Action Plan identifies underutilized data as a central\nissue for improving risk assessment, motivating this research. Accurate GIC\nprediction is generally not possible without information on the electrical\ncircuit, therefore we define a reproducible method based on open-source data,\nwhich enables risk analysts to collect their own substation component data.\nThis process converts OpenStreetMap (OSM) substation locations to\nhigh-resolution, component-level mapping of electricity transmission assets by\nutilizing an innovative web-browser platform to facilitate component\nannotation. As a case study example, we convert an initial 1,313 high-voltage\n(>115 kV) substations to 52,273 substation components via Google Earth APIs\nutilizing low-altitude, satellite, and Streetview imagery. We find that a total\nof 41,642 substation components (79.6%) connect to the highest substation\nvoltage levels (>345 kV) and are possibly susceptible to GIC, with a total of\n7,949 transformers identified. Compared to the initial OSM baseline, we provide\nnew detailed insights on voltage levels, line capacities, and substation\nconfigurations. Two validation workshops were undertaken to align the method\nand data with GIC assessment needs. The approach ensures consistency and rapid\nscalability, enabling users to quickly count components via a flexible\nweb-browser application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Space weather impact assessment is constrained by the lack of available asset\ninformation to undertake modeling of Geomagnetically Induced Currents (GICs) in\nExtra High Voltage electricity infrastructure networks. The U.S. National Space\nWeather Strategy and Action Plan identifies underutilized data as a central\nissue for improving risk assessment, motivating this research. Accurate GIC\nprediction is generally not possible without information on the electrical\ncircuit, therefore we define a reproducible method based on open-source data,\nwhich enables risk analysts to collect their own substation component data.\nThis process converts OpenStreetMap (OSM) substation locations to\nhigh-resolution, component-level mapping of electricity transmission assets by\nutilizing an innovative web-browser platform to facilitate component\nannotation. As a case study example, we convert an initial 1,313 high-voltage\n(>115 kV) substations to 52,273 substation components via Google Earth APIs\nutilizing low-altitude, satellite, and Streetview imagery. We find that a total\nof 41,642 substation components (79.6%) connect to the highest substation\nvoltage levels (>345 kV) and are possibly susceptible to GIC, with a total of\n7,949 transformers identified. Compared to the initial OSM baseline, we provide\nnew detailed insights on voltage levels, line capacities, and substation\nconfigurations. Two validation workshops were undertaken to align the method\nand data with GIC assessment needs. The approach ensures consistency and rapid\nscalability, enabling users to quickly count components via a flexible\nweb-browser application."
                },
                "authors": [
                    {
                        "name": "Edward J. Oughton"
                    },
                    {
                        "name": "Evan Alexander Peters"
                    },
                    {
                        "name": "Dennies Bor"
                    },
                    {
                        "name": "Noah Rivera"
                    },
                    {
                        "name": "C. Trevor Gaunt"
                    },
                    {
                        "name": "Robert Weigel"
                    }
                ],
                "author_detail": {
                    "name": "Robert Weigel"
                },
                "author": "Robert Weigel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18919v2",
                "updated": "2024-12-23T14:40:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    40,
                    26,
                    0,
                    358,
                    0
                ],
                "published": "2024-05-29T09:22:25Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    9,
                    22,
                    25,
                    2,
                    150,
                    0
                ],
                "title": "Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT\n  in SAGIN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT\n  in SAGIN"
                },
                "summary": "The rapid development of the aviation Internet of Things (IoT) has positioned\nin-flight connectivity (IFC) as one of its critical applications.\nSpace-air-ground integrated networks (SAGIN) are essential for ensuring the\nperformance of IFC by enabling seamless and reliable connectivity. However,\nmost existing research treats satellites merely as transparent forwarding nodes\nand overlooks their potential caching capabilities to enhance IFC data rates.\nIn this article, we explore an IFC-oriented SAGIN where satellites and ground\nstations (GSs) work together to transmit content to airborne passengers,\nthereby facilitating airborne communication. By categorizing files into cached\n(instantly accessible via satellites) and non-cached files (available only\nthrough GSs), this article pioneers the integration of multiple inter-satellite\nlinks (ISLs) into the IFC framework, thus innovating the content delivery\nprocess for both types of files. To minimize the average delay of content\ndelivery, we formulate the corresponding optimization problems: 1) For cached\nfiles, we propose an exact penalty-based method to determine the satellite\nassociation scheme. 2) For non-cached files, we present an efficient algorithm\nbased on alternating optimization to jointly optimize satellite association and\nGS bandwidth allocation. Our proposed framework is low in complexity, paving\nthe way for high-speed Internet connectivity for aviation passengers. Finally,\nsimulation results are provided to demonstrate the effectiveness of our\nproposed IFC framework for SAGIN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of the aviation Internet of Things (IoT) has positioned\nin-flight connectivity (IFC) as one of its critical applications.\nSpace-air-ground integrated networks (SAGIN) are essential for ensuring the\nperformance of IFC by enabling seamless and reliable connectivity. However,\nmost existing research treats satellites merely as transparent forwarding nodes\nand overlooks their potential caching capabilities to enhance IFC data rates.\nIn this article, we explore an IFC-oriented SAGIN where satellites and ground\nstations (GSs) work together to transmit content to airborne passengers,\nthereby facilitating airborne communication. By categorizing files into cached\n(instantly accessible via satellites) and non-cached files (available only\nthrough GSs), this article pioneers the integration of multiple inter-satellite\nlinks (ISLs) into the IFC framework, thus innovating the content delivery\nprocess for both types of files. To minimize the average delay of content\ndelivery, we formulate the corresponding optimization problems: 1) For cached\nfiles, we propose an exact penalty-based method to determine the satellite\nassociation scheme. 2) For non-cached files, we present an efficient algorithm\nbased on alternating optimization to jointly optimize satellite association and\nGS bandwidth allocation. Our proposed framework is low in complexity, paving\nthe way for high-speed Internet connectivity for aviation passengers. Finally,\nsimulation results are provided to demonstrate the effectiveness of our\nproposed IFC framework for SAGIN."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Chenyu Wu"
                    },
                    {
                        "name": "Shuai Han"
                    },
                    {
                        "name": "Weixiao Meng"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "14 pages, 13 figures. This work has been accepted by IEEE Internet of\n  Things Journal. It is expanded on our previous research presented at the IEEE\n  Globecom 2024: Q. Chen, C. Wu, S. Han, W. Meng, and T. Q. Quek, \"Exploiting\n  Inter-Satellite Links for In-Flight Connectivity Scheme in Space-Air-Ground\n  Integrated Networks,\" in Proc. GLOBECOM 2024, Cape Town, South Africa, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03408v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03408v3",
                "updated": "2024-12-23T12:55:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    12,
                    55,
                    21,
                    0,
                    358,
                    0
                ],
                "published": "2024-02-05T15:10:42Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    15,
                    10,
                    42,
                    0,
                    36,
                    0
                ],
                "title": "A Framework for Effective Invocation Methods of Various LLM Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Effective Invocation Methods of Various LLM Services"
                },
                "summary": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research."
                },
                "authors": [
                    {
                        "name": "Can Wang"
                    },
                    {
                        "name": "Dianbo Sui"
                    },
                    {
                        "name": "Bolin Zhang"
                    },
                    {
                        "name": "Xiaoyu Liu"
                    },
                    {
                        "name": "Jiabao Kang"
                    },
                    {
                        "name": "Zhidong Qiao"
                    },
                    {
                        "name": "Zhiying Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiying Tu"
                },
                "author": "Zhiying Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03408v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03408v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17464v1",
                "updated": "2024-12-23T10:41:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    41,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T10:41:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    41,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "CALLIC: Content Adaptive Learning for Lossless Image Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CALLIC: Content Adaptive Learning for Lossless Image Compression"
                },
                "summary": "Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression."
                },
                "authors": [
                    {
                        "name": "Daxin Li"
                    },
                    {
                        "name": "Yuanchao Bai"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Junjun Jiang"
                    },
                    {
                        "name": "Xianming Liu"
                    },
                    {
                        "name": "Wen Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Gao"
                },
                "author": "Wen Gao",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17246v1",
                "updated": "2024-12-23T03:38:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T03:38:46Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "title": "Fast and Live Model Auto Scaling with O(1) Host Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Live Model Auto Scaling with O(1) Host Caching"
                },
                "summary": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. We first show that data plane can be made fast with no/O(1) caching by\nloading parameters through the compute network between GPUs because: (1) its\nspeed is comparable host cache and is underutilized; (2) scaling multiple\ninstances requires no or O(1) caching with network-optimized multicast. Second,\nautoscaling can be made live by breaking the scaling abstraction from a\ncoarse-grained instance-level to a fine-grained layer-level. This allows us to\noffload the layer computation from the overloaded serving instances to the\nscaled instance with cooperative execution, thus handles cases even when the\ncompute network is not sufficiently fast. Our system BLITZSCALE reduces the\nserving tail latencies by up to 86% without caching, and we achieve comparable\nperformance (or even better) to an optimal setup where all the parameters are\ncached at all the host for autoscaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. We first show that data plane can be made fast with no/O(1) caching by\nloading parameters through the compute network between GPUs because: (1) its\nspeed is comparable host cache and is underutilized; (2) scaling multiple\ninstances requires no or O(1) caching with network-optimized multicast. Second,\nautoscaling can be made live by breaking the scaling abstraction from a\ncoarse-grained instance-level to a fine-grained layer-level. This allows us to\noffload the layer computation from the overloaded serving instances to the\nscaled instance with cooperative execution, thus handles cases even when the\ncompute network is not sufficiently fast. Our system BLITZSCALE reduces the\nserving tail latencies by up to 86% without caching, and we achieve comparable\nperformance (or even better) to an optimal setup where all the parameters are\ncached at all the host for autoscaling."
                },
                "authors": [
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05831v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05831v2",
                "updated": "2024-12-23T02:52:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    2,
                    52,
                    36,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-08T06:37:27Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    6,
                    37,
                    27,
                    6,
                    343,
                    0
                ],
                "title": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval"
                },
                "summary": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval."
                },
                "authors": [
                    {
                        "name": "Shanti Stewart"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Lie Lu"
                    },
                    {
                        "name": "Andrea Fanelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Fanelli"
                },
                "author": "Andrea Fanelli",
                "arxiv_comment": "Accepted at ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05831v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05831v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17203v1",
                "updated": "2024-12-23T00:46:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    0,
                    46,
                    53,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T00:46:53Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    0,
                    46,
                    53,
                    0,
                    358,
                    0
                ],
                "title": "Agile TLB Prefetching and Prediction Replacement Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agile TLB Prefetching and Prediction Replacement Policy"
                },
                "summary": "Virtual-to-physical address translation is a critical performance bottleneck\nin paging-based virtual memory systems. The Translation Lookaside Buffer (TLB)\naccelerates address translation by caching frequently accessed mappings, but\nTLB misses lead to costly page walks. Hardware and software techniques address\nthis challenge. Hardware approaches enhance TLB reach through system-level\nsupport, while software optimizations include TLB prefetching, replacement\npolicies, superpages, and page size adjustments. Prefetching Page Table Entries\n(PTEs) for future accesses reduces bottlenecks but may incur overhead from\nincorrect predictions. Integrating an Agile TLB Prefetcher (ATP) with SBFP\noptimizes performance by leveraging page table locality and dynamically\nidentifying essential free PTEs during page walks. Predictive replacement\npolicies further improve TLB performance. Traditional LRU replacement is\nlimited to near-instant references, while advanced policies like SRRIP, GHRP,\nSHiP, SDBP, and CHiRP enhance performance by targeting specific inefficiencies.\nCHiRP, tailored for L2 TLBs, surpasses other policies by leveraging control\nflow history to detect dead blocks, utilizing L2 TLB entries for learning\ninstead of sampling. These integrated techniques collectively address key\nchallenges in virtual memory management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual-to-physical address translation is a critical performance bottleneck\nin paging-based virtual memory systems. The Translation Lookaside Buffer (TLB)\naccelerates address translation by caching frequently accessed mappings, but\nTLB misses lead to costly page walks. Hardware and software techniques address\nthis challenge. Hardware approaches enhance TLB reach through system-level\nsupport, while software optimizations include TLB prefetching, replacement\npolicies, superpages, and page size adjustments. Prefetching Page Table Entries\n(PTEs) for future accesses reduces bottlenecks but may incur overhead from\nincorrect predictions. Integrating an Agile TLB Prefetcher (ATP) with SBFP\noptimizes performance by leveraging page table locality and dynamically\nidentifying essential free PTEs during page walks. Predictive replacement\npolicies further improve TLB performance. Traditional LRU replacement is\nlimited to near-instant references, while advanced policies like SRRIP, GHRP,\nSHiP, SDBP, and CHiRP enhance performance by targeting specific inefficiencies.\nCHiRP, tailored for L2 TLBs, surpasses other policies by leveraging control\nflow history to detect dead blocks, utilizing L2 TLB entries for learning\ninstead of sampling. These integrated techniques collectively address key\nchallenges in virtual memory management."
                },
                "authors": [
                    {
                        "name": "Melkamu Mersha"
                    },
                    {
                        "name": "Tsion Abay"
                    },
                    {
                        "name": "Mingziem Bitewa"
                    },
                    {
                        "name": "Gedare Bloom"
                    }
                ],
                "author_detail": {
                    "name": "Gedare Bloom"
                },
                "author": "Gedare Bloom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16897v1",
                "updated": "2024-12-22T07:14:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "published": "2024-12-22T07:14:45Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "title": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context"
                },
                "summary": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC"
                },
                "authors": [
                    {
                        "name": "Shuai Lyu"
                    },
                    {
                        "name": "Fangjian Liao"
                    },
                    {
                        "name": "Zeqi Ma"
                    },
                    {
                        "name": "Rongchen Zhang"
                    },
                    {
                        "name": "Dongmei Mo"
                    },
                    {
                        "name": "Waikeung Wong"
                    }
                ],
                "author_detail": {
                    "name": "Waikeung Wong"
                },
                "author": "Waikeung Wong",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17565v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17565v3",
                "updated": "2024-12-21T13:55:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    13,
                    55,
                    49,
                    5,
                    356,
                    0
                ],
                "published": "2024-06-25T14:02:08Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    14,
                    2,
                    8,
                    1,
                    177,
                    0
                ],
                "title": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic\n  Memory Pool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic\n  Memory Pool"
                },
                "summary": "Large language model (LLM) serving has transformed from stateless to stateful\nsystems, utilizing techniques like context caching and disaggregated inference.\nThese optimizations extend the lifespan and domain of the KV cache,\nnecessitating a new architectural approach. We present MemServe, a unified\nsystem that integrates both inter-request and intra-request optimizations.\nMemServe introduces MemPool, an elastic memory pool managing distributed memory\nand KV caches across serving instances. Using MemPool APIs, MemServe combines\ncontext caching with disaggregated inference for the first time, supported by a\nglobal scheduler that enhances cache reuse through a global prompt tree-based\nlocality-aware policy. Tests show that MemServe significantly improves job\ncompletion time and time-to-first-time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) serving has transformed from stateless to stateful\nsystems, utilizing techniques like context caching and disaggregated inference.\nThese optimizations extend the lifespan and domain of the KV cache,\nnecessitating a new architectural approach. We present MemServe, a unified\nsystem that integrates both inter-request and intra-request optimizations.\nMemServe introduces MemPool, an elastic memory pool managing distributed memory\nand KV caches across serving instances. Using MemPool APIs, MemServe combines\ncontext caching with disaggregated inference for the first time, supported by a\nglobal scheduler that enhances cache reuse through a global prompt tree-based\nlocality-aware policy. Tests show that MemServe significantly improves job\ncompletion time and time-to-first-time."
                },
                "authors": [
                    {
                        "name": "Cunchen Hu"
                    },
                    {
                        "name": "Heyang Huang"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Jiang Xu"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Sa Wang"
                    },
                    {
                        "name": "Yungang Bao"
                    },
                    {
                        "name": "Ninghui Sun"
                    },
                    {
                        "name": "Yizhou Shan"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Shan"
                },
                "author": "Yizhou Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17565v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17565v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16585v1",
                "updated": "2024-12-21T11:20:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    11,
                    20,
                    26,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-21T11:20:26Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    11,
                    20,
                    26,
                    5,
                    356,
                    0
                ],
                "title": "Parameterized Complexity of Caching in Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameterized Complexity of Caching in Networks"
                },
                "summary": "The fundamental caching problem in networks asks to find an allocation of\ncontents to a network of caches with the aim of maximizing the cache hit rate.\nDespite the problem's importance to a variety of research areas -- including\nnot only content delivery, but also edge intelligence and inference -- and the\nextensive body of work on empirical aspects of caching, very little is known\nabout the exact boundaries of tractability for the problem beyond its general\nNP-hardness. We close this gap by performing a comprehensive\ncomplexity-theoretic analysis of the problem through the lens of the\nparameterized complexity paradigm, which is designed to provide more precise\nstatements regarding algorithmic tractability than classical complexity. Our\nresults include algorithmic lower and upper bounds which together establish the\nconditions under which the caching problem becomes tractable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fundamental caching problem in networks asks to find an allocation of\ncontents to a network of caches with the aim of maximizing the cache hit rate.\nDespite the problem's importance to a variety of research areas -- including\nnot only content delivery, but also edge intelligence and inference -- and the\nextensive body of work on empirical aspects of caching, very little is known\nabout the exact boundaries of tractability for the problem beyond its general\nNP-hardness. We close this gap by performing a comprehensive\ncomplexity-theoretic analysis of the problem through the lens of the\nparameterized complexity paradigm, which is designed to provide more precise\nstatements regarding algorithmic tractability than classical complexity. Our\nresults include algorithmic lower and upper bounds which together establish the\nconditions under which the caching problem becomes tractable."
                },
                "authors": [
                    {
                        "name": "Robert Ganian"
                    },
                    {
                        "name": "Fionn Mc Inerney"
                    },
                    {
                        "name": "Dimitra Tsigkari"
                    }
                ],
                "author_detail": {
                    "name": "Dimitra Tsigkari"
                },
                "author": "Dimitra Tsigkari",
                "arxiv_comment": "A shorter version of this paper will appear in the proceedings of\n  AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v4",
                "updated": "2024-12-21T02:36:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    2,
                    36,
                    3,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiming Yang"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    },
                    {
                        "name": "Zonghong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Zonghong Dai"
                },
                "author": "Zonghong Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16434v1",
                "updated": "2024-12-21T01:48:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    1,
                    48,
                    52,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-21T01:48:52Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    1,
                    48,
                    52,
                    5,
                    356,
                    0
                ],
                "title": "SYMPHONY: Improving Memory Management for LLM Inference Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SYMPHONY: Improving Memory Management for LLM Inference Workloads"
                },
                "summary": "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile."
                },
                "authors": [
                    {
                        "name": "Saurabh Agarwal"
                    },
                    {
                        "name": "Anyong Mao"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16001v1",
                "updated": "2024-12-20T15:51:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    51,
                    42,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T15:51:42Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    51,
                    42,
                    4,
                    355,
                    0
                ],
                "title": "Multi-Strided Access Patterns to Boost Hardware Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Strided Access Patterns to Boost Hardware Prefetching"
                },
                "summary": "Important memory-bound kernels, such as linear algebra, convolutions, and\nstencils, rely on SIMD instructions as well as optimizations targeting improved\nvectorized data traversal and data re-use to attain satisfactory performance.\nOn on temporary CPU architectures, the hardware prefetcher is of key importance\nfor efficient utilization of the memory hierarchy. In this paper, we\ndemonstrate that transforming a memory access pattern consisting of a single\nstride to one that concurrently accesses multiple strides, can boost the\nutilization of the hardware prefetcher, and in turn improves the performance of\nmemory-bound kernels significantly. Using a set of micro-benchmarks, we\nestablish that accessing memory in a multi-strided manner enables more cache\nlines to be concurrently brought into the cache, resulting in improved cache\nhit ratios and higher effective memory bandwidth without the introduction of\ncostly software prefetch instructions. Subsequently, we show that multi-strided\nvariants of a collection of six memory-bound dense compute kernels outperform\nstate-of-the-art counterparts on three different micro-architectures. More\nspecifically, for kernels among which Matrix Vector Multiplication, Convolution\nStencil and kernels from PolyBench, we achieve significant speedups of up to\n12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and\n1.87x over OpenCV. The code transformation to take advantage of multi-strided\nmemory access is a natural extension of the loop unroll and loop interchange\ntechniques, allowing this method to be incorporated into compiler pipelines in\nthe future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Important memory-bound kernels, such as linear algebra, convolutions, and\nstencils, rely on SIMD instructions as well as optimizations targeting improved\nvectorized data traversal and data re-use to attain satisfactory performance.\nOn on temporary CPU architectures, the hardware prefetcher is of key importance\nfor efficient utilization of the memory hierarchy. In this paper, we\ndemonstrate that transforming a memory access pattern consisting of a single\nstride to one that concurrently accesses multiple strides, can boost the\nutilization of the hardware prefetcher, and in turn improves the performance of\nmemory-bound kernels significantly. Using a set of micro-benchmarks, we\nestablish that accessing memory in a multi-strided manner enables more cache\nlines to be concurrently brought into the cache, resulting in improved cache\nhit ratios and higher effective memory bandwidth without the introduction of\ncostly software prefetch instructions. Subsequently, we show that multi-strided\nvariants of a collection of six memory-bound dense compute kernels outperform\nstate-of-the-art counterparts on three different micro-architectures. More\nspecifically, for kernels among which Matrix Vector Multiplication, Convolution\nStencil and kernels from PolyBench, we achieve significant speedups of up to\n12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and\n1.87x over OpenCV. The code transformation to take advantage of multi-strided\nmemory access is a natural extension of the loop unroll and loop interchange\ntechniques, allowing this method to be incorporated into compiler pipelines in\nthe future."
                },
                "authors": [
                    {
                        "name": "Miguel O. Blom"
                    },
                    {
                        "name": "Kristian F. D. Rietveld"
                    },
                    {
                        "name": "Rob V. van Nieuwpoort"
                    }
                ],
                "author_detail": {
                    "name": "Rob V. van Nieuwpoort"
                },
                "author": "Rob V. van Nieuwpoort",
                "arxiv_comment": "12 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14485v2",
                "updated": "2024-12-20T15:18:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    18,
                    44,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-19T03:11:33Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    3,
                    11,
                    33,
                    3,
                    354,
                    0
                ],
                "title": "Towards Projected and Incremental Pseudo-Boolean Model Counting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Projected and Incremental Pseudo-Boolean Model Counting"
                },
                "summary": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption. In this work, our main\ncontribution is the introduction of the PB model counter PBCount2, the first\nexact PB model counter with support for projected and incremental model\ncounting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree\n(LOW-MD) computation ordering heuristic to support projected model counting and\na cache mechanism to enable incremental model counting. In our evaluations,\nPBCount2 completed at least 1.40x the number of benchmarks of competing methods\nfor projected model counting and at least 1.18x of competing methods in\nincremental model counting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption. In this work, our main\ncontribution is the introduction of the PB model counter PBCount2, the first\nexact PB model counter with support for projected and incremental model\ncounting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree\n(LOW-MD) computation ordering heuristic to support projected model counting and\na cache mechanism to enable incremental model counting. In our evaluations,\nPBCount2 completed at least 1.40x the number of benchmarks of competing methods\nfor projected model counting and at least 1.18x of competing methods in\nincremental model counting."
                },
                "authors": [
                    {
                        "name": "Suwei Yang"
                    },
                    {
                        "name": "Kuldeep S. Meel"
                    }
                ],
                "author_detail": {
                    "name": "Kuldeep S. Meel"
                },
                "author": "Kuldeep S. Meel",
                "arxiv_comment": "To appear in AAAI25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15605v1",
                "updated": "2024-12-20T06:58:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T06:58:32Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "title": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks"
                },
                "summary": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v3",
                "updated": "2024-12-19T23:52:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    23,
                    52,
                    16,
                    3,
                    354,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Madan Musuvathi"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12592v2",
                "updated": "2024-12-19T22:34:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    22,
                    34,
                    37,
                    3,
                    354,
                    0
                ],
                "published": "2024-08-22T17:56:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "Exposing Shadow Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Shadow Branches"
                },
                "summary": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation."
                },
                "authors": [
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14838v1",
                "updated": "2024-12-19T13:28:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs"
                },
                "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Xiabin Zhou"
                    },
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Minyan Zeng"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v3",
                "updated": "2024-12-19T12:38:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    12,
                    38,
                    23,
                    3,
                    354,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "In this version, we achieved a nearly lossless acceleration of 1.51\n  times for ToCa on FLUX in the appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14392v1",
                "updated": "2024-12-18T22:52:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    52,
                    12,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T22:52:12Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    52,
                    12,
                    2,
                    353,
                    0
                ],
                "title": "Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure\n  Integration in Machine Learning Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure\n  Integration in Machine Learning Systems"
                },
                "summary": "Machine learning (ML) systems that guarantee security and privacy often rely\non Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling\ncomputations on encrypted data without exposing sensitive information. However,\na critical limitation of FHE is its computational inefficiency, making it\nimpractical for large-scale applications. In this work, we propose\n\\textit{Nemesis}, a framework that accelerates FHE-based systems without\ncompromising accuracy or security. The design of Nemesis is inspired by Rache\n(SIGMOD'23), which introduced a caching mechanism for encrypted integers and\nscalars. Nemesis extends this idea with more advanced caching techniques and\nmathematical tools, enabling efficient operations over multi-slot FHE schemes\nand overcoming Rache's limitations to support general plaintext structures. We\nformally prove the security of Nemesis under standard cryptographic assumptions\nand evaluate its performance extensively on widely used datasets, including\nMNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis\nsignificantly reduces the computational overhead of FHE-based ML systems,\npaving the way for broader adoption of privacy-preserving technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) systems that guarantee security and privacy often rely\non Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling\ncomputations on encrypted data without exposing sensitive information. However,\na critical limitation of FHE is its computational inefficiency, making it\nimpractical for large-scale applications. In this work, we propose\n\\textit{Nemesis}, a framework that accelerates FHE-based systems without\ncompromising accuracy or security. The design of Nemesis is inspired by Rache\n(SIGMOD'23), which introduced a caching mechanism for encrypted integers and\nscalars. Nemesis extends this idea with more advanced caching techniques and\nmathematical tools, enabling efficient operations over multi-slot FHE schemes\nand overcoming Rache's limitations to support general plaintext structures. We\nformally prove the security of Nemesis under standard cryptographic assumptions\nand evaluate its performance extensively on widely used datasets, including\nMNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis\nsignificantly reduces the computational overhead of FHE-based ML systems,\npaving the way for broader adoption of privacy-preserving technologies."
                },
                "authors": [
                    {
                        "name": "Dongfang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongfang Zhao"
                },
                "author": "Dongfang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14363v1",
                "updated": "2024-12-18T22:01:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T22:01:55Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "title": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals"
                },
                "summary": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama families of models, we demonstrate\nthat ResQ outperforms recent uniform and mixed precision PTQ methods on a\nvariety of benchmarks, achieving up to 33% lower perplexity on Wikitext than\nthe next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code\nis available at https://github.com/utkarsh-dmx/project-resq.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama families of models, we demonstrate\nthat ResQ outperforms recent uniform and mixed precision PTQ methods on a\nvariety of benchmarks, achieving up to 33% lower perplexity on Wikitext than\nthe next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code\nis available at https://github.com/utkarsh-dmx/project-resq."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Sayeh Sharify"
                    },
                    {
                        "name": "Kaushik Roy"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "14 pages, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14335v1",
                "updated": "2024-12-18T21:09:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T21:09:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines"
                },
                "summary": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). C3 on average achieves only 21% of ideal speedup, this is\ndue to known challenges of compute and memory interference between concurrent\nGPU kernels (that is, sharing of GPU's compute units, caches and HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build Concurrent Communication CoLlectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). C3 on average achieves only 21% of ideal speedup, this is\ndue to known challenges of compute and memory interference between concurrent\nGPU kernels (that is, sharing of GPU's compute units, caches and HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build Concurrent Communication CoLlectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs."
                },
                "authors": [
                    {
                        "name": "Anirudha Agrawal"
                    },
                    {
                        "name": "Shaizeen Aga"
                    },
                    {
                        "name": "Suchita Pati"
                    },
                    {
                        "name": "Mahzabeen Islam"
                    }
                ],
                "author_detail": {
                    "name": "Mahzabeen Islam"
                },
                "author": "Mahzabeen Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v4",
                "updated": "2024-12-18T17:36:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    36,
                    36,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13779v1",
                "updated": "2024-12-18T12:16:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:16:41Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "title": "Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization"
                },
                "summary": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Yuying Wang"
                    },
                    {
                        "name": "Tianzhe Xiao"
                    },
                    {
                        "name": "Haozhao Wang"
                    },
                    {
                        "name": "Yining Qi"
                    },
                    {
                        "name": "Ruixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruixuan Li"
                },
                "author": "Ruixuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13771v1",
                "updated": "2024-12-18T12:07:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:07:58Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "title": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization"
                },
                "summary": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems."
                },
                "authors": [
                    {
                        "name": "Guanghan Li"
                    },
                    {
                        "name": "Xun Zhang"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Yifan Yin"
                    },
                    {
                        "name": "Guojun Yin"
                    },
                    {
                        "name": "Wei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Wei Lin"
                },
                "author": "Wei Lin",
                "arxiv_comment": "7 pages, 3 figures, AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v2",
                "updated": "2024-12-18T09:47:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    47,
                    25,
                    2,
                    353,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13649v1",
                "updated": "2024-12-18T09:27:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T09:27:33Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "title": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation"
                },
                "summary": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods."
                },
                "authors": [
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Zhenglin Wang"
                    },
                    {
                        "name": "Linhai Zhang"
                    },
                    {
                        "name": "Yilong Lai"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Deyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhou"
                },
                "author": "Deyu Zhou",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08584v2",
                "updated": "2024-12-18T07:45:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    7,
                    45,
                    11,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-11T07:24:21Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "title": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification"
                },
                "summary": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs."
                },
                "authors": [
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Hong Zhou"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13509v1",
                "updated": "2024-12-18T05:16:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T05:16:11Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "title": "Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data\n  Presentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data\n  Presentation"
                },
                "summary": "Understanding sensor data can be challenging for non-experts because of the\ncomplexity and unique semantic meanings of sensor modalities. This calls for\nintuitive and effective methods to present sensor information. However,\ncreating intuitive sensor data visualizations presents three key challenges:\nthe variability of sensor readings, gaps in domain comprehension, and the\ndynamic nature of sensor data. To address these issues, we develop Vivar, a\nnovel AR system that integrates multi-modal sensor data and presents 3D\nvolumetric content for visualization. In particular, we introduce a cross-modal\nembedding approach that maps sensor data into a pre-trained visual embedding\nspace through barycentric interpolation. This allows for accurate and\ncontinuous integration of multi-modal sensor information. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation. Our extensive experiments demonstrate that our system\nachieves 11$\\times$ latency reduction without compromising quality. A user\nstudy involving over 485 participants, including domain experts, demonstrates\nVivar's effectiveness in accuracy, consistency, and real-world applicability,\npaving the way for more intuitive sensor data visualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding sensor data can be challenging for non-experts because of the\ncomplexity and unique semantic meanings of sensor modalities. This calls for\nintuitive and effective methods to present sensor information. However,\ncreating intuitive sensor data visualizations presents three key challenges:\nthe variability of sensor readings, gaps in domain comprehension, and the\ndynamic nature of sensor data. To address these issues, we develop Vivar, a\nnovel AR system that integrates multi-modal sensor data and presents 3D\nvolumetric content for visualization. In particular, we introduce a cross-modal\nembedding approach that maps sensor data into a pre-trained visual embedding\nspace through barycentric interpolation. This allows for accurate and\ncontinuous integration of multi-modal sensor information. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation. Our extensive experiments demonstrate that our system\nachieves 11$\\times$ latency reduction without compromising quality. A user\nstudy involving over 485 participants, including domain experts, demonstrates\nVivar's effectiveness in accuracy, consistency, and real-world applicability,\npaving the way for more intuitive sensor data visualization."
                },
                "authors": [
                    {
                        "name": "Yunqi Guo"
                    },
                    {
                        "name": "Kaiyuan Hou"
                    },
                    {
                        "name": "Heming Fu"
                    },
                    {
                        "name": "Hongkai Chen"
                    },
                    {
                        "name": "Zhenyu Yan"
                    },
                    {
                        "name": "Guoliang Xing"
                    },
                    {
                        "name": "Xiaofan Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofan Jiang"
                },
                "author": "Xiaofan Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12486v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12486v2",
                "updated": "2024-12-18T05:08:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    8,
                    39,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-17T02:43:54Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    43,
                    54,
                    1,
                    352,
                    0
                ],
                "title": "Boosting Long-Context Management via Query-Guided Activation Refilling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Long-Context Management via Query-Guided Activation Refilling"
                },
                "summary": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Defu Lian"
                    }
                ],
                "author_detail": {
                    "name": "Defu Lian"
                },
                "author": "Defu Lian",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12486v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12486v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04012v1",
                "updated": "2024-12-18T00:35:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    0,
                    35,
                    16,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T00:35:16Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    0,
                    35,
                    16,
                    2,
                    353,
                    0
                ],
                "title": "FlexCache: Flexible Approximate Cache System for Video Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexCache: Flexible Approximate Cache System for Video Diffusion"
                },
                "summary": "Text-to-Video applications receive increasing attention from the public.\nAmong these, diffusion models have emerged as the most prominent approach,\noffering impressive quality in visual content generation. However, it still\nsuffers from substantial computational complexity, often requiring several\nminutes to generate a single video. While prior research has addressed the\ncomputational overhead in text-to-image diffusion models, the techniques\ndeveloped are not directly suitable for video diffusion models due to the\nsignificantly larger cache requirements and enhanced computational demands\nassociated with video generation.\n  We present FlexCache, a flexible approximate cache system that addresses the\nchallenges in two main designs. First, we compress the caches before saving\nthem to storage. Our compression strategy can reduce 6.7 times consumption on\naverage. Then we find that the approximate cache system can achieve higher hit\nrate and computation savings by decoupling the object and background. We\nfurther design a tailored cache replacement policy to support the two\ntechniques mentioned above better. Through our evaluation, FlexCache reaches\n1.26 times higher throughput and 25% lower cost compared to the\nstate-of-the-art diffusion approximate cache system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-Video applications receive increasing attention from the public.\nAmong these, diffusion models have emerged as the most prominent approach,\noffering impressive quality in visual content generation. However, it still\nsuffers from substantial computational complexity, often requiring several\nminutes to generate a single video. While prior research has addressed the\ncomputational overhead in text-to-image diffusion models, the techniques\ndeveloped are not directly suitable for video diffusion models due to the\nsignificantly larger cache requirements and enhanced computational demands\nassociated with video generation.\n  We present FlexCache, a flexible approximate cache system that addresses the\nchallenges in two main designs. First, we compress the caches before saving\nthem to storage. Our compression strategy can reduce 6.7 times consumption on\naverage. Then we find that the approximate cache system can achieve higher hit\nrate and computation savings by decoupling the object and background. We\nfurther design a tailored cache replacement policy to support the two\ntechniques mentioned above better. Through our evaluation, FlexCache reaches\n1.26 times higher throughput and 25% lower cost compared to the\nstate-of-the-art diffusion approximate cache system."
                },
                "authors": [
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Henry Tian"
                    },
                    {
                        "name": "Tim Lu"
                    },
                    {
                        "name": "Sihang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sihang Liu"
                },
                "author": "Sihang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v3",
                "updated": "2024-12-17T14:45:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    45,
                    12,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Code is available at https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12953v1",
                "updated": "2024-12-17T14:34:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    34,
                    51,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T14:34:51Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    34,
                    51,
                    1,
                    352,
                    0
                ],
                "title": "Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning"
                },
                "summary": "Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/."
                },
                "authors": [
                    {
                        "name": "Moritz Reuss"
                    },
                    {
                        "name": "Jyothish Pari"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Rudolf Lioutikov"
                    }
                ],
                "author_detail": {
                    "name": "Rudolf Lioutikov"
                },
                "author": "Rudolf Lioutikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12798v1",
                "updated": "2024-12-17T11:00:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    0,
                    56,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T11:00:56Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    0,
                    56,
                    1,
                    352,
                    0
                ],
                "title": "ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation"
                },
                "summary": "Instance segmentation algorithms in remote sensing are typically based on\nconventional methods, limiting their application to seen scenarios and\nclosed-set predictions. In this work, we propose a novel task called zero-shot\nremote sensing instance segmentation, aimed at identifying aerial objects that\nare absent from training data. Challenges arise when classifying aerial\ncategories with high inter-class similarity and intra-class variance. Besides,\nthe domain gap between vision-language models' pretraining datasets and remote\nsensing datasets hinders the zero-shot capabilities of the pretrained model\nwhen it is directly applied to remote sensing images. To address these\nchallenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote\nSensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our\napproach features a discrimination-enhanced classifier that uses refined\ntextual embeddings to increase the awareness of class disparities. Instead of\ndirect fine-tuning, we propose a knowledge-maintained adaptation strategy that\ndecouples semantic-related information to preserve the pretrained\nvision-language alignment while adjusting features to capture remote sensing\ndomain-specific visual cues. Additionally, we introduce a prior-injected\nprediction with cache bank of aerial visual prototypes to supplement the\nsemantic richness of text embeddings and seamlessly integrate aerial\nrepresentations, adapting to the remote sensing domain. We establish new\nexperimental protocols and benchmarks, and extensive experiments convincingly\ndemonstrate that ZoRI achieves the state-of-art performance on the zero-shot\nremote sensing instance segmentation task. Our code is available at\nhttps://github.com/HuangShiqi128/ZoRI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instance segmentation algorithms in remote sensing are typically based on\nconventional methods, limiting their application to seen scenarios and\nclosed-set predictions. In this work, we propose a novel task called zero-shot\nremote sensing instance segmentation, aimed at identifying aerial objects that\nare absent from training data. Challenges arise when classifying aerial\ncategories with high inter-class similarity and intra-class variance. Besides,\nthe domain gap between vision-language models' pretraining datasets and remote\nsensing datasets hinders the zero-shot capabilities of the pretrained model\nwhen it is directly applied to remote sensing images. To address these\nchallenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote\nSensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our\napproach features a discrimination-enhanced classifier that uses refined\ntextual embeddings to increase the awareness of class disparities. Instead of\ndirect fine-tuning, we propose a knowledge-maintained adaptation strategy that\ndecouples semantic-related information to preserve the pretrained\nvision-language alignment while adjusting features to capture remote sensing\ndomain-specific visual cues. Additionally, we introduce a prior-injected\nprediction with cache bank of aerial visual prototypes to supplement the\nsemantic richness of text embeddings and seamlessly integrate aerial\nrepresentations, adapting to the remote sensing domain. We establish new\nexperimental protocols and benchmarks, and extensive experiments convincingly\ndemonstrate that ZoRI achieves the state-of-art performance on the zero-shot\nremote sensing instance segmentation task. Our code is available at\nhttps://github.com/HuangShiqi128/ZoRI."
                },
                "authors": [
                    {
                        "name": "Shiqi Huang"
                    },
                    {
                        "name": "Shuting He"
                    },
                    {
                        "name": "Bihan Wen"
                    }
                ],
                "author_detail": {
                    "name": "Bihan Wen"
                },
                "author": "Bihan Wen",
                "arxiv_comment": "AAAI 2025, code see https://github.com/HuangShiqi128/ZoRI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12706v1",
                "updated": "2024-12-17T09:20:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T09:20:31Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "title": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression"
                },
                "summary": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture."
                },
                "authors": [
                    {
                        "name": "Jiebin Zhang"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Chuqiao Kuang"
                    },
                    {
                        "name": "Xiaoguang Li"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "13pages,7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v2",
                "updated": "2024-12-17T09:11:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    11,
                    47,
                    1,
                    352,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08585v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08585v3",
                "updated": "2024-12-17T05:40:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    40,
                    9,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-11T18:03:05Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    3,
                    5,
                    2,
                    346,
                    0
                ],
                "title": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs"
                },
                "summary": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "James Hensman"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Victor Ruhle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08585v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08585v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12543v1",
                "updated": "2024-12-17T05:09:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    9,
                    45,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T05:09:45Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    9,
                    45,
                    1,
                    352,
                    0
                ],
                "title": "Personalized Federated Deep Reinforcement Learning for Heterogeneous\n  Edge Content Caching Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Federated Deep Reinforcement Learning for Heterogeneous\n  Edge Content Caching Networks"
                },
                "summary": "Proactive caching is essential for minimizing latency and improving Quality\nof Experience (QoE) in multi-server edge networks. Federated Deep Reinforcement\nLearning (FDRL) is a promising approach for developing cache policies tailored\nto dynamic content requests. However, FDRL faces challenges such as an\nexpanding caching action space due to increased content numbers and difficulty\nin adapting global information to heterogeneous edge environments. In this\npaper, we propose a Personalized Federated Deep Reinforcement Learning\nframework for Caching, called PF-DRL-Ca, with the aim to maximize system\nutility while satisfying caching capability constraints. To manage the\nexpanding action space, we employ a new DRL algorithm, Multi-head Deep\nQ-Network (MH-DQN), which reshapes the action output layers of DQN into a\nmulti-head structure where each head generates a sub-dimensional action. We\nnext integrate the proposed MH-DQN into a personalized federated training\nframework, employing a layer-wise approach for training to derive a\npersonalized model that can adapt to heterogeneous environments while\nexploiting the global information to accelerate learning convergence. Our\nextensive experimental results demonstrate the superiority of MH-DQN over\ntraditional DRL algorithms on a single server, as well as the advantages of the\npersonal federated training architecture compared to other frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proactive caching is essential for minimizing latency and improving Quality\nof Experience (QoE) in multi-server edge networks. Federated Deep Reinforcement\nLearning (FDRL) is a promising approach for developing cache policies tailored\nto dynamic content requests. However, FDRL faces challenges such as an\nexpanding caching action space due to increased content numbers and difficulty\nin adapting global information to heterogeneous edge environments. In this\npaper, we propose a Personalized Federated Deep Reinforcement Learning\nframework for Caching, called PF-DRL-Ca, with the aim to maximize system\nutility while satisfying caching capability constraints. To manage the\nexpanding action space, we employ a new DRL algorithm, Multi-head Deep\nQ-Network (MH-DQN), which reshapes the action output layers of DQN into a\nmulti-head structure where each head generates a sub-dimensional action. We\nnext integrate the proposed MH-DQN into a personalized federated training\nframework, employing a layer-wise approach for training to derive a\npersonalized model that can adapt to heterogeneous environments while\nexploiting the global information to accelerate learning convergence. Our\nextensive experimental results demonstrate the superiority of MH-DQN over\ntraditional DRL algorithms on a single server, as well as the advantages of the\npersonal federated training architecture compared to other frameworks."
                },
                "authors": [
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Tan Li"
                    },
                    {
                        "name": "Hai Liu"
                    },
                    {
                        "name": "Tse-Tin Chan"
                    }
                ],
                "author_detail": {
                    "name": "Tse-Tin Chan"
                },
                "author": "Tse-Tin Chan",
                "arxiv_comment": "8 pages, 8 figures, WiOpt 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12488v1",
                "updated": "2024-12-17T02:44:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    44,
                    43,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T02:44:43Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    44,
                    43,
                    1,
                    352,
                    0
                ],
                "title": "A System for Microserving of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A System for Microserving of LLMs"
                },
                "summary": "The recent advances in LLMs bring a strong demand for efficient system\nsupport to improve overall serving efficiency. As LLM inference scales towards\nmultiple GPUs and even multiple compute nodes, various coordination patterns,\nsuch as prefill-decode disaggregation and context migration, arise in serving\nsystems. Most inference services today expose a coarse-grained request-level\nAPI with a pre-configured coordination strategy, limiting the ability to\ncustomize and dynamically reconfigure the coordination. In this paper, we\npropose LLM microserving, a multi-level architecture for structuring and\nprogramming LLM inference services. We introduces simple yet effective\nmicroserving APIs to support fine-grained sub-request level actions. A\nprogrammable router transforms user requests into sub-request calls, enabling\nthe dynamic reconfiguration of serving patterns. To support diverse execution\npatterns, we develop a unified KV cache interface that handles various KV\ncompute, transfer, and reuse scenarios. Our evaluation shows that LLM\nmicroserving can be reconfigured to support multiple disaggregation\norchestration strategies in a few lines of Python code while maintaining\nstate-of-the-art performance for LLM inference tasks. Additionally, it allows\nus to explore new strategy variants that reduce up to 47% of job completion\ntime compared to the existing strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advances in LLMs bring a strong demand for efficient system\nsupport to improve overall serving efficiency. As LLM inference scales towards\nmultiple GPUs and even multiple compute nodes, various coordination patterns,\nsuch as prefill-decode disaggregation and context migration, arise in serving\nsystems. Most inference services today expose a coarse-grained request-level\nAPI with a pre-configured coordination strategy, limiting the ability to\ncustomize and dynamically reconfigure the coordination. In this paper, we\npropose LLM microserving, a multi-level architecture for structuring and\nprogramming LLM inference services. We introduces simple yet effective\nmicroserving APIs to support fine-grained sub-request level actions. A\nprogrammable router transforms user requests into sub-request calls, enabling\nthe dynamic reconfiguration of serving patterns. To support diverse execution\npatterns, we develop a unified KV cache interface that handles various KV\ncompute, transfer, and reuse scenarios. Our evaluation shows that LLM\nmicroserving can be reconfigured to support multiple disaggregation\norchestration strategies in a few lines of Python code while maintaining\nstate-of-the-art performance for LLM inference tasks. Additionally, it allows\nus to explore new strategy variants that reduce up to 47% of job completion\ntime compared to the existing strategies."
                },
                "authors": [
                    {
                        "name": "Hongyi Jin"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Charlie F. Ruan"
                    },
                    {
                        "name": "Yingcheng Wang"
                    },
                    {
                        "name": "Todd C. Mowry"
                    },
                    {
                        "name": "Xupeng Miao"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Tianqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianqi Chen"
                },
                "author": "Tianqi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12444v1",
                "updated": "2024-12-17T01:12:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T01:12:35Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "title": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers"
                },
                "summary": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency."
                },
                "authors": [
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yanyu Li"
                    },
                    {
                        "name": "Yifan Gong"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Henghui Ding"
                    },
                    {
                        "name": "Zhihao Shu"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jiuxiang Gu"
                },
                "author": "Jiuxiang Gu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11828v1",
                "updated": "2024-12-16T14:49:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T14:49:32Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "title": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey"
                },
                "summary": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, based on the View Selection Problem, we propose a\nunified view on these problems. We identify the root causes of the complexity\nof these selection problems and provide a detailed analysis of techniques to\ncope with them. Our survey provides a modern classification of selection\nalgorithms known in the literature, including the latest ones based on Machine\nLearning. We provide a ground for the reuse of the selection techniques between\ndifferent optimization scenarios and highlight challenges and promising\ndirections in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, based on the View Selection Problem, we propose a\nunified view on these problems. We identify the root causes of the complexity\nof these selection problems and provide a detailed analysis of techniques to\ncope with them. Our survey provides a modern classification of selection\nalgorithms known in the literature, including the latest ones based on Machine\nLearning. We provide a ground for the reuse of the selection techniques between\ndifferent optimization scenarios and highlight challenges and promising\ndirections in the field."
                },
                "authors": [
                    {
                        "name": "Sergey Zinchenko"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11741v1",
                "updated": "2024-12-16T13:01:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T13:01:53Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "title": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation"
                },
                "summary": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments."
                },
                "authors": [
                    {
                        "name": "Hongxuan Zhang"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Jiaqi Zheng"
                    },
                    {
                        "name": "Chenyi Zhuang"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11706v1",
                "updated": "2024-12-16T12:28:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T12:28:22Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "title": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration"
                },
                "summary": "Video Diffusion Transformers (DiTs) have demonstrated significant potential\nfor generating high-fidelity videos but are computationally intensive. Existing\nacceleration methods include distillation, which requires costly retraining,\nand feature caching, which is highly sensitive to network architecture. Recent\ntoken reduction methods are training-free and architecture-agnostic, offering\ngreater flexibility and wider applicability. However, they enforce the same\nsequence length across different components, constraining their acceleration\npotential. We observe that intra-sequence redundancy in video DiTs varies\nacross features, blocks, and denoising timesteps. Building on this observation,\nwe propose Asymmetric Reduction and Restoration (AsymRnR), a training-free\napproach to accelerate video DiTs. It offers a flexible and adaptive strategy\nthat reduces the number of tokens based on their redundancy to enhance both\nacceleration and generation quality. We further propose matching cache to\nfacilitate faster processing. Integrated into state-of-the-art video DiTs,\nAsymRnR achieves a superior speedup without compromising the quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Diffusion Transformers (DiTs) have demonstrated significant potential\nfor generating high-fidelity videos but are computationally intensive. Existing\nacceleration methods include distillation, which requires costly retraining,\nand feature caching, which is highly sensitive to network architecture. Recent\ntoken reduction methods are training-free and architecture-agnostic, offering\ngreater flexibility and wider applicability. However, they enforce the same\nsequence length across different components, constraining their acceleration\npotential. We observe that intra-sequence redundancy in video DiTs varies\nacross features, blocks, and denoising timesteps. Building on this observation,\nwe propose Asymmetric Reduction and Restoration (AsymRnR), a training-free\napproach to accelerate video DiTs. It offers a flexible and adaptive strategy\nthat reduces the number of tokens based on their redundancy to enhance both\nacceleration and generation quality. We further propose matching cache to\nfacilitate faster processing. Integrated into state-of-the-art video DiTs,\nAsymRnR achieves a superior speedup without compromising the quality."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11685v1",
                "updated": "2024-12-16T11:55:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T11:55:26Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "title": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning"
                },
                "summary": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU."
                },
                "authors": [
                    {
                        "name": "Xingchi Chen"
                    },
                    {
                        "name": "Zhuoran Zheng"
                    },
                    {
                        "name": "Xuerui Li"
                    },
                    {
                        "name": "Yuying Chen"
                    },
                    {
                        "name": "Shu Wang"
                    },
                    {
                        "name": "Wenqi Ren"
                    }
                ],
                "author_detail": {
                    "name": "Wenqi Ren"
                },
                "author": "Wenqi Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14201v1",
                "updated": "2024-12-15T21:02:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "published": "2024-12-15T21:02:16Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "title": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models"
                },
                "summary": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint."
                },
                "authors": [
                    {
                        "name": "Boris Ruf"
                    },
                    {
                        "name": "Marcin Detyniecki"
                    }
                ],
                "author_detail": {
                    "name": "Marcin Detyniecki"
                },
                "author": "Marcin Detyniecki",
                "arxiv_comment": "Presented at the 18th IEEE International Workshop on Multimedia\n  Technologies for E-Learning (MTEL), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.02388v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.02388v3",
                "updated": "2024-12-15T03:29:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    3,
                    29,
                    54,
                    6,
                    350,
                    0
                ],
                "published": "2023-05-03T19:07:06Z",
                "published_parsed": [
                    2023,
                    5,
                    3,
                    19,
                    7,
                    6,
                    2,
                    123,
                    0
                ],
                "title": "PULSE: Accelerating Distributed Pointer-Traversals on Disaggregated\n  Memory (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PULSE: Accelerating Distributed Pointer-Traversals on Disaggregated\n  Memory (Extended Version)"
                },
                "summary": "Caches at CPU nodes in disaggregated memory architectures amortize the high\ndata access latency over the network. However, such caches are fundamentally\nunable to improve performance for workloads requiring pointer traversals across\nlinked data structures. We argue for accelerating these pointer traversals\ncloser to disaggregated memory in a manner that preserves expressiveness for\nsupporting various linked structures, ensures energy efficiency and\nperformance, and supports distributed execution. We design PULSE, a distributed\npointer-traversal framework for rack-scale disaggregated memory to meet all the\nabove requirements. Our evaluation of PULSE shows that it enables low-latency,\nhigh-throughput, and energy-efficient execution for a wide range of pointer\ntraversal workloads on disaggregated memory that fare poorly with caching\nalone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caches at CPU nodes in disaggregated memory architectures amortize the high\ndata access latency over the network. However, such caches are fundamentally\nunable to improve performance for workloads requiring pointer traversals across\nlinked data structures. We argue for accelerating these pointer traversals\ncloser to disaggregated memory in a manner that preserves expressiveness for\nsupporting various linked structures, ensures energy efficiency and\nperformance, and supports distributed execution. We design PULSE, a distributed\npointer-traversal framework for rack-scale disaggregated memory to meet all the\nabove requirements. Our evaluation of PULSE shows that it enables low-latency,\nhigh-throughput, and energy-efficient execution for a wide range of pointer\ntraversal workloads on disaggregated memory that fare poorly with caching\nalone."
                },
                "authors": [
                    {
                        "name": "Yupeng Tang"
                    },
                    {
                        "name": "Seung-seob Lee"
                    },
                    {
                        "name": "Abhishek Bhattacharjee"
                    },
                    {
                        "name": "Anurag Khandelwal"
                    }
                ],
                "author_detail": {
                    "name": "Anurag Khandelwal"
                },
                "author": "Anurag Khandelwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.02388v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.02388v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11021v1",
                "updated": "2024-12-15T02:30:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    2,
                    30,
                    9,
                    6,
                    350,
                    0
                ],
                "published": "2024-12-15T02:30:09Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    2,
                    30,
                    9,
                    6,
                    350,
                    0
                ],
                "title": "SparseMap: Loop Mapping for Sparse CNNs on Streaming Coarse-grained\n  Reconfigurable Array",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseMap: Loop Mapping for Sparse CNNs on Streaming Coarse-grained\n  Reconfigurable Array"
                },
                "summary": "Streaming coarse-grained reconfgurable array (CGRA) is a promising\narchitecture for data/computing-intensive applications because of its\nfexibility, high throughput and efcient memory system. However,when\naccelerating sparse CNNs, the irregular input data demands inside sparse CNNs\nwould cause excessive caching operations (COPs) and multi-cycle internal\ndependencies (MCIDs) between operations, declining the throughput of the\nstreaming CGRA. We propose a mapping method for sparse CNNs onto streaming\nCGRA, SparseMap, which incorporates an efcient I/O data management along with\noperation scheduling and binding, to reduce the COPs and MCIDs, thereby\nensuring the optimal throughput of streaming CGRA.The experimental results show\nSparseMap reduces 92.5% COPs and 46.0 % MCIDs while achieves the same or even\nsmaller initiation interval (II) compared to previous works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming coarse-grained reconfgurable array (CGRA) is a promising\narchitecture for data/computing-intensive applications because of its\nfexibility, high throughput and efcient memory system. However,when\naccelerating sparse CNNs, the irregular input data demands inside sparse CNNs\nwould cause excessive caching operations (COPs) and multi-cycle internal\ndependencies (MCIDs) between operations, declining the throughput of the\nstreaming CGRA. We propose a mapping method for sparse CNNs onto streaming\nCGRA, SparseMap, which incorporates an efcient I/O data management along with\noperation scheduling and binding, to reduce the COPs and MCIDs, thereby\nensuring the optimal throughput of streaming CGRA.The experimental results show\nSparseMap reduces 92.5% COPs and 46.0 % MCIDs while achieves the same or even\nsmaller initiation interval (II) compared to previous works."
                },
                "authors": [
                    {
                        "name": "Xiaobing Ni"
                    },
                    {
                        "name": "Mengke Ge"
                    },
                    {
                        "name": "Jiaheng Ruan"
                    },
                    {
                        "name": "Song Chen"
                    },
                    {
                        "name": "Yi Kang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Kang"
                },
                "author": "Yi Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15246v1",
                "updated": "2024-12-14T06:47:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    14,
                    6,
                    47,
                    56,
                    5,
                    349,
                    0
                ],
                "published": "2024-12-14T06:47:56Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    6,
                    47,
                    56,
                    5,
                    349,
                    0
                ],
                "title": "Accelerating Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Retrieval-Augmented Generation"
                },
                "summary": "An evolving solution to address hallucination and enhance accuracy in large\nlanguage models (LLMs) is Retrieval-Augmented Generation (RAG), which involves\naugmenting LLMs with information retrieved from an external knowledge source,\nsuch as the web. This paper profiles several RAG execution pipelines and\ndemystifies the complex interplay between their retrieval and generation\nphases. We demonstrate that while exact retrieval schemes are expensive, they\ncan reduce inference time compared to approximate retrieval variants because an\nexact retrieval model can send a smaller but more accurate list of documents to\nthe generative model while maintaining the same end-to-end accuracy. This\nobservation motivates the acceleration of the exact nearest neighbor search for\nRAG.\n  In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL\ndevice that implements a scale-out near-memory acceleration architecture with a\nnovel cache-coherent interface between the host CPU and near-memory\naccelerators. IKS offers 13.4-27.9x faster exact nearest neighbor search over a\n512GB vector database compared with executing the search on Intel Sapphire\nRapids CPUs. This higher search performance translates to 1.7-26.3x lower\nend-to-end inference time for representative RAG applications. IKS is\ninherently a memory expander; its internal DRAM can be disaggregated and used\nfor other applications running on the server to prevent DRAM, which is the most\nexpensive component in today's servers, from being stranded.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An evolving solution to address hallucination and enhance accuracy in large\nlanguage models (LLMs) is Retrieval-Augmented Generation (RAG), which involves\naugmenting LLMs with information retrieved from an external knowledge source,\nsuch as the web. This paper profiles several RAG execution pipelines and\ndemystifies the complex interplay between their retrieval and generation\nphases. We demonstrate that while exact retrieval schemes are expensive, they\ncan reduce inference time compared to approximate retrieval variants because an\nexact retrieval model can send a smaller but more accurate list of documents to\nthe generative model while maintaining the same end-to-end accuracy. This\nobservation motivates the acceleration of the exact nearest neighbor search for\nRAG.\n  In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL\ndevice that implements a scale-out near-memory acceleration architecture with a\nnovel cache-coherent interface between the host CPU and near-memory\naccelerators. IKS offers 13.4-27.9x faster exact nearest neighbor search over a\n512GB vector database compared with executing the search on Intel Sapphire\nRapids CPUs. This higher search performance translates to 1.7-26.3x lower\nend-to-end inference time for representative RAG applications. IKS is\ninherently a memory expander; its internal DRAM can be disaggregated and used\nfor other applications running on the server to prevent DRAM, which is the most\nexpensive component in today's servers, from being stranded."
                },
                "authors": [
                    {
                        "name": "Derrick Quinn"
                    },
                    {
                        "name": "Mohammad Nouri"
                    },
                    {
                        "name": "Neel Patel"
                    },
                    {
                        "name": "John Salihu"
                    },
                    {
                        "name": "Alireza Salemi"
                    },
                    {
                        "name": "Sukhan Lee"
                    },
                    {
                        "name": "Hamed Zamani"
                    },
                    {
                        "name": "Mohammad Alian"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Alian"
                },
                "author": "Mohammad Alian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10685v1",
                "updated": "2024-12-14T05:20:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    14,
                    5,
                    20,
                    50,
                    5,
                    349,
                    0
                ],
                "published": "2024-12-14T05:20:50Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    5,
                    20,
                    50,
                    5,
                    349,
                    0
                ],
                "title": "RMCSA Algorithm for Congestion-Aware and Service Latency Aware Dynamic\n  Service Provisioning in Software-Defined SDM-EONs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RMCSA Algorithm for Congestion-Aware and Service Latency Aware Dynamic\n  Service Provisioning in Software-Defined SDM-EONs"
                },
                "summary": "The implementation of 5G and the future deployment of 6G necessitate the\nutilization of optical networks that possess substantial capacity and exhibit\nminimal latency. The dynamic arrival and departure of connection requests in\noptical networks result in particular central links experiencing more traffic\nand congestion than non-central links. The occurrence of congested links leads\nto service blocking despite the availability of resources within the network,\nrestricting the efficient utilization of network resources. The available\nalgorithms in the literature that aim to balance load among network links offer\na trade-off between blocking performance and algorithmic complexity, thus\nincreasing service provisioning time. This work proposes a dynamic\nrouting-based congestion-aware routing, modulation, core, and spectrum\nassignment (RMCSA) algorithm for space division multiplexing elastic optical\nnetworks (SDM-EONs). The algorithm finds alternative candidate paths based on\nreal-time link occupancy metrics to minimize blocking due to link congestion\nunder dynamic traffic scenarios. As a result, the algorithm reduces the\nformation of congestion hotspots in the network owing to link-betweenness\ncentrality. We have performed extensive simulations using two realistic network\ntopologies to compare the performance of the proposed algorithm with relevant\nRMCSA algorithms available in the literature. The simulation results verify the\nsuperior performance of our proposed algorithm compared to the benchmark Yen's\nK-shortest paths and K-Disjoint shortest paths RMCSA algorithms in connection\nblocking ratio and spectrum utilization efficiency. To expedite the\nroute-finding process, we present a novel caching strategy that allows the\nproposed algorithm to demonstrate a much-reduced service delay time compared to\nthe recently developed adaptive link weight-based load-balancing RMCSA\nalgorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The implementation of 5G and the future deployment of 6G necessitate the\nutilization of optical networks that possess substantial capacity and exhibit\nminimal latency. The dynamic arrival and departure of connection requests in\noptical networks result in particular central links experiencing more traffic\nand congestion than non-central links. The occurrence of congested links leads\nto service blocking despite the availability of resources within the network,\nrestricting the efficient utilization of network resources. The available\nalgorithms in the literature that aim to balance load among network links offer\na trade-off between blocking performance and algorithmic complexity, thus\nincreasing service provisioning time. This work proposes a dynamic\nrouting-based congestion-aware routing, modulation, core, and spectrum\nassignment (RMCSA) algorithm for space division multiplexing elastic optical\nnetworks (SDM-EONs). The algorithm finds alternative candidate paths based on\nreal-time link occupancy metrics to minimize blocking due to link congestion\nunder dynamic traffic scenarios. As a result, the algorithm reduces the\nformation of congestion hotspots in the network owing to link-betweenness\ncentrality. We have performed extensive simulations using two realistic network\ntopologies to compare the performance of the proposed algorithm with relevant\nRMCSA algorithms available in the literature. The simulation results verify the\nsuperior performance of our proposed algorithm compared to the benchmark Yen's\nK-shortest paths and K-Disjoint shortest paths RMCSA algorithms in connection\nblocking ratio and spectrum utilization efficiency. To expedite the\nroute-finding process, we present a novel caching strategy that allows the\nproposed algorithm to demonstrate a much-reduced service delay time compared to\nthe recently developed adaptive link weight-based load-balancing RMCSA\nalgorithm."
                },
                "authors": [
                    {
                        "name": "Baljinder Singh Heera"
                    },
                    {
                        "name": "Shrinivas Petale"
                    },
                    {
                        "name": "Yatindra Nath Singh"
                    },
                    {
                        "name": "Suresh Subramaniam"
                    }
                ],
                "author_detail": {
                    "name": "Suresh Subramaniam"
                },
                "author": "Suresh Subramaniam",
                "arxiv_comment": "The preliminary work was presented at ONDM 2023 conference.\n  https://doi.org/10.23919/ONDM57372.2023.10144866",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10319v1",
                "updated": "2024-12-13T17:59:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:59:52Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods"
                },
                "summary": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench."
                },
                "authors": [
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Surin Ahn"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10302v1",
                "updated": "2024-12-13T17:37:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    48,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:37:48Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    48,
                    4,
                    348,
                    0
                ],
                "title": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding"
                },
                "summary": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2."
                },
                "authors": [
                    {
                        "name": "Zhiyu Wu"
                    },
                    {
                        "name": "Xiaokang Chen"
                    },
                    {
                        "name": "Zizheng Pan"
                    },
                    {
                        "name": "Xingchao Liu"
                    },
                    {
                        "name": "Wen Liu"
                    },
                    {
                        "name": "Damai Dai"
                    },
                    {
                        "name": "Huazuo Gao"
                    },
                    {
                        "name": "Yiyang Ma"
                    },
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Bingxuan Wang"
                    },
                    {
                        "name": "Zhenda Xie"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Kai Hu"
                    },
                    {
                        "name": "Jiawei Wang"
                    },
                    {
                        "name": "Yaofeng Sun"
                    },
                    {
                        "name": "Yukun Li"
                    },
                    {
                        "name": "Yishi Piao"
                    },
                    {
                        "name": "Kang Guan"
                    },
                    {
                        "name": "Aixin Liu"
                    },
                    {
                        "name": "Xin Xie"
                    },
                    {
                        "name": "Yuxiang You"
                    },
                    {
                        "name": "Kai Dong"
                    },
                    {
                        "name": "Xingkai Yu"
                    },
                    {
                        "name": "Haowei Zhang"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Yisong Wang"
                    },
                    {
                        "name": "Chong Ruan"
                    }
                ],
                "author_detail": {
                    "name": "Chong Ruan"
                },
                "author": "Chong Ruan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18566v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18566v2",
                "updated": "2024-12-13T16:13:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    13,
                    39,
                    4,
                    348,
                    0
                ],
                "published": "2024-11-27T18:09:29Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "title": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software"
                },
                "summary": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software."
                },
                "authors": [
                    {
                        "name": "Oliver Maximilian Zobel"
                    },
                    {
                        "name": "Johannes Maierhofer"
                    },
                    {
                        "name": "Andreas Köstler"
                    },
                    {
                        "name": "Daniel J. Rixen"
                    }
                ],
                "author_detail": {
                    "name": "Daniel J. Rixen"
                },
                "author": "Daniel J. Rixen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18566v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18566v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10153v1",
                "updated": "2024-12-13T14:11:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T14:11:42Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "title": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector"
                },
                "summary": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhang"
                    },
                    {
                        "name": "Shuzhao Xie"
                    },
                    {
                        "name": "Chengwei Ren"
                    },
                    {
                        "name": "Siyi Xie"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Shijia Ge"
                    },
                    {
                        "name": "Mingzi Wang"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12021v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12021v2",
                "updated": "2024-12-13T14:08:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    8,
                    55,
                    4,
                    348,
                    0
                ],
                "published": "2024-09-18T14:31:33Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "title": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues"
                },
                "summary": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized)."
                },
                "authors": [
                    {
                        "name": "Thore Thießen"
                    },
                    {
                        "name": "Jan Vahrenhold"
                    }
                ],
                "author_detail": {
                    "name": "Jan Vahrenhold"
                },
                "author": "Jan Vahrenhold",
                "arxiv_doi": "10.4230/LIPIcs.ISAAC.2024.55",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4230/LIPIcs.ISAAC.2024.55",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.12021v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12021v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "23 pages, full version of the paper in ISAAC 2024; minor changes",
                "arxiv_journal_ref": "Thore Thie{\\ss}en and Jan Vahrenhold. Optimal offline ORAM with\n  perfect security via simple oblivious priority queues. In 35th International\n  Symposium on Algorithms and Computation (ISAAC 2024), 18 pages. 2024",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.04695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04695v1",
                "updated": "2025-01-08T18:58:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    58,
                    22,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T18:58:22Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    58,
                    22,
                    2,
                    8,
                    0
                ],
                "title": "Re-ranking the Context for Multimodal Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-ranking the Context for Multimodal Retrieval Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating external knowledge to generate a response within a context with\nimproved accuracy and reduced hallucinations. However, multi-modal RAG systems\nface unique challenges: (i) the retrieval process may select irrelevant entries\nto user query (e.g., images, documents), and (ii) vision-language models or\nmulti-modal language models like GPT-4o may hallucinate when processing these\nentries to generate RAG output. In this paper, we aim to address the first\nchallenge, i.e, improving the selection of relevant context from the\nknowledge-base in retrieval phase of the multi-modal RAG. Specifically, we\nleverage the relevancy score (RS) measure designed in our previous work for\nevaluating the RAG performance to select more relevant entries in retrieval\nprocess. The retrieval based on embeddings, say CLIP-based embedding, and\ncosine similarity usually perform poorly particularly for multi-modal data. We\nshow that by using a more advanced relevancy measure, one can enhance the\nretrieval process by selecting more relevant pieces from the knowledge-base and\neliminate the irrelevant pieces from the context by adaptively selecting\nup-to-$k$ entries instead of fixed number of entries. Our evaluation using COCO\ndataset demonstrates significant enhancement in selecting relevant context and\naccuracy of the generated response.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating external knowledge to generate a response within a context with\nimproved accuracy and reduced hallucinations. However, multi-modal RAG systems\nface unique challenges: (i) the retrieval process may select irrelevant entries\nto user query (e.g., images, documents), and (ii) vision-language models or\nmulti-modal language models like GPT-4o may hallucinate when processing these\nentries to generate RAG output. In this paper, we aim to address the first\nchallenge, i.e, improving the selection of relevant context from the\nknowledge-base in retrieval phase of the multi-modal RAG. Specifically, we\nleverage the relevancy score (RS) measure designed in our previous work for\nevaluating the RAG performance to select more relevant entries in retrieval\nprocess. The retrieval based on embeddings, say CLIP-based embedding, and\ncosine similarity usually perform poorly particularly for multi-modal data. We\nshow that by using a more advanced relevancy measure, one can enhance the\nretrieval process by selecting more relevant pieces from the knowledge-base and\neliminate the irrelevant pieces from the context by adaptively selecting\nup-to-$k$ entries instead of fixed number of entries. Our evaluation using COCO\ndataset demonstrates significant enhancement in selecting relevant context and\naccuracy of the generated response."
                },
                "authors": [
                    {
                        "name": "Matin Mortaheb"
                    },
                    {
                        "name": "Mohammad A. Amir Khojastepour"
                    },
                    {
                        "name": "Srimat T. Chakradhar"
                    },
                    {
                        "name": "Sennur Ulukus"
                    }
                ],
                "author_detail": {
                    "name": "Sennur Ulukus"
                },
                "author": "Sennur Ulukus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04694v1",
                "updated": "2025-01-08T18:58:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    58,
                    15,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T18:58:15Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    58,
                    15,
                    2,
                    8,
                    0
                ],
                "title": "EpiCoder: Encompassing Diversity and Complexity in Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EpiCoder: Encompassing Diversity and Complexity in Code Generation"
                },
                "summary": "Effective instruction tuning is indispensable for optimizing code LLMs,\naligning model behavior with user expectations and enhancing model performance\nin real-world applications. However, most existing methods focus on code\nsnippets, which are limited to specific functionalities and rigid structures,\nrestricting the complexity and diversity of the synthesized data. To address\nthese limitations, we introduce a novel feature tree-based synthesis framework\ninspired by Abstract Syntax Trees (AST). Unlike AST, which captures syntactic\nstructure of code, our framework models semantic relationships between code\nelements, enabling the generation of more nuanced and diverse data. The feature\ntree is constructed from raw data and refined iteratively to increase the\nquantity and diversity of the extracted features. This process enables the\nidentification of more complex patterns and relationships within the code. By\nsampling subtrees with controlled depth and breadth, our framework allows\nprecise adjustments to the complexity of the generated code, supporting a wide\nrange of tasks from simple function-level operations to intricate multi-file\nscenarios. We fine-tuned widely-used base models to create the EpiCoder series,\nachieving state-of-the-art performance at both the function and file levels\nacross multiple benchmarks. Notably, empirical evidence indicates that our\napproach shows significant potential in synthesizing highly complex\nrepository-level code data. Further analysis elucidates the merits of this\napproach by rigorously assessing data complexity and diversity through software\nengineering principles and LLM-as-a-judge method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective instruction tuning is indispensable for optimizing code LLMs,\naligning model behavior with user expectations and enhancing model performance\nin real-world applications. However, most existing methods focus on code\nsnippets, which are limited to specific functionalities and rigid structures,\nrestricting the complexity and diversity of the synthesized data. To address\nthese limitations, we introduce a novel feature tree-based synthesis framework\ninspired by Abstract Syntax Trees (AST). Unlike AST, which captures syntactic\nstructure of code, our framework models semantic relationships between code\nelements, enabling the generation of more nuanced and diverse data. The feature\ntree is constructed from raw data and refined iteratively to increase the\nquantity and diversity of the extracted features. This process enables the\nidentification of more complex patterns and relationships within the code. By\nsampling subtrees with controlled depth and breadth, our framework allows\nprecise adjustments to the complexity of the generated code, supporting a wide\nrange of tasks from simple function-level operations to intricate multi-file\nscenarios. We fine-tuned widely-used base models to create the EpiCoder series,\nachieving state-of-the-art performance at both the function and file levels\nacross multiple benchmarks. Notably, empirical evidence indicates that our\napproach shows significant potential in synthesizing highly complex\nrepository-level code data. Further analysis elucidates the merits of this\napproach by rigorously assessing data complexity and diversity through software\nengineering principles and LLM-as-a-judge method."
                },
                "authors": [
                    {
                        "name": "Yaoxiang Wang"
                    },
                    {
                        "name": "Haoling Li"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Jie Wu"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Wenxiang Hu"
                    },
                    {
                        "name": "Zhongxin Guo"
                    },
                    {
                        "name": "Yangyu Huang"
                    },
                    {
                        "name": "Ying Xin"
                    },
                    {
                        "name": "Yujiu Yang"
                    },
                    {
                        "name": "Jinsong Su"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Scarlett Li"
                    }
                ],
                "author_detail": {
                    "name": "Scarlett Li"
                },
                "author": "Scarlett Li",
                "arxiv_comment": "40 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04689v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04689v1",
                "updated": "2025-01-08T18:52:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    52,
                    3,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T18:52:03Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    52,
                    3,
                    2,
                    8,
                    0
                ],
                "title": "SPAR3D: Stable Point-Aware Reconstruction of 3D Objects from Single\n  Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPAR3D: Stable Point-Aware Reconstruction of 3D Objects from Single\n  Images"
                },
                "summary": "We study the problem of single-image 3D object reconstruction. Recent works\nhave diverged into two directions: regression-based modeling and generative\nmodeling. Regression methods efficiently infer visible surfaces, but struggle\nwith occluded regions. Generative methods handle uncertain regions better by\nmodeling distributions, but are computationally expensive and the generation is\noften misaligned with visible surfaces. In this paper, we present SPAR3D, a\nnovel two-stage approach aiming to take the best of both directions. The first\nstage of SPAR3D generates sparse 3D point clouds using a lightweight point\ndiffusion model, which has a fast sampling speed. The second stage uses both\nthe sampled point cloud and the input image to create highly detailed meshes.\nOur two-stage design enables probabilistic modeling of the ill-posed\nsingle-image 3D task while maintaining high computational efficiency and great\noutput fidelity. Using point clouds as an intermediate representation further\nallows for interactive user edits. Evaluated on diverse datasets, SPAR3D\ndemonstrates superior performance over previous state-of-the-art methods, at an\ninference speed of 0.7 seconds. Project page with code and model:\nhttps://spar3d.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of single-image 3D object reconstruction. Recent works\nhave diverged into two directions: regression-based modeling and generative\nmodeling. Regression methods efficiently infer visible surfaces, but struggle\nwith occluded regions. Generative methods handle uncertain regions better by\nmodeling distributions, but are computationally expensive and the generation is\noften misaligned with visible surfaces. In this paper, we present SPAR3D, a\nnovel two-stage approach aiming to take the best of both directions. The first\nstage of SPAR3D generates sparse 3D point clouds using a lightweight point\ndiffusion model, which has a fast sampling speed. The second stage uses both\nthe sampled point cloud and the input image to create highly detailed meshes.\nOur two-stage design enables probabilistic modeling of the ill-posed\nsingle-image 3D task while maintaining high computational efficiency and great\noutput fidelity. Using point clouds as an intermediate representation further\nallows for interactive user edits. Evaluated on diverse datasets, SPAR3D\ndemonstrates superior performance over previous state-of-the-art methods, at an\ninference speed of 0.7 seconds. Project page with code and model:\nhttps://spar3d.github.io"
                },
                "authors": [
                    {
                        "name": "Zixuan Huang"
                    },
                    {
                        "name": "Mark Boss"
                    },
                    {
                        "name": "Aaryaman Vasishta"
                    },
                    {
                        "name": "James M. Rehg"
                    },
                    {
                        "name": "Varun Jampani"
                    }
                ],
                "author_detail": {
                    "name": "Varun Jampani"
                },
                "author": "Varun Jampani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04689v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04689v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04686v1",
                "updated": "2025-01-08T18:49:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    49,
                    41,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T18:49:41Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    49,
                    41,
                    2,
                    8,
                    0
                ],
                "title": "URSA: Understanding and Verifying Chain-of-thought Reasoning in\n  Multimodal Mathematics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "URSA: Understanding and Verifying Chain-of-thought Reasoning in\n  Multimodal Mathematics"
                },
                "summary": "Chain-of-thought (CoT) reasoning has been widely applied in the mathematical\nreasoning of Large Language Models (LLMs). Recently, the introduction of\nderivative process supervision on CoT trajectories has sparked discussions on\nenhancing scaling capabilities during test time, thereby boosting the potential\nof these models. However, in multimodal mathematical reasoning, the scarcity of\nhigh-quality CoT training data has hindered existing models from achieving\nhigh-precision CoT reasoning and has limited the realization of reasoning\npotential during test time. In this work, we propose a three-module synthesis\nstrategy that integrates CoT distillation, trajectory-format rewriting, and\nformat unification. It results in a high-quality CoT reasoning instruction\nfine-tuning dataset in multimodal mathematics, MMathCoT-1M. We comprehensively\nvalidate the state-of-the-art (SOTA) performance of the trained URSA-7B model\non multiple multimodal mathematical benchmarks. For test-time scaling, we\nintroduce a data synthesis strategy that automatically generates process\nannotation datasets, known as DualMath-1.1M, focusing on both interpretation\nand logic. By further training URSA-7B on DualMath-1.1M, we transition from CoT\nreasoning capabilities to robust supervision abilities. The trained URSA-RM-7B\nacts as a verifier, effectively enhancing the performance of URSA-7B at test\ntime. URSA-RM-7B also demonstrates excellent out-of-distribution (OOD)\nverifying capabilities, showcasing its generalization. Model weights, training\ndata and code will be open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought (CoT) reasoning has been widely applied in the mathematical\nreasoning of Large Language Models (LLMs). Recently, the introduction of\nderivative process supervision on CoT trajectories has sparked discussions on\nenhancing scaling capabilities during test time, thereby boosting the potential\nof these models. However, in multimodal mathematical reasoning, the scarcity of\nhigh-quality CoT training data has hindered existing models from achieving\nhigh-precision CoT reasoning and has limited the realization of reasoning\npotential during test time. In this work, we propose a three-module synthesis\nstrategy that integrates CoT distillation, trajectory-format rewriting, and\nformat unification. It results in a high-quality CoT reasoning instruction\nfine-tuning dataset in multimodal mathematics, MMathCoT-1M. We comprehensively\nvalidate the state-of-the-art (SOTA) performance of the trained URSA-7B model\non multiple multimodal mathematical benchmarks. For test-time scaling, we\nintroduce a data synthesis strategy that automatically generates process\nannotation datasets, known as DualMath-1.1M, focusing on both interpretation\nand logic. By further training URSA-7B on DualMath-1.1M, we transition from CoT\nreasoning capabilities to robust supervision abilities. The trained URSA-RM-7B\nacts as a verifier, effectively enhancing the performance of URSA-7B at test\ntime. URSA-RM-7B also demonstrates excellent out-of-distribution (OOD)\nverifying capabilities, showcasing its generalization. Model weights, training\ndata and code will be open-sourced."
                },
                "authors": [
                    {
                        "name": "Ruilin Luo"
                    },
                    {
                        "name": "Zhuofan Zheng"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Yiyao Yu"
                    },
                    {
                        "name": "Xinzhe Ni"
                    },
                    {
                        "name": "Zicheng Lin"
                    },
                    {
                        "name": "Jin Zeng"
                    },
                    {
                        "name": "Yujiu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yujiu Yang"
                },
                "author": "Yujiu Yang",
                "arxiv_comment": "27 pages, 10 tables, 17 figures. The training data has been released.\n  The code and model are currently undergoing internal review. They will be\n  made available soon. Project url: https://ursa-math.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04682v1",
                "updated": "2025-01-08T18:42:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    42,
                    48,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T18:42:48Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    42,
                    48,
                    2,
                    8,
                    0
                ],
                "title": "Towards System 2 Reasoning in LLMs: Learning How to Think With Meta\n  Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards System 2 Reasoning in LLMs: Learning How to Think With Meta\n  Chain-of-Thought"
                },
                "summary": "We propose a novel framework, Meta Chain-of-Thought (Meta-CoT), which extends\ntraditional Chain-of-Thought (CoT) by explicitly modeling the underlying\nreasoning required to arrive at a particular CoT. We present empirical evidence\nfrom state-of-the-art models exhibiting behaviors consistent with in-context\nsearch, and explore methods for producing Meta-CoT via process supervision,\nsynthetic data generation, and search algorithms. Finally, we outline a\nconcrete pipeline for training a model to produce Meta-CoTs, incorporating\ninstruction tuning with linearized search traces and reinforcement learning\npost-training. Finally, we discuss open research questions, including scaling\nlaws, verifier roles, and the potential for discovering novel reasoning\nalgorithms. This work provides a theoretical and practical roadmap to enable\nMeta-CoT in LLMs, paving the way for more powerful and human-like reasoning in\nartificial intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel framework, Meta Chain-of-Thought (Meta-CoT), which extends\ntraditional Chain-of-Thought (CoT) by explicitly modeling the underlying\nreasoning required to arrive at a particular CoT. We present empirical evidence\nfrom state-of-the-art models exhibiting behaviors consistent with in-context\nsearch, and explore methods for producing Meta-CoT via process supervision,\nsynthetic data generation, and search algorithms. Finally, we outline a\nconcrete pipeline for training a model to produce Meta-CoTs, incorporating\ninstruction tuning with linearized search traces and reinforcement learning\npost-training. Finally, we discuss open research questions, including scaling\nlaws, verifier roles, and the potential for discovering novel reasoning\nalgorithms. This work provides a theoretical and practical roadmap to enable\nMeta-CoT in LLMs, paving the way for more powerful and human-like reasoning in\nartificial intelligence."
                },
                "authors": [
                    {
                        "name": "Violet Xiang"
                    },
                    {
                        "name": "Charlie Snell"
                    },
                    {
                        "name": "Kanishk Gandhi"
                    },
                    {
                        "name": "Alon Albalak"
                    },
                    {
                        "name": "Anikait Singh"
                    },
                    {
                        "name": "Chase Blagden"
                    },
                    {
                        "name": "Duy Phung"
                    },
                    {
                        "name": "Rafael Rafailov"
                    },
                    {
                        "name": "Nathan Lile"
                    },
                    {
                        "name": "Dakota Mahan"
                    },
                    {
                        "name": "Louis Castricato"
                    },
                    {
                        "name": "Jan-Philipp Franken"
                    },
                    {
                        "name": "Nick Haber"
                    },
                    {
                        "name": "Chelsea Finn"
                    }
                ],
                "author_detail": {
                    "name": "Chelsea Finn"
                },
                "author": "Chelsea Finn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12245v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12245v2",
                "updated": "2025-01-08T18:39:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    39,
                    11,
                    2,
                    8,
                    0
                ],
                "published": "2024-12-16T19:00:00Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    19,
                    0,
                    0,
                    0,
                    351,
                    0
                ],
                "title": "The redshift dependence of the inferred $H_0$ in a local void solution\n  to the Hubble tension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The redshift dependence of the inferred $H_0$ in a local void solution\n  to the Hubble tension"
                },
                "summary": "Galaxy number counts suggest that we are located within the Gpc-scale KBC\nvoid. The Hubble tension might arise due to gravitationally driven outflow from\nthis void, as explored in detail by Haslbauer et al. We explore how the impact\nof the void on redshift decays at large distances. We define $H_0(z)$ as the\npresent expansion rate $H_0$ that would be inferred from observations in a\nnarrow redshift range centred on $z$. We find $H_0(z)$ in three different ways,\nall of which give similar results. We then compare these results with the\nobservations of Jia et al., who were careful to minimise the impact of\ncorrelations between $H_0$ measurements from data in different redshift bins.\nWe find reasonable agreement with their results for the Gaussian and\nExponential void underdensity profiles, although the agreement is less good in\nthe Maxwell-Boltzmann case. The latter profile causes severe disagreement with\nthe observed bulk flow curve at $z < 0.1$ (Mazurenko et al.), so the tension\nwith higher redshift data further highlights that the deepest part of the KBC\nvoid is probably near its centre. The observations show a decline of $H_0(z)$\ntowards the background $Planck$ value in qualitative agreement with the\nconsidered models, even if we use a larger void. The good overall agreement\nwith the recent results of Jia et al. suggests that the local supervoid evident\nfrom the galaxy luminosity density out to a Gpc might also solve the Hubble\ntension while retaining a low background $H_0$ consistent with $Planck$ data,\nassuming enhanced structure formation on $>100$ Mpc scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Galaxy number counts suggest that we are located within the Gpc-scale KBC\nvoid. The Hubble tension might arise due to gravitationally driven outflow from\nthis void, as explored in detail by Haslbauer et al. We explore how the impact\nof the void on redshift decays at large distances. We define $H_0(z)$ as the\npresent expansion rate $H_0$ that would be inferred from observations in a\nnarrow redshift range centred on $z$. We find $H_0(z)$ in three different ways,\nall of which give similar results. We then compare these results with the\nobservations of Jia et al., who were careful to minimise the impact of\ncorrelations between $H_0$ measurements from data in different redshift bins.\nWe find reasonable agreement with their results for the Gaussian and\nExponential void underdensity profiles, although the agreement is less good in\nthe Maxwell-Boltzmann case. The latter profile causes severe disagreement with\nthe observed bulk flow curve at $z < 0.1$ (Mazurenko et al.), so the tension\nwith higher redshift data further highlights that the deepest part of the KBC\nvoid is probably near its centre. The observations show a decline of $H_0(z)$\ntowards the background $Planck$ value in qualitative agreement with the\nconsidered models, even if we use a larger void. The good overall agreement\nwith the recent results of Jia et al. suggests that the local supervoid evident\nfrom the galaxy luminosity density out to a Gpc might also solve the Hubble\ntension while retaining a low background $H_0$ consistent with $Planck$ data,\nassuming enhanced structure formation on $>100$ Mpc scales."
                },
                "authors": [
                    {
                        "name": "Sergij Mazurenko"
                    },
                    {
                        "name": "Indranil Banik"
                    },
                    {
                        "name": "Pavel Kroupa"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Kroupa"
                },
                "author": "Pavel Kroupa",
                "arxiv_doi": "10.1093/mnras/stae2758",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/stae2758",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.12245v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12245v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 5 figures, no tables. Published in the Monthly Notices of\n  the Royal Astronomical Society in this form",
                "arxiv_journal_ref": "MNRAS, volume 536, issue 4, pages 3232 - 3241 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04675v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04675v1",
                "updated": "2025-01-08T18:33:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    33,
                    17,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T18:33:17Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    33,
                    17,
                    2,
                    8,
                    0
                ],
                "title": "Enhancing Financial VQA in Vision Language Models using Intermediate\n  Structured Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Financial VQA in Vision Language Models using Intermediate\n  Structured Representations"
                },
                "summary": "Chart interpretation is crucial for visual data analysis, but accurately\nextracting information from charts poses significant challenges for automated\nmodels. This study investigates the fine-tuning of DEPLOT, a modality\nconversion module that translates the image of a plot or chart to a linearized\ntable, on a custom dataset of 50,000 bar charts. The dataset comprises simple,\nstacked, and grouped bar charts, targeting the unique structural features of\nthese visualizations. The finetuned DEPLOT model is evaluated against its base\nversion using a test set of 1,000 images and two metrics: Relative Mapping\nSimilarity (RMS), which measures categorical mapping accuracy, and Relative\nNumber Set Similarity (RNSS), which evaluates numerical interpretation\naccuracy. To further explore the reasoning capabilities of large language\nmodels (LLMs), we curate an additional set of 100 bar chart images paired with\nquestion answer sets. Our findings demonstrate that providing a structured\nintermediate table alongside the image significantly enhances LLM reasoning\nperformance compared to direct image queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chart interpretation is crucial for visual data analysis, but accurately\nextracting information from charts poses significant challenges for automated\nmodels. This study investigates the fine-tuning of DEPLOT, a modality\nconversion module that translates the image of a plot or chart to a linearized\ntable, on a custom dataset of 50,000 bar charts. The dataset comprises simple,\nstacked, and grouped bar charts, targeting the unique structural features of\nthese visualizations. The finetuned DEPLOT model is evaluated against its base\nversion using a test set of 1,000 images and two metrics: Relative Mapping\nSimilarity (RMS), which measures categorical mapping accuracy, and Relative\nNumber Set Similarity (RNSS), which evaluates numerical interpretation\naccuracy. To further explore the reasoning capabilities of large language\nmodels (LLMs), we curate an additional set of 100 bar chart images paired with\nquestion answer sets. Our findings demonstrate that providing a structured\nintermediate table alongside the image significantly enhances LLM reasoning\nperformance compared to direct image queries."
                },
                "authors": [
                    {
                        "name": "Archita Srivastava"
                    },
                    {
                        "name": "Abhas Kumar"
                    },
                    {
                        "name": "Rajesh Kumar"
                    },
                    {
                        "name": "Prabhakar Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Prabhakar Srinivasan"
                },
                "author": "Prabhakar Srinivasan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04675v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04675v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04670v1",
                "updated": "2025-01-08T18:30:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    30,
                    53,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T18:30:53Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    30,
                    53,
                    2,
                    8,
                    0
                ],
                "title": "Are They the Same? Exploring Visual Correspondence Shortcomings of\n  Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are They the Same? Exploring Visual Correspondence Shortcomings of\n  Multimodal LLMs"
                },
                "summary": "Recent advancements in multimodal models have shown a strong ability in\nvisual perception, reasoning abilities, and vision-language understanding.\nHowever, studies on visual matching ability are missing, where finding the\nvisual correspondence of objects is essential in vision research. Our research\nreveals that the matching capabilities in recent multimodal LLMs (MLLMs) still\nexhibit systematic shortcomings, even with current strong MLLMs models, GPT-4o.\nIn particular, we construct a Multimodal Visual Matching (MMVM) benchmark to\nfairly benchmark over 30 different MLLMs. The MMVM benchmark is built from 15\nopen-source datasets and Internet videos with manual annotation. We categorize\nthe data samples of MMVM benchmark into eight aspects based on the required\ncues and capabilities to more comprehensively evaluate and analyze current\nMLLMs. In addition, we have designed an automatic annotation pipeline to\ngenerate the MMVM SFT dataset, including 220K visual matching data with\nreasoning annotation. Finally, we present CoLVA, a novel contrastive MLLM with\ntwo novel technical designs: fine-grained vision expert with object-level\ncontrastive learning and instruction augmentation strategy. CoLVA achieves\n51.06\\% overall accuracy (OA) on the MMVM benchmark, surpassing GPT-4o and\nbaseline by 8.41\\% and 23.58\\% OA, respectively. The results show the\neffectiveness of our MMVM SFT dataset and our novel technical designs. Code,\nbenchmark, dataset, and models are available at\nhttps://github.com/zhouyiks/CoLVA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in multimodal models have shown a strong ability in\nvisual perception, reasoning abilities, and vision-language understanding.\nHowever, studies on visual matching ability are missing, where finding the\nvisual correspondence of objects is essential in vision research. Our research\nreveals that the matching capabilities in recent multimodal LLMs (MLLMs) still\nexhibit systematic shortcomings, even with current strong MLLMs models, GPT-4o.\nIn particular, we construct a Multimodal Visual Matching (MMVM) benchmark to\nfairly benchmark over 30 different MLLMs. The MMVM benchmark is built from 15\nopen-source datasets and Internet videos with manual annotation. We categorize\nthe data samples of MMVM benchmark into eight aspects based on the required\ncues and capabilities to more comprehensively evaluate and analyze current\nMLLMs. In addition, we have designed an automatic annotation pipeline to\ngenerate the MMVM SFT dataset, including 220K visual matching data with\nreasoning annotation. Finally, we present CoLVA, a novel contrastive MLLM with\ntwo novel technical designs: fine-grained vision expert with object-level\ncontrastive learning and instruction augmentation strategy. CoLVA achieves\n51.06\\% overall accuracy (OA) on the MMVM benchmark, surpassing GPT-4o and\nbaseline by 8.41\\% and 23.58\\% OA, respectively. The results show the\neffectiveness of our MMVM SFT dataset and our novel technical designs. Code,\nbenchmark, dataset, and models are available at\nhttps://github.com/zhouyiks/CoLVA."
                },
                "authors": [
                    {
                        "name": "Yikang Zhou"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Shilin Xu"
                    },
                    {
                        "name": "Shihao Chen"
                    },
                    {
                        "name": "Qianyu Zhou"
                    },
                    {
                        "name": "Yunhai Tong"
                    },
                    {
                        "name": "Shunping Ji"
                    },
                    {
                        "name": "Jiangning Zhang"
                    },
                    {
                        "name": "Xiangtai Li"
                    },
                    {
                        "name": "Lu Qi"
                    }
                ],
                "author_detail": {
                    "name": "Lu Qi"
                },
                "author": "Lu Qi",
                "arxiv_comment": "project page: https://zhouyiks.github.io/projects/CoLVA/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17194v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17194v3",
                "updated": "2025-01-08T18:18:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    18,
                    51,
                    2,
                    8,
                    0
                ],
                "published": "2024-10-22T17:13:34Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    17,
                    13,
                    34,
                    1,
                    296,
                    0
                ],
                "title": "Representation Shattering in Transformers: A Synthetic Study with\n  Knowledge Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation Shattering in Transformers: A Synthetic Study with\n  Knowledge Editing"
                },
                "summary": "Knowledge Editing (KE) algorithms alter models' weights to perform targeted\nupdates to incorrect, outdated, or otherwise unwanted factual associations. To\nbetter identify the possibilities and limitations of these approaches, recent\nwork has shown that applying KE can adversely affect models' factual recall\naccuracy and diminish their general reasoning abilities. While these studies\ngive broad insights into the potential harms of KE algorithms, e.g., via\nperformance evaluations on benchmarks, we argue little is understood as to why\nsuch destructive failures occur. Is it possible KE methods distort\nrepresentations of concepts beyond the targeted fact, hence hampering abilities\nat broad? If so, what is the extent of this distortion? Motivated by such\nquestions, we define a novel synthetic task wherein a Transformer is trained\nfrom scratch to internalize a \"structured\" knowledge graph. The structure\nenforces relationships between entities of the graph, such that editing a\nfactual association has \"trickling effects\" on other entities in the graph\n(e.g., altering X's parent is Y to Z affects who X's siblings' parent is).\nThrough evaluations of edited models and analysis of extracted representations,\nwe show that KE inadvertently affects representations of entities beyond the\ntargeted one, distorting relevant structures that allow a model to infer unseen\nknowledge about an entity. We call this phenomenon representation shattering\nand demonstrate that it results in degradation of factual recall and reasoning\nperformance more broadly. To corroborate our findings in a more naturalistic\nsetup, we perform preliminary experiments with pre-trained Llama and Mamba\nmodels, reproducing the representation shattering effect therein as well.\nOverall, our work yields a precise mechanistic hypothesis to explain why KE has\nadverse effects on model abilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Editing (KE) algorithms alter models' weights to perform targeted\nupdates to incorrect, outdated, or otherwise unwanted factual associations. To\nbetter identify the possibilities and limitations of these approaches, recent\nwork has shown that applying KE can adversely affect models' factual recall\naccuracy and diminish their general reasoning abilities. While these studies\ngive broad insights into the potential harms of KE algorithms, e.g., via\nperformance evaluations on benchmarks, we argue little is understood as to why\nsuch destructive failures occur. Is it possible KE methods distort\nrepresentations of concepts beyond the targeted fact, hence hampering abilities\nat broad? If so, what is the extent of this distortion? Motivated by such\nquestions, we define a novel synthetic task wherein a Transformer is trained\nfrom scratch to internalize a \"structured\" knowledge graph. The structure\nenforces relationships between entities of the graph, such that editing a\nfactual association has \"trickling effects\" on other entities in the graph\n(e.g., altering X's parent is Y to Z affects who X's siblings' parent is).\nThrough evaluations of edited models and analysis of extracted representations,\nwe show that KE inadvertently affects representations of entities beyond the\ntargeted one, distorting relevant structures that allow a model to infer unseen\nknowledge about an entity. We call this phenomenon representation shattering\nand demonstrate that it results in degradation of factual recall and reasoning\nperformance more broadly. To corroborate our findings in a more naturalistic\nsetup, we perform preliminary experiments with pre-trained Llama and Mamba\nmodels, reproducing the representation shattering effect therein as well.\nOverall, our work yields a precise mechanistic hypothesis to explain why KE has\nadverse effects on model abilities."
                },
                "authors": [
                    {
                        "name": "Kento Nishi"
                    },
                    {
                        "name": "Maya Okawa"
                    },
                    {
                        "name": "Rahul Ramesh"
                    },
                    {
                        "name": "Mikail Khona"
                    },
                    {
                        "name": "Hidenori Tanaka"
                    },
                    {
                        "name": "Ekdeep Singh Lubana"
                    }
                ],
                "author_detail": {
                    "name": "Ekdeep Singh Lubana"
                },
                "author": "Ekdeep Singh Lubana",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17194v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17194v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04661v1",
                "updated": "2025-01-08T18:15:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    15,
                    10,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T18:15:10Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    15,
                    10,
                    2,
                    8,
                    0
                ],
                "title": "Assessing Language Comprehension in Large Language Models Using\n  Construction Grammar",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Language Comprehension in Large Language Models Using\n  Construction Grammar"
                },
                "summary": "Large Language Models, despite their significant capabilities, are known to\nfail in surprising and unpredictable ways. Evaluating their true\n`understanding' of language is particularly challenging due to the extensive\nweb-scale data they are trained on. Therefore, we construct an evaluation to\nsystematically assess natural language understanding (NLU) in LLMs by\nleveraging Construction Grammar (CxG), which provides insights into the meaning\ncaptured by linguistic elements known as constructions (Cxns). CxG is\nwell-suited for this purpose because provides a theoretical basis to construct\ntargeted evaluation sets. These datasets are carefully constructed to include\nexamples which are unlikely to appear in pre-training data, yet intuitive and\neasy for humans to understand, enabling a more targeted and reliable\nassessment. Our experiments focus on downstream natural language inference and\nreasoning tasks by comparing LLMs' understanding of the underlying meanings\ncommunicated through 8 unique Cxns with that of humans. The results show that\nwhile LLMs demonstrate some knowledge of constructional information, even the\nlatest models including GPT-o1 struggle with abstract meanings conveyed by\nthese Cxns, as demonstrated in cases where test sentences are dissimilar to\ntheir pre-training data. We argue that such cases provide a more accurate test\nof true language understanding, highlighting key limitations in LLMs' semantic\ncapabilities. We make our novel dataset and associated experimental data\nincluding prompts and model responses publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models, despite their significant capabilities, are known to\nfail in surprising and unpredictable ways. Evaluating their true\n`understanding' of language is particularly challenging due to the extensive\nweb-scale data they are trained on. Therefore, we construct an evaluation to\nsystematically assess natural language understanding (NLU) in LLMs by\nleveraging Construction Grammar (CxG), which provides insights into the meaning\ncaptured by linguistic elements known as constructions (Cxns). CxG is\nwell-suited for this purpose because provides a theoretical basis to construct\ntargeted evaluation sets. These datasets are carefully constructed to include\nexamples which are unlikely to appear in pre-training data, yet intuitive and\neasy for humans to understand, enabling a more targeted and reliable\nassessment. Our experiments focus on downstream natural language inference and\nreasoning tasks by comparing LLMs' understanding of the underlying meanings\ncommunicated through 8 unique Cxns with that of humans. The results show that\nwhile LLMs demonstrate some knowledge of constructional information, even the\nlatest models including GPT-o1 struggle with abstract meanings conveyed by\nthese Cxns, as demonstrated in cases where test sentences are dissimilar to\ntheir pre-training data. We argue that such cases provide a more accurate test\nof true language understanding, highlighting key limitations in LLMs' semantic\ncapabilities. We make our novel dataset and associated experimental data\nincluding prompts and model responses publicly available."
                },
                "authors": [
                    {
                        "name": "Wesley Scivetti"
                    },
                    {
                        "name": "Melissa Torgbi"
                    },
                    {
                        "name": "Austin Blodgett"
                    },
                    {
                        "name": "Mollie Shichman"
                    },
                    {
                        "name": "Taylor Hudson"
                    },
                    {
                        "name": "Claire Bonial"
                    },
                    {
                        "name": "Harish Tayyar Madabushi"
                    }
                ],
                "author_detail": {
                    "name": "Harish Tayyar Madabushi"
                },
                "author": "Harish Tayyar Madabushi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04652v1",
                "updated": "2025-01-08T18:05:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    5,
                    30,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T18:05:30Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    5,
                    30,
                    2,
                    8,
                    0
                ],
                "title": "Multi-task retriever fine-tuning for domain-specific and efficient RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-task retriever fine-tuning for domain-specific and efficient RAG"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has become ubiquitous when deploying\nLarge Language Models (LLMs), as it can address typical limitations such as\ngenerating hallucinated or outdated information. However, when building\nreal-world RAG applications, practical issues arise. First, the retrieved\ninformation is generally domain-specific. Since it is computationally expensive\nto fine-tune LLMs, it is more feasible to fine-tune the retriever to improve\nthe quality of the data included in the LLM input. Second, as more applications\nare deployed in the same real-world system, one cannot afford to deploy\nseparate retrievers. Moreover, these RAG applications normally retrieve\ndifferent kinds of data. Our solution is to instruction fine-tune a small\nretriever encoder on a variety of domain-specific tasks to allow us to deploy\none encoder that can serve many use cases, thereby achieving low-cost,\nscalability, and speed. We show how this encoder generalizes to out-of-domain\nsettings as well as to an unseen retrieval task on real-world enterprise use\ncases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has become ubiquitous when deploying\nLarge Language Models (LLMs), as it can address typical limitations such as\ngenerating hallucinated or outdated information. However, when building\nreal-world RAG applications, practical issues arise. First, the retrieved\ninformation is generally domain-specific. Since it is computationally expensive\nto fine-tune LLMs, it is more feasible to fine-tune the retriever to improve\nthe quality of the data included in the LLM input. Second, as more applications\nare deployed in the same real-world system, one cannot afford to deploy\nseparate retrievers. Moreover, these RAG applications normally retrieve\ndifferent kinds of data. Our solution is to instruction fine-tune a small\nretriever encoder on a variety of domain-specific tasks to allow us to deploy\none encoder that can serve many use cases, thereby achieving low-cost,\nscalability, and speed. We show how this encoder generalizes to out-of-domain\nsettings as well as to an unseen retrieval task on real-world enterprise use\ncases."
                },
                "authors": [
                    {
                        "name": "Patrice Béchard"
                    },
                    {
                        "name": "Orlando Marquez Ayala"
                    }
                ],
                "author_detail": {
                    "name": "Orlando Marquez Ayala"
                },
                "author": "Orlando Marquez Ayala",
                "arxiv_comment": "9 pages, 2 figures. Submitted to NAACL 2025 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04648v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04648v1",
                "updated": "2025-01-08T18:01:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    1,
                    49,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T18:01:49Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    1,
                    49,
                    2,
                    8,
                    0
                ],
                "title": "FlairGPT: Repurposing LLMs for Interior Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlairGPT: Repurposing LLMs for Interior Designs"
                },
                "summary": "Interior design involves the careful selection and arrangement of objects to\ncreate an aesthetically pleasing, functional, and harmonized space that aligns\nwith the client's design brief. This task is particularly challenging, as a\nsuccessful design must not only incorporate all the necessary objects in a\ncohesive style, but also ensure they are arranged in a way that maximizes\naccessibility, while adhering to a variety of affordability and usage\nconsiderations. Data-driven solutions have been proposed, but these are\ntypically room- or domain-specific and lack explainability in their design\ndesign considerations used in producing the final layout. In this paper, we\ninvestigate if large language models (LLMs) can be directly utilized for\ninterior design. While we find that LLMs are not yet capable of generating\ncomplete layouts, they can be effectively leveraged in a structured manner,\ninspired by the workflow of interior designers. By systematically probing LLMs,\nwe can reliably generate a list of objects along with relevant constraints that\nguide their placement. We translate this information into a design layout\ngraph, which is then solved using an off-the-shelf constrained optimization\nsetup to generate the final layouts. We benchmark our algorithm in various\ndesign configurations against existing LLM-based methods and human designs, and\nevaluate the results using a variety of quantitative and qualitative metrics\nalong with user studies. In summary, we demonstrate that LLMs, when used in a\nstructured manner, can effectively generate diverse high-quality layouts,\nmaking them a viable solution for creating large-scale virtual scenes. Project\nwebpage at https://flairgpt.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interior design involves the careful selection and arrangement of objects to\ncreate an aesthetically pleasing, functional, and harmonized space that aligns\nwith the client's design brief. This task is particularly challenging, as a\nsuccessful design must not only incorporate all the necessary objects in a\ncohesive style, but also ensure they are arranged in a way that maximizes\naccessibility, while adhering to a variety of affordability and usage\nconsiderations. Data-driven solutions have been proposed, but these are\ntypically room- or domain-specific and lack explainability in their design\ndesign considerations used in producing the final layout. In this paper, we\ninvestigate if large language models (LLMs) can be directly utilized for\ninterior design. While we find that LLMs are not yet capable of generating\ncomplete layouts, they can be effectively leveraged in a structured manner,\ninspired by the workflow of interior designers. By systematically probing LLMs,\nwe can reliably generate a list of objects along with relevant constraints that\nguide their placement. We translate this information into a design layout\ngraph, which is then solved using an off-the-shelf constrained optimization\nsetup to generate the final layouts. We benchmark our algorithm in various\ndesign configurations against existing LLM-based methods and human designs, and\nevaluate the results using a variety of quantitative and qualitative metrics\nalong with user studies. In summary, we demonstrate that LLMs, when used in a\nstructured manner, can effectively generate diverse high-quality layouts,\nmaking them a viable solution for creating large-scale virtual scenes. Project\nwebpage at https://flairgpt.github.io/"
                },
                "authors": [
                    {
                        "name": "Gabrielle Littlefair"
                    },
                    {
                        "name": "Niladri Shekhar Dutt"
                    },
                    {
                        "name": "Niloy J. Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Niloy J. Mitra"
                },
                "author": "Niloy J. Mitra",
                "arxiv_comment": "Accepted at EUROGRAPHICS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04648v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04648v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15861v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15861v2",
                "updated": "2025-01-08T17:41:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    17,
                    41,
                    51,
                    2,
                    8,
                    0
                ],
                "published": "2024-09-24T08:33:41Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    33,
                    41,
                    1,
                    268,
                    0
                ],
                "title": "A Zero-Shot Open-Vocabulary Pipeline for Dialogue Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Zero-Shot Open-Vocabulary Pipeline for Dialogue Understanding"
                },
                "summary": "Dialogue State Tracking (DST) is crucial for understanding user needs and\nexecuting appropriate system actions in task-oriented dialogues. Majority of\nexisting DST methods are designed to work within predefined ontologies and\nassume the availability of gold domain labels, struggling with adapting to new\nslots values. While Large Language Models (LLMs)-based systems show promising\nzero-shot DST performance, they either require extensive computational\nresources or they underperform existing fully-trained systems, limiting their\npracticality. To address these limitations, we propose a zero-shot,\nopen-vocabulary system that integrates domain classification and DST in a\nsingle pipeline. Our approach includes reformulating DST as a\nquestion-answering task for less capable models and employing self-refining\nprompts for more adaptable ones. Our system does not rely on fixed slot values\ndefined in the ontology allowing the system to adapt dynamically. We compare\nour approach with existing SOTA, and show that it provides up to 20% better\nJoint Goal Accuracy (JGA) over previous methods on datasets like Multi-WOZ 2.1,\nwith up to 90% fewer requests to the LLM API.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialogue State Tracking (DST) is crucial for understanding user needs and\nexecuting appropriate system actions in task-oriented dialogues. Majority of\nexisting DST methods are designed to work within predefined ontologies and\nassume the availability of gold domain labels, struggling with adapting to new\nslots values. While Large Language Models (LLMs)-based systems show promising\nzero-shot DST performance, they either require extensive computational\nresources or they underperform existing fully-trained systems, limiting their\npracticality. To address these limitations, we propose a zero-shot,\nopen-vocabulary system that integrates domain classification and DST in a\nsingle pipeline. Our approach includes reformulating DST as a\nquestion-answering task for less capable models and employing self-refining\nprompts for more adaptable ones. Our system does not rely on fixed slot values\ndefined in the ontology allowing the system to adapt dynamically. We compare\nour approach with existing SOTA, and show that it provides up to 20% better\nJoint Goal Accuracy (JGA) over previous methods on datasets like Multi-WOZ 2.1,\nwith up to 90% fewer requests to the LLM API."
                },
                "authors": [
                    {
                        "name": "Abdulfattah Safa"
                    },
                    {
                        "name": "Gözde Gül Şahin"
                    }
                ],
                "author_detail": {
                    "name": "Gözde Gül Şahin"
                },
                "author": "Gözde Gül Şahin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15861v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15861v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04635v1",
                "updated": "2025-01-08T17:29:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    17,
                    29,
                    46,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T17:29:46Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    17,
                    29,
                    46,
                    2,
                    8,
                    0
                ],
                "title": "Knowledge Retrieval Based on Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Retrieval Based on Generative AI"
                },
                "summary": "This study develops a question-answering system based on Retrieval-Augmented\nGeneration (RAG) using Chinese Wikipedia and Lawbank as retrieval sources.\nUsing TTQA and TMMLU+ as evaluation datasets, the system employs BGE-M3 for\ndense vector retrieval to obtain highly relevant search results and\nBGE-reranker to reorder these results based on query relevance. The most\npertinent retrieval outcomes serve as reference knowledge for a Large Language\nModel (LLM), enhancing its ability to answer questions and establishing a\nknowledge retrieval system grounded in generative AI.\n  The system's effectiveness is assessed through a two-stage evaluation:\nautomatic and assisted performance evaluations. The automatic evaluation\ncalculates accuracy by comparing the model's auto-generated labels with ground\ntruth answers, measuring performance under standardized conditions without\nhuman intervention. The assisted performance evaluation involves 20\nfinance-related multiple-choice questions answered by 20 participants without\nfinancial backgrounds. Initially, participants answer independently. Later,\nthey receive system-generated reference information to assist in answering,\nexamining whether the system improves accuracy when assistance is provided.\n  The main contributions of this research are: (1) Enhanced LLM Capability: By\nintegrating BGE-M3 and BGE-reranker, the system retrieves and reorders highly\nrelevant results, reduces hallucinations, and dynamically accesses authorized\nor public knowledge sources. (2) Improved Data Privacy: A customized RAG\narchitecture enables local operation of the LLM, eliminating the need to send\nprivate data to external servers. This approach enhances data security, reduces\nreliance on commercial services, lowers operational costs, and mitigates\nprivacy risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study develops a question-answering system based on Retrieval-Augmented\nGeneration (RAG) using Chinese Wikipedia and Lawbank as retrieval sources.\nUsing TTQA and TMMLU+ as evaluation datasets, the system employs BGE-M3 for\ndense vector retrieval to obtain highly relevant search results and\nBGE-reranker to reorder these results based on query relevance. The most\npertinent retrieval outcomes serve as reference knowledge for a Large Language\nModel (LLM), enhancing its ability to answer questions and establishing a\nknowledge retrieval system grounded in generative AI.\n  The system's effectiveness is assessed through a two-stage evaluation:\nautomatic and assisted performance evaluations. The automatic evaluation\ncalculates accuracy by comparing the model's auto-generated labels with ground\ntruth answers, measuring performance under standardized conditions without\nhuman intervention. The assisted performance evaluation involves 20\nfinance-related multiple-choice questions answered by 20 participants without\nfinancial backgrounds. Initially, participants answer independently. Later,\nthey receive system-generated reference information to assist in answering,\nexamining whether the system improves accuracy when assistance is provided.\n  The main contributions of this research are: (1) Enhanced LLM Capability: By\nintegrating BGE-M3 and BGE-reranker, the system retrieves and reorders highly\nrelevant results, reduces hallucinations, and dynamically accesses authorized\nor public knowledge sources. (2) Improved Data Privacy: A customized RAG\narchitecture enables local operation of the LLM, eliminating the need to send\nprivate data to external servers. This approach enhances data security, reduces\nreliance on commercial services, lowers operational costs, and mitigates\nprivacy risks."
                },
                "authors": [
                    {
                        "name": "Te-Lun Yang"
                    },
                    {
                        "name": "Jyi-Shane Liu"
                    },
                    {
                        "name": "Yuen-Hsien Tseng"
                    },
                    {
                        "name": "Jyh-Shing Roger Jang"
                    }
                ],
                "author_detail": {
                    "name": "Jyh-Shing Roger Jang"
                },
                "author": "Jyh-Shing Roger Jang",
                "arxiv_comment": "8 pages, 13 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04185v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04185v4",
                "updated": "2025-01-08T17:11:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    17,
                    11,
                    53,
                    2,
                    8,
                    0
                ],
                "published": "2024-07-04T23:26:56Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    23,
                    26,
                    56,
                    3,
                    186,
                    0
                ],
                "title": "HAF-RM: A Hybrid Alignment Framework for Reward Model Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HAF-RM: A Hybrid Alignment Framework for Reward Model Training"
                },
                "summary": "The reward model has become increasingly important in alignment, assessment,\nand data construction for large language models (LLMs). Most existing\nresearchers focus on enhancing reward models through data improvements,\nfollowing the conventional training framework for reward models that directly\noptimizes the predicted rewards. In this paper, we propose a hybrid alignment\nframework HaF-RM for reward model training by introducing an additional\nconstraint on token-level policy probabilities in addition to the reward score.\nIt can simultaneously supervise the internal preference model at the token\nlevel and optimize the mapping layer of the reward model at the sequence level.\nExperiment results on five datasets sufficiently show the validity and\neffectiveness of our proposed hybrid framework for training a high-quality\nreward model. By decoupling the reward modeling procedure and incorporating\nhybrid supervision, our HaF-RM framework offers a principled and effective\napproach to enhancing the performance and alignment of reward models, a\ncritical component in the responsible development of powerful language models.\nWe release our code at https://haf-rm.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reward model has become increasingly important in alignment, assessment,\nand data construction for large language models (LLMs). Most existing\nresearchers focus on enhancing reward models through data improvements,\nfollowing the conventional training framework for reward models that directly\noptimizes the predicted rewards. In this paper, we propose a hybrid alignment\nframework HaF-RM for reward model training by introducing an additional\nconstraint on token-level policy probabilities in addition to the reward score.\nIt can simultaneously supervise the internal preference model at the token\nlevel and optimize the mapping layer of the reward model at the sequence level.\nExperiment results on five datasets sufficiently show the validity and\neffectiveness of our proposed hybrid framework for training a high-quality\nreward model. By decoupling the reward modeling procedure and incorporating\nhybrid supervision, our HaF-RM framework offers a principled and effective\napproach to enhancing the performance and alignment of reward models, a\ncritical component in the responsible development of powerful language models.\nWe release our code at https://haf-rm.github.io."
                },
                "authors": [
                    {
                        "name": "Shujun Liu"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    },
                    {
                        "name": "Yuhang Lai"
                    },
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Shengbin Yue"
                    },
                    {
                        "name": "Zengfeng Huang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04185v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04185v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04615v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04615v1",
                "updated": "2025-01-08T16:57:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    16,
                    57,
                    18,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T16:57:18Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    16,
                    57,
                    18,
                    2,
                    8,
                    0
                ],
                "title": "Doubly Robust and Efficient Calibration of Prediction Sets for Censored\n  Time-to-Event Outcomes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doubly Robust and Efficient Calibration of Prediction Sets for Censored\n  Time-to-Event Outcomes"
                },
                "summary": "Our objective is to construct well-calibrated prediction sets for a\ntime-to-event outcome subject to right-censoring with guaranteed coverage. Our\napproach is inspired by modern conformal inference literature, in that, unlike\nclassical frameworks, we obviate the need for a well-specified parametric or\nsemi-parametric survival model to accomplish our goal. In contrast to existing\nconformal prediction methods for survival data, which restrict censoring to be\nof Type I, whereby potential censoring times are assumed to be fully observed\non all units in both training and validation samples, we consider the more\ncommon right-censoring setting in which either only the censoring time or only\nthe event time of primary interest is directly observed, whichever comes first.\nUnder a standard conditional independence assumption between the potential\nsurvival and censoring times given covariates, we propose and analyze two\nmethods to construct valid and efficient lower predictive bounds for the\nsurvival time of a future observation. The proposed methods build upon modern\nsemiparametric efficiency theory for censored data, in that the first approach\nincorporates inverse-probability-of-censoring weighting (IPCW), while the\nsecond approach is based on augmented-inverse-probability-of-censoring\nweighting (AIPCW). For both methods, we formally establish asymptotic coverage\nguarantees, and demonstrate both via theory and empirical experiments that\nAIPCW substantially improves efficiency over IPCW in the sense that its\ncoverage error bound is of second-order mixed bias type, that is \\emph{doubly\nrobust}, and therefore guaranteed to be asymptotically negligible relative to\nthe coverage error of IPCW.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our objective is to construct well-calibrated prediction sets for a\ntime-to-event outcome subject to right-censoring with guaranteed coverage. Our\napproach is inspired by modern conformal inference literature, in that, unlike\nclassical frameworks, we obviate the need for a well-specified parametric or\nsemi-parametric survival model to accomplish our goal. In contrast to existing\nconformal prediction methods for survival data, which restrict censoring to be\nof Type I, whereby potential censoring times are assumed to be fully observed\non all units in both training and validation samples, we consider the more\ncommon right-censoring setting in which either only the censoring time or only\nthe event time of primary interest is directly observed, whichever comes first.\nUnder a standard conditional independence assumption between the potential\nsurvival and censoring times given covariates, we propose and analyze two\nmethods to construct valid and efficient lower predictive bounds for the\nsurvival time of a future observation. The proposed methods build upon modern\nsemiparametric efficiency theory for censored data, in that the first approach\nincorporates inverse-probability-of-censoring weighting (IPCW), while the\nsecond approach is based on augmented-inverse-probability-of-censoring\nweighting (AIPCW). For both methods, we formally establish asymptotic coverage\nguarantees, and demonstrate both via theory and empirical experiments that\nAIPCW substantially improves efficiency over IPCW in the sense that its\ncoverage error bound is of second-order mixed bias type, that is \\emph{doubly\nrobust}, and therefore guaranteed to be asymptotically negligible relative to\nthe coverage error of IPCW."
                },
                "authors": [
                    {
                        "name": "Rebecca Farina"
                    },
                    {
                        "name": "Arun Kumar Kuchibhotla"
                    },
                    {
                        "name": "Eric J. Tchetgen Tchetgen"
                    }
                ],
                "author_detail": {
                    "name": "Eric J. Tchetgen Tchetgen"
                },
                "author": "Eric J. Tchetgen Tchetgen",
                "arxiv_comment": "36 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04615v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04606v1",
                "updated": "2025-01-08T16:41:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    16,
                    41,
                    31,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T16:41:31Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    16,
                    41,
                    31,
                    2,
                    8,
                    0
                ],
                "title": "Enhancing Low-Cost Video Editing with Lightweight Adaptors and\n  Temporal-Aware Inversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Low-Cost Video Editing with Lightweight Adaptors and\n  Temporal-Aware Inversion"
                },
                "summary": "Recent advancements in text-to-image (T2I) generation using diffusion models\nhave enabled cost-effective video-editing applications by leveraging\npre-trained models, eliminating the need for resource-intensive training.\nHowever, the frame-independence of T2I generation often results in poor\ntemporal consistency. Existing methods address this issue through temporal\nlayer fine-tuning or inference-based temporal propagation, but these approaches\nsuffer from high training costs or limited temporal coherence. To address these\nchallenges, we propose a General and Efficient Adapter (GE-Adapter) that\nintegrates temporal-spatial and semantic consistency with Baliteral DDIM\ninversion. This framework introduces three key components: (1) Frame-based\nTemporal Consistency Blocks (FTC Blocks) to capture frame-specific features and\nenforce smooth inter-frame transitions via temporally-aware loss functions; (2)\nChannel-dependent Spatial Consistency Blocks (SCD Blocks) employing bilateral\nfilters to enhance spatial coherence by reducing noise and artifacts; and (3)\nToken-based Semantic Consistency Module (TSC Module) to maintain semantic\nalignment using shared prompt tokens and frame-specific tokens. Our method\nsignificantly improves perceptual quality, text-image alignment, and temporal\ncoherence, as demonstrated on the MSR-VTT dataset. Additionally, it achieves\nenhanced fidelity and frame-to-frame coherence, offering a practical solution\nfor T2V editing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in text-to-image (T2I) generation using diffusion models\nhave enabled cost-effective video-editing applications by leveraging\npre-trained models, eliminating the need for resource-intensive training.\nHowever, the frame-independence of T2I generation often results in poor\ntemporal consistency. Existing methods address this issue through temporal\nlayer fine-tuning or inference-based temporal propagation, but these approaches\nsuffer from high training costs or limited temporal coherence. To address these\nchallenges, we propose a General and Efficient Adapter (GE-Adapter) that\nintegrates temporal-spatial and semantic consistency with Baliteral DDIM\ninversion. This framework introduces three key components: (1) Frame-based\nTemporal Consistency Blocks (FTC Blocks) to capture frame-specific features and\nenforce smooth inter-frame transitions via temporally-aware loss functions; (2)\nChannel-dependent Spatial Consistency Blocks (SCD Blocks) employing bilateral\nfilters to enhance spatial coherence by reducing noise and artifacts; and (3)\nToken-based Semantic Consistency Module (TSC Module) to maintain semantic\nalignment using shared prompt tokens and frame-specific tokens. Our method\nsignificantly improves perceptual quality, text-image alignment, and temporal\ncoherence, as demonstrated on the MSR-VTT dataset. Additionally, it achieves\nenhanced fidelity and frame-to-frame coherence, offering a practical solution\nfor T2V editing."
                },
                "authors": [
                    {
                        "name": "Yangfan He"
                    },
                    {
                        "name": "Sida Li"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Jianhui Wang"
                    },
                    {
                        "name": "Binxu Li"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Jun Yin"
                    },
                    {
                        "name": "Miao Zhang"
                    },
                    {
                        "name": "Xueqian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xueqian Wang"
                },
                "author": "Xueqian Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03304v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03304v2",
                "updated": "2025-01-08T16:41:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    16,
                    41,
                    3,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-06T16:04:56Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    16,
                    4,
                    56,
                    0,
                    6,
                    0
                ],
                "title": "LiLMaps: Learnable Implicit Language Maps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiLMaps: Learnable Implicit Language Maps"
                },
                "summary": "One of the current trends in robotics is to employ large language models\n(LLMs) to provide non-predefined command execution and natural human-robot\ninteraction. It is useful to have an environment map together with its language\nrepresentation, which can be further utilized by LLMs. Such a comprehensive\nscene representation enables numerous ways of interaction with the map for\nautonomously operating robots. In this work, we present an approach that\nenhances incremental implicit mapping through the integration of\nvision-language features. Specifically, we (i) propose a decoder optimization\ntechnique for implicit language maps which can be used when new objects appear\non the scene, and (ii) address the problem of inconsistent vision-language\npredictions between different viewing positions. Our experiments demonstrate\nthe effectiveness of LiLMaps and solid improvements in performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the current trends in robotics is to employ large language models\n(LLMs) to provide non-predefined command execution and natural human-robot\ninteraction. It is useful to have an environment map together with its language\nrepresentation, which can be further utilized by LLMs. Such a comprehensive\nscene representation enables numerous ways of interaction with the map for\nautonomously operating robots. In this work, we present an approach that\nenhances incremental implicit mapping through the integration of\nvision-language features. Specifically, we (i) propose a decoder optimization\ntechnique for implicit language maps which can be used when new objects appear\non the scene, and (ii) address the problem of inconsistent vision-language\npredictions between different viewing positions. Our experiments demonstrate\nthe effectiveness of LiLMaps and solid improvements in performance."
                },
                "authors": [
                    {
                        "name": "Evgenii Kruzhkov"
                    },
                    {
                        "name": "Sven Behnke"
                    }
                ],
                "author_detail": {
                    "name": "Sven Behnke"
                },
                "author": "Sven Behnke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03304v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03304v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.17464v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.17464v3",
                "updated": "2025-01-08T16:27:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    16,
                    27,
                    29,
                    2,
                    8,
                    0
                ],
                "published": "2024-01-30T21:53:30Z",
                "published_parsed": [
                    2024,
                    1,
                    30,
                    21,
                    53,
                    30,
                    1,
                    30,
                    0
                ],
                "title": "Efficient Tool Use with Chain-of-Abstraction Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Tool Use with Chain-of-Abstraction Reasoning"
                },
                "summary": "To achieve faithful reasoning that aligns with human expectations, large\nlanguage models (LLMs) need to ground their reasoning to real-world knowledge\n(e.g., web facts, math and physical rules). Tools help LLMs access this\nexternal knowledge, but there remains challenges for fine-tuning LLM agents\n(e.g., Toolformer) to invoke tools in multi-step reasoning problems, where\ninter-connected tool calls require holistic and efficient tool usage planning.\n  In this work, we propose a new method for LLMs to better leverage tools in\nmulti-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to\nfirst decode reasoning chains with abstract placeholders, and then call domain\ntools to reify each reasoning chain by filling in specific knowledge. This\nplanning with abstract chains enables LLMs to learn more general reasoning\nstrategies, which are robust to shifts of domain knowledge (e.g., math results)\nrelevant to different reasoning questions. It also allows LLMs to perform\ndecoding and calling of external tools in parallel, which avoids the inference\ndelay caused by waiting for tool responses. In mathematical reasoning and Wiki\nQA domains, we show that our method consistently outperforms previous\nchain-of-thought and tool-augmented baselines on both in-distribution and\nout-of-distribution test sets, with an average ~6% absolute QA accuracy\nimprovement. LLM agents trained with our method also show more efficient tool\nuse, with inference speed being on average ~1.4x faster than baseline\ntool-augmented LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To achieve faithful reasoning that aligns with human expectations, large\nlanguage models (LLMs) need to ground their reasoning to real-world knowledge\n(e.g., web facts, math and physical rules). Tools help LLMs access this\nexternal knowledge, but there remains challenges for fine-tuning LLM agents\n(e.g., Toolformer) to invoke tools in multi-step reasoning problems, where\ninter-connected tool calls require holistic and efficient tool usage planning.\n  In this work, we propose a new method for LLMs to better leverage tools in\nmulti-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to\nfirst decode reasoning chains with abstract placeholders, and then call domain\ntools to reify each reasoning chain by filling in specific knowledge. This\nplanning with abstract chains enables LLMs to learn more general reasoning\nstrategies, which are robust to shifts of domain knowledge (e.g., math results)\nrelevant to different reasoning questions. It also allows LLMs to perform\ndecoding and calling of external tools in parallel, which avoids the inference\ndelay caused by waiting for tool responses. In mathematical reasoning and Wiki\nQA domains, we show that our method consistently outperforms previous\nchain-of-thought and tool-augmented baselines on both in-distribution and\nout-of-distribution test sets, with an average ~6% absolute QA accuracy\nimprovement. LLM agents trained with our method also show more efficient tool\nuse, with inference speed being on average ~1.4x faster than baseline\ntool-augmented LLMs."
                },
                "authors": [
                    {
                        "name": "Silin Gao"
                    },
                    {
                        "name": "Jane Dwivedi-Yu"
                    },
                    {
                        "name": "Ping Yu"
                    },
                    {
                        "name": "Xiaoqing Ellen Tan"
                    },
                    {
                        "name": "Ramakanth Pasunuru"
                    },
                    {
                        "name": "Olga Golovneva"
                    },
                    {
                        "name": "Koustuv Sinha"
                    },
                    {
                        "name": "Asli Celikyilmaz"
                    },
                    {
                        "name": "Antoine Bosselut"
                    },
                    {
                        "name": "Tianlu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Tianlu Wang"
                },
                "author": "Tianlu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.17464v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.17464v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00957v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00957v3",
                "updated": "2025-01-08T16:07:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    16,
                    7,
                    35,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-01T21:23:22Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    21,
                    23,
                    22,
                    2,
                    1,
                    0
                ],
                "title": "Generative AI and LLMs in Industry: A text-mining Analysis and Critical\n  Evaluation of Guidelines and Policy Statements Across Fourteen Industrial\n  Sectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI and LLMs in Industry: A text-mining Analysis and Critical\n  Evaluation of Guidelines and Policy Statements Across Fourteen Industrial\n  Sectors"
                },
                "summary": "The rise of Generative AI (GAI) and Large Language Models (LLMs) has\ntransformed industrial landscapes, offering unprecedented opportunities for\nefficiency and innovation while raising critical ethical, regulatory, and\noperational challenges. This study conducts a text-based analysis of 160\nguidelines and policy statements across fourteen industrial sectors, utilizing\nsystematic methods and text-mining techniques to evaluate the governance of\nthese technologies. By examining global directives, industry practices, and\nsector-specific policies, the paper highlights the complexities of balancing\ninnovation with ethical accountability and equitable access. The findings\nprovide actionable insights and recommendations for fostering responsible,\ntransparent, and safe integration of GAI and LLMs in diverse industry contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Generative AI (GAI) and Large Language Models (LLMs) has\ntransformed industrial landscapes, offering unprecedented opportunities for\nefficiency and innovation while raising critical ethical, regulatory, and\noperational challenges. This study conducts a text-based analysis of 160\nguidelines and policy statements across fourteen industrial sectors, utilizing\nsystematic methods and text-mining techniques to evaluate the governance of\nthese technologies. By examining global directives, industry practices, and\nsector-specific policies, the paper highlights the complexities of balancing\ninnovation with ethical accountability and equitable access. The findings\nprovide actionable insights and recommendations for fostering responsible,\ntransparent, and safe integration of GAI and LLMs in diverse industry contexts."
                },
                "authors": [
                    {
                        "name": "Junfeng Jiao"
                    },
                    {
                        "name": "Saleh Afroogh"
                    },
                    {
                        "name": "Kevin Chen"
                    },
                    {
                        "name": "David Atkinson"
                    },
                    {
                        "name": "Amit Dhurandhar"
                    }
                ],
                "author_detail": {
                    "name": "Amit Dhurandhar"
                },
                "author": "Amit Dhurandhar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00957v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00957v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.12553v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.12553v3",
                "updated": "2025-01-08T16:02:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    16,
                    2,
                    24,
                    2,
                    8,
                    0
                ],
                "published": "2023-01-29T22:00:53Z",
                "published_parsed": [
                    2023,
                    1,
                    29,
                    22,
                    0,
                    53,
                    6,
                    29,
                    0
                ],
                "title": "Asymptotic Inference for Multi-Stage Stationary Treatment Policy with\n  Variable Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymptotic Inference for Multi-Stage Stationary Treatment Policy with\n  Variable Selection"
                },
                "summary": "Dynamic treatment regimes or policies are a sequence of decision functions\nover multiple stages that are tailored to individual features. One important\nclass of treatment policies in practice, namely multi-stage stationary\ntreatment policies, prescribes treatment assignment probabilities using the\nsame decision function across stages, where the decision is based on the same\nset of features consisting of time-evolving variables (e.g., routinely\ncollected disease biomarkers). Although there has been extensive literature on\nconstructing valid inference for the value function associated with dynamic\ntreatment policies, little work has focused on the policies themselves,\nespecially in the presence of high-dimensional feature variables. We aim to\nfill the gap in this work. Specifically, we first estimate the multi-stage\nstationary treatment policy using an augmented inverse probability weighted\nestimator for the value function to increase asymptotic efficiency, and further\napply a penalty to select important feature variables. We then construct\none-step improvements of the policy parameter estimators for valid inference.\nTheoretically, we show that the improved estimators are asymptotically normal,\neven if nuisance parameters are estimated at a slow convergence rate and the\ndimension of the feature variables increases with the sample size. Our\nnumerical studies demonstrate that the proposed method estimates a sparse\npolicy with a near-optimal value function and conducts valid inference for the\npolicy parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic treatment regimes or policies are a sequence of decision functions\nover multiple stages that are tailored to individual features. One important\nclass of treatment policies in practice, namely multi-stage stationary\ntreatment policies, prescribes treatment assignment probabilities using the\nsame decision function across stages, where the decision is based on the same\nset of features consisting of time-evolving variables (e.g., routinely\ncollected disease biomarkers). Although there has been extensive literature on\nconstructing valid inference for the value function associated with dynamic\ntreatment policies, little work has focused on the policies themselves,\nespecially in the presence of high-dimensional feature variables. We aim to\nfill the gap in this work. Specifically, we first estimate the multi-stage\nstationary treatment policy using an augmented inverse probability weighted\nestimator for the value function to increase asymptotic efficiency, and further\napply a penalty to select important feature variables. We then construct\none-step improvements of the policy parameter estimators for valid inference.\nTheoretically, we show that the improved estimators are asymptotically normal,\neven if nuisance parameters are estimated at a slow convergence rate and the\ndimension of the feature variables increases with the sample size. Our\nnumerical studies demonstrate that the proposed method estimates a sparse\npolicy with a near-optimal value function and conducts valid inference for the\npolicy parameters."
                },
                "authors": [
                    {
                        "name": "Daiqi Gao"
                    },
                    {
                        "name": "Yufeng Liu"
                    },
                    {
                        "name": "Donglin Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Donglin Zeng"
                },
                "author": "Donglin Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.12553v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.12553v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15939v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15939v6",
                "updated": "2025-01-08T15:56:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    15,
                    56,
                    21,
                    2,
                    8,
                    0
                ],
                "published": "2024-08-28T17:01:55Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    1,
                    55,
                    2,
                    241,
                    0
                ],
                "title": "The Tidal Torque Theory Revisited. I. Protohalo Angular Momentum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Tidal Torque Theory Revisited. I. Protohalo Angular Momentum"
                },
                "summary": "In the tidal torque theory, the angular momentum (AM) of dark matter halos\narises from the tidal torque suffered by non-spherical protohalos due to their\nsurrounding mass fluctuations. This theory was implemented in the peak model\nwhere collapsing patches are ellipsoidal. However, the delimitation of the\nellipsoids adopted was uncertain, and the protohalo mass used was doubtful. In\naddition, the numerical average of the AM of individual protohalos compromised\nthe traceability of the functionality of the result. Last but not least, the\nhalo AM was derived without taking into account shell-crossing and mergers.\nHere, we re-derive the protohalo AM in the peak model, using the natural\ndelimitation of ellipsoids from the accurate protohalo masses and following a\nnovel approach that clarifies the origin of its functionality. As a bonus, we\nobtain a very simple fully analytic expression of the protohalo AM. These\nresults will be used in Paper II to infer the rotational properties of relaxed\nhalos, accounting for those non-linear effects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the tidal torque theory, the angular momentum (AM) of dark matter halos\narises from the tidal torque suffered by non-spherical protohalos due to their\nsurrounding mass fluctuations. This theory was implemented in the peak model\nwhere collapsing patches are ellipsoidal. However, the delimitation of the\nellipsoids adopted was uncertain, and the protohalo mass used was doubtful. In\naddition, the numerical average of the AM of individual protohalos compromised\nthe traceability of the functionality of the result. Last but not least, the\nhalo AM was derived without taking into account shell-crossing and mergers.\nHere, we re-derive the protohalo AM in the peak model, using the natural\ndelimitation of ellipsoids from the accurate protohalo masses and following a\nnovel approach that clarifies the origin of its functionality. As a bonus, we\nobtain a very simple fully analytic expression of the protohalo AM. These\nresults will be used in Paper II to infer the rotational properties of relaxed\nhalos, accounting for those non-linear effects."
                },
                "authors": [
                    {
                        "name": "Eduard Salvador-Solé"
                    },
                    {
                        "name": "Alberto Manrique"
                    }
                ],
                "author_detail": {
                    "name": "Alberto Manrique"
                },
                "author": "Alberto Manrique",
                "arxiv_comment": "14 pages, 2 figures, submitted to The Astrophysical Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15939v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15939v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18367v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18367v3",
                "updated": "2025-01-08T15:30:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    15,
                    30,
                    11,
                    2,
                    8,
                    0
                ],
                "published": "2024-12-24T11:50:18Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    11,
                    50,
                    18,
                    1,
                    359,
                    0
                ],
                "title": "Towards Global AI Inclusivity: A Large-Scale Multilingual Terminology\n  Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Global AI Inclusivity: A Large-Scale Multilingual Terminology\n  Dataset"
                },
                "summary": "The field of machine translation has achieved significant advancements, yet\ndomain-specific terminology translation, particularly in AI, remains\nchallenging. We introduced GIST, a large-scale multilingual AI terminology\ndataset containing 5K terms extracted from top AI conference papers spanning\n2000 to 2023. The terms were translated into Arabic, Chinese, French, Japanese,\nand Russian using a hybrid framework that combines LLMs for extraction with\nhuman expertise for translation. The dataset's quality was benchmarked against\nexisting resources, demonstrating superior translation accuracy through\ncrowdsourced evaluation. GIST was integrated into translation workflows using\npost-translation refinement methods that required no retraining, where LLM\nprompting consistently improved BLEU and COMET scores. A web demonstration on\nthe ACL Anthology platform highlights its practical application, showcasing\nimproved accessibility for non-English speakers. This work aims to address\ncritical gaps in AI terminology resources and fosters global inclusivity and\ncollaboration in AI research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of machine translation has achieved significant advancements, yet\ndomain-specific terminology translation, particularly in AI, remains\nchallenging. We introduced GIST, a large-scale multilingual AI terminology\ndataset containing 5K terms extracted from top AI conference papers spanning\n2000 to 2023. The terms were translated into Arabic, Chinese, French, Japanese,\nand Russian using a hybrid framework that combines LLMs for extraction with\nhuman expertise for translation. The dataset's quality was benchmarked against\nexisting resources, demonstrating superior translation accuracy through\ncrowdsourced evaluation. GIST was integrated into translation workflows using\npost-translation refinement methods that required no retraining, where LLM\nprompting consistently improved BLEU and COMET scores. A web demonstration on\nthe ACL Anthology platform highlights its practical application, showcasing\nimproved accessibility for non-English speakers. This work aims to address\ncritical gaps in AI terminology resources and fosters global inclusivity and\ncollaboration in AI research."
                },
                "authors": [
                    {
                        "name": "Jiarui Liu"
                    },
                    {
                        "name": "Iman Ouzzani"
                    },
                    {
                        "name": "Wenkai Li"
                    },
                    {
                        "name": "Lechen Zhang"
                    },
                    {
                        "name": "Tianyue Ou"
                    },
                    {
                        "name": "Houda Bouamor"
                    },
                    {
                        "name": "Zhijing Jin"
                    },
                    {
                        "name": "Mona Diab"
                    }
                ],
                "author_detail": {
                    "name": "Mona Diab"
                },
                "author": "Mona Diab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18367v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18367v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18205v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18205v4",
                "updated": "2025-01-08T15:18:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    15,
                    18,
                    15,
                    2,
                    8,
                    0
                ],
                "published": "2024-02-28T09:51:55Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    9,
                    51,
                    55,
                    2,
                    59,
                    0
                ],
                "title": "Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging"
                },
                "summary": "Logs produced by extensive software systems are integral to monitoring system\nbehaviors. Advanced log analysis facilitates the detection, alerting, and\ndiagnosis of system faults. Log parsing, which entails transforming raw log\nmessages into structured templates, constitutes a critical phase in the\nautomation of log analytics. Existing log parsers fail to identify the correct\ntemplates due to reliance on human-made rules. Besides, These methods focus on\nstatistical features while ignoring semantic information in log messages. To\naddress these challenges, we introduce a cutting-edge \\textbf{L}og parsing\nframework with \\textbf{E}ntropy sampling and Chain-of-Thought \\textbf{M}erging\n(Lemur). Specifically, to discard the tedious manual rules. We propose a novel\nsampling method inspired by information entropy, which efficiently clusters\ntypical logs. Furthermore, to enhance the merging of log templates, we design a\nchain-of-thought method for large language models (LLMs). LLMs exhibit\nexceptional semantic comprehension, deftly distinguishing between parameters\nand invariant tokens. We have conducted experiments on large-scale public\ndatasets. Extensive evaluation demonstrates that Lemur achieves the\nstate-of-the-art performance and impressive efficiency. The Code is available\nat https://github.com/zwpride/lemur.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logs produced by extensive software systems are integral to monitoring system\nbehaviors. Advanced log analysis facilitates the detection, alerting, and\ndiagnosis of system faults. Log parsing, which entails transforming raw log\nmessages into structured templates, constitutes a critical phase in the\nautomation of log analytics. Existing log parsers fail to identify the correct\ntemplates due to reliance on human-made rules. Besides, These methods focus on\nstatistical features while ignoring semantic information in log messages. To\naddress these challenges, we introduce a cutting-edge \\textbf{L}og parsing\nframework with \\textbf{E}ntropy sampling and Chain-of-Thought \\textbf{M}erging\n(Lemur). Specifically, to discard the tedious manual rules. We propose a novel\nsampling method inspired by information entropy, which efficiently clusters\ntypical logs. Furthermore, to enhance the merging of log templates, we design a\nchain-of-thought method for large language models (LLMs). LLMs exhibit\nexceptional semantic comprehension, deftly distinguishing between parameters\nand invariant tokens. We have conducted experiments on large-scale public\ndatasets. Extensive evaluation demonstrates that Lemur achieves the\nstate-of-the-art performance and impressive efficiency. The Code is available\nat https://github.com/zwpride/lemur."
                },
                "authors": [
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Hongcheng Guo"
                    },
                    {
                        "name": "Anjie Le"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Zhoujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhoujun Li"
                },
                "author": "Zhoujun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18205v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18205v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.10150v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.10150v4",
                "updated": "2025-01-08T15:15:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    15,
                    15,
                    12,
                    2,
                    8,
                    0
                ],
                "published": "2024-01-18T17:22:37Z",
                "published_parsed": [
                    2024,
                    1,
                    18,
                    17,
                    22,
                    37,
                    3,
                    18,
                    0
                ],
                "title": "Motion-Zero: Zero-Shot Moving Object Control Framework for\n  Diffusion-Based Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motion-Zero: Zero-Shot Moving Object Control Framework for\n  Diffusion-Based Video Generation"
                },
                "summary": "Recent large-scale pre-trained diffusion models have demonstrated a powerful\ngenerative ability to produce high-quality videos from detailed text\ndescriptions. However, exerting control over the motion of objects in videos\ngenerated by any video diffusion model is a challenging problem. In this paper,\nwe propose a novel zero-shot moving object trajectory control framework,\nMotion-Zero, to enable a bounding-box-trajectories-controlled text-to-video\ndiffusion model. To this end, an initial noise prior module is designed to\nprovide a position-based prior to improve the stability of the appearance of\nthe moving object and the accuracy of position. In addition, based on the\nattention map of the U-net, spatial constraints are directly applied to the\ndenoising process of diffusion models, which further ensures the positional and\nspatial consistency of moving objects during the inference. Furthermore,\ntemporal consistency is guaranteed with a proposed shift temporal attention\nmechanism. Our method can be flexibly applied to various state-of-the-art video\ndiffusion models without any training process. Extensive experiments\ndemonstrate our proposed method can control the motion trajectories of objects\nand generate high-quality videos. Our project page is\nhttps://vpx-ecnu.github.io/MotionZero-website/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large-scale pre-trained diffusion models have demonstrated a powerful\ngenerative ability to produce high-quality videos from detailed text\ndescriptions. However, exerting control over the motion of objects in videos\ngenerated by any video diffusion model is a challenging problem. In this paper,\nwe propose a novel zero-shot moving object trajectory control framework,\nMotion-Zero, to enable a bounding-box-trajectories-controlled text-to-video\ndiffusion model. To this end, an initial noise prior module is designed to\nprovide a position-based prior to improve the stability of the appearance of\nthe moving object and the accuracy of position. In addition, based on the\nattention map of the U-net, spatial constraints are directly applied to the\ndenoising process of diffusion models, which further ensures the positional and\nspatial consistency of moving objects during the inference. Furthermore,\ntemporal consistency is guaranteed with a proposed shift temporal attention\nmechanism. Our method can be flexibly applied to various state-of-the-art video\ndiffusion models without any training process. Extensive experiments\ndemonstrate our proposed method can control the motion trajectories of objects\nand generate high-quality videos. Our project page is\nhttps://vpx-ecnu.github.io/MotionZero-website/"
                },
                "authors": [
                    {
                        "name": "Changgu Chen"
                    },
                    {
                        "name": "Junwei Shu"
                    },
                    {
                        "name": "Gaoqi He"
                    },
                    {
                        "name": "Changbo Wang"
                    },
                    {
                        "name": "Yang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yang Li"
                },
                "author": "Yang Li",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.10150v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.10150v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04556v1",
                "updated": "2025-01-08T15:07:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    15,
                    7,
                    6,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T15:07:06Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    15,
                    7,
                    6,
                    2,
                    8,
                    0
                ],
                "title": "Nonlinear coupling between magnetar QPOs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonlinear coupling between magnetar QPOs"
                },
                "summary": "The quasi-periodic oscillations (QPOs) observed in the tails of magnetar\ngiant $\\gamma$-ray flares have long been interpreted as normal oscillation\nmodes of these stars. However, most studies modelling QPOs have neglected some\nkey features in the analyses of the signals, namely that QPOs appear to be\ndetectable only intermittently and exhibit drifts in their frequencies. These\nare typical characteristics of nonlinear mode coupling, where, at leading\norder, the modes couple and evolve collectively as triplets. Using a\nrepresentative triplet of modes we solve the system's nonlinear equations of\nmotion analytically and argue that the coupling is likely axial-axial-polar in\nnature, with the observed intermittence and frequency drifts providing a way to\ninfer details of the magnetar's internal magnetic-field geometry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quasi-periodic oscillations (QPOs) observed in the tails of magnetar\ngiant $\\gamma$-ray flares have long been interpreted as normal oscillation\nmodes of these stars. However, most studies modelling QPOs have neglected some\nkey features in the analyses of the signals, namely that QPOs appear to be\ndetectable only intermittently and exhibit drifts in their frequencies. These\nare typical characteristics of nonlinear mode coupling, where, at leading\norder, the modes couple and evolve collectively as triplets. Using a\nrepresentative triplet of modes we solve the system's nonlinear equations of\nmotion analytically and argue that the coupling is likely axial-axial-polar in\nnature, with the observed intermittence and frequency drifts providing a way to\ninfer details of the magnetar's internal magnetic-field geometry."
                },
                "authors": [
                    {
                        "name": "Pantelis Pnigouras"
                    },
                    {
                        "name": "Samuel K. Lander"
                    }
                ],
                "author_detail": {
                    "name": "Samuel K. Lander"
                },
                "author": "Samuel K. Lander",
                "arxiv_comment": "8 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02654v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02654v2",
                "updated": "2025-01-08T14:53:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    14,
                    53,
                    41,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-05T20:39:52Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    20,
                    39,
                    52,
                    6,
                    5,
                    0
                ],
                "title": "Tougher Text, Smarter Models: Raising the Bar for Adversarial Defence\n  Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tougher Text, Smarter Models: Raising the Bar for Adversarial Defence\n  Benchmarks"
                },
                "summary": "Recent advancements in natural language processing have highlighted the\nvulnerability of deep learning models to adversarial attacks. While various\ndefence mechanisms have been proposed, there is a lack of comprehensive\nbenchmarks that evaluate these defences across diverse datasets, models, and\ntasks. In this work, we address this gap by presenting an extensive benchmark\nfor textual adversarial defence that significantly expands upon previous work.\nOur benchmark incorporates a wide range of datasets, evaluates state-of-the-art\ndefence mechanisms, and extends the assessment to include critical tasks such\nas single-sentence classification, similarity and paraphrase identification,\nnatural language inference, and commonsense reasoning. This work not only\nserves as a valuable resource for researchers and practitioners in the field of\nadversarial robustness but also identifies key areas for future research in\ntextual adversarial defence. By establishing a new standard for benchmarking in\nthis domain, we aim to accelerate progress towards more robust and reliable\nnatural language processing systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in natural language processing have highlighted the\nvulnerability of deep learning models to adversarial attacks. While various\ndefence mechanisms have been proposed, there is a lack of comprehensive\nbenchmarks that evaluate these defences across diverse datasets, models, and\ntasks. In this work, we address this gap by presenting an extensive benchmark\nfor textual adversarial defence that significantly expands upon previous work.\nOur benchmark incorporates a wide range of datasets, evaluates state-of-the-art\ndefence mechanisms, and extends the assessment to include critical tasks such\nas single-sentence classification, similarity and paraphrase identification,\nnatural language inference, and commonsense reasoning. This work not only\nserves as a valuable resource for researchers and practitioners in the field of\nadversarial robustness but also identifies key areas for future research in\ntextual adversarial defence. By establishing a new standard for benchmarking in\nthis domain, we aim to accelerate progress towards more robust and reliable\nnatural language processing systems."
                },
                "authors": [
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "arxiv_comment": "Will be presented as an oral in-person presentation at the conference\n  of COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02654v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02654v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04543v1",
                "updated": "2025-01-08T14:46:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    14,
                    46,
                    37,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T14:46:37Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    14,
                    46,
                    37,
                    2,
                    8,
                    0
                ],
                "title": "The Impostor is Among Us: Can Large Language Models Capture the\n  Complexity of Human Personas?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impostor is Among Us: Can Large Language Models Capture the\n  Complexity of Human Personas?"
                },
                "summary": "Large Language Models (LLMs) created new opportunities for generating\npersonas, which are expected to streamline and accelerate the human-centered\ndesign process. Yet, AI-generated personas may not accurately represent actual\nuser experiences, as they can miss contextual and emotional insights critical\nto understanding real users' needs and behaviors. This paper examines the\ndifferences in how users perceive personas created by LLMs compared to those\ncrafted by humans regarding their credibility for design. We gathered ten\nhuman-crafted personas developed by HCI experts according to relevant\nattributes established in related work. Then, we systematically generated ten\npersonas and compared them with human-crafted ones in a survey. The results\nshowed that participants differentiated between human-created and AI-generated\npersonas, with the latter being perceived as more informative and consistent.\nHowever, participants noted that the AI-generated personas tended to follow\nstereotypes, highlighting the need for a greater emphasis on diversity when\nutilizing LLMs for persona creation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) created new opportunities for generating\npersonas, which are expected to streamline and accelerate the human-centered\ndesign process. Yet, AI-generated personas may not accurately represent actual\nuser experiences, as they can miss contextual and emotional insights critical\nto understanding real users' needs and behaviors. This paper examines the\ndifferences in how users perceive personas created by LLMs compared to those\ncrafted by humans regarding their credibility for design. We gathered ten\nhuman-crafted personas developed by HCI experts according to relevant\nattributes established in related work. Then, we systematically generated ten\npersonas and compared them with human-crafted ones in a survey. The results\nshowed that participants differentiated between human-created and AI-generated\npersonas, with the latter being perceived as more informative and consistent.\nHowever, participants noted that the AI-generated personas tended to follow\nstereotypes, highlighting the need for a greater emphasis on diversity when\nutilizing LLMs for persona creation."
                },
                "authors": [
                    {
                        "name": "Christopher Lazik"
                    },
                    {
                        "name": "Christopher Katins"
                    },
                    {
                        "name": "Charlotte Kauter"
                    },
                    {
                        "name": "Jonas Jakob"
                    },
                    {
                        "name": "Caroline Jay"
                    },
                    {
                        "name": "Lars Grunske"
                    },
                    {
                        "name": "Thomas Kosch"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Kosch"
                },
                "author": "Thomas Kosch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13375v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13375v2",
                "updated": "2025-01-08T14:41:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    14,
                    41,
                    4,
                    2,
                    8,
                    0
                ],
                "published": "2024-12-17T23:18:06Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    23,
                    18,
                    6,
                    1,
                    352,
                    0
                ],
                "title": "Extending LLMs to New Languages: A Case Study of Llama and Persian\n  Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending LLMs to New Languages: A Case Study of Llama and Persian\n  Adaptation"
                },
                "summary": "Large language models (LLMs) have made great progress in classification and\ntext generation tasks. However, they are mainly trained on English data and\noften struggle with low-resource languages. In this study, we explore adding a\nnew language, i.e., Persian, to Llama (a model with a limited understanding of\nPersian) using parameter-efficient fine-tuning. We employ a multi-stage\napproach involving pretraining on monolingual Persian data, aligning\nrepresentations through bilingual pretraining and instruction datasets, and\ninstruction-tuning with task-specific datasets. We evaluate the model's\nperformance at each stage on generation and classification tasks. Our findings\nsuggest that incorporating the Persian language, through bilingual data\nalignment, can enhance classification accuracy for Persian tasks, with no\nadverse impact and sometimes even improvements on English tasks. Additionally,\nthe results highlight the model's initial strength as a critical factor when\nworking with limited training data, with cross-lingual alignment offering\nminimal benefits for the low-resource language. Knowledge transfer from English\nto Persian has a marginal effect, primarily benefiting simple classification\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made great progress in classification and\ntext generation tasks. However, they are mainly trained on English data and\noften struggle with low-resource languages. In this study, we explore adding a\nnew language, i.e., Persian, to Llama (a model with a limited understanding of\nPersian) using parameter-efficient fine-tuning. We employ a multi-stage\napproach involving pretraining on monolingual Persian data, aligning\nrepresentations through bilingual pretraining and instruction datasets, and\ninstruction-tuning with task-specific datasets. We evaluate the model's\nperformance at each stage on generation and classification tasks. Our findings\nsuggest that incorporating the Persian language, through bilingual data\nalignment, can enhance classification accuracy for Persian tasks, with no\nadverse impact and sometimes even improvements on English tasks. Additionally,\nthe results highlight the model's initial strength as a critical factor when\nworking with limited training data, with cross-lingual alignment offering\nminimal benefits for the low-resource language. Knowledge transfer from English\nto Persian has a marginal effect, primarily benefiting simple classification\ntasks."
                },
                "authors": [
                    {
                        "name": "Samin Mahdizadeh Sani"
                    },
                    {
                        "name": "Pouya Sadeghi"
                    },
                    {
                        "name": "Thuy-Trang Vu"
                    },
                    {
                        "name": "Yadollah Yaghoobzadeh"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    }
                ],
                "author_detail": {
                    "name": "Gholamreza Haffari"
                },
                "author": "Gholamreza Haffari",
                "arxiv_comment": "accepted at COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13375v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13375v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00599v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00599v2",
                "updated": "2025-01-08T14:38:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    14,
                    38,
                    30,
                    2,
                    8,
                    0
                ],
                "published": "2024-12-31T18:56:46Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    18,
                    56,
                    46,
                    1,
                    366,
                    0
                ],
                "title": "VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with\n  Video LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with\n  Video LLM"
                },
                "summary": "Video Large Language Models (Video LLMs) have recently exhibited remarkable\ncapabilities in general video understanding. However, they mainly focus on\nholistic comprehension and struggle with capturing fine-grained spatial and\ntemporal details. Besides, the lack of high-quality object-level video\ninstruction data and a comprehensive benchmark further hinders their\nadvancements. To tackle these challenges, we introduce the VideoRefer Suite to\nempower Video LLM for finer-level spatial-temporal video understanding, i.e.,\nenabling perception and reasoning on any objects throughout the video.\nSpecially, we thoroughly develop VideoRefer Suite across three essential\naspects: dataset, model, and benchmark. Firstly, we introduce a multi-agent\ndata engine to meticulously curate a large-scale, high-quality object-level\nvideo instruction dataset, termed VideoRefer-700K. Next, we present the\nVideoRefer model, which equips a versatile spatial-temporal object encoder to\ncapture precise regional and sequential representations. Finally, we\nmeticulously create a VideoRefer-Bench to comprehensively assess the\nspatial-temporal understanding capability of a Video LLM, evaluating it across\nvarious aspects. Extensive experiments and analyses demonstrate that our\nVideoRefer model not only achieves promising performance on video referring\nbenchmarks but also facilitates general video understanding capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (Video LLMs) have recently exhibited remarkable\ncapabilities in general video understanding. However, they mainly focus on\nholistic comprehension and struggle with capturing fine-grained spatial and\ntemporal details. Besides, the lack of high-quality object-level video\ninstruction data and a comprehensive benchmark further hinders their\nadvancements. To tackle these challenges, we introduce the VideoRefer Suite to\nempower Video LLM for finer-level spatial-temporal video understanding, i.e.,\nenabling perception and reasoning on any objects throughout the video.\nSpecially, we thoroughly develop VideoRefer Suite across three essential\naspects: dataset, model, and benchmark. Firstly, we introduce a multi-agent\ndata engine to meticulously curate a large-scale, high-quality object-level\nvideo instruction dataset, termed VideoRefer-700K. Next, we present the\nVideoRefer model, which equips a versatile spatial-temporal object encoder to\ncapture precise regional and sequential representations. Finally, we\nmeticulously create a VideoRefer-Bench to comprehensively assess the\nspatial-temporal understanding capability of a Video LLM, evaluating it across\nvarious aspects. Extensive experiments and analyses demonstrate that our\nVideoRefer model not only achieves promising performance on video referring\nbenchmarks but also facilitates general video understanding capabilities."
                },
                "authors": [
                    {
                        "name": "Yuqian Yuan"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Wentong Li"
                    },
                    {
                        "name": "Zesen Cheng"
                    },
                    {
                        "name": "Boqiang Zhang"
                    },
                    {
                        "name": "Long Li"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Deli Zhao"
                    },
                    {
                        "name": "Wenqiao Zhang"
                    },
                    {
                        "name": "Yueting Zhuang"
                    },
                    {
                        "name": "Jianke Zhu"
                    },
                    {
                        "name": "Lidong Bing"
                    }
                ],
                "author_detail": {
                    "name": "Lidong Bing"
                },
                "author": "Lidong Bing",
                "arxiv_comment": "17 pages, 14 figures, technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00599v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00599v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04531v1",
                "updated": "2025-01-08T14:23:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    14,
                    23,
                    15,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T14:23:15Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    14,
                    23,
                    15,
                    2,
                    8,
                    0
                ],
                "title": "The ENUBET monitored neutrino beam and its implementation at CERN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ENUBET monitored neutrino beam and its implementation at CERN"
                },
                "summary": "The ENUBET project recently concluded the R&D for a site independent design\nof a monitored neutrino beam for high precision cross section measurements, in\nwhich the neutrino flux is inferred from the measurement of charged leptons in\nan instrumented decay tunnel. In this phase three fundamental results were\nobtained and will be discussed here: 1) a beamline not requiring a horn and\nrelying on static focusing elements allows to perform a $\\nu_e$ cross section\nmeasurement in the DUNE energy range with 1% statistical uncertainty employing\n$10^{20}$ 400 GeV protons on target (pot) and a neutrino detector of the size\nof ProtoDUNE; 2) the instrumentation of the decay tunnel, based on a cost\neffective sampling calorimeter solution, has been tested with a large scale\nprototype achieving the performance required to identify positrons and muons\nfrom kaon decays with high signal-to-noise ratio; 3) the systematics budget on\nthe neutrino flux is constrained at the 1% level by fitting the charged leptons\nobservables measured in the decay tunnel. Based on these successful results\nENUBET is now pursuing a study for a site dependent implementation at CERN in\nthe framework of Physics Beyond Colliders. In this context a new beamline, able\nto enrich the neutrino flux at the energy of HK and to reduce by more than a\nfactor 3 the needed pot, has been designed and is being optimized. The civil\nengineering and radioprotection studies for the siting of ENUBET in the North\nArea towards the two ProtoDUNEs are also in the scope of this work, with the\ngoal of proposing a neutrino cross section experiment in 2026. The combined use\nof both the neutrino detectors and of the improved beamline would allow to\nperform cross section measurements with unprecedented precision in about 5\nyears with a proton request compatible with the needs of other users after CERN\nLong Shutdown 3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ENUBET project recently concluded the R&D for a site independent design\nof a monitored neutrino beam for high precision cross section measurements, in\nwhich the neutrino flux is inferred from the measurement of charged leptons in\nan instrumented decay tunnel. In this phase three fundamental results were\nobtained and will be discussed here: 1) a beamline not requiring a horn and\nrelying on static focusing elements allows to perform a $\\nu_e$ cross section\nmeasurement in the DUNE energy range with 1% statistical uncertainty employing\n$10^{20}$ 400 GeV protons on target (pot) and a neutrino detector of the size\nof ProtoDUNE; 2) the instrumentation of the decay tunnel, based on a cost\neffective sampling calorimeter solution, has been tested with a large scale\nprototype achieving the performance required to identify positrons and muons\nfrom kaon decays with high signal-to-noise ratio; 3) the systematics budget on\nthe neutrino flux is constrained at the 1% level by fitting the charged leptons\nobservables measured in the decay tunnel. Based on these successful results\nENUBET is now pursuing a study for a site dependent implementation at CERN in\nthe framework of Physics Beyond Colliders. In this context a new beamline, able\nto enrich the neutrino flux at the energy of HK and to reduce by more than a\nfactor 3 the needed pot, has been designed and is being optimized. The civil\nengineering and radioprotection studies for the siting of ENUBET in the North\nArea towards the two ProtoDUNEs are also in the scope of this work, with the\ngoal of proposing a neutrino cross section experiment in 2026. The combined use\nof both the neutrino detectors and of the improved beamline would allow to\nperform cross section measurements with unprecedented precision in about 5\nyears with a proton request compatible with the needs of other users after CERN\nLong Shutdown 3."
                },
                "authors": [
                    {
                        "name": "ENUBET collaboration"
                    },
                    {
                        "name": "L. Halić"
                    },
                    {
                        "name": "F. Acerbi"
                    },
                    {
                        "name": "I. Angelis"
                    },
                    {
                        "name": "L. Bomben"
                    },
                    {
                        "name": "M. Bonesini"
                    },
                    {
                        "name": "F. Bramati"
                    },
                    {
                        "name": "A. Branca"
                    },
                    {
                        "name": "C. Brizzolari"
                    },
                    {
                        "name": "G. Brunetti"
                    },
                    {
                        "name": "M. Calviani"
                    },
                    {
                        "name": "S. Capelli"
                    },
                    {
                        "name": "M. Capitani"
                    },
                    {
                        "name": "S. Carturan"
                    },
                    {
                        "name": "M. G. Catanesi"
                    },
                    {
                        "name": "S. Cecchini"
                    },
                    {
                        "name": "N. Charitonidis"
                    },
                    {
                        "name": "F. Cindolo"
                    },
                    {
                        "name": "G. Cogo"
                    },
                    {
                        "name": "G. Collazuol"
                    },
                    {
                        "name": "F. Dal Corso"
                    },
                    {
                        "name": "C. Delogu"
                    },
                    {
                        "name": "G. De Rosa"
                    },
                    {
                        "name": "A. Falcone"
                    },
                    {
                        "name": "B. Goddard"
                    },
                    {
                        "name": "A. Gola"
                    },
                    {
                        "name": "D. Guffanti"
                    },
                    {
                        "name": "F. Iacob"
                    },
                    {
                        "name": "M. A. Jebramcik"
                    },
                    {
                        "name": "C. Jollet"
                    },
                    {
                        "name": "V. Kain"
                    },
                    {
                        "name": "A. Kallitsopoulou"
                    },
                    {
                        "name": "B. Kliček"
                    },
                    {
                        "name": "Y. Kudenko"
                    },
                    {
                        "name": "Ch. Lampoudis"
                    },
                    {
                        "name": "M. Laveder"
                    },
                    {
                        "name": "P. Legou"
                    },
                    {
                        "name": "A. Longhin"
                    },
                    {
                        "name": "L. Ludovici"
                    },
                    {
                        "name": "E. Lutsenko"
                    },
                    {
                        "name": "L. Magaletti"
                    },
                    {
                        "name": "G. Mandrioli"
                    },
                    {
                        "name": "S. Marangoni"
                    },
                    {
                        "name": "A. Margotti"
                    },
                    {
                        "name": "V. Mascagna"
                    },
                    {
                        "name": "N. Mauri"
                    },
                    {
                        "name": "J. McElwee"
                    },
                    {
                        "name": "L. Meazza"
                    },
                    {
                        "name": "A. Meregaglia"
                    },
                    {
                        "name": "M. Mezzetto"
                    },
                    {
                        "name": "M. Nessi"
                    },
                    {
                        "name": "A. Paoloni"
                    },
                    {
                        "name": "M. Pari"
                    },
                    {
                        "name": "T. Papaevangelou"
                    },
                    {
                        "name": "E. G. Parozzi"
                    },
                    {
                        "name": "L. Pasqualini"
                    },
                    {
                        "name": "G. Paternoster"
                    },
                    {
                        "name": "L. Patrizii"
                    },
                    {
                        "name": "M. Pozzato"
                    },
                    {
                        "name": "M. Prest"
                    },
                    {
                        "name": "F. Pupilli"
                    },
                    {
                        "name": "E. Radicioni"
                    },
                    {
                        "name": "A. C. Ruggeri"
                    },
                    {
                        "name": "G. Saibene"
                    },
                    {
                        "name": "D. Sampsonidis"
                    },
                    {
                        "name": "A. Scanu"
                    },
                    {
                        "name": "C. Scian"
                    },
                    {
                        "name": "G. Sirri"
                    },
                    {
                        "name": "R. Speziali"
                    },
                    {
                        "name": "M. Stipčević"
                    },
                    {
                        "name": "M. Tenti"
                    },
                    {
                        "name": "F. Terranova"
                    },
                    {
                        "name": "M. Torti"
                    },
                    {
                        "name": "S. E. Tzamarias"
                    },
                    {
                        "name": "E. Vallazza"
                    },
                    {
                        "name": "F. Velotti"
                    },
                    {
                        "name": "L. Votano"
                    }
                ],
                "author_detail": {
                    "name": "L. Votano"
                },
                "author": "L. Votano",
                "arxiv_comment": "Conference proceedings for the 25th International Workshop on\n  Neutrinos from Accelerators (NuFact 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04529v1",
                "updated": "2025-01-08T14:21:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    14,
                    21,
                    3,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T14:21:03Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    14,
                    21,
                    3,
                    2,
                    8,
                    0
                ],
                "title": "A Plug-and-Play Bregman ADMM Module for Inferring Event Branches in\n  Temporal Point Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Plug-and-Play Bregman ADMM Module for Inferring Event Branches in\n  Temporal Point Processes"
                },
                "summary": "An event sequence generated by a temporal point process is often associated\nwith a hidden and structured event branching process that captures the\ntriggering relations between its historical and current events. In this study,\nwe design a new plug-and-play module based on the Bregman ADMM (BADMM)\nalgorithm, which infers event branches associated with event sequences in the\nmaximum likelihood estimation framework of temporal point processes (TPPs).\nSpecifically, we formulate the inference of event branches as an optimization\nproblem for the event transition matrix under sparse and low-rank constraints,\nwhich is embedded in existing TPP models or their learning paradigms. We can\nimplement this optimization problem based on subspace clustering and sparse\ngroup-lasso, respectively, and solve it using the Bregman ADMM algorithm, whose\nunrolling leads to the proposed BADMM module. When learning a classic TPP\n(e.g., Hawkes process) by the expectation-maximization algorithm, the BADMM\nmodule helps derive structured responsibility matrices in the E-step.\nSimilarly, the BADMM module helps derive low-rank and sparse attention maps for\nthe neural TPPs with self-attention layers. The structured responsibility\nmatrices and attention maps, which work as learned event transition matrices,\nindicate event branches, e.g., inferring isolated events and those key events\ntriggering many subsequent events. Experiments on both synthetic and real-world\ndata show that plugging our BADMM module into existing TPP models and learning\nparadigms can improve model performance and provide us with interpretable\nstructured event branches. The code is available at\n\\url{https://github.com/qingmeiwangdaily/BADMM_TPP}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An event sequence generated by a temporal point process is often associated\nwith a hidden and structured event branching process that captures the\ntriggering relations between its historical and current events. In this study,\nwe design a new plug-and-play module based on the Bregman ADMM (BADMM)\nalgorithm, which infers event branches associated with event sequences in the\nmaximum likelihood estimation framework of temporal point processes (TPPs).\nSpecifically, we formulate the inference of event branches as an optimization\nproblem for the event transition matrix under sparse and low-rank constraints,\nwhich is embedded in existing TPP models or their learning paradigms. We can\nimplement this optimization problem based on subspace clustering and sparse\ngroup-lasso, respectively, and solve it using the Bregman ADMM algorithm, whose\nunrolling leads to the proposed BADMM module. When learning a classic TPP\n(e.g., Hawkes process) by the expectation-maximization algorithm, the BADMM\nmodule helps derive structured responsibility matrices in the E-step.\nSimilarly, the BADMM module helps derive low-rank and sparse attention maps for\nthe neural TPPs with self-attention layers. The structured responsibility\nmatrices and attention maps, which work as learned event transition matrices,\nindicate event branches, e.g., inferring isolated events and those key events\ntriggering many subsequent events. Experiments on both synthetic and real-world\ndata show that plugging our BADMM module into existing TPP models and learning\nparadigms can improve model performance and provide us with interpretable\nstructured event branches. The code is available at\n\\url{https://github.com/qingmeiwangdaily/BADMM_TPP}."
                },
                "authors": [
                    {
                        "name": "Qingmei Wang"
                    },
                    {
                        "name": "Yuxin Wu"
                    },
                    {
                        "name": "Yujie Long"
                    },
                    {
                        "name": "Jing Huang"
                    },
                    {
                        "name": "Fengyuan Ran"
                    },
                    {
                        "name": "Bing Su"
                    },
                    {
                        "name": "Hongteng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hongteng Xu"
                },
                "author": "Hongteng Xu",
                "arxiv_comment": "Accepted at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "60G55, 62M10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04520v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04520v1",
                "updated": "2025-01-08T14:13:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    14,
                    13,
                    12,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T14:13:12Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    14,
                    13,
                    12,
                    2,
                    8,
                    0
                ],
                "title": "Inferring resource competition in microbial communities from time series",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring resource competition in microbial communities from time series"
                },
                "summary": "The competition for resources is a defining feature of microbial communities.\nIn many contexts, from soils to host-associated communities, highly diverse\nmicrobes are organized into metabolic groups or guilds with similar resource\npreferences. The resource preferences of individual taxa that give rise to\nthese guilds are critical for understanding fluxes of resources through the\ncommunity and the structure of diversity in the system. However, inferring the\nmetabolic capabilities of individual taxa, and their competition with other\ntaxa, within a community is challenging and unresolved. Here we address this\ngap in knowledge by leveraging dynamic measurements of abundances in\ncommunities. We show that simple correlations are often misleading in\npredicting resource competition. We show that spectral methods such as the\ncross-power spectral density (CPSD) and coherence that account for time-delayed\neffects are superior metrics for inferring the structure of resource\ncompetition in communities. We first demonstrate this fact on synthetic data\ngenerated from consumer-resource models with time-dependent resource\navailability, where taxa are organized into groups or guilds with similar\nresource preferences. By applying spectral methods to oceanic plankton\ntime-series data, we demonstrate that these methods detect interaction\nstructures among species with similar genomic sequences. Our results indicate\nthat analyzing temporal data across multiple timescales can reveal the\nunderlying structure of resource competition within communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The competition for resources is a defining feature of microbial communities.\nIn many contexts, from soils to host-associated communities, highly diverse\nmicrobes are organized into metabolic groups or guilds with similar resource\npreferences. The resource preferences of individual taxa that give rise to\nthese guilds are critical for understanding fluxes of resources through the\ncommunity and the structure of diversity in the system. However, inferring the\nmetabolic capabilities of individual taxa, and their competition with other\ntaxa, within a community is challenging and unresolved. Here we address this\ngap in knowledge by leveraging dynamic measurements of abundances in\ncommunities. We show that simple correlations are often misleading in\npredicting resource competition. We show that spectral methods such as the\ncross-power spectral density (CPSD) and coherence that account for time-delayed\neffects are superior metrics for inferring the structure of resource\ncompetition in communities. We first demonstrate this fact on synthetic data\ngenerated from consumer-resource models with time-dependent resource\navailability, where taxa are organized into groups or guilds with similar\nresource preferences. By applying spectral methods to oceanic plankton\ntime-series data, we demonstrate that these methods detect interaction\nstructures among species with similar genomic sequences. Our results indicate\nthat analyzing temporal data across multiple timescales can reveal the\nunderlying structure of resource competition within communities."
                },
                "authors": [
                    {
                        "name": "Xiaowen Chen"
                    },
                    {
                        "name": "Kyle Crocker"
                    },
                    {
                        "name": "Seppe Kuehn"
                    },
                    {
                        "name": "Aleksandra M. Walczak"
                    },
                    {
                        "name": "Thierry Mora"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Mora"
                },
                "author": "Thierry Mora",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04520v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04520v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04519v1",
                "updated": "2025-01-08T14:12:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    14,
                    12,
                    57,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T14:12:57Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    14,
                    12,
                    57,
                    2,
                    8,
                    0
                ],
                "title": "rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep\n  Thinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep\n  Thinking"
                },
                "summary": "We present rStar-Math to demonstrate that small language models (SLMs) can\nrival or even surpass the math reasoning capability of OpenAI o1, without\ndistillation from superior models. rStar-Math achieves this by exercising \"deep\nthinking\" through Monte Carlo Tree Search (MCTS), where a math policy SLM\nperforms test-time search guided by an SLM-based process reward model.\nrStar-Math introduces three innovations to tackle the challenges in training\nthe two SLMs: (1) a novel code-augmented CoT data sythesis method, which\nperforms extensive MCTS rollouts to generate step-by-step verified reasoning\ntrajectories used to train the policy SLM; (2) a novel process reward model\ntraining method that avoids na\\\"ive step-level score annotation, yielding a\nmore effective process preference model (PPM); (3) a self-evolution recipe in\nwhich the policy SLM and PPM are built from scratch and iteratively evolved to\nimprove reasoning capabilities. Through 4 rounds of self-evolution with\nmillions of synthesized solutions for 747k math problems, rStar-Math boosts\nSLMs' math reasoning to state-of-the-art levels. On the MATH benchmark, it\nimproves Qwen2.5-Math-7B from 58.8% to 90.0% and Phi3-mini-3.8B from 41.4% to\n86.4%, surpassing o1-preview by +4.5% and +0.9%. On the USA Math Olympiad\n(AIME), rStar-Math solves an average of 53.3% (8/15) of problems, ranking among\nthe top 20% the brightest high school math students. Code and data will be\navailable at https://github.com/microsoft/rStar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present rStar-Math to demonstrate that small language models (SLMs) can\nrival or even surpass the math reasoning capability of OpenAI o1, without\ndistillation from superior models. rStar-Math achieves this by exercising \"deep\nthinking\" through Monte Carlo Tree Search (MCTS), where a math policy SLM\nperforms test-time search guided by an SLM-based process reward model.\nrStar-Math introduces three innovations to tackle the challenges in training\nthe two SLMs: (1) a novel code-augmented CoT data sythesis method, which\nperforms extensive MCTS rollouts to generate step-by-step verified reasoning\ntrajectories used to train the policy SLM; (2) a novel process reward model\ntraining method that avoids na\\\"ive step-level score annotation, yielding a\nmore effective process preference model (PPM); (3) a self-evolution recipe in\nwhich the policy SLM and PPM are built from scratch and iteratively evolved to\nimprove reasoning capabilities. Through 4 rounds of self-evolution with\nmillions of synthesized solutions for 747k math problems, rStar-Math boosts\nSLMs' math reasoning to state-of-the-art levels. On the MATH benchmark, it\nimproves Qwen2.5-Math-7B from 58.8% to 90.0% and Phi3-mini-3.8B from 41.4% to\n86.4%, surpassing o1-preview by +4.5% and +0.9%. On the USA Math Olympiad\n(AIME), rStar-Math solves an average of 53.3% (8/15) of problems, ranking among\nthe top 20% the brightest high school math students. Code and data will be\navailable at https://github.com/microsoft/rStar."
                },
                "authors": [
                    {
                        "name": "Xinyu Guan"
                    },
                    {
                        "name": "Li Lyna Zhang"
                    },
                    {
                        "name": "Yifei Liu"
                    },
                    {
                        "name": "Ning Shang"
                    },
                    {
                        "name": "Youran Sun"
                    },
                    {
                        "name": "Yi Zhu"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13867v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13867v2",
                "updated": "2025-01-08T14:08:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    14,
                    8,
                    11,
                    2,
                    8,
                    0
                ],
                "published": "2024-05-22T17:48:17Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    17,
                    48,
                    17,
                    2,
                    143,
                    0
                ],
                "title": "Scaling-laws for Large Time-series Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-laws for Large Time-series Models"
                },
                "summary": "Scaling laws for large language models (LLMs) have provided useful guidance\nin training ever larger models for predictable performance gains. Time series\nforecasting shares a similar sequential structure to language, and is amenable\nto large-scale transformer architectures. Here we show that foundational\ndecoder-only time series transformer models exhibit analogous scaling-behavior\nto LLMs, with architectural details (aspect ratio and number of heads) having a\nminimal effect over broad ranges. We assemble a large corpus of heterogenous\ntime series data on which to train, and establish for the first time power-law\nscaling with parameter count, dataset size, and training compute, spanning five\norders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling laws for large language models (LLMs) have provided useful guidance\nin training ever larger models for predictable performance gains. Time series\nforecasting shares a similar sequential structure to language, and is amenable\nto large-scale transformer architectures. Here we show that foundational\ndecoder-only time series transformer models exhibit analogous scaling-behavior\nto LLMs, with architectural details (aspect ratio and number of heads) having a\nminimal effect over broad ranges. We assemble a large corpus of heterogenous\ntime series data on which to train, and establish for the first time power-law\nscaling with parameter count, dataset size, and training compute, spanning five\norders of magnitude."
                },
                "authors": [
                    {
                        "name": "Thomas D. P. Edwards"
                    },
                    {
                        "name": "James Alvey"
                    },
                    {
                        "name": "Justin Alsing"
                    },
                    {
                        "name": "Nam H. Nguyen"
                    },
                    {
                        "name": "Benjamin D. Wandelt"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin D. Wandelt"
                },
                "author": "Benjamin D. Wandelt",
                "arxiv_comment": "4 main pages (16 total), 4 figures; Accepted for oral presentation in\n  Time Series in the Age of Large Models (TSALM) Workshop at Neurips 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13867v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13867v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04513v1",
                "updated": "2025-01-08T14:00:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    14,
                    0,
                    7,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T14:00:07Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    14,
                    0,
                    7,
                    2,
                    8,
                    0
                ],
                "title": "Improving Image Captioning by Mimicking Human Reformulation Feedback at\n  Inference-time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Image Captioning by Mimicking Human Reformulation Feedback at\n  Inference-time"
                },
                "summary": "Incorporating automatically predicted human feedback into the process of\ntraining generative models has attracted substantial recent interest, while\nfeedback at inference time has received less attention. The typical feedback at\ntraining time, i.e., preferences of choice given two samples, does not\nnaturally transfer to the inference phase. We introduce a novel type of\nfeedback -- caption reformulations -- and train models to mimic reformulation\nfeedback based on human annotations. Our method does not require training the\nimage captioning model itself, thereby demanding substantially less\ncomputational effort. We experiment with two types of reformulation feedback:\nfirst, we collect a dataset of human reformulations that correct errors in the\ngenerated captions. We find that incorporating reformulation models trained on\nthis data into the inference phase of existing image captioning models results\nin improved captions, especially when the original captions are of low quality.\nWe apply our method to non-English image captioning, a domain where robust\nmodels are less prevalent, and gain substantial improvement. Second, we apply\nreformulations to style transfer. Quantitative evaluations reveal\nstate-of-the-art performance on German image captioning and English style\ntransfer, while human validation with a detailed comparative framework exposes\nthe specific axes of improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incorporating automatically predicted human feedback into the process of\ntraining generative models has attracted substantial recent interest, while\nfeedback at inference time has received less attention. The typical feedback at\ntraining time, i.e., preferences of choice given two samples, does not\nnaturally transfer to the inference phase. We introduce a novel type of\nfeedback -- caption reformulations -- and train models to mimic reformulation\nfeedback based on human annotations. Our method does not require training the\nimage captioning model itself, thereby demanding substantially less\ncomputational effort. We experiment with two types of reformulation feedback:\nfirst, we collect a dataset of human reformulations that correct errors in the\ngenerated captions. We find that incorporating reformulation models trained on\nthis data into the inference phase of existing image captioning models results\nin improved captions, especially when the original captions are of low quality.\nWe apply our method to non-English image captioning, a domain where robust\nmodels are less prevalent, and gain substantial improvement. Second, we apply\nreformulations to style transfer. Quantitative evaluations reveal\nstate-of-the-art performance on German image captioning and English style\ntransfer, while human validation with a detailed comparative framework exposes\nthe specific axes of improvement."
                },
                "authors": [
                    {
                        "name": "Uri Berger"
                    },
                    {
                        "name": "Omri Abend"
                    },
                    {
                        "name": "Lea Frermann"
                    },
                    {
                        "name": "Gabriel Stanovsky"
                    }
                ],
                "author_detail": {
                    "name": "Gabriel Stanovsky"
                },
                "author": "Gabriel Stanovsky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12809v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12809v2",
                "updated": "2025-01-08T13:59:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    13,
                    59,
                    28,
                    2,
                    8,
                    0
                ],
                "published": "2024-09-19T14:34:20Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    14,
                    34,
                    20,
                    3,
                    263,
                    0
                ],
                "title": "Don't be Fooled: The Misinformation Effect of Explanations in Human-AI\n  Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't be Fooled: The Misinformation Effect of Explanations in Human-AI\n  Collaboration"
                },
                "summary": "Across various applications, humans increasingly use black-box artificial\nintelligence (AI) systems without insight into these systems' reasoning. To\ncounter this opacity, explainable AI (XAI) methods promise enhanced\ntransparency and interpretability. While recent studies have explored how XAI\naffects human-AI collaboration, few have examined the potential pitfalls caused\nby incorrect explanations. The implications for humans can be far-reaching but\nhave not been explored extensively. To investigate this, we ran a study (n=160)\non AI-assisted decision-making in which humans were supported by XAI. Our\nfindings reveal a misinformation effect when incorrect explanations accompany\ncorrect AI advice with implications post-collaboration. This effect causes\nhumans to infer flawed reasoning strategies, hindering task execution and\ndemonstrating impaired procedural knowledge. Additionally, incorrect\nexplanations compromise human-AI team-performance during collaboration. With\nour work, we contribute to HCI by providing empirical evidence for the negative\nconsequences of incorrect explanations on humans post-collaboration and\noutlining guidelines for designers of AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Across various applications, humans increasingly use black-box artificial\nintelligence (AI) systems without insight into these systems' reasoning. To\ncounter this opacity, explainable AI (XAI) methods promise enhanced\ntransparency and interpretability. While recent studies have explored how XAI\naffects human-AI collaboration, few have examined the potential pitfalls caused\nby incorrect explanations. The implications for humans can be far-reaching but\nhave not been explored extensively. To investigate this, we ran a study (n=160)\non AI-assisted decision-making in which humans were supported by XAI. Our\nfindings reveal a misinformation effect when incorrect explanations accompany\ncorrect AI advice with implications post-collaboration. This effect causes\nhumans to infer flawed reasoning strategies, hindering task execution and\ndemonstrating impaired procedural knowledge. Additionally, incorrect\nexplanations compromise human-AI team-performance during collaboration. With\nour work, we contribute to HCI by providing empirical evidence for the negative\nconsequences of incorrect explanations on humans post-collaboration and\noutlining guidelines for designers of AI."
                },
                "authors": [
                    {
                        "name": "Philipp Spitzer"
                    },
                    {
                        "name": "Joshua Holstein"
                    },
                    {
                        "name": "Katelyn Morrison"
                    },
                    {
                        "name": "Kenneth Holstein"
                    },
                    {
                        "name": "Gerhard Satzger"
                    },
                    {
                        "name": "Niklas Kühl"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Kühl"
                },
                "author": "Niklas Kühl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12809v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12809v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04510v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04510v1",
                "updated": "2025-01-08T13:56:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    13,
                    56,
                    17,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T13:56:17Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    13,
                    56,
                    17,
                    2,
                    8,
                    0
                ],
                "title": "CGP-Tuning: Structure-Aware Soft Prompt Tuning for Code Vulnerability\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CGP-Tuning: Structure-Aware Soft Prompt Tuning for Code Vulnerability\n  Detection"
                },
                "summary": "Large language models (LLMs) have been proposed as powerful tools for\ndetecting software vulnerabilities, where task-specific fine-tuning is\ntypically employed to provide vulnerability-specific knowledge to the LLMs for\nthis purpose. However, traditional full-parameter fine-tuning is inefficient\nfor modern, complex LLMs, which contain billions of parameters.\n  Soft prompt tuning has been suggested as a more efficient alternative for\nfine-tuning LLMs in general cases. However, pure soft prompt tuning treats\nsource code as plain text, losing structural information inherent in source\ncode. Meanwhile, graph-enhanced soft prompt tuning methods, which aim to\naddress this issue, are unable to preserve the rich semantic information within\ncode graphs, as they are primarily designed for general graph-related tasks and\nfocus more on adjacency information. They also fail to ensure computational\nefficiency while accounting for graph-text interactions.\n  This paper, therefore, introduces a new code graph-enhanced, structure-aware\nsoft prompt tuning method for vulnerability detection, referred to as\nCGP-Tuning. It employs innovative type-aware embeddings to capture the rich\nsemantic information within code graphs, along with a novel and efficient\ncross-modal alignment module that achieves linear computational cost while\nincorporating graph-text interactions. The proposed CGP-Tuning is evaluated on\nthe latest DiverseVul dataset and the most recent open-source code LLMs,\nCodeLlama and CodeGemma. Experimental results demonstrate that CGP-Tuning\noutperforms the best state-of-the-art method by an average of 3.5 percentage\npoints in accuracy, without compromising its vulnerability detection\ncapabilities for long source code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been proposed as powerful tools for\ndetecting software vulnerabilities, where task-specific fine-tuning is\ntypically employed to provide vulnerability-specific knowledge to the LLMs for\nthis purpose. However, traditional full-parameter fine-tuning is inefficient\nfor modern, complex LLMs, which contain billions of parameters.\n  Soft prompt tuning has been suggested as a more efficient alternative for\nfine-tuning LLMs in general cases. However, pure soft prompt tuning treats\nsource code as plain text, losing structural information inherent in source\ncode. Meanwhile, graph-enhanced soft prompt tuning methods, which aim to\naddress this issue, are unable to preserve the rich semantic information within\ncode graphs, as they are primarily designed for general graph-related tasks and\nfocus more on adjacency information. They also fail to ensure computational\nefficiency while accounting for graph-text interactions.\n  This paper, therefore, introduces a new code graph-enhanced, structure-aware\nsoft prompt tuning method for vulnerability detection, referred to as\nCGP-Tuning. It employs innovative type-aware embeddings to capture the rich\nsemantic information within code graphs, along with a novel and efficient\ncross-modal alignment module that achieves linear computational cost while\nincorporating graph-text interactions. The proposed CGP-Tuning is evaluated on\nthe latest DiverseVul dataset and the most recent open-source code LLMs,\nCodeLlama and CodeGemma. Experimental results demonstrate that CGP-Tuning\noutperforms the best state-of-the-art method by an average of 3.5 percentage\npoints in accuracy, without compromising its vulnerability detection\ncapabilities for long source code."
                },
                "authors": [
                    {
                        "name": "Ruijun Feng"
                    },
                    {
                        "name": "Hammond Pearce"
                    },
                    {
                        "name": "Pietro Liguori"
                    },
                    {
                        "name": "Yulei Sui"
                    }
                ],
                "author_detail": {
                    "name": "Yulei Sui"
                },
                "author": "Yulei Sui",
                "arxiv_comment": "14 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04510v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02634v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02634v2",
                "updated": "2025-01-08T13:35:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    13,
                    35,
                    14,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-05T19:44:36Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    19,
                    44,
                    36,
                    6,
                    5,
                    0
                ],
                "title": "Optimal Inference of Asynchronous Boolean Network Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Inference of Asynchronous Boolean Network Models"
                },
                "summary": "Associations between phenotype and genomic and epigenomic markers are often\nderived by correlation. Systems Biology aims to make more robust connections\nand uncover broader insights by modeling the cellular mechanisms that produce a\nphenotype. The question of choosing the modeling methodology is of central\nimportance. A model that does not capture biological reality closely enough\nwill not explain the system's behavior. At the same time, highly detailed\nmodels suffer from computational limitations and are likely to overfit the\ndata. Boolean networks strike a balance between complexity and descriptiveness\nand thus have received increasing interest. We previously described an\nalgorithm for fitting Boolean networks to high-throughout experimental data\nthat finds the optimal network with respect to the information in a given\ndataset. In this work, we describe a simple extension that enables the modeling\nof asynchronous dynamics, i.e. different reaction times for different network\nnodes. Our approach greatly simplifies the construction of Boolean network\nmodels for time-series datasets, where asynchronicty often occurs. We\ndemonstrate our methodology by integrating real data from transcriptomic\nexperiments, and provide an implementation that can be used by the community\nfor network reconstruction using any high-throughout dataset. Our approach\nsignificantly expands the applicability of the Boolean network model to\nexperimental data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Associations between phenotype and genomic and epigenomic markers are often\nderived by correlation. Systems Biology aims to make more robust connections\nand uncover broader insights by modeling the cellular mechanisms that produce a\nphenotype. The question of choosing the modeling methodology is of central\nimportance. A model that does not capture biological reality closely enough\nwill not explain the system's behavior. At the same time, highly detailed\nmodels suffer from computational limitations and are likely to overfit the\ndata. Boolean networks strike a balance between complexity and descriptiveness\nand thus have received increasing interest. We previously described an\nalgorithm for fitting Boolean networks to high-throughout experimental data\nthat finds the optimal network with respect to the information in a given\ndataset. In this work, we describe a simple extension that enables the modeling\nof asynchronous dynamics, i.e. different reaction times for different network\nnodes. Our approach greatly simplifies the construction of Boolean network\nmodels for time-series datasets, where asynchronicty often occurs. We\ndemonstrate our methodology by integrating real data from transcriptomic\nexperiments, and provide an implementation that can be used by the community\nfor network reconstruction using any high-throughout dataset. Our approach\nsignificantly expands the applicability of the Boolean network model to\nexperimental data."
                },
                "authors": [
                    {
                        "name": "Guy Karlebach"
                    }
                ],
                "author_detail": {
                    "name": "Guy Karlebach"
                },
                "author": "Guy Karlebach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02634v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02634v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.MN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.MN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04492v1",
                "updated": "2025-01-08T13:25:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    13,
                    25,
                    21,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T13:25:21Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    13,
                    25,
                    21,
                    2,
                    8,
                    0
                ],
                "title": "ALMA observations of CO isotopologues towards six obscured post-AGB\n  stars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALMA observations of CO isotopologues towards six obscured post-AGB\n  stars"
                },
                "summary": "Low- and intermediate-mass stars evolve through the asymptotic giant branch\n(AGB), when an efficient mass-loss process removes a significant fraction of\ntheir initial mass. A substantial increase in the mass-loss rate at the end of\nthe AGB is observed for at least some stars for unknown reasons. This creates\npost-AGB objects that are completely enshrouded in thick dusty envelopes and\nmight be associated with binary interactions. We observed the $J=2-1$ line of\n$^{13}$CO, C$^{17}$O, and C$^{18}$O with the Atacama Large Millimeter /\nsubmillimeter Array (ALMA) towards six obscured post-AGB stars (four C-rich and\ntwo O-rich sources) to constrain the properties of their circumstellar\nenvelopes, recent mass-loss histories, and initial mass of the central stars.\nBased on the inferred $^{17}$O/$^{18}$O isotopic ratios, we find all stars to\nhave relatively low initial masses ($< 2~M_\\odot$) contrary to suggestions in\nthe literature of higher masses for some sources. We infer a mass for HD~187885\n$\\sim 1.15~M_\\odot$, which is relatively low for a carbon star. For all but one\nsource (GLMP~950), we observe kinematic components with velocities $\\gtrsim\n30$~km~s$^{-1}$, which are faster than typical AGB wind expansion velocities.\nFor most sources, these higher-velocity outflows display point-symmetric\nmorphologies. The case of Hen~3-1475 is particularly spectacular, with the\nhigh-velocity molecular outflow interleaved with the high-velocity outflow of\nionised gas observed at optical wavelengths. Based on the size of the emission\nregions of the slow components of the outflows, we derive typical kinematic\nages associated with the C$^{18}$O~$J=2-1$ emission $\\lesssim 1500$~years and\nobtain relatively high associated mass-loss rates ($\\gtrsim10^{-4}~M_\\odot~{\\rm\nyr}^{-1}$). The sources with known spectral types are found to have evolved\nfaster than expected based on stellar evolutionary models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low- and intermediate-mass stars evolve through the asymptotic giant branch\n(AGB), when an efficient mass-loss process removes a significant fraction of\ntheir initial mass. A substantial increase in the mass-loss rate at the end of\nthe AGB is observed for at least some stars for unknown reasons. This creates\npost-AGB objects that are completely enshrouded in thick dusty envelopes and\nmight be associated with binary interactions. We observed the $J=2-1$ line of\n$^{13}$CO, C$^{17}$O, and C$^{18}$O with the Atacama Large Millimeter /\nsubmillimeter Array (ALMA) towards six obscured post-AGB stars (four C-rich and\ntwo O-rich sources) to constrain the properties of their circumstellar\nenvelopes, recent mass-loss histories, and initial mass of the central stars.\nBased on the inferred $^{17}$O/$^{18}$O isotopic ratios, we find all stars to\nhave relatively low initial masses ($< 2~M_\\odot$) contrary to suggestions in\nthe literature of higher masses for some sources. We infer a mass for HD~187885\n$\\sim 1.15~M_\\odot$, which is relatively low for a carbon star. For all but one\nsource (GLMP~950), we observe kinematic components with velocities $\\gtrsim\n30$~km~s$^{-1}$, which are faster than typical AGB wind expansion velocities.\nFor most sources, these higher-velocity outflows display point-symmetric\nmorphologies. The case of Hen~3-1475 is particularly spectacular, with the\nhigh-velocity molecular outflow interleaved with the high-velocity outflow of\nionised gas observed at optical wavelengths. Based on the size of the emission\nregions of the slow components of the outflows, we derive typical kinematic\nages associated with the C$^{18}$O~$J=2-1$ emission $\\lesssim 1500$~years and\nobtain relatively high associated mass-loss rates ($\\gtrsim10^{-4}~M_\\odot~{\\rm\nyr}^{-1}$). The sources with known spectral types are found to have evolved\nfaster than expected based on stellar evolutionary models."
                },
                "authors": [
                    {
                        "name": "T. Khouri"
                    },
                    {
                        "name": "D. Tafoya"
                    },
                    {
                        "name": "W. H. T. Vlemmings"
                    },
                    {
                        "name": "H. Olofsson"
                    },
                    {
                        "name": "C. Sánchez Contreras"
                    },
                    {
                        "name": "J. Alcolea"
                    },
                    {
                        "name": "J. F. Gómez"
                    },
                    {
                        "name": "L. Velilla-Prieto"
                    },
                    {
                        "name": "R. Sahai"
                    },
                    {
                        "name": "M. Santander-García"
                    },
                    {
                        "name": "V. Bujarrabal"
                    },
                    {
                        "name": "A. Karakas"
                    },
                    {
                        "name": "M. Saberi"
                    },
                    {
                        "name": "I. Gallardo Cava"
                    },
                    {
                        "name": "H. Imai"
                    },
                    {
                        "name": "A. F. Pérez-Sánchez"
                    }
                ],
                "author_detail": {
                    "name": "A. F. Pérez-Sánchez"
                },
                "author": "A. F. Pérez-Sánchez",
                "arxiv_comment": "Accepted for publication in Astronomy & Astrophysics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04489v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04489v1",
                "updated": "2025-01-08T13:16:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    13,
                    16,
                    22,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T13:16:22Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    13,
                    16,
                    22,
                    2,
                    8,
                    0
                ],
                "title": "Collaborative Inference Acceleration with Non-Penetrative Tensor\n  Partitioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Inference Acceleration with Non-Penetrative Tensor\n  Partitioning"
                },
                "summary": "The inference of large-sized images on Internet of Things (IoT) devices is\ncommonly hindered by limited resources, while there are often stringent latency\nrequirements for Deep Neural Network (DNN) inference. Currently, this problem\nis generally addressed by collaborative inference, where the large-sized image\nis partitioned into multiple tiles, and each tile is assigned to an IoT device\nfor processing. However, since significant latency will be incurred due to the\ncommunication overhead caused by tile sharing, the existing collaborative\ninference strategy is inefficient for convolutional computation, which is\nindispensable for any DNN. To reduce it, we propose Non-Penetrative Tensor\nPartitioning (NPTP), a fine-grained tensor partitioning method that reduces the\ncommunication latency by minimizing the communication load of tiles shared,\nthereby reducing inference latency. We evaluate NPTP with four widely-adopted\nDNN models. Experimental results demonstrate that NPTP achieves a 1.44-1.68x\ninference speedup relative to CoEdge, a state-of-the-art (SOTA) collaborative\ninference algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference of large-sized images on Internet of Things (IoT) devices is\ncommonly hindered by limited resources, while there are often stringent latency\nrequirements for Deep Neural Network (DNN) inference. Currently, this problem\nis generally addressed by collaborative inference, where the large-sized image\nis partitioned into multiple tiles, and each tile is assigned to an IoT device\nfor processing. However, since significant latency will be incurred due to the\ncommunication overhead caused by tile sharing, the existing collaborative\ninference strategy is inefficient for convolutional computation, which is\nindispensable for any DNN. To reduce it, we propose Non-Penetrative Tensor\nPartitioning (NPTP), a fine-grained tensor partitioning method that reduces the\ncommunication latency by minimizing the communication load of tiles shared,\nthereby reducing inference latency. We evaluate NPTP with four widely-adopted\nDNN models. Experimental results demonstrate that NPTP achieves a 1.44-1.68x\ninference speedup relative to CoEdge, a state-of-the-art (SOTA) collaborative\ninference algorithm."
                },
                "authors": [
                    {
                        "name": "Zhibang Liu"
                    },
                    {
                        "name": "Chaonong Xu"
                    },
                    {
                        "name": "Zhenjie Lv"
                    },
                    {
                        "name": "Zhizhuo Liu"
                    },
                    {
                        "name": "Suyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Suyu Zhao"
                },
                "author": "Suyu Zhao",
                "arxiv_comment": "Accepted by the 2025 IEEE International Conference on Acoustics,\n  Speech, and Signal Processing (ICASSP 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04489v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04489v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04487v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04487v1",
                "updated": "2025-01-08T13:14:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    13,
                    14,
                    5,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T13:14:05Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    13,
                    14,
                    5,
                    2,
                    8,
                    0
                ],
                "title": "Integrating remote sensing data assimilation, deep learning and large\n  language model for interactive wheat breeding yield prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating remote sensing data assimilation, deep learning and large\n  language model for interactive wheat breeding yield prediction"
                },
                "summary": "Yield is one of the core goals of crop breeding. By predicting the potential\nyield of different breeding materials, breeders can screen these materials at\nvarious growth stages to select the best performing. Based on unmanned aerial\nvehicle remote sensing technology, high-throughput crop phenotyping data in\nbreeding areas is collected to provide data support for the breeding decisions\nof breeders. However, the accuracy of current yield predictions still requires\nimprovement, and the usability and user-friendliness of yield forecasting tools\nremain suboptimal. To address these challenges, this study introduces a hybrid\nmethod and tool for crop yield prediction, designed to allow breeders to\ninteractively and accurately predict wheat yield by chatting with a large\nlanguage model (LLM). First, the newly designed data assimilation algorithm is\nused to assimilate the leaf area index into the WOFOST model. Then, selected\noutputs from the assimilation process, along with remote sensing inversion\nresults, are used to drive the time-series temporal fusion transformer model\nfor wheat yield prediction. Finally, based on this hybrid method and leveraging\nan LLM with retrieval augmented generation technology, we developed an\ninteractive yield prediction Web tool that is user-friendly and supports\nsustainable data updates. This tool integrates multi-source data to assist\nbreeding decision-making. This study aims to accelerate the identification of\nhigh-yield materials in the breeding process, enhance breeding efficiency, and\nenable more scientific and smart breeding decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yield is one of the core goals of crop breeding. By predicting the potential\nyield of different breeding materials, breeders can screen these materials at\nvarious growth stages to select the best performing. Based on unmanned aerial\nvehicle remote sensing technology, high-throughput crop phenotyping data in\nbreeding areas is collected to provide data support for the breeding decisions\nof breeders. However, the accuracy of current yield predictions still requires\nimprovement, and the usability and user-friendliness of yield forecasting tools\nremain suboptimal. To address these challenges, this study introduces a hybrid\nmethod and tool for crop yield prediction, designed to allow breeders to\ninteractively and accurately predict wheat yield by chatting with a large\nlanguage model (LLM). First, the newly designed data assimilation algorithm is\nused to assimilate the leaf area index into the WOFOST model. Then, selected\noutputs from the assimilation process, along with remote sensing inversion\nresults, are used to drive the time-series temporal fusion transformer model\nfor wheat yield prediction. Finally, based on this hybrid method and leveraging\nan LLM with retrieval augmented generation technology, we developed an\ninteractive yield prediction Web tool that is user-friendly and supports\nsustainable data updates. This tool integrates multi-source data to assist\nbreeding decision-making. This study aims to accelerate the identification of\nhigh-yield materials in the breeding process, enhance breeding efficiency, and\nenable more scientific and smart breeding decisions."
                },
                "authors": [
                    {
                        "name": "Guofeng Yang"
                    },
                    {
                        "name": "Nanfei Jin"
                    },
                    {
                        "name": "Wenjie Ai"
                    },
                    {
                        "name": "Zhonghua Zheng"
                    },
                    {
                        "name": "Yuhong He"
                    },
                    {
                        "name": "Yong He"
                    }
                ],
                "author_detail": {
                    "name": "Yong He"
                },
                "author": "Yong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04487v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04486v1",
                "updated": "2025-01-08T13:13:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    13,
                    13,
                    52,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T13:13:52Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    13,
                    13,
                    52,
                    2,
                    8,
                    0
                ],
                "title": "MB-TaylorFormer V2: Improved Multi-branch Linear Transformer Expanded by\n  Taylor Formula for Image Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MB-TaylorFormer V2: Improved Multi-branch Linear Transformer Expanded by\n  Taylor Formula for Image Restoration"
                },
                "summary": "Recently, Transformer networks have demonstrated outstanding performance in\nthe field of image restoration due to the global receptive field and\nadaptability to input. However, the quadratic computational complexity of\nSoftmax-attention poses a significant limitation on its extensive application\nin image restoration tasks, particularly for high-resolution images. To tackle\nthis challenge, we propose a novel variant of the Transformer. This variant\nleverages the Taylor expansion to approximate the Softmax-attention and\nutilizes the concept of norm-preserving mapping to approximate the remainder of\nthe first-order Taylor expansion, resulting in a linear computational\ncomplexity. Moreover, we introduce a multi-branch architecture featuring\nmulti-scale patch embedding into the proposed Transformer, which has four\ndistinct advantages: 1) various sizes of the receptive field; 2) multi-level\nsemantic information; 3) flexible shapes of the receptive field; 4) accelerated\ntraining and inference speed. Hence, the proposed model, named the second\nversion of Taylor formula expansion-based Transformer (for short\nMB-TaylorFormer V2) has the capability to concurrently process coarse-to-fine\nfeatures, capture long-distance pixel interactions with limited computational\ncost, and improve the approximation of the Taylor expansion remainder.\nExperimental results across diverse image restoration benchmarks demonstrate\nthat MB-TaylorFormer V2 achieves state-of-the-art performance in multiple image\nrestoration tasks, such as image dehazing, deraining, desnowing, motion\ndeblurring, and denoising, with very little computational overhead. The source\ncode is available at https://github.com/FVL2020/MB-TaylorFormerV2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Transformer networks have demonstrated outstanding performance in\nthe field of image restoration due to the global receptive field and\nadaptability to input. However, the quadratic computational complexity of\nSoftmax-attention poses a significant limitation on its extensive application\nin image restoration tasks, particularly for high-resolution images. To tackle\nthis challenge, we propose a novel variant of the Transformer. This variant\nleverages the Taylor expansion to approximate the Softmax-attention and\nutilizes the concept of norm-preserving mapping to approximate the remainder of\nthe first-order Taylor expansion, resulting in a linear computational\ncomplexity. Moreover, we introduce a multi-branch architecture featuring\nmulti-scale patch embedding into the proposed Transformer, which has four\ndistinct advantages: 1) various sizes of the receptive field; 2) multi-level\nsemantic information; 3) flexible shapes of the receptive field; 4) accelerated\ntraining and inference speed. Hence, the proposed model, named the second\nversion of Taylor formula expansion-based Transformer (for short\nMB-TaylorFormer V2) has the capability to concurrently process coarse-to-fine\nfeatures, capture long-distance pixel interactions with limited computational\ncost, and improve the approximation of the Taylor expansion remainder.\nExperimental results across diverse image restoration benchmarks demonstrate\nthat MB-TaylorFormer V2 achieves state-of-the-art performance in multiple image\nrestoration tasks, such as image dehazing, deraining, desnowing, motion\ndeblurring, and denoising, with very little computational overhead. The source\ncode is available at https://github.com/FVL2020/MB-TaylorFormerV2."
                },
                "authors": [
                    {
                        "name": "Zhi Jin"
                    },
                    {
                        "name": "Yuwei Qiu"
                    },
                    {
                        "name": "Kaihao Zhang"
                    },
                    {
                        "name": "Hongdong Li"
                    },
                    {
                        "name": "Wenhan Luo"
                    }
                ],
                "author_detail": {
                    "name": "Wenhan Luo"
                },
                "author": "Wenhan Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06159v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06159v2",
                "updated": "2025-01-08T13:06:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    13,
                    6,
                    27,
                    2,
                    8,
                    0
                ],
                "published": "2024-11-09T12:06:40Z",
                "published_parsed": [
                    2024,
                    11,
                    9,
                    12,
                    6,
                    40,
                    5,
                    314,
                    0
                ],
                "title": "Mixture of Knowledge Minigraph Agents for Literature Review Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Knowledge Minigraph Agents for Literature Review Generation"
                },
                "summary": "Literature reviews play a crucial role in scientific research for\nunderstanding the current state of research, identifying gaps, and guiding\nfuture studies on specific topics. However, the process of conducting a\ncomprehensive literature review is yet time-consuming. This paper proposes a\nnovel framework, collaborative knowledge minigraph agents (CKMAs), to automate\nscholarly literature reviews. A novel prompt-based algorithm, the knowledge\nminigraph construction agent (KMCA), is designed to identify relations between\nconcepts from academic literature and automatically constructs knowledge\nminigraphs. By leveraging the capabilities of large language models on\nconstructed knowledge minigraphs, the multiple path summarization agent (MPSA)\nefficiently organizes concepts and relations from different viewpoints to\ngenerate literature review paragraphs. We evaluate CKMAs on three benchmark\ndatasets. Experimental results show the effectiveness of the proposed method,\nfurther revealing promising applications of LLMs in scientific research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Literature reviews play a crucial role in scientific research for\nunderstanding the current state of research, identifying gaps, and guiding\nfuture studies on specific topics. However, the process of conducting a\ncomprehensive literature review is yet time-consuming. This paper proposes a\nnovel framework, collaborative knowledge minigraph agents (CKMAs), to automate\nscholarly literature reviews. A novel prompt-based algorithm, the knowledge\nminigraph construction agent (KMCA), is designed to identify relations between\nconcepts from academic literature and automatically constructs knowledge\nminigraphs. By leveraging the capabilities of large language models on\nconstructed knowledge minigraphs, the multiple path summarization agent (MPSA)\nefficiently organizes concepts and relations from different viewpoints to\ngenerate literature review paragraphs. We evaluate CKMAs on three benchmark\ndatasets. Experimental results show the effectiveness of the proposed method,\nfurther revealing promising applications of LLMs in scientific research."
                },
                "authors": [
                    {
                        "name": "Zhi Zhang"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Sheng-hua Zhong"
                    },
                    {
                        "name": "Gong Chen"
                    },
                    {
                        "name": "Yu Yang"
                    },
                    {
                        "name": "Jiannong Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jiannong Cao"
                },
                "author": "Jiannong Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06159v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06159v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04478v1",
                "updated": "2025-01-08T13:00:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    13,
                    0,
                    38,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T13:00:38Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    13,
                    0,
                    38,
                    2,
                    8,
                    0
                ],
                "title": "Taxonomy of amorphous ternary phase diagrams: the importance of\n  interaction parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taxonomy of amorphous ternary phase diagrams: the importance of\n  interaction parameters"
                },
                "summary": "Phase diagrams offer a comprehensive representation of the thermodynamics of\nmulti-component systems. However, for ternary amorphous systems, the different\npossible phase diagram types and their existence conditions are still unclear.\nBased on the systematic screening of the parameter space for three amorphous\nternary material systems representative of polymer, small molecule, and solvent\nmaterials, we report identifying twenty-one phase diagram types generated using\nFlory-Huggins theory. The proposed classification relies on the number of\nimmiscible material pairs, distinct miscibility gaps, and three-phase regions.\nWe infer existence rules by mapping the type of phase diagram to the range of\ninteraction parameters in the three-dimensional parameter space. Depending on\nthe number of immiscible pairs (0, 1, 2, or 3), four well-known phase-diagram\ntypes are found to be very likely. However, numerous uncommon phase diagrams\nare observed within a small parameter window around the critical interaction\nparameter values. Regarding the processability window, the size of the miscible\nregion becomes sensitive to interaction parameter variations, mainly when they\nare close to critical values. The sensitivity decreases for materials with\nincreasing molar size. Finally, the paper includes a successful comparison of\nsimulated phase diagrams with experimental data, showcasing the real-world\nrelevance of this theoretical analysis and its applicability for optimizing\nsolution processing methods efficiently.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phase diagrams offer a comprehensive representation of the thermodynamics of\nmulti-component systems. However, for ternary amorphous systems, the different\npossible phase diagram types and their existence conditions are still unclear.\nBased on the systematic screening of the parameter space for three amorphous\nternary material systems representative of polymer, small molecule, and solvent\nmaterials, we report identifying twenty-one phase diagram types generated using\nFlory-Huggins theory. The proposed classification relies on the number of\nimmiscible material pairs, distinct miscibility gaps, and three-phase regions.\nWe infer existence rules by mapping the type of phase diagram to the range of\ninteraction parameters in the three-dimensional parameter space. Depending on\nthe number of immiscible pairs (0, 1, 2, or 3), four well-known phase-diagram\ntypes are found to be very likely. However, numerous uncommon phase diagrams\nare observed within a small parameter window around the critical interaction\nparameter values. Regarding the processability window, the size of the miscible\nregion becomes sensitive to interaction parameter variations, mainly when they\nare close to critical values. The sensitivity decreases for materials with\nincreasing molar size. Finally, the paper includes a successful comparison of\nsimulated phase diagrams with experimental data, showcasing the real-world\nrelevance of this theoretical analysis and its applicability for optimizing\nsolution processing methods efficiently."
                },
                "authors": [
                    {
                        "name": "Yasin Ameslon"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Jens Harting"
                    },
                    {
                        "name": "Olivier J. J. Ronsin"
                    },
                    {
                        "name": "Olga Wodo"
                    }
                ],
                "author_detail": {
                    "name": "Olga Wodo"
                },
                "author": "Olga Wodo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04475v1",
                "updated": "2025-01-08T12:57:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    12,
                    57,
                    9,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T12:57:09Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    12,
                    57,
                    9,
                    2,
                    8,
                    0
                ],
                "title": "ART: Distribution-Free and Model-Agnostic Changepoint Detection with\n  Finite-Sample Guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ART: Distribution-Free and Model-Agnostic Changepoint Detection with\n  Finite-Sample Guarantees"
                },
                "summary": "We introduce ART, a distribution-free and model-agnostic framework for\nchangepoint detection that provides finite-sample guarantees. ART transforms\nindependent observations into real-valued scores via a symmetric function,\nensuring exchangeability in the absence of changepoints. These scores are then\nranked and aggregated to detect distributional changes. The resulting test\noffers exact Type-I error control, agnostic to specific distributional or model\nassumptions. Moreover, ART seamlessly extends to multi-scale settings, enabling\nrobust multiple changepoint estimation and post-detection inference with\nfinite-sample error rate control. By locally ranking the scores and performing\naggregations across multiple prespecified intervals, ART identifies changepoint\nintervals and refines subsequent inference while maintaining its\ndistribution-free and model-agnostic nature. This adaptability makes ART as a\nreliable and versatile tool for modern changepoint analysis, particularly in\nhigh-dimensional data contexts and applications leveraging machine learning\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ART, a distribution-free and model-agnostic framework for\nchangepoint detection that provides finite-sample guarantees. ART transforms\nindependent observations into real-valued scores via a symmetric function,\nensuring exchangeability in the absence of changepoints. These scores are then\nranked and aggregated to detect distributional changes. The resulting test\noffers exact Type-I error control, agnostic to specific distributional or model\nassumptions. Moreover, ART seamlessly extends to multi-scale settings, enabling\nrobust multiple changepoint estimation and post-detection inference with\nfinite-sample error rate control. By locally ranking the scores and performing\naggregations across multiple prespecified intervals, ART identifies changepoint\nintervals and refines subsequent inference while maintaining its\ndistribution-free and model-agnostic nature. This adaptability makes ART as a\nreliable and versatile tool for modern changepoint analysis, particularly in\nhigh-dimensional data contexts and applications leveraging machine learning\nmethods."
                },
                "authors": [
                    {
                        "name": "Xiaolong Cui"
                    },
                    {
                        "name": "Haoyu Geng"
                    },
                    {
                        "name": "Guanghui Wang"
                    },
                    {
                        "name": "Zhaojun Wang"
                    },
                    {
                        "name": "Changliang Zou"
                    }
                ],
                "author_detail": {
                    "name": "Changliang Zou"
                },
                "author": "Changliang Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04473v1",
                "updated": "2025-01-08T12:54:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    12,
                    54,
                    5,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T12:54:05Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    12,
                    54,
                    5,
                    2,
                    8,
                    0
                ],
                "title": "When LLMs Struggle: Reference-less Translation Evaluation for\n  Low-resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When LLMs Struggle: Reference-less Translation Evaluation for\n  Low-resource Languages"
                },
                "summary": "This paper investigates the reference-less evaluation of machine translation\nfor low-resource language pairs, known as quality estimation (QE).\nSegment-level QE is a challenging cross-lingual language understanding task\nthat provides a quality score (0-100) to the translated output. We\ncomprehensively evaluate large language models (LLMs) in zero/few-shot\nscenarios and perform instruction fine-tuning using a novel prompt based on\nannotation guidelines. Our results indicate that prompt-based approaches are\noutperformed by the encoder-based fine-tuned QE models. Our error analysis\nreveals tokenization issues, along with errors due to transliteration and named\nentities, and argues for refinement in LLM pre-training for cross-lingual\ntasks. We release the data, and models trained publicly for further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the reference-less evaluation of machine translation\nfor low-resource language pairs, known as quality estimation (QE).\nSegment-level QE is a challenging cross-lingual language understanding task\nthat provides a quality score (0-100) to the translated output. We\ncomprehensively evaluate large language models (LLMs) in zero/few-shot\nscenarios and perform instruction fine-tuning using a novel prompt based on\nannotation guidelines. Our results indicate that prompt-based approaches are\noutperformed by the encoder-based fine-tuned QE models. Our error analysis\nreveals tokenization issues, along with errors due to transliteration and named\nentities, and argues for refinement in LLM pre-training for cross-lingual\ntasks. We release the data, and models trained publicly for further research."
                },
                "authors": [
                    {
                        "name": "Archchana Sindhujan"
                    },
                    {
                        "name": "Diptesh Kanojia"
                    },
                    {
                        "name": "Constantin Orasan"
                    },
                    {
                        "name": "Shenbin Qian"
                    }
                ],
                "author_detail": {
                    "name": "Shenbin Qian"
                },
                "author": "Shenbin Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08023v2",
                "updated": "2025-01-08T12:40:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    12,
                    40,
                    56,
                    2,
                    8,
                    0
                ],
                "published": "2024-09-12T13:05:28Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    13,
                    5,
                    28,
                    3,
                    256,
                    0
                ],
                "title": "Edge-Wise Graph-Instructed Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge-Wise Graph-Instructed Neural Networks"
                },
                "summary": "The problem of multi-task regression over graph nodes has been recently\napproached through Graph-Instructed Neural Network (GINN), which is a promising\narchitecture belonging to the subset of message-passing graph neural networks.\nIn this work, we discuss the limitations of the Graph-Instructed (GI) layer,\nand we formalize a novel edge-wise GI (EWGI) layer. We discuss the advantages\nof the EWGI layer and we provide numerical evidence that EWGINNs perform better\nthan GINNs over some graph-structured input data, like the ones inferred from\nthe Barabasi-Albert graph, and improve the training regularization on graphs\nwith chaotic connectivity, like the ones inferred from the Erdos-Renyi graph.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The problem of multi-task regression over graph nodes has been recently\napproached through Graph-Instructed Neural Network (GINN), which is a promising\narchitecture belonging to the subset of message-passing graph neural networks.\nIn this work, we discuss the limitations of the Graph-Instructed (GI) layer,\nand we formalize a novel edge-wise GI (EWGI) layer. We discuss the advantages\nof the EWGI layer and we provide numerical evidence that EWGINNs perform better\nthan GINNs over some graph-structured input data, like the ones inferred from\nthe Barabasi-Albert graph, and improve the training regularization on graphs\nwith chaotic connectivity, like the ones inferred from the Erdos-Renyi graph."
                },
                "authors": [
                    {
                        "name": "Francesco Della Santa"
                    },
                    {
                        "name": "Antonio Mastropietro"
                    },
                    {
                        "name": "Sandra Pieraccini"
                    },
                    {
                        "name": "Francesco Vaccarino"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Vaccarino"
                },
                "author": "Francesco Vaccarino",
                "arxiv_doi": "10.1016/j.jocs.2024.102518",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jocs.2024.102518",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.08023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "05C21, 65D15, 68T07, 90C35",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16149v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16149v4",
                "updated": "2025-01-08T12:40:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    12,
                    40,
                    27,
                    2,
                    8,
                    0
                ],
                "published": "2024-03-24T13:43:43Z",
                "published_parsed": [
                    2024,
                    3,
                    24,
                    13,
                    43,
                    43,
                    6,
                    84,
                    0
                ],
                "title": "Analyzing Consumer IoT Traffic from Security and Privacy Perspectives: a\n  Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Consumer IoT Traffic from Security and Privacy Perspectives: a\n  Comprehensive Survey"
                },
                "summary": "The Consumer Internet of Things (CIoT), a notable segment within the IoT\ndomain, involves the integration of IoT technology into consumer electronics\nand devices, such as smart homes and smart wearables. Compared to traditional\nIoT fields, CIoT differs notably in target users, product types, and design\napproaches. While offering convenience to users, it also raises new security\nand privacy concerns. Network traffic analysis, a widely used technique in the\nsecurity community, has been extensively applied to investigate these concerns\nabout CIoT. Compared to network traffic analysis in other fields such as mobile\napps and websites, CIoT presents unique characteristics, introducing new\nchallenges and research opportunities. Researchers have made significant\ncontributions in this area. To aid researchers in understanding the application\nof traffic analysis tools for studying CIoT security and privacy risks, this\nsurvey reviews 303 publications on traffic analysis within the CIoT security\nand privacy domain from January 2018 to June 2024, focusing on three research\nquestions. Our work: 1) outlines the CIoT traffic analysis process and\nhighlights its differences from general network traffic analysis. 2) summarizes\nand classifies existing research into four categories according to its\napplication objectives: device fingerprinting, user activity inference,\nmalicious traffic detection, and measurement. 3) explores emerging challenges\nand potential future research directions based on each step of the CIoT traffic\nanalysis process. This will provide new insights to the community and guide the\nindustry towards safer product designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Consumer Internet of Things (CIoT), a notable segment within the IoT\ndomain, involves the integration of IoT technology into consumer electronics\nand devices, such as smart homes and smart wearables. Compared to traditional\nIoT fields, CIoT differs notably in target users, product types, and design\napproaches. While offering convenience to users, it also raises new security\nand privacy concerns. Network traffic analysis, a widely used technique in the\nsecurity community, has been extensively applied to investigate these concerns\nabout CIoT. Compared to network traffic analysis in other fields such as mobile\napps and websites, CIoT presents unique characteristics, introducing new\nchallenges and research opportunities. Researchers have made significant\ncontributions in this area. To aid researchers in understanding the application\nof traffic analysis tools for studying CIoT security and privacy risks, this\nsurvey reviews 303 publications on traffic analysis within the CIoT security\nand privacy domain from January 2018 to June 2024, focusing on three research\nquestions. Our work: 1) outlines the CIoT traffic analysis process and\nhighlights its differences from general network traffic analysis. 2) summarizes\nand classifies existing research into four categories according to its\napplication objectives: device fingerprinting, user activity inference,\nmalicious traffic detection, and measurement. 3) explores emerging challenges\nand potential future research directions based on each step of the CIoT traffic\nanalysis process. This will provide new insights to the community and guide the\nindustry towards safer product designs."
                },
                "authors": [
                    {
                        "name": "Yan Jia"
                    },
                    {
                        "name": "Yuxin Song"
                    },
                    {
                        "name": "Zihou Liu"
                    },
                    {
                        "name": "Qingyin Tan"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Zheli Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheli Liu"
                },
                "author": "Zheli Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.16149v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16149v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04455v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04455v1",
                "updated": "2025-01-08T12:18:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    12,
                    18,
                    11,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T12:18:11Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    12,
                    18,
                    11,
                    2,
                    8,
                    0
                ],
                "title": "Hidden Entity Detection from GitHub Leveraging Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hidden Entity Detection from GitHub Leveraging Large Language Models"
                },
                "summary": "Named entity recognition is an important task when constructing knowledge\nbases from unstructured data sources. Whereas entity detection methods mostly\nrely on extensive training data, Large Language Models (LLMs) have paved the\nway towards approaches that rely on zero-shot learning (ZSL) or few-shot\nlearning (FSL) by taking advantage of the capabilities LLMs acquired during\npretraining. Specifically, in very specialized scenarios where large-scale\ntraining data is not available, ZSL / FSL opens new opportunities. This paper\nfollows this recent trend and investigates the potential of leveraging Large\nLanguage Models (LLMs) in such scenarios to automatically detect datasets and\nsoftware within textual content from GitHub repositories. While existing\nmethods focused solely on named entities, this study aims to broaden the scope\nby incorporating resources such as repositories and online hubs where entities\nare also represented by URLs. The study explores different FSL prompt learning\napproaches to enhance the LLMs' ability to identify dataset and software\nmentions within repository texts. Through analyses of LLM effectiveness and\nlearning strategies, this paper offers insights into the potential of advanced\nlanguage models for automated entity detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named entity recognition is an important task when constructing knowledge\nbases from unstructured data sources. Whereas entity detection methods mostly\nrely on extensive training data, Large Language Models (LLMs) have paved the\nway towards approaches that rely on zero-shot learning (ZSL) or few-shot\nlearning (FSL) by taking advantage of the capabilities LLMs acquired during\npretraining. Specifically, in very specialized scenarios where large-scale\ntraining data is not available, ZSL / FSL opens new opportunities. This paper\nfollows this recent trend and investigates the potential of leveraging Large\nLanguage Models (LLMs) in such scenarios to automatically detect datasets and\nsoftware within textual content from GitHub repositories. While existing\nmethods focused solely on named entities, this study aims to broaden the scope\nby incorporating resources such as repositories and online hubs where entities\nare also represented by URLs. The study explores different FSL prompt learning\napproaches to enhance the LLMs' ability to identify dataset and software\nmentions within repository texts. Through analyses of LLM effectiveness and\nlearning strategies, this paper offers insights into the potential of advanced\nlanguage models for automated entity detection."
                },
                "authors": [
                    {
                        "name": "Lu Gan"
                    },
                    {
                        "name": "Martin Blum"
                    },
                    {
                        "name": "Danilo Dessi"
                    },
                    {
                        "name": "Brigitte Mathiak"
                    },
                    {
                        "name": "Ralf Schenkel"
                    },
                    {
                        "name": "Stefan Dietze"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Dietze"
                },
                "author": "Stefan Dietze",
                "arxiv_comment": "accepted by KDD2024 workshop DL4KG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04455v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04455v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04437v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04437v1",
                "updated": "2025-01-08T11:37:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    11,
                    37,
                    35,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T11:37:35Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    11,
                    37,
                    35,
                    2,
                    8,
                    0
                ],
                "title": "Integrating LLMs with ITS: Recent Advances, Potentials, Challenges, and\n  Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating LLMs with ITS: Recent Advances, Potentials, Challenges, and\n  Future Directions"
                },
                "summary": "Intelligent Transportation Systems (ITS) are crucial for the development and\noperation of smart cities, addressing key challenges in efficiency,\nproductivity, and environmental sustainability. This paper comprehensively\nreviews the transformative potential of Large Language Models (LLMs) in\noptimizing ITS. Initially, we provide an extensive overview of ITS,\nhighlighting its components, operational principles, and overall effectiveness.\nWe then delve into the theoretical background of various LLM techniques, such\nas GPT, T5, CTRL, and BERT, elucidating their relevance to ITS applications.\nFollowing this, we examine the wide-ranging applications of LLMs within ITS,\nincluding traffic flow prediction, vehicle detection and classification,\nautonomous driving, traffic sign recognition, and pedestrian detection. Our\nanalysis reveals how these advanced models can significantly enhance traffic\nmanagement and safety. Finally, we explore the challenges and limitations LLMs\nface in ITS, such as data availability, computational constraints, and ethical\nconsiderations. We also present several future research directions and\npotential innovations to address these challenges. This paper aims to guide\nresearchers and practitioners through the complexities and opportunities of\nintegrating LLMs in ITS, offering a roadmap to create more efficient,\nsustainable, and responsive next-generation transportation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent Transportation Systems (ITS) are crucial for the development and\noperation of smart cities, addressing key challenges in efficiency,\nproductivity, and environmental sustainability. This paper comprehensively\nreviews the transformative potential of Large Language Models (LLMs) in\noptimizing ITS. Initially, we provide an extensive overview of ITS,\nhighlighting its components, operational principles, and overall effectiveness.\nWe then delve into the theoretical background of various LLM techniques, such\nas GPT, T5, CTRL, and BERT, elucidating their relevance to ITS applications.\nFollowing this, we examine the wide-ranging applications of LLMs within ITS,\nincluding traffic flow prediction, vehicle detection and classification,\nautonomous driving, traffic sign recognition, and pedestrian detection. Our\nanalysis reveals how these advanced models can significantly enhance traffic\nmanagement and safety. Finally, we explore the challenges and limitations LLMs\nface in ITS, such as data availability, computational constraints, and ethical\nconsiderations. We also present several future research directions and\npotential innovations to address these challenges. This paper aims to guide\nresearchers and practitioners through the complexities and opportunities of\nintegrating LLMs in ITS, offering a roadmap to create more efficient,\nsustainable, and responsive next-generation transportation systems."
                },
                "authors": [
                    {
                        "name": "Doaa Mahmud"
                    },
                    {
                        "name": "Hadeel Hajmohamed"
                    },
                    {
                        "name": "Shamma Almentheri"
                    },
                    {
                        "name": "Shamma Alqaydi"
                    },
                    {
                        "name": "Lameya Aldhaheri"
                    },
                    {
                        "name": "Ruhul Amin Khalil"
                    },
                    {
                        "name": "Nasir Saeed"
                    }
                ],
                "author_detail": {
                    "name": "Nasir Saeed"
                },
                "author": "Nasir Saeed",
                "arxiv_comment": "Accepted for publication in IEEE Transactions on Intelligent\n  Transportation Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04437v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04437v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04436v1",
                "updated": "2025-01-08T11:37:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    11,
                    37,
                    6,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T11:37:06Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    11,
                    37,
                    6,
                    2,
                    8,
                    0
                ],
                "title": "Federated Fine-Tuning of LLMs: Framework Comparison and Research\n  Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Fine-Tuning of LLMs: Framework Comparison and Research\n  Directions"
                },
                "summary": "Federated learning (FL) provides a privacy-preserving solution for\nfine-tuning pre-trained large language models (LLMs) using distributed private\ndatasets, enabling task-specific adaptation while preserving data privacy.\nHowever, fine-tuning the extensive parameters in LLMs is particularly\nchallenging in resource-constrained federated scenarios due to the significant\ncommunication and computational costs. To gain a deeper understanding of how\nthese challenges can be addressed, this article conducts a comparative analysis\nthree advanced federated LLM (FedLLM) frameworks that integrate knowledge\ndistillation (KD) and split learning (SL) to mitigate these issues: 1) FedLLMs,\nwhere clients upload model parameters or gradients to enable straightforward\nand effective fine-tuning; 2) KD-FedLLMs, which leverage KD for efficient\nknowledge sharing via logits; and 3) Split-FedLLMs, which split the LLMs into\ntwo parts, with one part executed on the client and the other one on the\nserver, to balance the computational load. Each framework is evaluated based on\nkey performance metrics, including model accuracy, communication overhead, and\nclient-side computational load, offering insights into their effectiveness for\nvarious federated fine-tuning scenarios. Through this analysis, we identify\nframework-specific optimization opportunities to enhance the efficiency of\nFedLLMs and discuss broader research directions, highlighting open\nopportunities to better adapt FedLLMs for real-world applications. A use case\nis presented to demonstrate the performance comparison of these three\nframeworks under varying configurations and settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) provides a privacy-preserving solution for\nfine-tuning pre-trained large language models (LLMs) using distributed private\ndatasets, enabling task-specific adaptation while preserving data privacy.\nHowever, fine-tuning the extensive parameters in LLMs is particularly\nchallenging in resource-constrained federated scenarios due to the significant\ncommunication and computational costs. To gain a deeper understanding of how\nthese challenges can be addressed, this article conducts a comparative analysis\nthree advanced federated LLM (FedLLM) frameworks that integrate knowledge\ndistillation (KD) and split learning (SL) to mitigate these issues: 1) FedLLMs,\nwhere clients upload model parameters or gradients to enable straightforward\nand effective fine-tuning; 2) KD-FedLLMs, which leverage KD for efficient\nknowledge sharing via logits; and 3) Split-FedLLMs, which split the LLMs into\ntwo parts, with one part executed on the client and the other one on the\nserver, to balance the computational load. Each framework is evaluated based on\nkey performance metrics, including model accuracy, communication overhead, and\nclient-side computational load, offering insights into their effectiveness for\nvarious federated fine-tuning scenarios. Through this analysis, we identify\nframework-specific optimization opportunities to enhance the efficiency of\nFedLLMs and discuss broader research directions, highlighting open\nopportunities to better adapt FedLLMs for real-world applications. A use case\nis presented to demonstrate the performance comparison of these three\nframeworks under varying configurations and settings."
                },
                "authors": [
                    {
                        "name": "Na Yan"
                    },
                    {
                        "name": "Yang Su"
                    },
                    {
                        "name": "Yansha Deng"
                    },
                    {
                        "name": "Robert Schober"
                    }
                ],
                "author_detail": {
                    "name": "Robert Schober"
                },
                "author": "Robert Schober",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04431v1",
                "updated": "2025-01-08T11:26:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    11,
                    26,
                    54,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T11:26:54Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    11,
                    26,
                    54,
                    2,
                    8,
                    0
                ],
                "title": "Simultaneous MOKE imaging and measurement of magneto-resistance with\n  vector magnet: a low noise customized setup for low field magnetic devices\n  and thin films characterization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous MOKE imaging and measurement of magneto-resistance with\n  vector magnet: a low noise customized setup for low field magnetic devices\n  and thin films characterization"
                },
                "summary": "Here we report a custom design setup for the simultaneous measurement of\nmagneto-resistance and MOKE imaging in longitudinal configuration for\ncharacterization of magnetic thin-films and sensors. The setup is designed so\nas to cope with a small signal to noise ratio of initial images and to allow\nsensitive magnetoresistance (MR) measurement at low field. An improved\ndifferential algorithm is used to get a good enough contrast of the magnetic\nimages and get rid of beam illumination or small camera gain fluctuations. Home\nmade power supply and pre-amplifier stage were designed so as to reduce the low\nfrequency noise as well as the thermal electrical noise. A vector magnet is\nused to produce rotational magnetic field so as to study the magnetic\nanisotropy and calculate the anisotropic constants in magnetic thin films.\nMagnetoresistive sensors patterned on epitaxial La$_{0.67}$Sr$_{0.33}$MnO$_3$\n(LSMO) thin film have been characterized with this setup. The Images of the\nmagnetization reversal process as well as the local magnetization loops deduced\nfrom these images provided evidence of a magnetic uniaxial anisotropy induced\nby the vicinal substrate. The magnetic anistropic constant of the films was\nthen inferred from the MR measurements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Here we report a custom design setup for the simultaneous measurement of\nmagneto-resistance and MOKE imaging in longitudinal configuration for\ncharacterization of magnetic thin-films and sensors. The setup is designed so\nas to cope with a small signal to noise ratio of initial images and to allow\nsensitive magnetoresistance (MR) measurement at low field. An improved\ndifferential algorithm is used to get a good enough contrast of the magnetic\nimages and get rid of beam illumination or small camera gain fluctuations. Home\nmade power supply and pre-amplifier stage were designed so as to reduce the low\nfrequency noise as well as the thermal electrical noise. A vector magnet is\nused to produce rotational magnetic field so as to study the magnetic\nanisotropy and calculate the anisotropic constants in magnetic thin films.\nMagnetoresistive sensors patterned on epitaxial La$_{0.67}$Sr$_{0.33}$MnO$_3$\n(LSMO) thin film have been characterized with this setup. The Images of the\nmagnetization reversal process as well as the local magnetization loops deduced\nfrom these images provided evidence of a magnetic uniaxial anisotropy induced\nby the vicinal substrate. The magnetic anistropic constant of the films was\nthen inferred from the MR measurements."
                },
                "authors": [
                    {
                        "name": "Imtiaz Noor Bhatti"
                    },
                    {
                        "name": "Ilyas Noor Bhatti"
                    },
                    {
                        "name": "L. G. Enger"
                    },
                    {
                        "name": "P. Victor"
                    },
                    {
                        "name": "B. Guillet"
                    },
                    {
                        "name": "M. Lam Chok Sing"
                    },
                    {
                        "name": "O. Rousseau"
                    },
                    {
                        "name": "V. Pierron"
                    },
                    {
                        "name": "S. Lebargy"
                    },
                    {
                        "name": "J. Camarero"
                    },
                    {
                        "name": "L. Mechin"
                    },
                    {
                        "name": "S. Flament"
                    }
                ],
                "author_detail": {
                    "name": "S. Flament"
                },
                "author": "S. Flament",
                "arxiv_comment": "9 Pages, 9 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11189v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11189v2",
                "updated": "2025-01-08T11:24:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    11,
                    24,
                    17,
                    2,
                    8,
                    0
                ],
                "published": "2024-12-15T13:48:39Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    13,
                    48,
                    39,
                    6,
                    350,
                    0
                ],
                "title": "Leveraging Large Language Models for Active Merchant Non-player\n  Characters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Active Merchant Non-player\n  Characters"
                },
                "summary": "We highlight two significant issues leading to the passivity of current\nmerchant non-player characters (NPCs): pricing and communication. While\nimmersive interactions have been a focus, negotiations between merchant NPCs\nand players on item prices have not received sufficient attention. First, we\ndefine passive pricing as the limited ability of merchants to modify predefined\nitem prices. Second, passive communication means that merchants can only\ninteract with players in a scripted manner. To tackle these issues and create\nan active merchant NPC, we propose a merchant framework based on large language\nmodels (LLMs), called MART, which consists of an appraiser module and a\nnegotiator module. We conducted two experiments to guide game developers in\nselecting appropriate implementations by comparing different training methods\nand LLM sizes. Our findings indicate that finetuning methods, such as\nsupervised finetuning (SFT) and knowledge distillation (KD), are effective in\nusing smaller LLMs to implement active merchant NPCs. Additionally, we found\nthree irregular cases arising from the responses of LLMs. We expect our\nfindings to guide developers in using LLMs for developing active merchant NPCs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We highlight two significant issues leading to the passivity of current\nmerchant non-player characters (NPCs): pricing and communication. While\nimmersive interactions have been a focus, negotiations between merchant NPCs\nand players on item prices have not received sufficient attention. First, we\ndefine passive pricing as the limited ability of merchants to modify predefined\nitem prices. Second, passive communication means that merchants can only\ninteract with players in a scripted manner. To tackle these issues and create\nan active merchant NPC, we propose a merchant framework based on large language\nmodels (LLMs), called MART, which consists of an appraiser module and a\nnegotiator module. We conducted two experiments to guide game developers in\nselecting appropriate implementations by comparing different training methods\nand LLM sizes. Our findings indicate that finetuning methods, such as\nsupervised finetuning (SFT) and knowledge distillation (KD), are effective in\nusing smaller LLMs to implement active merchant NPCs. Additionally, we found\nthree irregular cases arising from the responses of LLMs. We expect our\nfindings to guide developers in using LLMs for developing active merchant NPCs."
                },
                "authors": [
                    {
                        "name": "Byungjun Kim"
                    },
                    {
                        "name": "Minju Kim"
                    },
                    {
                        "name": "Dayeon Seo"
                    },
                    {
                        "name": "Bugeun Kim"
                    }
                ],
                "author_detail": {
                    "name": "Bugeun Kim"
                },
                "author": "Bugeun Kim",
                "arxiv_comment": "Under review / Modified the links to code and dataset",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11189v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11189v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10587v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10587v3",
                "updated": "2025-01-08T11:21:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    11,
                    21,
                    12,
                    2,
                    8,
                    0
                ],
                "published": "2024-05-17T07:22:02Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    7,
                    22,
                    2,
                    4,
                    138,
                    0
                ],
                "title": "RDRec: Rationale Distillation for LLM-based Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RDRec: Rationale Distillation for LLM-based Recommendation"
                },
                "summary": "Large language model (LLM)-based recommender models that bridge users and\nitems through textual prompts for effective semantic reasoning have gained\nconsiderable attention. However, few methods consider the underlying rationales\nbehind interactions, such as user preferences and item attributes, limiting the\nreasoning capability of LLMs for recommendations. This paper proposes a\nrationale distillation recommender (RDRec), a compact model designed to learn\nrationales generated by a larger language model (LM). By leveraging rationales\nfrom reviews related to users and items, RDRec remarkably specifies their\nprofiles for recommendations. Experiments show that RDRec achieves\nstate-of-the-art (SOTA) performance in both top-N and sequential\nrecommendations. Our source code is released at\nhttps://github.com/WangXFng/RDRec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM)-based recommender models that bridge users and\nitems through textual prompts for effective semantic reasoning have gained\nconsiderable attention. However, few methods consider the underlying rationales\nbehind interactions, such as user preferences and item attributes, limiting the\nreasoning capability of LLMs for recommendations. This paper proposes a\nrationale distillation recommender (RDRec), a compact model designed to learn\nrationales generated by a larger language model (LM). By leveraging rationales\nfrom reviews related to users and items, RDRec remarkably specifies their\nprofiles for recommendations. Experiments show that RDRec achieves\nstate-of-the-art (SOTA) performance in both top-N and sequential\nrecommendations. Our source code is released at\nhttps://github.com/WangXFng/RDRec."
                },
                "authors": [
                    {
                        "name": "Xinfeng Wang"
                    },
                    {
                        "name": "Jin Cui"
                    },
                    {
                        "name": "Yoshimi Suzuki"
                    },
                    {
                        "name": "Fumiyo Fukumoto"
                    }
                ],
                "author_detail": {
                    "name": "Fumiyo Fukumoto"
                },
                "author": "Fumiyo Fukumoto",
                "arxiv_comment": "10 pages. Accepted to ACL 2024 Main as a short paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10587v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10587v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04425v1",
                "updated": "2025-01-08T11:18:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    11,
                    18,
                    36,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T11:18:36Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    11,
                    18,
                    36,
                    2,
                    8,
                    0
                ],
                "title": "End-to-End Bangla AI for Solving Math Olympiad Problem Benchmark:\n  Leveraging Large Language Model Using Integrated Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End Bangla AI for Solving Math Olympiad Problem Benchmark:\n  Leveraging Large Language Model Using Integrated Approach"
                },
                "summary": "This work introduces systematic approach for enhancing large language models\n(LLMs) to address Bangla AI mathematical challenges. Through the assessment of\ndiverse LLM configurations, fine-tuning with specific datasets, and the\nimplementation of Retrieval-Augmented Generation (RAG), we enhanced the model's\nreasoning precision in a multilingual setting. Crucial discoveries indicate\nthat customized prompting, dataset augmentation, and iterative reasoning\nimprove the model's efficiency regarding Olympiad-level mathematical\nchallenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work introduces systematic approach for enhancing large language models\n(LLMs) to address Bangla AI mathematical challenges. Through the assessment of\ndiverse LLM configurations, fine-tuning with specific datasets, and the\nimplementation of Retrieval-Augmented Generation (RAG), we enhanced the model's\nreasoning precision in a multilingual setting. Crucial discoveries indicate\nthat customized prompting, dataset augmentation, and iterative reasoning\nimprove the model's efficiency regarding Olympiad-level mathematical\nchallenges."
                },
                "authors": [
                    {
                        "name": "H. M. Shadman Tabib"
                    },
                    {
                        "name": "Jaber Ahmed Deedar"
                    }
                ],
                "author_detail": {
                    "name": "Jaber Ahmed Deedar"
                },
                "author": "Jaber Ahmed Deedar",
                "arxiv_doi": "10.5121/ijnlc.2024.13604",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5121/ijnlc.2024.13604",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.04425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IJNLC,vol:13, Issue:5/6, page 49-59,2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04420v1",
                "updated": "2025-01-08T11:08:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    11,
                    8,
                    58,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T11:08:58Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    11,
                    8,
                    58,
                    2,
                    8,
                    0
                ],
                "title": "A Closer Look on Gender Stereotypes in Movie Recommender Systems and\n  Their Implications with Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Closer Look on Gender Stereotypes in Movie Recommender Systems and\n  Their Implications with Privacy"
                },
                "summary": "The movie recommender system typically leverages user feedback to provide\npersonalized recommendations that align with user preferences and increase\nbusiness revenue. This study investigates the impact of gender stereotypes on\nsuch systems through a specific attack scenario. In this scenario, an attacker\ndetermines users' gender, a private attribute, by exploiting gender stereotypes\nabout movie preferences and analyzing users' feedback data, which is either\npublicly available or observed within the system. The study consists of two\nphases. In the first phase, a user study involving 630 participants identified\ngender stereotypes associated with movie genres, which often influence viewing\nchoices. In the second phase, four inference algorithms were applied to detect\ngender stereotypes by combining the findings from the first phase with users'\nfeedback data. Results showed that these algorithms performed more effectively\nthan relying solely on feedback data for gender inference. Additionally, we\nquantified the extent of gender stereotypes to evaluate their broader impact on\ndigital computational science. The latter part of the study utilized two major\nmovie recommender datasets: MovieLens 1M and Yahoo!Movie. Detailed experimental\ninformation is available on our GitHub repository:\nhttps://github.com/fr-iit/GSMRS",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The movie recommender system typically leverages user feedback to provide\npersonalized recommendations that align with user preferences and increase\nbusiness revenue. This study investigates the impact of gender stereotypes on\nsuch systems through a specific attack scenario. In this scenario, an attacker\ndetermines users' gender, a private attribute, by exploiting gender stereotypes\nabout movie preferences and analyzing users' feedback data, which is either\npublicly available or observed within the system. The study consists of two\nphases. In the first phase, a user study involving 630 participants identified\ngender stereotypes associated with movie genres, which often influence viewing\nchoices. In the second phase, four inference algorithms were applied to detect\ngender stereotypes by combining the findings from the first phase with users'\nfeedback data. Results showed that these algorithms performed more effectively\nthan relying solely on feedback data for gender inference. Additionally, we\nquantified the extent of gender stereotypes to evaluate their broader impact on\ndigital computational science. The latter part of the study utilized two major\nmovie recommender datasets: MovieLens 1M and Yahoo!Movie. Detailed experimental\ninformation is available on our GitHub repository:\nhttps://github.com/fr-iit/GSMRS"
                },
                "authors": [
                    {
                        "name": "Falguni Roy"
                    },
                    {
                        "name": "Yiduo Shen"
                    },
                    {
                        "name": "Na Zhao"
                    },
                    {
                        "name": "Xiaofeng Ding"
                    },
                    {
                        "name": "Md. Omar Faruk"
                    }
                ],
                "author_detail": {
                    "name": "Md. Omar Faruk"
                },
                "author": "Md. Omar Faruk",
                "arxiv_comment": "19 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03256v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03256v2",
                "updated": "2025-01-08T11:05:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    11,
                    5,
                    4,
                    2,
                    8,
                    0
                ],
                "published": "2024-11-05T16:54:47Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    16,
                    54,
                    47,
                    1,
                    310,
                    0
                ],
                "title": "How the StarDICE photometric calibration of standard stars can improve\n  cosmological constraints?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How the StarDICE photometric calibration of standard stars can improve\n  cosmological constraints?"
                },
                "summary": "The number of type Ia supernova (SNe Ia) observations will grow significantly\nwithin the next decade, mainly thanks to the Legacy Survey of Space and Time\n(LSST) undertaken by the Vera Rubin Observatory in Chile. With this\nimprovement, statistical uncertainties will decrease, and flux calibration will\nbecome the main uncertainty for the characterization of dark energy. Currently,\nthe astronomical flux scale is anchored on the numerical models of white dwarf\natmospheres from the CALSPEC catalog, and every error on the model can induce a\nbias over cosmological parameters inference. The StarDICE experiment proposes a\nnew calibration reference that only relies on observations from the optical\nwatt defined by the NIST towards the magnitude of standard stars. It is\ncurrently operating at l'Observatoire de Haute-Provence and has been collecting\ndata since the beginning of 2023. To overcome the photometric calibration\nuncertainty and reach a sub-percent precision, the instrument throughput has\nbeen calibrated with a Collimated Beam Projector. It will be monitored on-site\nwith a LED-based artificial star source calibrated with NIST photodiodes. In\nthis proceeding, we will first illustrate how an error in the photometric\ncalibration can impact the SNe Ia distance moduli and thus bias the measurement\nof cosmological parameters. Then we will present the StarDICE experiment and\nhow we can recalibrate the CALSPEC catalog at the millimagnitude level on the\nNIST scale with photometric analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The number of type Ia supernova (SNe Ia) observations will grow significantly\nwithin the next decade, mainly thanks to the Legacy Survey of Space and Time\n(LSST) undertaken by the Vera Rubin Observatory in Chile. With this\nimprovement, statistical uncertainties will decrease, and flux calibration will\nbecome the main uncertainty for the characterization of dark energy. Currently,\nthe astronomical flux scale is anchored on the numerical models of white dwarf\natmospheres from the CALSPEC catalog, and every error on the model can induce a\nbias over cosmological parameters inference. The StarDICE experiment proposes a\nnew calibration reference that only relies on observations from the optical\nwatt defined by the NIST towards the magnitude of standard stars. It is\ncurrently operating at l'Observatoire de Haute-Provence and has been collecting\ndata since the beginning of 2023. To overcome the photometric calibration\nuncertainty and reach a sub-percent precision, the instrument throughput has\nbeen calibrated with a Collimated Beam Projector. It will be monitored on-site\nwith a LED-based artificial star source calibrated with NIST photodiodes. In\nthis proceeding, we will first illustrate how an error in the photometric\ncalibration can impact the SNe Ia distance moduli and thus bias the measurement\nof cosmological parameters. Then we will present the StarDICE experiment and\nhow we can recalibrate the CALSPEC catalog at the millimagnitude level on the\nNIST scale with photometric analysis."
                },
                "authors": [
                    {
                        "name": "T. Souverin"
                    },
                    {
                        "name": "J. Neveu"
                    },
                    {
                        "name": "M. Betoule"
                    },
                    {
                        "name": "S. Bongard"
                    },
                    {
                        "name": "P. E. Blanc"
                    },
                    {
                        "name": "J. Cohen Tanugi"
                    },
                    {
                        "name": "S. Dagoret-Campagne"
                    },
                    {
                        "name": "F. Feinstein"
                    },
                    {
                        "name": "M. Ferrari"
                    },
                    {
                        "name": "F. Hazenberg"
                    },
                    {
                        "name": "C. Juramy"
                    },
                    {
                        "name": "L. Le Guillou"
                    },
                    {
                        "name": "A. Le Van Suu"
                    },
                    {
                        "name": "M. Moniez"
                    },
                    {
                        "name": "E. Nuss"
                    },
                    {
                        "name": "B. Plez"
                    },
                    {
                        "name": "N. Regnault"
                    },
                    {
                        "name": "E. Sepulveda"
                    },
                    {
                        "name": "K. Sommer"
                    }
                ],
                "author_detail": {
                    "name": "K. Sommer"
                },
                "author": "K. Sommer",
                "arxiv_comment": "4 pages, 3 figures, contribution to the 2024 Cosmology session of the\n  58th Rencontres de Moriond",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03256v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03256v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.14520v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.14520v4",
                "updated": "2025-01-08T11:03:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    11,
                    3,
                    0,
                    2,
                    8,
                    0
                ],
                "published": "2024-03-21T16:17:57Z",
                "published_parsed": [
                    2024,
                    3,
                    21,
                    16,
                    17,
                    57,
                    3,
                    81,
                    0
                ],
                "title": "Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient\n  Inference"
                },
                "summary": "In recent years, the application of multimodal large language models (MLLM)\nin various fields has achieved remarkable success. However, as the foundation\nmodel for many downstream tasks, current MLLMs are composed of the well-known\nTransformer network, which has a less efficient quadratic computation\ncomplexity. To improve the efficiency of such basic models, we propose Cobra, a\nlinear computational complexity MLLM. Specifically, Cobra integrates the\nefficient Mamba language model into the visual modality. Moreover, we explore\nand study various modal fusion schemes to create an effective multi-modal\nMamba. Extensive experiments demonstrate that (1) Cobra achieves extremely\ncompetitive performance with current computationally efficient state-of-the-art\nmethods, e.g., LLaVA-Phi, TinyLLaVA, and MobileVLM v2, and has faster speed due\nto Cobra's linear sequential modeling. (2) Interestingly, the results of\nclosed-set challenging prediction benchmarks show that Cobra performs well in\novercoming visual illusions and spatial relationship judgments. (3) Notably,\nCobra even achieves comparable performance to LLaVA with about 43% of the\nnumber of parameters. We will make all codes of Cobra open-source and hope that\nthe proposed method can facilitate future research on complexity problems in\nMLLM. Our project page is available at: https://sites.google.com/view/cobravlm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the application of multimodal large language models (MLLM)\nin various fields has achieved remarkable success. However, as the foundation\nmodel for many downstream tasks, current MLLMs are composed of the well-known\nTransformer network, which has a less efficient quadratic computation\ncomplexity. To improve the efficiency of such basic models, we propose Cobra, a\nlinear computational complexity MLLM. Specifically, Cobra integrates the\nefficient Mamba language model into the visual modality. Moreover, we explore\nand study various modal fusion schemes to create an effective multi-modal\nMamba. Extensive experiments demonstrate that (1) Cobra achieves extremely\ncompetitive performance with current computationally efficient state-of-the-art\nmethods, e.g., LLaVA-Phi, TinyLLaVA, and MobileVLM v2, and has faster speed due\nto Cobra's linear sequential modeling. (2) Interestingly, the results of\nclosed-set challenging prediction benchmarks show that Cobra performs well in\novercoming visual illusions and spatial relationship judgments. (3) Notably,\nCobra even achieves comparable performance to LLaVA with about 43% of the\nnumber of parameters. We will make all codes of Cobra open-source and hope that\nthe proposed method can facilitate future research on complexity problems in\nMLLM. Our project page is available at: https://sites.google.com/view/cobravlm."
                },
                "authors": [
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Pengxiang Ding"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Donglin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Donglin Wang"
                },
                "author": "Donglin Wang",
                "arxiv_comment": "Accepted to the Thirty-Ninth AAAI Conference on Artificial\n  Intelligence (AAAI-25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.14520v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.14520v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03535v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03535v2",
                "updated": "2025-01-08T10:34:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    34,
                    54,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-07T05:15:46Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    5,
                    15,
                    46,
                    1,
                    7,
                    0
                ],
                "title": "SenseRAG: Constructing Environmental Knowledge Bases with Proactive\n  Querying for LLM-Based Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SenseRAG: Constructing Environmental Knowledge Bases with Proactive\n  Querying for LLM-Based Autonomous Driving"
                },
                "summary": "This study addresses the critical need for enhanced situational awareness in\nautonomous driving (AD) by leveraging the contextual reasoning capabilities of\nlarge language models (LLMs). Unlike traditional perception systems that rely\non rigid, label-based annotations, it integrates real-time, multimodal sensor\ndata into a unified, LLMs-readable knowledge base, enabling LLMs to dynamically\nunderstand and respond to complex driving environments. To overcome the\ninherent latency and modality limitations of LLMs, a proactive\nRetrieval-Augmented Generation (RAG) is designed for AD, combined with a\nchain-of-thought prompting mechanism, ensuring rapid and context-rich\nunderstanding. Experimental results using real-world Vehicle-to-everything\n(V2X) datasets demonstrate significant improvements in perception and\nprediction performance, highlighting the potential of this framework to enhance\nsafety, adaptability, and decision-making in next-generation AD systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study addresses the critical need for enhanced situational awareness in\nautonomous driving (AD) by leveraging the contextual reasoning capabilities of\nlarge language models (LLMs). Unlike traditional perception systems that rely\non rigid, label-based annotations, it integrates real-time, multimodal sensor\ndata into a unified, LLMs-readable knowledge base, enabling LLMs to dynamically\nunderstand and respond to complex driving environments. To overcome the\ninherent latency and modality limitations of LLMs, a proactive\nRetrieval-Augmented Generation (RAG) is designed for AD, combined with a\nchain-of-thought prompting mechanism, ensuring rapid and context-rich\nunderstanding. Experimental results using real-world Vehicle-to-everything\n(V2X) datasets demonstrate significant improvements in perception and\nprediction performance, highlighting the potential of this framework to enhance\nsafety, adaptability, and decision-making in next-generation AD systems."
                },
                "authors": [
                    {
                        "name": "Xuewen Luo"
                    },
                    {
                        "name": "Fan Ding"
                    },
                    {
                        "name": "Fengze Yang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Junnyong Loo"
                    },
                    {
                        "name": "Hwa Hui Tew"
                    },
                    {
                        "name": "Chenxi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chenxi Liu"
                },
                "author": "Chenxi Liu",
                "arxiv_comment": "This paper has been accepted for presentation at WACV Workshop LLMAD\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03535v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03535v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04400v1",
                "updated": "2025-01-08T10:28:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    28,
                    31,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T10:28:31Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    28,
                    31,
                    2,
                    8,
                    0
                ],
                "title": "Non-intrusive reduced-order modeling for dynamical systems with\n  spatially localized features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-intrusive reduced-order modeling for dynamical systems with\n  spatially localized features"
                },
                "summary": "This work presents a non-intrusive reduced-order modeling framework for\ndynamical systems with spatially localized features characterized by slow\nsingular value decay. The proposed approach builds upon two existing\nmethodologies for reduced and full-order non-intrusive modeling, namely\nOperator Inference (OpInf) and sparse Full-Order Model (sFOM) inference. We\ndecompose the domain into two complementary subdomains which exhibit fast and\nslow singular value decay. The dynamics of the subdomain exhibiting slow\nsingular value decay are learned with sFOM while the dynamics with\nintrinsically low dimensionality on the complementary subdomain are learned\nwith OpInf. The resulting, coupled OpInf-sFOM formulation leverages the\ncomputational efficiency of OpInf and the high resolution of sFOM, and thus\nenables fast non-intrusive predictions for conditions beyond those sampled in\nthe training data set. A novel regularization technique with a closed-form\nsolution based on the Gershgorin disk theorem is introduced to promote stable\nsFOM and OpInf models. We also provide a data-driven indicator for the\nsubdomain selection and ensure solution smoothness over the interface via a\npost-processing interpolation step. We evaluate the efficiency of the approach\nin terms of offline and online speedup through a quantitative, parametric\ncomputational cost analysis. We demonstrate the coupled OpInf-sFOM formulation\nfor two test cases: a one-dimensional Burgers' model for which accurate\npredictions beyond the span of the training snapshots are presented, and a\ntwo-dimensional parametric model for the Pine Island Glacier ice thickness\ndynamics, for which the OpInf-sFOM model achieves an average prediction error\non the order of $1 \\%$ with an online speedup factor of approximately $8\\times$\ncompared to the numerical simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a non-intrusive reduced-order modeling framework for\ndynamical systems with spatially localized features characterized by slow\nsingular value decay. The proposed approach builds upon two existing\nmethodologies for reduced and full-order non-intrusive modeling, namely\nOperator Inference (OpInf) and sparse Full-Order Model (sFOM) inference. We\ndecompose the domain into two complementary subdomains which exhibit fast and\nslow singular value decay. The dynamics of the subdomain exhibiting slow\nsingular value decay are learned with sFOM while the dynamics with\nintrinsically low dimensionality on the complementary subdomain are learned\nwith OpInf. The resulting, coupled OpInf-sFOM formulation leverages the\ncomputational efficiency of OpInf and the high resolution of sFOM, and thus\nenables fast non-intrusive predictions for conditions beyond those sampled in\nthe training data set. A novel regularization technique with a closed-form\nsolution based on the Gershgorin disk theorem is introduced to promote stable\nsFOM and OpInf models. We also provide a data-driven indicator for the\nsubdomain selection and ensure solution smoothness over the interface via a\npost-processing interpolation step. We evaluate the efficiency of the approach\nin terms of offline and online speedup through a quantitative, parametric\ncomputational cost analysis. We demonstrate the coupled OpInf-sFOM formulation\nfor two test cases: a one-dimensional Burgers' model for which accurate\npredictions beyond the span of the training snapshots are presented, and a\ntwo-dimensional parametric model for the Pine Island Glacier ice thickness\ndynamics, for which the OpInf-sFOM model achieves an average prediction error\non the order of $1 \\%$ with an online speedup factor of approximately $8\\times$\ncompared to the numerical simulation."
                },
                "authors": [
                    {
                        "name": "Leonidas Gkimisis"
                    },
                    {
                        "name": "Nicole Aretz"
                    },
                    {
                        "name": "Marco Tezzele"
                    },
                    {
                        "name": "Thomas Richter"
                    },
                    {
                        "name": "Peter Benner"
                    },
                    {
                        "name": "Karen E. Willcox"
                    }
                ],
                "author_detail": {
                    "name": "Karen E. Willcox"
                },
                "author": "Karen E. Willcox",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04393v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04393v1",
                "updated": "2025-01-08T10:10:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    10,
                    29,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T10:10:29Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    10,
                    29,
                    2,
                    8,
                    0
                ],
                "title": "SEO: Stochastic Experience Optimization for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEO: Stochastic Experience Optimization for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) can benefit from useful experiences to improve\ntheir performance on specific tasks. However, finding helpful experiences for\ndifferent LLMs is not obvious, since it is unclear what experiences suit\nspecific LLMs. Previous studies intended to automatically find useful\nexperiences using LLMs, while it is difficult to ensure the effectiveness of\nthe obtained experience. In this paper, we propose Stochastic Experience\nOptimization (SEO), an iterative approach that finds optimized model-specific\nexperience without modifying model parameters through experience update in\nnatural language. In SEO, we propose a stochastic validation method to ensure\nthe update direction of experience, avoiding unavailing updates. Experimental\nresults on three tasks for three LLMs demonstrate that experiences optimized by\nSEO can achieve consistently improved performance. Further analysis indicates\nthat SEO-optimized experience can generalize to out-of-distribution data,\nboosting the performance of LLMs on similar tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can benefit from useful experiences to improve\ntheir performance on specific tasks. However, finding helpful experiences for\ndifferent LLMs is not obvious, since it is unclear what experiences suit\nspecific LLMs. Previous studies intended to automatically find useful\nexperiences using LLMs, while it is difficult to ensure the effectiveness of\nthe obtained experience. In this paper, we propose Stochastic Experience\nOptimization (SEO), an iterative approach that finds optimized model-specific\nexperience without modifying model parameters through experience update in\nnatural language. In SEO, we propose a stochastic validation method to ensure\nthe update direction of experience, avoiding unavailing updates. Experimental\nresults on three tasks for three LLMs demonstrate that experiences optimized by\nSEO can achieve consistently improved performance. Further analysis indicates\nthat SEO-optimized experience can generalize to out-of-distribution data,\nboosting the performance of LLMs on similar tasks."
                },
                "authors": [
                    {
                        "name": "Jitao Xu"
                    },
                    {
                        "name": "Hongyun Zhou"
                    },
                    {
                        "name": "Lei Shen"
                    },
                    {
                        "name": "Conghui Zhu"
                    },
                    {
                        "name": "Jin Huang"
                    },
                    {
                        "name": "Yitao Duan"
                    }
                ],
                "author_detail": {
                    "name": "Yitao Duan"
                },
                "author": "Yitao Duan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04393v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.13789v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.13789v3",
                "updated": "2025-01-08T10:01:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    1,
                    20,
                    2,
                    8,
                    0
                ],
                "published": "2023-12-21T12:26:11Z",
                "published_parsed": [
                    2023,
                    12,
                    21,
                    12,
                    26,
                    11,
                    3,
                    355,
                    0
                ],
                "title": "TinySAM: Pushing the Envelope for Efficient Segment Anything Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinySAM: Pushing the Envelope for Efficient Segment Anything Model"
                },
                "summary": "Recently segment anything model (SAM) has shown powerful segmentation\ncapability and has drawn great attention in computer vision fields. Massive\nfollowing works have developed various applications based on the pre-trained\nSAM and achieved impressive performance on downstream vision tasks. However,\nSAM consists of heavy architectures and requires massive computational\ncapacity, which hinders the further application of SAM on computation\nconstrained edge devices. To this end, in this paper we propose a framework to\nobtain a tiny segment anything model (TinySAM) while maintaining the strong\nzero-shot performance. We first propose a full-stage knowledge distillation\nmethod with hard prompt sampling and hard mask weighting strategy to distill a\nlightweight student model. We also adapt the post-training quantization to the\nprompt-based segmentation task and further reduce the computational cost.\nMoreover, a hierarchical segmenting everything strategy is proposed to\naccelerate the everything inference by $2\\times$ with almost no performance\ndegradation. With all these proposed methods, our TinySAM leads to orders of\nmagnitude computational reduction and pushes the envelope for efficient segment\nanything task. Extensive experiments on various zero-shot transfer tasks\ndemonstrate the significantly advantageous performance of our TinySAM against\ncounterpart methods. Codes are available at\nhttps://github.com/xinghaochen/TinySAM and\nhttps://gitee.com/mindspore/models/tree/master/research/cv/TinySAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently segment anything model (SAM) has shown powerful segmentation\ncapability and has drawn great attention in computer vision fields. Massive\nfollowing works have developed various applications based on the pre-trained\nSAM and achieved impressive performance on downstream vision tasks. However,\nSAM consists of heavy architectures and requires massive computational\ncapacity, which hinders the further application of SAM on computation\nconstrained edge devices. To this end, in this paper we propose a framework to\nobtain a tiny segment anything model (TinySAM) while maintaining the strong\nzero-shot performance. We first propose a full-stage knowledge distillation\nmethod with hard prompt sampling and hard mask weighting strategy to distill a\nlightweight student model. We also adapt the post-training quantization to the\nprompt-based segmentation task and further reduce the computational cost.\nMoreover, a hierarchical segmenting everything strategy is proposed to\naccelerate the everything inference by $2\\times$ with almost no performance\ndegradation. With all these proposed methods, our TinySAM leads to orders of\nmagnitude computational reduction and pushes the envelope for efficient segment\nanything task. Extensive experiments on various zero-shot transfer tasks\ndemonstrate the significantly advantageous performance of our TinySAM against\ncounterpart methods. Codes are available at\nhttps://github.com/xinghaochen/TinySAM and\nhttps://gitee.com/mindspore/models/tree/master/research/cv/TinySAM."
                },
                "authors": [
                    {
                        "name": "Han Shu"
                    },
                    {
                        "name": "Wenshuo Li"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Yiman Zhang"
                    },
                    {
                        "name": "Yihao Chen"
                    },
                    {
                        "name": "Houqiang Li"
                    },
                    {
                        "name": "Yunhe Wang"
                    },
                    {
                        "name": "Xinghao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xinghao Chen"
                },
                "author": "Xinghao Chen",
                "arxiv_comment": "AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.13789v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.13789v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01508v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01508v2",
                "updated": "2025-01-08T09:52:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    9,
                    52,
                    29,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-02T19:30:53Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    19,
                    30,
                    53,
                    3,
                    2,
                    0
                ],
                "title": "Garbage in Garbage out: Impacts of data quality on criminal network\n  intervention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Garbage in Garbage out: Impacts of data quality on criminal network\n  intervention"
                },
                "summary": "Criminal networks such as human trafficking rings are threats to the rule of\nlaw, democracy and public safety in our global society. Network science\nprovides invaluable tools to identify key players and design interventions for\nLaw Enforcement Agencies (LEAs), e.g., to dismantle their organisation.\nHowever, poor data quality and the adaptiveness of criminal networks through\nself-organization make effective disruption extremely challenging. Although\nthere exists a large body of work building and applying network scientific\ntools to attack criminal networks, these work often implicitly assume that the\nnetwork measurements are accurate and complete. Moreover, there is thus far no\ncomprehensive understanding of the impacts of data quality on the downstream\neffectiveness of interventions. This work investigates the relationship between\ndata quality and intervention effectiveness based on classical graph theoretic\nand machine learning-based approaches. Decentralization emerges as a major\nfactor in network robustness, particularly under conditions of incomplete data,\nwhich renders attack strategies largely ineffective. Moreover, the robustness\nof centralized networks can be boosted using simple heuristics, making targeted\nattack more infeasible. Consequently, we advocate for a more cautious\napplication of network science in disrupting criminal networks, the continuous\ndevelopment of an interoperable intelligence ecosystem, and the creation of\nnovel network inference techniques to address data quality challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Criminal networks such as human trafficking rings are threats to the rule of\nlaw, democracy and public safety in our global society. Network science\nprovides invaluable tools to identify key players and design interventions for\nLaw Enforcement Agencies (LEAs), e.g., to dismantle their organisation.\nHowever, poor data quality and the adaptiveness of criminal networks through\nself-organization make effective disruption extremely challenging. Although\nthere exists a large body of work building and applying network scientific\ntools to attack criminal networks, these work often implicitly assume that the\nnetwork measurements are accurate and complete. Moreover, there is thus far no\ncomprehensive understanding of the impacts of data quality on the downstream\neffectiveness of interventions. This work investigates the relationship between\ndata quality and intervention effectiveness based on classical graph theoretic\nand machine learning-based approaches. Decentralization emerges as a major\nfactor in network robustness, particularly under conditions of incomplete data,\nwhich renders attack strategies largely ineffective. Moreover, the robustness\nof centralized networks can be boosted using simple heuristics, making targeted\nattack more infeasible. Consequently, we advocate for a more cautious\napplication of network science in disrupting criminal networks, the continuous\ndevelopment of an interoperable intelligence ecosystem, and the creation of\nnovel network inference techniques to address data quality challenges."
                },
                "authors": [
                    {
                        "name": "Wang Ngai Yeung"
                    },
                    {
                        "name": "Riccardo Di Clemente"
                    },
                    {
                        "name": "Renaud Lambiotte"
                    }
                ],
                "author_detail": {
                    "name": "Renaud Lambiotte"
                },
                "author": "Renaud Lambiotte",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01508v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01508v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.4; J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15209v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15209v3",
                "updated": "2025-01-08T09:29:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    9,
                    29,
                    10,
                    2,
                    8,
                    0
                ],
                "published": "2024-03-22T13:50:27Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    13,
                    50,
                    27,
                    4,
                    82,
                    0
                ],
                "title": "MSCoTDet: Language-driven Multi-modal Fusion for Improved Multispectral\n  Pedestrian Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSCoTDet: Language-driven Multi-modal Fusion for Improved Multispectral\n  Pedestrian Detection"
                },
                "summary": "Multispectral pedestrian detection is attractive for around-the-clock\napplications due to the complementary information between RGB and thermal\nmodalities. However, current models often fail to detect pedestrians in certain\ncases (e.g., thermal-obscured pedestrians), particularly due to the modality\nbias learned from statistically biased datasets. In this paper, we investigate\nhow to mitigate modality bias in multispectral pedestrian detection using Large\nLanguage Models (LLMs). Accordingly, we design a Multispectral Chain-of-Thought\n(MSCoT) prompting strategy, which prompts the LLM to perform multispectral\npedestrian detection. Moreover, we propose a novel Multispectral\nChain-of-Thought Detection (MSCoTDet) framework that integrates MSCoT prompting\ninto multispectral pedestrian detection. To this end, we design a\nLanguage-driven Multi-modal Fusion (LMF) strategy that enables fusing the\noutputs of MSCoT prompting with the detection results of vision-based\nmultispectral pedestrian detection models. Extensive experiments validate that\nMSCoTDet effectively mitigates modality biases and improves multispectral\npedestrian detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multispectral pedestrian detection is attractive for around-the-clock\napplications due to the complementary information between RGB and thermal\nmodalities. However, current models often fail to detect pedestrians in certain\ncases (e.g., thermal-obscured pedestrians), particularly due to the modality\nbias learned from statistically biased datasets. In this paper, we investigate\nhow to mitigate modality bias in multispectral pedestrian detection using Large\nLanguage Models (LLMs). Accordingly, we design a Multispectral Chain-of-Thought\n(MSCoT) prompting strategy, which prompts the LLM to perform multispectral\npedestrian detection. Moreover, we propose a novel Multispectral\nChain-of-Thought Detection (MSCoTDet) framework that integrates MSCoT prompting\ninto multispectral pedestrian detection. To this end, we design a\nLanguage-driven Multi-modal Fusion (LMF) strategy that enables fusing the\noutputs of MSCoT prompting with the detection results of vision-based\nmultispectral pedestrian detection models. Extensive experiments validate that\nMSCoTDet effectively mitigates modality biases and improves multispectral\npedestrian detection."
                },
                "authors": [
                    {
                        "name": "Taeheon Kim"
                    },
                    {
                        "name": "Sangyun Chung"
                    },
                    {
                        "name": "Damin Yeom"
                    },
                    {
                        "name": "Youngjoon Yu"
                    },
                    {
                        "name": "Hak Gu Kim"
                    },
                    {
                        "name": "Yong Man Ro"
                    }
                ],
                "author_detail": {
                    "name": "Yong Man Ro"
                },
                "author": "Yong Man Ro",
                "arxiv_doi": "10.1109/TCSVT.2024.3524645",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TCSVT.2024.3524645",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.15209v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15209v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "IEEE Transactions on Circuits and Systems for Video Technology\n  (TCSVT)",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15267v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15267v2",
                "updated": "2025-01-08T09:18:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    9,
                    18,
                    5,
                    2,
                    8,
                    0
                ],
                "published": "2024-12-17T05:04:57Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    4,
                    57,
                    1,
                    352,
                    0
                ],
                "title": "Toxicity Detection towards Adaptability to Changing Perturbations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toxicity Detection towards Adaptability to Changing Perturbations"
                },
                "summary": "Toxicity detection is crucial for maintaining the peace of the society. While\nexisting methods perform well on normal toxic contents or those generated by\nspecific perturbation methods, they are vulnerable to evolving perturbation\npatterns. However, in real-world scenarios, malicious users tend to create new\nperturbation patterns for fooling the detectors. For example, some users may\ncircumvent the detector of large language models (LLMs) by adding `I am a\nscientist' at the beginning of the prompt. In this paper, we introduce a novel\nproblem, i.e., continual learning jailbreak perturbation patterns, into the\ntoxicity detection field. To tackle this problem, we first construct a new\ndataset generated by 9 types of perturbation patterns, 7 of them are summarized\nfrom prior work and 2 of them are developed by us. We then systematically\nvalidate the vulnerability of current methods on this new perturbation\npattern-aware dataset via both the zero-shot and fine tuned cross-pattern\ndetection. Upon this, we present the domain incremental learning paradigm and\nthe corresponding benchmark to ensure the detector's robustness to dynamically\nemerging types of perturbed toxic text. Our code and dataset are provided in\nthe appendix and will be publicly available at GitHub, by which we wish to\noffer new research opportunities for the security-relevant communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toxicity detection is crucial for maintaining the peace of the society. While\nexisting methods perform well on normal toxic contents or those generated by\nspecific perturbation methods, they are vulnerable to evolving perturbation\npatterns. However, in real-world scenarios, malicious users tend to create new\nperturbation patterns for fooling the detectors. For example, some users may\ncircumvent the detector of large language models (LLMs) by adding `I am a\nscientist' at the beginning of the prompt. In this paper, we introduce a novel\nproblem, i.e., continual learning jailbreak perturbation patterns, into the\ntoxicity detection field. To tackle this problem, we first construct a new\ndataset generated by 9 types of perturbation patterns, 7 of them are summarized\nfrom prior work and 2 of them are developed by us. We then systematically\nvalidate the vulnerability of current methods on this new perturbation\npattern-aware dataset via both the zero-shot and fine tuned cross-pattern\ndetection. Upon this, we present the domain incremental learning paradigm and\nthe corresponding benchmark to ensure the detector's robustness to dynamically\nemerging types of perturbed toxic text. Our code and dataset are provided in\nthe appendix and will be publicly available at GitHub, by which we wish to\noffer new research opportunities for the security-relevant communities."
                },
                "authors": [
                    {
                        "name": "Hankun Kang"
                    },
                    {
                        "name": "Jianhao Chen"
                    },
                    {
                        "name": "Yongqi Li"
                    },
                    {
                        "name": "Xin Miao"
                    },
                    {
                        "name": "Mayi Xu"
                    },
                    {
                        "name": "Ming Zhong"
                    },
                    {
                        "name": "Yuanyuan Zhu"
                    },
                    {
                        "name": "Tieyun Qian"
                    }
                ],
                "author_detail": {
                    "name": "Tieyun Qian"
                },
                "author": "Tieyun Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15267v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15267v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19549v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19549v2",
                "updated": "2025-01-08T08:30:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    8,
                    30,
                    3,
                    2,
                    8,
                    0
                ],
                "published": "2024-07-28T18:01:22Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    18,
                    1,
                    22,
                    6,
                    210,
                    0
                ],
                "title": "Constraints on the fuzzy dark matter mass window from high-redshift\n  observables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraints on the fuzzy dark matter mass window from high-redshift\n  observables"
                },
                "summary": "We use a combination of high-redshift observables to extract the strongest\nconstraints to date on the fraction of axion fuzzy dark matter (FDM) in the\nmass window $10^{-26}\\,\\mathrm{eV}\\!\\lesssim\\!\nm_\\mathrm{FDM}\\!\\lesssim\\!10^{-23}\\,\\mathrm{eV}$. These observables include\nultraviolet luminosity functions (UVLFs) at redshifts $4-10$ measured by the\nHubble Space Telescope, a constraint on the neutral hydrogen fraction from\nhigh-redshift quasar spectroscopy, the cosmic microwave background optical\ndepth to reionization measurement from Planck and upper bounds on the 21cm\npower spectrum from HERA. In order to calculate these signals for FDM\ncosmology, we use the 21cmFirstCLASS code to interface between AxiCLASS and\n21cmFAST and consistently account for the full cosmic history from\nrecombination to reionization. To facilitate a full Bayesian likelihood\nanalysis, we developed a machine-learning based pipeline, which is both\naccurate, and enables a swift statistical inference, orders of magnitude faster\nthan a brute force approach. We find that FDM of mass $m_\\mathrm{FDM} \\!=\n\\!10^{-23} \\, \\mathrm{eV}$ is bound to less than $16\\%$ of the total dark\nmatter, where the constrains strengthen towards smaller masses, reaching down\nto $1\\%$ for $m_\\mathrm{FDM}\\! =\\! 10^{-26} \\, \\mathrm{eV}$, both at $95\\%$\nconfidence level. In addition, we forecast that a future detection of the 21cm\npower spectrum with HERA will lower the upper bound at $m_\\mathrm{FDM}\\! =\\!\n10^{-23} \\, \\mathrm{eV}$ to $\\lesssim\\!1\\%$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We use a combination of high-redshift observables to extract the strongest\nconstraints to date on the fraction of axion fuzzy dark matter (FDM) in the\nmass window $10^{-26}\\,\\mathrm{eV}\\!\\lesssim\\!\nm_\\mathrm{FDM}\\!\\lesssim\\!10^{-23}\\,\\mathrm{eV}$. These observables include\nultraviolet luminosity functions (UVLFs) at redshifts $4-10$ measured by the\nHubble Space Telescope, a constraint on the neutral hydrogen fraction from\nhigh-redshift quasar spectroscopy, the cosmic microwave background optical\ndepth to reionization measurement from Planck and upper bounds on the 21cm\npower spectrum from HERA. In order to calculate these signals for FDM\ncosmology, we use the 21cmFirstCLASS code to interface between AxiCLASS and\n21cmFAST and consistently account for the full cosmic history from\nrecombination to reionization. To facilitate a full Bayesian likelihood\nanalysis, we developed a machine-learning based pipeline, which is both\naccurate, and enables a swift statistical inference, orders of magnitude faster\nthan a brute force approach. We find that FDM of mass $m_\\mathrm{FDM} \\!=\n\\!10^{-23} \\, \\mathrm{eV}$ is bound to less than $16\\%$ of the total dark\nmatter, where the constrains strengthen towards smaller masses, reaching down\nto $1\\%$ for $m_\\mathrm{FDM}\\! =\\! 10^{-26} \\, \\mathrm{eV}$, both at $95\\%$\nconfidence level. In addition, we forecast that a future detection of the 21cm\npower spectrum with HERA will lower the upper bound at $m_\\mathrm{FDM}\\! =\\!\n10^{-23} \\, \\mathrm{eV}$ to $\\lesssim\\!1\\%$."
                },
                "authors": [
                    {
                        "name": "Hovav Lazare"
                    },
                    {
                        "name": "Jordan Flitter"
                    },
                    {
                        "name": "Ely D. Kovetz"
                    }
                ],
                "author_detail": {
                    "name": "Ely D. Kovetz"
                },
                "author": "Ely D. Kovetz",
                "arxiv_comment": "19 pages, 16 figures, 2 tables. Version accepted for publication in\n  PRD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19549v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19549v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04341v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04341v1",
                "updated": "2025-01-08T08:26:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    8,
                    26,
                    56,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T08:26:56Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    8,
                    26,
                    56,
                    2,
                    8,
                    0
                ],
                "title": "Understanding Before Reasoning: Enhancing Chain-of-Thought with\n  Iterative Summarization Pre-Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Before Reasoning: Enhancing Chain-of-Thought with\n  Iterative Summarization Pre-Prompting"
                },
                "summary": "Chain-of-Thought (CoT) Prompting is a dominant paradigm in Large Language\nModels (LLMs) to enhance complex reasoning. It guides LLMs to present\nmulti-step reasoning, rather than generating the final answer directly.\nHowever, CoT encounters difficulties when key information required for\nreasoning is implicit or missing. This occurs because CoT emphasizes the\nsequence of reasoning steps while overlooking the early extraction of essential\ninformation. We propose a pre-prompting method called Iterative Summarization\nPre-Prompting (ISP^2) to refine LLM reasoning when key information is not\nexplicitly provided. First, entities and their corresponding descriptions are\nextracted to form potential key information pairs. Next, we use a reliability\nrating to assess these pairs, then merge the two lowest-ranked pairs into a new\nentity description. This process is repeated until a unique key information\npair is obtained. Finally, that pair, along with the original question, is fed\ninto LLMs to produce the answer. Extensive experiments demonstrate a 7.1%\nimprovement compared to existing methods. Unlike traditional prompting, ISP^2\nadopts an inductive approach with pre-prompting, offering flexible integration\ninto diverse reasoning frameworks. The code is available at\nhttps://github.com/zdhgreat/ISP-2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) Prompting is a dominant paradigm in Large Language\nModels (LLMs) to enhance complex reasoning. It guides LLMs to present\nmulti-step reasoning, rather than generating the final answer directly.\nHowever, CoT encounters difficulties when key information required for\nreasoning is implicit or missing. This occurs because CoT emphasizes the\nsequence of reasoning steps while overlooking the early extraction of essential\ninformation. We propose a pre-prompting method called Iterative Summarization\nPre-Prompting (ISP^2) to refine LLM reasoning when key information is not\nexplicitly provided. First, entities and their corresponding descriptions are\nextracted to form potential key information pairs. Next, we use a reliability\nrating to assess these pairs, then merge the two lowest-ranked pairs into a new\nentity description. This process is repeated until a unique key information\npair is obtained. Finally, that pair, along with the original question, is fed\ninto LLMs to produce the answer. Extensive experiments demonstrate a 7.1%\nimprovement compared to existing methods. Unlike traditional prompting, ISP^2\nadopts an inductive approach with pre-prompting, offering flexible integration\ninto diverse reasoning frameworks. The code is available at\nhttps://github.com/zdhgreat/ISP-2."
                },
                "authors": [
                    {
                        "name": "Dong-Hai Zhu"
                    },
                    {
                        "name": "Yu-Jie Xiong"
                    },
                    {
                        "name": "Jia-Chen Zhang"
                    },
                    {
                        "name": "Xi-Jiong Xie"
                    },
                    {
                        "name": "Chun-Ming Xia"
                    }
                ],
                "author_detail": {
                    "name": "Chun-Ming Xia"
                },
                "author": "Chun-Ming Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04341v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04341v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04338v1",
                "updated": "2025-01-08T08:21:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    8,
                    21,
                    16,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T08:21:16Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    8,
                    21,
                    16,
                    2,
                    8,
                    0
                ],
                "title": "Inference of noise intensity and phase response from noisy synchronous\n  oscillators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference of noise intensity and phase response from noisy synchronous\n  oscillators"
                },
                "summary": "Numerous biological and microscale systems exhibit synchronization in noisy\nenvironments. The theory of such noisy oscillators and their synchronization\nhas been developed and experimentally demonstrated, but inferring the noise\nintensity and phase response is not always straightforward. In this study, we\npropose a useful formula that enables us to infer the noise intensity and phase\nresponse of a noisy oscillator synchronized with periodic external forcing.\nThrough asymptotic approximations for small noise, we show that noisy\nsynchronous oscillators satisfy a simple relationship among the noise intensity\nand measurable quantities, i.e., the stationary distribution of the oscillation\nphase and stationary probability current obtained as the average phase\nvelocity, which is verified through systematic numerical analysis. The proposed\nformula facilitates a unified analysis and design of synchronous oscillators in\nweakly noisy environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerous biological and microscale systems exhibit synchronization in noisy\nenvironments. The theory of such noisy oscillators and their synchronization\nhas been developed and experimentally demonstrated, but inferring the noise\nintensity and phase response is not always straightforward. In this study, we\npropose a useful formula that enables us to infer the noise intensity and phase\nresponse of a noisy oscillator synchronized with periodic external forcing.\nThrough asymptotic approximations for small noise, we show that noisy\nsynchronous oscillators satisfy a simple relationship among the noise intensity\nand measurable quantities, i.e., the stationary distribution of the oscillation\nphase and stationary probability current obtained as the average phase\nvelocity, which is verified through systematic numerical analysis. The proposed\nformula facilitates a unified analysis and design of synchronous oscillators in\nweakly noisy environments."
                },
                "authors": [
                    {
                        "name": "Hisa-Aki Tanaka"
                    },
                    {
                        "name": "Somei Suga"
                    },
                    {
                        "name": "Akira Keida"
                    },
                    {
                        "name": "Hiroya Nakao"
                    },
                    {
                        "name": "Yutaka Jitsumatsu"
                    },
                    {
                        "name": "István Z. Kiss"
                    }
                ],
                "author_detail": {
                    "name": "István Z. Kiss"
                },
                "author": "István Z. Kiss",
                "arxiv_comment": "16 pages, 5 figures, to appear in PRR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nlin.AO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nlin.AO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04336v1",
                "updated": "2025-01-08T08:15:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    8,
                    15,
                    29,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T08:15:29Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    8,
                    15,
                    29,
                    2,
                    8,
                    0
                ],
                "title": "Building a Mind Palace: Structuring Environment-Grounded Semantic Graphs\n  for Effective Long Video Analysis with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building a Mind Palace: Structuring Environment-Grounded Semantic Graphs\n  for Effective Long Video Analysis with LLMs"
                },
                "summary": "Long-form video understanding with Large Vision Language Models is challenged\nby the need to analyze temporally dispersed yet spatially concentrated key\nmoments within limited context windows. In this work, we introduce\nVideoMindPalace, a new framework inspired by the \"Mind Palace\", which organizes\ncritical video moments into a topologically structured semantic graph.\nVideoMindPalace organizes key information through (i) hand-object tracking and\ninteraction, (ii) clustered activity zones representing specific areas of\nrecurring activities, and (iii) environment layout mapping, allowing natural\nlanguage parsing by LLMs to provide grounded insights on spatio-temporal and 3D\ncontext. In addition, we propose the Video MindPalace Benchmark (VMB), to\nassess human-like reasoning, including spatial localization, temporal\nreasoning, and layout-aware sequential understanding. Evaluated on VMB and\nestablished video QA datasets, including EgoSchema, NExT-QA, IntentQA, and the\nActive Memories Benchmark, VideoMindPalace demonstrates notable gains in\nspatio-temporal coherence and human-aligned reasoning, advancing long-form\nvideo analysis capabilities in VLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-form video understanding with Large Vision Language Models is challenged\nby the need to analyze temporally dispersed yet spatially concentrated key\nmoments within limited context windows. In this work, we introduce\nVideoMindPalace, a new framework inspired by the \"Mind Palace\", which organizes\ncritical video moments into a topologically structured semantic graph.\nVideoMindPalace organizes key information through (i) hand-object tracking and\ninteraction, (ii) clustered activity zones representing specific areas of\nrecurring activities, and (iii) environment layout mapping, allowing natural\nlanguage parsing by LLMs to provide grounded insights on spatio-temporal and 3D\ncontext. In addition, we propose the Video MindPalace Benchmark (VMB), to\nassess human-like reasoning, including spatial localization, temporal\nreasoning, and layout-aware sequential understanding. Evaluated on VMB and\nestablished video QA datasets, including EgoSchema, NExT-QA, IntentQA, and the\nActive Memories Benchmark, VideoMindPalace demonstrates notable gains in\nspatio-temporal coherence and human-aligned reasoning, advancing long-form\nvideo analysis capabilities in VLMs."
                },
                "authors": [
                    {
                        "name": "Zeyi Huang"
                    },
                    {
                        "name": "Yuyang Ji"
                    },
                    {
                        "name": "Xiaofang Wang"
                    },
                    {
                        "name": "Nikhil Mehta"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Donghyun Lee"
                    },
                    {
                        "name": "Sigmund Vanvalkenburgh"
                    },
                    {
                        "name": "Shengxin Zha"
                    },
                    {
                        "name": "Bolin Lai"
                    },
                    {
                        "name": "Licheng Yu"
                    },
                    {
                        "name": "Ning Zhang"
                    },
                    {
                        "name": "Yong Jae Lee"
                    },
                    {
                        "name": "Miao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Miao Liu"
                },
                "author": "Miao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04330v1",
                "updated": "2025-01-08T08:05:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    8,
                    5,
                    17,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T08:05:17Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    8,
                    5,
                    17,
                    2,
                    8,
                    0
                ],
                "title": "Neural Parameter Estimation with Incomplete Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Parameter Estimation with Incomplete Data"
                },
                "summary": "Advancements in artificial intelligence (AI) and deep learning have led to\nneural networks being used to generate lightning-speed answers to complex\nquestions, to paint like Monet, or to write like Proust. Leveraging their\ncomputational speed and flexibility, neural networks are also being used to\nfacilitate fast, likelihood-free statistical inference. However, it is not\nstraightforward to use neural networks with data that for various reasons are\nincomplete, which precludes their use in many applications. A recently proposed\napproach to remedy this issue inputs an appropriately padded data vector and a\nvector that encodes the missingness pattern to a neural network. While\ncomputationally efficient, this \"masking\" approach can result in statistically\ninefficient inferences. Here, we propose an alternative approach that is based\non the Monte Carlo expectation-maximization (EM) algorithm. Our EM approach is\nlikelihood-free, substantially faster than the conventional EM algorithm as it\ndoes not require numerical optimization at each iteration, and more\nstatistically efficient than the masking approach. This research represents a\nprototype problem that indicates how improvements could be made in AI by\nintroducing Bayesian statistical thinking. We compare the two approaches to\nmissingness using simulated incomplete data from two models: a spatial Gaussian\nprocess model, and a spatial Potts model. The utility of the methodology is\nshown on Arctic sea-ice data and cryptocurrency data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in artificial intelligence (AI) and deep learning have led to\nneural networks being used to generate lightning-speed answers to complex\nquestions, to paint like Monet, or to write like Proust. Leveraging their\ncomputational speed and flexibility, neural networks are also being used to\nfacilitate fast, likelihood-free statistical inference. However, it is not\nstraightforward to use neural networks with data that for various reasons are\nincomplete, which precludes their use in many applications. A recently proposed\napproach to remedy this issue inputs an appropriately padded data vector and a\nvector that encodes the missingness pattern to a neural network. While\ncomputationally efficient, this \"masking\" approach can result in statistically\ninefficient inferences. Here, we propose an alternative approach that is based\non the Monte Carlo expectation-maximization (EM) algorithm. Our EM approach is\nlikelihood-free, substantially faster than the conventional EM algorithm as it\ndoes not require numerical optimization at each iteration, and more\nstatistically efficient than the masking approach. This research represents a\nprototype problem that indicates how improvements could be made in AI by\nintroducing Bayesian statistical thinking. We compare the two approaches to\nmissingness using simulated incomplete data from two models: a spatial Gaussian\nprocess model, and a spatial Potts model. The utility of the methodology is\nshown on Arctic sea-ice data and cryptocurrency data."
                },
                "authors": [
                    {
                        "name": "Matthew Sainsbury-Dale"
                    },
                    {
                        "name": "Andrew Zammit-Mangion"
                    },
                    {
                        "name": "Noel Cressie"
                    },
                    {
                        "name": "Raphaël Huser"
                    }
                ],
                "author_detail": {
                    "name": "Raphaël Huser"
                },
                "author": "Raphaël Huser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04327v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04327v1",
                "updated": "2025-01-08T07:59:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    59,
                    40,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T07:59:40Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    59,
                    40,
                    2,
                    8,
                    0
                ],
                "title": "Machine Learning Enhanced Quantum State Tomography on FPGA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Learning Enhanced Quantum State Tomography on FPGA"
                },
                "summary": "Machine learning techniques have opened new avenues for real-time quantum\nstate tomography (QST). In this work, we demonstrate the deployment of machine\nlearning-based QST onto edge devices, specifically utilizing field programmable\ngate arrays (FPGAs). This implementation is realized using the {\\it Vitis AI\nIntegrated Development Environment} provided by AMD\\textsuperscript\n\\textregistered~Inc. Compared to the Graphics Processing Unit (GPU)-based\nmachine learning QST, our FPGA-based one reduces the average inference time by\nan order of magnitude, from 38 ms to 2.94 ms, but only sacrifices the average\nfidelity about $1\\% $ reduction (from 0.99 to 0.98). The FPGA-based QST offers\na highly efficient and precise tool for diagnosing quantum states, marking a\nsignificant advancement in the practical applications for quantum information\nprocessing and quantum sensing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning techniques have opened new avenues for real-time quantum\nstate tomography (QST). In this work, we demonstrate the deployment of machine\nlearning-based QST onto edge devices, specifically utilizing field programmable\ngate arrays (FPGAs). This implementation is realized using the {\\it Vitis AI\nIntegrated Development Environment} provided by AMD\\textsuperscript\n\\textregistered~Inc. Compared to the Graphics Processing Unit (GPU)-based\nmachine learning QST, our FPGA-based one reduces the average inference time by\nan order of magnitude, from 38 ms to 2.94 ms, but only sacrifices the average\nfidelity about $1\\% $ reduction (from 0.99 to 0.98). The FPGA-based QST offers\na highly efficient and precise tool for diagnosing quantum states, marking a\nsignificant advancement in the practical applications for quantum information\nprocessing and quantum sensing."
                },
                "authors": [
                    {
                        "name": "Hsun-Chung Wu"
                    },
                    {
                        "name": "Hsien-Yi Hsieh"
                    },
                    {
                        "name": "Zhi-Kai Xu"
                    },
                    {
                        "name": "Hua Li Chen"
                    },
                    {
                        "name": "Zi-Hao Shi"
                    },
                    {
                        "name": "Po-Han Wang"
                    },
                    {
                        "name": "Popo Yang"
                    },
                    {
                        "name": "Ole Steuernagel"
                    },
                    {
                        "name": "Chien-Ming Wu"
                    },
                    {
                        "name": "Ray-Kuang Lee"
                    }
                ],
                "author_detail": {
                    "name": "Ray-Kuang Lee"
                },
                "author": "Ray-Kuang Lee",
                "arxiv_comment": "6 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04327v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04327v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14978v2",
                "updated": "2025-01-08T07:53:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    53,
                    15,
                    2,
                    8,
                    0
                ],
                "published": "2024-09-23T12:57:24Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    12,
                    57,
                    24,
                    0,
                    267,
                    0
                ],
                "title": "TS-HTFA: Advancing Time Series Forecasting via Hierarchical Text-Free\n  Alignment with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TS-HTFA: Advancing Time Series Forecasting via Hierarchical Text-Free\n  Alignment with Large Language Models"
                },
                "summary": "Given the significant potential of large language models (LLMs) in sequence\nmodeling, emerging studies have begun applying them to time-series forecasting.\nDespite notable progress, existing methods still face two critical challenges:\n1) their reliance on large amounts of paired text data, limiting the model\napplicability, and 2) a substantial modality gap between text and time series,\nleading to insufficient alignment and suboptimal performance. In this paper, we\nintroduce \\textbf{H}ierarchical \\textbf{T}ext-\\textbf{F}ree \\textbf{A}lignment\n(\\textbf{TS-HTFA}), a novel method that leverages hierarchical alignment to\nfully exploit the representation capacity of LLMs while eliminating the\ndependence on text data. Specifically, we replace paired text data with\nadaptive virtual text based on QR decomposition word embeddings and learnable\nprompt. Furthermore, we establish comprehensive cross-modal alignment at three\nlevels: input, feature, and output. Extensive experiments on multiple\ntime-series benchmarks demonstrate that HTFA achieves state-of-the-art\nperformance, significantly improving prediction accuracy and generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given the significant potential of large language models (LLMs) in sequence\nmodeling, emerging studies have begun applying them to time-series forecasting.\nDespite notable progress, existing methods still face two critical challenges:\n1) their reliance on large amounts of paired text data, limiting the model\napplicability, and 2) a substantial modality gap between text and time series,\nleading to insufficient alignment and suboptimal performance. In this paper, we\nintroduce \\textbf{H}ierarchical \\textbf{T}ext-\\textbf{F}ree \\textbf{A}lignment\n(\\textbf{TS-HTFA}), a novel method that leverages hierarchical alignment to\nfully exploit the representation capacity of LLMs while eliminating the\ndependence on text data. Specifically, we replace paired text data with\nadaptive virtual text based on QR decomposition word embeddings and learnable\nprompt. Furthermore, we establish comprehensive cross-modal alignment at three\nlevels: input, feature, and output. Extensive experiments on multiple\ntime-series benchmarks demonstrate that HTFA achieves state-of-the-art\nperformance, significantly improving prediction accuracy and generalization."
                },
                "authors": [
                    {
                        "name": "Pengfei Wang"
                    },
                    {
                        "name": "Huanran Zheng"
                    },
                    {
                        "name": "Qi'ao Xu"
                    },
                    {
                        "name": "Silong Dai"
                    },
                    {
                        "name": "Yiqiao Wang"
                    },
                    {
                        "name": "Wenjing Yue"
                    },
                    {
                        "name": "Wei Zhu"
                    },
                    {
                        "name": "Tianwen Qian"
                    },
                    {
                        "name": "Xiaoling Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoling Wang"
                },
                "author": "Xiaoling Wang",
                "arxiv_comment": "19 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04323v1",
                "updated": "2025-01-08T07:47:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    47,
                    43,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T07:47:43Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    47,
                    43,
                    2,
                    8,
                    0
                ],
                "title": "Navigating the Designs of Privacy-Preserving Fine-tuning for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating the Designs of Privacy-Preserving Fine-tuning for Large\n  Language Models"
                },
                "summary": "Instruction tuning has proven effective in enhancing Large Language Models'\n(LLMs) performance on downstream tasks. However, real-world fine-tuning faces\ninherent conflicts between model providers' intellectual property protection,\nclients' data privacy requirements, and tuning costs. While recent approaches\nlike split learning and offsite tuning demonstrate promising architectures for\nprivacy-preserving fine-tuning, there is a gap in systematically addressing the\nmultidimensional trade-offs required for diverse real-world deployments. We\npropose several indicative evaluation metrics to guide design trade-offs for\nprivacy-preserving fine-tuning and a series of example designs, collectively\nnamed GuardedTuning; they result from novel combinations of system\narchitectures with adapted privacy-enhancement methods and emerging computation\ntechniques. Each design represents distinct trade-offs across model utility,\nprivacy guarantees, and costs. Experimental results demonstrate that these\ndesigns protect against data reconstruction attacks while maintaining\ncompetitive fine-tuning performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning has proven effective in enhancing Large Language Models'\n(LLMs) performance on downstream tasks. However, real-world fine-tuning faces\ninherent conflicts between model providers' intellectual property protection,\nclients' data privacy requirements, and tuning costs. While recent approaches\nlike split learning and offsite tuning demonstrate promising architectures for\nprivacy-preserving fine-tuning, there is a gap in systematically addressing the\nmultidimensional trade-offs required for diverse real-world deployments. We\npropose several indicative evaluation metrics to guide design trade-offs for\nprivacy-preserving fine-tuning and a series of example designs, collectively\nnamed GuardedTuning; they result from novel combinations of system\narchitectures with adapted privacy-enhancement methods and emerging computation\ntechniques. Each design represents distinct trade-offs across model utility,\nprivacy guarantees, and costs. Experimental results demonstrate that these\ndesigns protect against data reconstruction attacks while maintaining\ncompetitive fine-tuning performance."
                },
                "authors": [
                    {
                        "name": "Shi Haonan"
                    },
                    {
                        "name": "Ouyang Tu"
                    },
                    {
                        "name": "Wang An"
                    }
                ],
                "author_detail": {
                    "name": "Wang An"
                },
                "author": "Wang An",
                "arxiv_comment": "4 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04319v1",
                "updated": "2025-01-08T07:32:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    32,
                    54,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T07:32:54Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    32,
                    54,
                    2,
                    8,
                    0
                ],
                "title": "VerifBFL: Leveraging zk-SNARKs for A Verifiable Blockchained Federated\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VerifBFL: Leveraging zk-SNARKs for A Verifiable Blockchained Federated\n  Learning"
                },
                "summary": "Blockchain-based Federated Learning (FL) is an emerging decentralized machine\nlearning paradigm that enables model training without relying on a central\nserver. Although some BFL frameworks are considered privacy-preserving, they\nare still vulnerable to various attacks, including inference and model\npoisoning. Additionally, most of these solutions employ strong trust\nassumptions among all participating entities or introduce incentive mechanisms\nto encourage collaboration, making them susceptible to multiple security flaws.\nThis work presents VerifBFL, a trustless, privacy-preserving, and verifiable\nfederated learning framework that integrates blockchain technology and\ncryptographic protocols. By employing zero-knowledge Succinct Non-Interactive\nArgument of Knowledge (zk-SNARKs) and incrementally verifiable computation\n(IVC), VerifBFL ensures the verifiability of both local training and\naggregation processes. The proofs of training and aggregation are verified\non-chain, guaranteeing the integrity and auditability of each participant's\ncontributions. To protect training data from inference attacks, VerifBFL\nleverages differential privacy. Finally, to demonstrate the efficiency of the\nproposed protocols, we built a proof of concept using emerging tools. The\nresults show that generating proofs for local training and aggregation in\nVerifBFL takes less than 81s and 2s, respectively, while verifying them\non-chain takes less than 0.6s.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blockchain-based Federated Learning (FL) is an emerging decentralized machine\nlearning paradigm that enables model training without relying on a central\nserver. Although some BFL frameworks are considered privacy-preserving, they\nare still vulnerable to various attacks, including inference and model\npoisoning. Additionally, most of these solutions employ strong trust\nassumptions among all participating entities or introduce incentive mechanisms\nto encourage collaboration, making them susceptible to multiple security flaws.\nThis work presents VerifBFL, a trustless, privacy-preserving, and verifiable\nfederated learning framework that integrates blockchain technology and\ncryptographic protocols. By employing zero-knowledge Succinct Non-Interactive\nArgument of Knowledge (zk-SNARKs) and incrementally verifiable computation\n(IVC), VerifBFL ensures the verifiability of both local training and\naggregation processes. The proofs of training and aggregation are verified\non-chain, guaranteeing the integrity and auditability of each participant's\ncontributions. To protect training data from inference attacks, VerifBFL\nleverages differential privacy. Finally, to demonstrate the efficiency of the\nproposed protocols, we built a proof of concept using emerging tools. The\nresults show that generating proofs for local training and aggregation in\nVerifBFL takes less than 81s and 2s, respectively, while verifying them\non-chain takes less than 0.6s."
                },
                "authors": [
                    {
                        "name": "Ahmed Ayoub Bellachia"
                    },
                    {
                        "name": "Mouhamed Amine Bouchiha"
                    },
                    {
                        "name": "Yacine Ghamri-Doudane"
                    },
                    {
                        "name": "Mourad Rabah"
                    }
                ],
                "author_detail": {
                    "name": "Mourad Rabah"
                },
                "author": "Mourad Rabah",
                "arxiv_comment": "Paper accepted at NOMS'25 (9 pages, 6 Figures)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04316v1",
                "updated": "2025-01-08T07:28:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    28,
                    10,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T07:28:10Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    28,
                    10,
                    2,
                    8,
                    0
                ],
                "title": "Who Does the Giant Number Pile Like Best: Analyzing Fairness in Hiring\n  Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who Does the Giant Number Pile Like Best: Analyzing Fairness in Hiring\n  Contexts"
                },
                "summary": "Large language models (LLMs) are increasingly being deployed in high-stakes\napplications like hiring, yet their potential for unfair decision-making and\noutcomes remains understudied, particularly in generative settings. In this\nwork, we examine the fairness of LLM-based hiring systems through two\nreal-world tasks: resume summarization and retrieval. By constructing a\nsynthetic resume dataset and curating job postings, we investigate whether\nmodel behavior differs across demographic groups and is sensitive to\ndemographic perturbations. Our findings reveal that race-based differences\nappear in approximately 10% of generated summaries, while gender-based\ndifferences occur in only 1%. In the retrieval setting, all evaluated models\ndisplay non-uniform selection patterns across demographic groups and exhibit\nhigh sensitivity to both gender and race-based perturbations. Surprisingly,\nretrieval models demonstrate comparable sensitivity to non-demographic changes,\nsuggesting that fairness issues may stem, in part, from general brittleness\nissues. Overall, our results indicate that LLM-based hiring systems, especially\nat the retrieval stage, can exhibit notable biases that lead to discriminatory\noutcomes in real-world contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being deployed in high-stakes\napplications like hiring, yet their potential for unfair decision-making and\noutcomes remains understudied, particularly in generative settings. In this\nwork, we examine the fairness of LLM-based hiring systems through two\nreal-world tasks: resume summarization and retrieval. By constructing a\nsynthetic resume dataset and curating job postings, we investigate whether\nmodel behavior differs across demographic groups and is sensitive to\ndemographic perturbations. Our findings reveal that race-based differences\nappear in approximately 10% of generated summaries, while gender-based\ndifferences occur in only 1%. In the retrieval setting, all evaluated models\ndisplay non-uniform selection patterns across demographic groups and exhibit\nhigh sensitivity to both gender and race-based perturbations. Surprisingly,\nretrieval models demonstrate comparable sensitivity to non-demographic changes,\nsuggesting that fairness issues may stem, in part, from general brittleness\nissues. Overall, our results indicate that LLM-based hiring systems, especially\nat the retrieval stage, can exhibit notable biases that lead to discriminatory\noutcomes in real-world contexts."
                },
                "authors": [
                    {
                        "name": "Preethi Seshadri"
                    },
                    {
                        "name": "Seraphina Goldfarb-Tarrant"
                    }
                ],
                "author_detail": {
                    "name": "Seraphina Goldfarb-Tarrant"
                },
                "author": "Seraphina Goldfarb-Tarrant",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07464v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07464v2",
                "updated": "2025-01-08T07:25:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    25,
                    55,
                    2,
                    8,
                    0
                ],
                "published": "2024-11-12T00:57:30Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    0,
                    57,
                    30,
                    1,
                    317,
                    0
                ],
                "title": "BudgetMLAgent: A Cost-Effective LLM Multi-Agent system for Automating\n  Machine Learning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BudgetMLAgent: A Cost-Effective LLM Multi-Agent system for Automating\n  Machine Learning Tasks"
                },
                "summary": "Large Language Models (LLMs) excel in diverse applications including\ngeneration of code snippets, but often struggle with generating code for\ncomplex Machine Learning (ML) tasks. Although existing LLM single-agent based\nsystems give varying performance depending on the task complexity, they purely\nrely on larger and expensive models such as GPT-4. Our investigation reveals\nthat no-cost and low-cost models such as Gemini-Pro, Mixtral and CodeLlama\nperform far worse than GPT-4 in a single-agent setting. With the motivation of\ndeveloping a cost-efficient LLM based solution for solving ML tasks, we propose\nan LLM Multi-Agent based system which leverages combination of experts using\nprofiling, efficient retrieval of past observations, LLM cascades, and\nask-the-expert calls. Through empirical analysis on ML engineering tasks in the\nMLAgentBench benchmark, we demonstrate the effectiveness of our system, using\nno-cost models, namely Gemini as the base LLM, paired with GPT-4 in cascade and\nexpert to serve occasional ask-the-expert calls for planning. With 94.2\\%\nreduction in the cost (from \\$0.931 per run cost averaged over all tasks for\nGPT-4 single agent system to \\$0.054), our system is able to yield better\naverage success rate of 32.95\\% as compared to GPT-4 single-agent system\nyielding 22.72\\% success rate averaged over all the tasks of MLAgentBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in diverse applications including\ngeneration of code snippets, but often struggle with generating code for\ncomplex Machine Learning (ML) tasks. Although existing LLM single-agent based\nsystems give varying performance depending on the task complexity, they purely\nrely on larger and expensive models such as GPT-4. Our investigation reveals\nthat no-cost and low-cost models such as Gemini-Pro, Mixtral and CodeLlama\nperform far worse than GPT-4 in a single-agent setting. With the motivation of\ndeveloping a cost-efficient LLM based solution for solving ML tasks, we propose\nan LLM Multi-Agent based system which leverages combination of experts using\nprofiling, efficient retrieval of past observations, LLM cascades, and\nask-the-expert calls. Through empirical analysis on ML engineering tasks in the\nMLAgentBench benchmark, we demonstrate the effectiveness of our system, using\nno-cost models, namely Gemini as the base LLM, paired with GPT-4 in cascade and\nexpert to serve occasional ask-the-expert calls for planning. With 94.2\\%\nreduction in the cost (from \\$0.931 per run cost averaged over all tasks for\nGPT-4 single agent system to \\$0.054), our system is able to yield better\naverage success rate of 32.95\\% as compared to GPT-4 single-agent system\nyielding 22.72\\% success rate averaged over all the tasks of MLAgentBench."
                },
                "authors": [
                    {
                        "name": "Shubham Gandhi"
                    },
                    {
                        "name": "Manasi Patwardhan"
                    },
                    {
                        "name": "Lovekesh Vig"
                    },
                    {
                        "name": "Gautam Shroff"
                    }
                ],
                "author_detail": {
                    "name": "Gautam Shroff"
                },
                "author": "Gautam Shroff",
                "arxiv_comment": "Presented at AIMLSystems '24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07464v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07464v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T42",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1; I.2.2; I.2.5; I.2.7; I.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14418v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14418v3",
                "updated": "2025-01-08T07:23:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    23,
                    56,
                    2,
                    8,
                    0
                ],
                "published": "2024-08-26T17:04:00Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    4,
                    0,
                    0,
                    239,
                    0
                ],
                "title": "MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR\n  Errors with LLM-generated Synthetic Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR\n  Errors with LLM-generated Synthetic Dialogues"
                },
                "summary": "Automatic Speech Recognition (ASR) systems are pivotal in transcribing speech\ninto text, yet the errors they introduce can significantly degrade the\nperformance of downstream tasks like summarization. This issue is particularly\npronounced in clinical dialogue summarization, a low-resource domain where\nsupervised data for fine-tuning is scarce, necessitating the use of ASR models\nas black-box solutions. Employing conventional data augmentation for enhancing\nthe noise robustness of summarization models is not feasible either due to the\nunavailability of sufficient medical dialogue audio recordings and\ncorresponding ASR transcripts. To address this challenge, we propose MEDSAGE,\nan approach for generating synthetic samples for data augmentation using Large\nLanguage Models (LLMs). Specifically, we leverage the in-context learning\ncapabilities of LLMs and instruct them to generate ASR-like errors based on a\nfew available medical dialogue examples with audio recordings. Experimental\nresults show that LLMs can effectively model ASR noise, and incorporating this\nnoisy data into the training process significantly improves the robustness and\naccuracy of medical dialogue summarization systems. This approach addresses the\nchallenges of noisy ASR outputs in critical applications, offering a robust\nsolution to enhance the reliability of clinical dialogue summarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Speech Recognition (ASR) systems are pivotal in transcribing speech\ninto text, yet the errors they introduce can significantly degrade the\nperformance of downstream tasks like summarization. This issue is particularly\npronounced in clinical dialogue summarization, a low-resource domain where\nsupervised data for fine-tuning is scarce, necessitating the use of ASR models\nas black-box solutions. Employing conventional data augmentation for enhancing\nthe noise robustness of summarization models is not feasible either due to the\nunavailability of sufficient medical dialogue audio recordings and\ncorresponding ASR transcripts. To address this challenge, we propose MEDSAGE,\nan approach for generating synthetic samples for data augmentation using Large\nLanguage Models (LLMs). Specifically, we leverage the in-context learning\ncapabilities of LLMs and instruct them to generate ASR-like errors based on a\nfew available medical dialogue examples with audio recordings. Experimental\nresults show that LLMs can effectively model ASR noise, and incorporating this\nnoisy data into the training process significantly improves the robustness and\naccuracy of medical dialogue summarization systems. This approach addresses the\nchallenges of noisy ASR outputs in critical applications, offering a robust\nsolution to enhance the reliability of clinical dialogue summarization."
                },
                "authors": [
                    {
                        "name": "Kuluhan Binici"
                    },
                    {
                        "name": "Abhinav Ramesh Kashyap"
                    },
                    {
                        "name": "Viktor Schlegel"
                    },
                    {
                        "name": "Andy T. Liu"
                    },
                    {
                        "name": "Vijay Prakash Dwivedi"
                    },
                    {
                        "name": "Thanh-Tung Nguyen"
                    },
                    {
                        "name": "Xiaoxue Gao"
                    },
                    {
                        "name": "Nancy F. Chen"
                    },
                    {
                        "name": "Stefan Winkler"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Winkler"
                },
                "author": "Stefan Winkler",
                "arxiv_comment": "Accepted by the Thirty-Ninth AAAI Conference on Artificial\n  Intelligence (AAAI-25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14418v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14418v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11102v2",
                "updated": "2025-01-08T07:22:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    22,
                    51,
                    2,
                    8,
                    0
                ],
                "published": "2024-12-15T07:49:31Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    7,
                    49,
                    31,
                    6,
                    350,
                    0
                ],
                "title": "Empowering LLMs to Understand and Generate Complex Vector Graphics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering LLMs to Understand and Generate Complex Vector Graphics"
                },
                "summary": "The unprecedented advancements in Large Language Models (LLMs) have\nprofoundly impacted natural language processing but have yet to fully embrace\nthe realm of scalable vector graphics (SVG) generation. While LLMs encode\npartial knowledge of SVG data from web pages during training, recent findings\nsuggest that semantically ambiguous and tokenized representations within LLMs\nmay result in hallucinations in vector primitive predictions. Additionally, LLM\ntraining typically lacks modeling and understanding of the rendering sequence\nof vector paths, which can lead to occlusion between output vector primitives.\nIn this paper, we present LLM4SVG, an initial yet substantial step toward\nbridging this gap by enabling LLMs to better understand and generate vector\ngraphics. LLM4SVG facilitates a deeper understanding of SVG components through\nlearnable semantic tokens, which precisely encode these tokens and their\ncorresponding properties to generate semantically aligned SVG outputs. Using a\nseries of learnable semantic tokens, a structured dataset for instruction\nfollowing is developed to support comprehension and generation across two\nprimary tasks. Our method introduces a modular architecture to existing large\nlanguage models, integrating semantic tags, vector instruction encoders,\nfine-tuned commands, and powerful LLMs to tightly combine geometric,\nappearance, and language information. To overcome the scarcity of SVG-text\ninstruction data, we developed an automated data generation pipeline that\ncollected a massive dataset of more than 250k SVG data and 580k SVG-text\ninstructions, which facilitated the adoption of the two-stage training strategy\npopular in LLM development. By exploring various training strategies, we\ndeveloped LLM4SVG, which significantly moves beyond optimized rendering-based\napproaches and language-model-based baselines to achieve remarkable results in\nhuman evaluation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The unprecedented advancements in Large Language Models (LLMs) have\nprofoundly impacted natural language processing but have yet to fully embrace\nthe realm of scalable vector graphics (SVG) generation. While LLMs encode\npartial knowledge of SVG data from web pages during training, recent findings\nsuggest that semantically ambiguous and tokenized representations within LLMs\nmay result in hallucinations in vector primitive predictions. Additionally, LLM\ntraining typically lacks modeling and understanding of the rendering sequence\nof vector paths, which can lead to occlusion between output vector primitives.\nIn this paper, we present LLM4SVG, an initial yet substantial step toward\nbridging this gap by enabling LLMs to better understand and generate vector\ngraphics. LLM4SVG facilitates a deeper understanding of SVG components through\nlearnable semantic tokens, which precisely encode these tokens and their\ncorresponding properties to generate semantically aligned SVG outputs. Using a\nseries of learnable semantic tokens, a structured dataset for instruction\nfollowing is developed to support comprehension and generation across two\nprimary tasks. Our method introduces a modular architecture to existing large\nlanguage models, integrating semantic tags, vector instruction encoders,\nfine-tuned commands, and powerful LLMs to tightly combine geometric,\nappearance, and language information. To overcome the scarcity of SVG-text\ninstruction data, we developed an automated data generation pipeline that\ncollected a massive dataset of more than 250k SVG data and 580k SVG-text\ninstructions, which facilitated the adoption of the two-stage training strategy\npopular in LLM development. By exploring various training strategies, we\ndeveloped LLM4SVG, which significantly moves beyond optimized rendering-based\napproaches and language-model-based baselines to achieve remarkable results in\nhuman evaluation tasks."
                },
                "authors": [
                    {
                        "name": "Ximing Xing"
                    },
                    {
                        "name": "Juncheng Hu"
                    },
                    {
                        "name": "Guotao Liang"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Dong Xu"
                    },
                    {
                        "name": "Qian Yu"
                    }
                ],
                "author_detail": {
                    "name": "Qian Yu"
                },
                "author": "Qian Yu",
                "arxiv_comment": "Project Page: https://ximinng.github.io/LLM4SVGProject/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04315v1",
                "updated": "2025-01-08T07:13:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    13,
                    52,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T07:13:52Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    13,
                    52,
                    2,
                    8,
                    0
                ],
                "title": "RoRA: Efficient Fine-Tuning of LLM with Reliability Optimization for\n  Rank Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoRA: Efficient Fine-Tuning of LLM with Reliability Optimization for\n  Rank Adaptation"
                },
                "summary": "Fine-tuning helps large language models (LLM) recover degraded information\nand enhance task performance.Although Low-Rank Adaptation (LoRA) is widely used\nand effective for fine-tuning, we have observed that its scaling factor can\nlimit or even reduce performance as the rank size increases. To address this\nissue, we propose RoRA (Rank-adaptive Reliability Optimization), a simple yet\neffective method for optimizing LoRA's scaling factor. By replacing $\\alpha/r$\nwith $\\alpha/\\sqrt{r}$, RoRA ensures improved performance as rank size\nincreases. Moreover, RoRA enhances low-rank adaptation in fine-tuning\nuncompressed models and excels in the more challenging task of accuracy\nrecovery when fine-tuning pruned models. Extensive experiments demonstrate the\neffectiveness of RoRA in fine-tuning both uncompressed and pruned models. RoRA\nsurpasses the state-of-the-art (SOTA) in average accuracy and robustness on\nLLaMA-7B/13B, LLaMA2-7B, and LLaMA3-8B, specifically outperforming LoRA and\nDoRA by 6.5% and 2.9% on LLaMA-7B, respectively. In pruned model fine-tuning,\nRoRA shows significant advantages; for SHEARED-LLAMA-1.3, a LLaMA-7B with 81.4%\npruning, RoRA achieves 5.7% higher average accuracy than LoRA and 3.9% higher\nthan DoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning helps large language models (LLM) recover degraded information\nand enhance task performance.Although Low-Rank Adaptation (LoRA) is widely used\nand effective for fine-tuning, we have observed that its scaling factor can\nlimit or even reduce performance as the rank size increases. To address this\nissue, we propose RoRA (Rank-adaptive Reliability Optimization), a simple yet\neffective method for optimizing LoRA's scaling factor. By replacing $\\alpha/r$\nwith $\\alpha/\\sqrt{r}$, RoRA ensures improved performance as rank size\nincreases. Moreover, RoRA enhances low-rank adaptation in fine-tuning\nuncompressed models and excels in the more challenging task of accuracy\nrecovery when fine-tuning pruned models. Extensive experiments demonstrate the\neffectiveness of RoRA in fine-tuning both uncompressed and pruned models. RoRA\nsurpasses the state-of-the-art (SOTA) in average accuracy and robustness on\nLLaMA-7B/13B, LLaMA2-7B, and LLaMA3-8B, specifically outperforming LoRA and\nDoRA by 6.5% and 2.9% on LLaMA-7B, respectively. In pruned model fine-tuning,\nRoRA shows significant advantages; for SHEARED-LLAMA-1.3, a LLaMA-7B with 81.4%\npruning, RoRA achieves 5.7% higher average accuracy than LoRA and 3.9% higher\nthan DoRA."
                },
                "authors": [
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Zhenglun Kong"
                    },
                    {
                        "name": "Peiyan Dong"
                    },
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Geng Yuan"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Wenbin Zhang"
                    },
                    {
                        "name": "Xue Lin"
                    },
                    {
                        "name": "Dong Huang"
                    },
                    {
                        "name": "Yanzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanzhi Wang"
                },
                "author": "Yanzhi Wang",
                "arxiv_comment": "ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04312v1",
                "updated": "2025-01-08T07:07:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    7,
                    22,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T07:07:22Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    7,
                    22,
                    2,
                    8,
                    0
                ],
                "title": "Your Fix Is My Exploit: Enabling Comprehensive DL Library API Fuzzing\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Your Fix Is My Exploit: Enabling Comprehensive DL Library API Fuzzing\n  with Large Language Models"
                },
                "summary": "Deep learning (DL) libraries, widely used in AI applications, often contain\nvulnerabilities like buffer overflows and use-after-free errors. Traditional\nfuzzing struggles with the complexity and API diversity of DL libraries such as\nTensorFlow and PyTorch, which feature over 1,000 APIs. Testing all these APIs\nis challenging due to complex inputs and varied usage patterns. While large\nlanguage models (LLMs) show promise in code understanding and generation,\nexisting LLM-based fuzzers lack deep knowledge of API edge cases and struggle\nwith test input generation. To address this, we propose DFUZZ, an LLM-driven\nfuzzing approach for DL libraries. DFUZZ leverages two insights: (1) LLMs can\nreason about error-triggering edge cases from API code and apply this knowledge\nto untested APIs, and (2) LLMs can accurately synthesize test programs to\nautomate API testing. By providing LLMs with a \"white-box view\" of APIs, DFUZZ\nenhances reasoning and generation for comprehensive fuzzing. Experimental\nresults show that DFUZZ outperforms state-of-the-art fuzzers in API coverage\nfor TensorFlow and PyTorch, uncovering 37 bugs, with 8 fixed and 19 under\ndeveloper investigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning (DL) libraries, widely used in AI applications, often contain\nvulnerabilities like buffer overflows and use-after-free errors. Traditional\nfuzzing struggles with the complexity and API diversity of DL libraries such as\nTensorFlow and PyTorch, which feature over 1,000 APIs. Testing all these APIs\nis challenging due to complex inputs and varied usage patterns. While large\nlanguage models (LLMs) show promise in code understanding and generation,\nexisting LLM-based fuzzers lack deep knowledge of API edge cases and struggle\nwith test input generation. To address this, we propose DFUZZ, an LLM-driven\nfuzzing approach for DL libraries. DFUZZ leverages two insights: (1) LLMs can\nreason about error-triggering edge cases from API code and apply this knowledge\nto untested APIs, and (2) LLMs can accurately synthesize test programs to\nautomate API testing. By providing LLMs with a \"white-box view\" of APIs, DFUZZ\nenhances reasoning and generation for comprehensive fuzzing. Experimental\nresults show that DFUZZ outperforms state-of-the-art fuzzers in API coverage\nfor TensorFlow and PyTorch, uncovering 37 bugs, with 8 fixed and 19 under\ndeveloper investigation."
                },
                "authors": [
                    {
                        "name": "Kunpeng Zhang"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Jitao Han"
                    },
                    {
                        "name": "Xiaogang Zhu"
                    },
                    {
                        "name": "Xian Li"
                    },
                    {
                        "name": "Shaohua Wang"
                    },
                    {
                        "name": "Sheng Wen"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Wen"
                },
                "author": "Sheng Wen",
                "arxiv_journal_ref": "2025 IEEE/ACM 47th International Conference on Software\n  Engineering (ICSE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18046v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18046v2",
                "updated": "2025-01-08T07:06:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    6,
                    9,
                    2,
                    8,
                    0
                ],
                "published": "2024-11-27T04:31:22Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    4,
                    31,
                    22,
                    2,
                    332,
                    0
                ],
                "title": "Probing orbits of stellar mass objects deep in galactic nuclei with\n  quasi-periodic eruptions -- III: Long term evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing orbits of stellar mass objects deep in galactic nuclei with\n  quasi-periodic eruptions -- III: Long term evolution"
                },
                "summary": "Quasi-periodic eruptions (QPEs) are intense repeating soft X-ray bursts with\nrecurrence times about a few hours to a few weeks from galactic nuclei. More\nand more analyses show that QPEs are the result of collisions between a stellar\nmass object (SMO, a stellar mass black hole or a main sequence star) and an\naccretion disk around a supermassive black hole (SMBH) in galactic nuclei. QPEs\nhave shown to be invaluable in probing the orbits of SMOs in the vicinity of\nSMBHs, and further inferring the formation of extreme mass ratio inspirals\n(EMRIs). In this paper, we extend previous orbital analyses in Refs.\narXiv:2401.11190, arXiv:2405.06429 by including extra effects, the SMO orbital\ndecay due to collisions with the disk and the disk precession. We find clear\nBayes evidence for orbital decay in GSN 069 and for disk precession in\neRO-QPE2, the two most stable QPE sources. The detection of these effects\nprovides informative constraints on the SMBH mass, the radiation efficiency of\nQPEs, the SMO nature, the accretion disk surface density and the accretion disk\nviscosity. With tighter constraints on the SMO orbital parameters, we further\nconfirm that these two QPE EMRIs are nearly circular orbiters which are\nconsistent with the wet EMRI formation channel prediction, but are incompatible\nwith either the dry loss-cone channel or the Hills mechanism. Combining all the\nQPE sources available, we find the QPE EMRIs can be divided into two\npopulations according to their orbital eccentricities, where the orbital\nperiods and the SMBH masses in the low-eccentricity population follow a scaling\nrelation $T_{\\rm obt}\\propto M_{\\bullet}^n$ with $n\\approx 0.8$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quasi-periodic eruptions (QPEs) are intense repeating soft X-ray bursts with\nrecurrence times about a few hours to a few weeks from galactic nuclei. More\nand more analyses show that QPEs are the result of collisions between a stellar\nmass object (SMO, a stellar mass black hole or a main sequence star) and an\naccretion disk around a supermassive black hole (SMBH) in galactic nuclei. QPEs\nhave shown to be invaluable in probing the orbits of SMOs in the vicinity of\nSMBHs, and further inferring the formation of extreme mass ratio inspirals\n(EMRIs). In this paper, we extend previous orbital analyses in Refs.\narXiv:2401.11190, arXiv:2405.06429 by including extra effects, the SMO orbital\ndecay due to collisions with the disk and the disk precession. We find clear\nBayes evidence for orbital decay in GSN 069 and for disk precession in\neRO-QPE2, the two most stable QPE sources. The detection of these effects\nprovides informative constraints on the SMBH mass, the radiation efficiency of\nQPEs, the SMO nature, the accretion disk surface density and the accretion disk\nviscosity. With tighter constraints on the SMO orbital parameters, we further\nconfirm that these two QPE EMRIs are nearly circular orbiters which are\nconsistent with the wet EMRI formation channel prediction, but are incompatible\nwith either the dry loss-cone channel or the Hills mechanism. Combining all the\nQPE sources available, we find the QPE EMRIs can be divided into two\npopulations according to their orbital eccentricities, where the orbital\nperiods and the SMBH masses in the low-eccentricity population follow a scaling\nrelation $T_{\\rm obt}\\propto M_{\\bullet}^n$ with $n\\approx 0.8$."
                },
                "authors": [
                    {
                        "name": "Cong Zhou"
                    },
                    {
                        "name": "Yuhe Zeng"
                    },
                    {
                        "name": "Zhen Pan"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Pan"
                },
                "author": "Zhen Pan",
                "arxiv_comment": "17 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18046v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18046v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13720v2",
                "updated": "2025-01-08T07:03:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    3,
                    42,
                    2,
                    8,
                    0
                ],
                "published": "2024-12-18T11:00:58Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    11,
                    0,
                    58,
                    2,
                    353,
                    0
                ],
                "title": "Federated Learning and RAG Integration: A Scalable Approach for Medical\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning and RAG Integration: A Scalable Approach for Medical\n  Large Language Models"
                },
                "summary": "This study analyzes the performance of domain-specific Large Language Models\n(LLMs) for the medical field by integrating Retrieval-Augmented Generation\n(RAG) systems within a federated learning framework. Leveraging the inherent\nadvantages of federated learning, such as preserving data privacy and enabling\ndistributed computation, this research explores the integration of RAG systems\nwith models trained under varying client configurations to optimize\nperformance. Experimental results demonstrate that the federated learning-based\nmodels integrated with RAG systems consistently outperform their non-integrated\ncounterparts across all evaluation metrics. This study highlights the potential\nof combining federated learning and RAG systems for developing domain-specific\nLLMs in the medical field, providing a scalable and privacy-preserving solution\nfor enhancing text generation capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study analyzes the performance of domain-specific Large Language Models\n(LLMs) for the medical field by integrating Retrieval-Augmented Generation\n(RAG) systems within a federated learning framework. Leveraging the inherent\nadvantages of federated learning, such as preserving data privacy and enabling\ndistributed computation, this research explores the integration of RAG systems\nwith models trained under varying client configurations to optimize\nperformance. Experimental results demonstrate that the federated learning-based\nmodels integrated with RAG systems consistently outperform their non-integrated\ncounterparts across all evaluation metrics. This study highlights the potential\nof combining federated learning and RAG systems for developing domain-specific\nLLMs in the medical field, providing a scalable and privacy-preserving solution\nfor enhancing text generation capabilities."
                },
                "authors": [
                    {
                        "name": "Jincheol Jung"
                    },
                    {
                        "name": "Hongju Jeong"
                    },
                    {
                        "name": "Eui-Nam Huh"
                    }
                ],
                "author_detail": {
                    "name": "Eui-Nam Huh"
                },
                "author": "Eui-Nam Huh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02996v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02996v2",
                "updated": "2025-01-08T07:00:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    0,
                    1,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-06T13:14:31Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    13,
                    14,
                    31,
                    0,
                    6,
                    0
                ],
                "title": "Comparing the Extrinsic Orbital Hall Effect in Centrosymmetric and\n  Noncentrosymmetric Systems: Insights from Bilayer Transition Metal\n  Dichalcogenides",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing the Extrinsic Orbital Hall Effect in Centrosymmetric and\n  Noncentrosymmetric Systems: Insights from Bilayer Transition Metal\n  Dichalcogenides"
                },
                "summary": "Both intrinsic and extrinsic orbital Hall effects (OHE) in bilayer transition\nmetal dichalcogenides (TMDs) are investigated in the presence of short-range\ndisorder using quantum kinetic theory. Bilayer TMDs provide an ideal platform\nto study the effects of inversion symmetry breaking on transport properties due\nto their unique structural and electronic characteristics. While bilayer TMDs\nare naturally inversion symmetric, applying a finite gate voltage to create a\nbias between the layers effectively breaks this symmetry. Our findings reveal\nthat slightly away from the band edges, the extrinsic OHE becomes the dominant\ncontribution in both inversion-symmetric and asymmetric cases, with its\nprominence increasing significantly as the Fermi energy rises. Furthermore, we\ndemonstrate that breaking inversion symmetry greatly enhances the extrinsic\nOHE. This enhancement arises from the fundamentally distinct behavior of\norbital angular momentum (OAM) in centrosymmetric systems, where intraband\ncomponents vanish due to symmetry constraints. As a result, in centrosymmetric\nsystems, only the off-diagonal components of the density matrix contribute to\nthe extrinsic OHE. In contrast, in noncentrosymmetric systems, both diagonal\nand off-diagonal components play a role. Our study suggests that in\nexperimentally relevant, highly doped systems, the OHE is predominantly\nextrinsic in nature, regardless of whether the system is centrosymmetric or\nnoncentrosymmetric. Importantly, we infer that even a weakly breaking of\ninversion symmetry can lead to a dramatic enhancement of the OHE, a finding\nwith significant implications for experimental investigations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Both intrinsic and extrinsic orbital Hall effects (OHE) in bilayer transition\nmetal dichalcogenides (TMDs) are investigated in the presence of short-range\ndisorder using quantum kinetic theory. Bilayer TMDs provide an ideal platform\nto study the effects of inversion symmetry breaking on transport properties due\nto their unique structural and electronic characteristics. While bilayer TMDs\nare naturally inversion symmetric, applying a finite gate voltage to create a\nbias between the layers effectively breaks this symmetry. Our findings reveal\nthat slightly away from the band edges, the extrinsic OHE becomes the dominant\ncontribution in both inversion-symmetric and asymmetric cases, with its\nprominence increasing significantly as the Fermi energy rises. Furthermore, we\ndemonstrate that breaking inversion symmetry greatly enhances the extrinsic\nOHE. This enhancement arises from the fundamentally distinct behavior of\norbital angular momentum (OAM) in centrosymmetric systems, where intraband\ncomponents vanish due to symmetry constraints. As a result, in centrosymmetric\nsystems, only the off-diagonal components of the density matrix contribute to\nthe extrinsic OHE. In contrast, in noncentrosymmetric systems, both diagonal\nand off-diagonal components play a role. Our study suggests that in\nexperimentally relevant, highly doped systems, the OHE is predominantly\nextrinsic in nature, regardless of whether the system is centrosymmetric or\nnoncentrosymmetric. Importantly, we infer that even a weakly breaking of\ninversion symmetry can lead to a dramatic enhancement of the OHE, a finding\nwith significant implications for experimental investigations."
                },
                "authors": [
                    {
                        "name": "Azadeh Faridi"
                    },
                    {
                        "name": "Reza Asgari"
                    }
                ],
                "author_detail": {
                    "name": "Reza Asgari"
                },
                "author": "Reza Asgari",
                "arxiv_comment": "12 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02996v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02996v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04306v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04306v1",
                "updated": "2025-01-08T06:44:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    6,
                    44,
                    2,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T06:44:02Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    6,
                    44,
                    2,
                    2,
                    8,
                    0
                ],
                "title": "LLM4SR: A Survey on Large Language Models for Scientific Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4SR: A Survey on Large Language Models for Scientific Research"
                },
                "summary": "In recent years, the rapid advancement of Large Language Models (LLMs) has\ntransformed the landscape of scientific research, offering unprecedented\nsupport across various stages of the research cycle. This paper presents the\nfirst systematic survey dedicated to exploring how LLMs are revolutionizing the\nscientific research process. We analyze the unique roles LLMs play across four\ncritical stages of research: hypothesis discovery, experiment planning and\nimplementation, scientific writing, and peer reviewing. Our review\ncomprehensively showcases the task-specific methodologies and evaluation\nbenchmarks. By identifying current challenges and proposing future research\ndirections, this survey not only highlights the transformative potential of\nLLMs, but also aims to inspire and guide researchers and practitioners in\nleveraging LLMs to advance scientific inquiry. Resources are available at the\nfollowing repository: https://github.com/du-nlp-lab/LLM4SR",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the rapid advancement of Large Language Models (LLMs) has\ntransformed the landscape of scientific research, offering unprecedented\nsupport across various stages of the research cycle. This paper presents the\nfirst systematic survey dedicated to exploring how LLMs are revolutionizing the\nscientific research process. We analyze the unique roles LLMs play across four\ncritical stages of research: hypothesis discovery, experiment planning and\nimplementation, scientific writing, and peer reviewing. Our review\ncomprehensively showcases the task-specific methodologies and evaluation\nbenchmarks. By identifying current challenges and proposing future research\ndirections, this survey not only highlights the transformative potential of\nLLMs, but also aims to inspire and guide researchers and practitioners in\nleveraging LLMs to advance scientific inquiry. Resources are available at the\nfollowing repository: https://github.com/du-nlp-lab/LLM4SR"
                },
                "authors": [
                    {
                        "name": "Ziming Luo"
                    },
                    {
                        "name": "Zonglin Yang"
                    },
                    {
                        "name": "Zexin Xu"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Xinya Du"
                    }
                ],
                "author_detail": {
                    "name": "Xinya Du"
                },
                "author": "Xinya Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04306v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04306v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.00398v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.00398v3",
                "updated": "2025-01-08T06:35:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    6,
                    35,
                    7,
                    2,
                    8,
                    0
                ],
                "published": "2023-06-01T07:00:07Z",
                "published_parsed": [
                    2023,
                    6,
                    1,
                    7,
                    0,
                    7,
                    3,
                    152,
                    0
                ],
                "title": "Preference-grounded Token-level Guidance for Language Model Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference-grounded Token-level Guidance for Language Model Fine-tuning"
                },
                "summary": "Aligning language models (LMs) with preferences is an important problem in\nnatural language generation. A key challenge is that preferences are typically\nprovided at the sequence level while LM training and generation both occur at\nthe token level. There is, therefore, a granularity mismatch between the\npreference and the LM training losses, which may complicate the learning\nproblem. In this paper, we address this issue by developing an alternate\ntraining process, where we iterate between grounding the sequence-level\npreference into token-level training guidance, and improving the LM with the\nlearned guidance. For guidance learning, we design a framework that extends the\npairwise-preference learning in imitation learning to both variable-length LM\ngeneration and the utilization of the preference among multiple generations.\nFor LM training, based on the amount of supervised data, we present two\nminimalist learning objectives that utilize the learned guidance. In\nexperiments, our method performs competitively on two distinct representative\nLM tasks -- discrete-prompt generation and text summarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning language models (LMs) with preferences is an important problem in\nnatural language generation. A key challenge is that preferences are typically\nprovided at the sequence level while LM training and generation both occur at\nthe token level. There is, therefore, a granularity mismatch between the\npreference and the LM training losses, which may complicate the learning\nproblem. In this paper, we address this issue by developing an alternate\ntraining process, where we iterate between grounding the sequence-level\npreference into token-level training guidance, and improving the LM with the\nlearned guidance. For guidance learning, we design a framework that extends the\npairwise-preference learning in imitation learning to both variable-length LM\ngeneration and the utilization of the preference among multiple generations.\nFor LM training, based on the amount of supervised data, we present two\nminimalist learning objectives that utilize the learned guidance. In\nexperiments, our method performs competitively on two distinct representative\nLM tasks -- discrete-prompt generation and text summarization."
                },
                "authors": [
                    {
                        "name": "Shentao Yang"
                    },
                    {
                        "name": "Shujian Zhang"
                    },
                    {
                        "name": "Congying Xia"
                    },
                    {
                        "name": "Yihao Feng"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Mingyuan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Mingyuan Zhou"
                },
                "author": "Mingyuan Zhou",
                "arxiv_comment": "v2: 37th Conference on Neural Information Processing Systems (NeurIPS\n  2023); v3: update on scaling up to PPO + LLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.00398v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.00398v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04288v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04288v1",
                "updated": "2025-01-08T05:27:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    5,
                    27,
                    16,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T05:27:16Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    5,
                    27,
                    16,
                    2,
                    8,
                    0
                ],
                "title": "An Analysis of Model Robustness across Concurrent Distribution Shifts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Analysis of Model Robustness across Concurrent Distribution Shifts"
                },
                "summary": "Machine learning models, meticulously optimized for source data, often fail\nto predict target data when faced with distribution shifts (DSs). Previous\nbenchmarking studies, though extensive, have mainly focused on simple DSs.\nRecognizing that DSs often occur in more complex forms in real-world scenarios,\nwe broadened our study to include multiple concurrent shifts, such as unseen\ndomain shifts combined with spurious correlations. We evaluated 26 algorithms\nthat range from simple heuristic augmentations to zero-shot inference using\nfoundation models, across 168 source-target pairs from eight datasets. Our\nanalysis of over 100K models reveals that (i) concurrent DSs typically worsen\nperformance compared to a single shift, with certain exceptions, (ii) if a\nmodel improves generalization for one distribution shift, it tends to be\neffective for others, and (iii) heuristic data augmentations achieve the best\noverall performance on both synthetic and real-world datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning models, meticulously optimized for source data, often fail\nto predict target data when faced with distribution shifts (DSs). Previous\nbenchmarking studies, though extensive, have mainly focused on simple DSs.\nRecognizing that DSs often occur in more complex forms in real-world scenarios,\nwe broadened our study to include multiple concurrent shifts, such as unseen\ndomain shifts combined with spurious correlations. We evaluated 26 algorithms\nthat range from simple heuristic augmentations to zero-shot inference using\nfoundation models, across 168 source-target pairs from eight datasets. Our\nanalysis of over 100K models reveals that (i) concurrent DSs typically worsen\nperformance compared to a single shift, with certain exceptions, (ii) if a\nmodel improves generalization for one distribution shift, it tends to be\neffective for others, and (iii) heuristic data augmentations achieve the best\noverall performance on both synthetic and real-world datasets."
                },
                "authors": [
                    {
                        "name": "Myeongho Jeon"
                    },
                    {
                        "name": "Suhwan Choi"
                    },
                    {
                        "name": "Hyoje Lee"
                    },
                    {
                        "name": "Teresa Yeo"
                    }
                ],
                "author_detail": {
                    "name": "Teresa Yeo"
                },
                "author": "Teresa Yeo",
                "arxiv_comment": "Accepted to TMLR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04288v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04288v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04287v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04287v1",
                "updated": "2025-01-08T05:25:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    5,
                    25,
                    14,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T05:25:14Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    5,
                    25,
                    14,
                    2,
                    8,
                    0
                ],
                "title": "ElasticZO: A Memory-Efficient On-Device Learning with Combined Zeroth-\n  and First-Order Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ElasticZO: A Memory-Efficient On-Device Learning with Combined Zeroth-\n  and First-Order Optimization"
                },
                "summary": "Zeroth-order (ZO) optimization is being recognized as a simple yet powerful\nalternative to standard backpropagation (BP)-based training. Notably, ZO\noptimization allows for training with only forward passes and (almost) the same\nmemory as inference, making it well-suited for edge devices with limited\ncomputing and memory resources. In this paper, we propose ZO-based on-device\nlearning (ODL) methods for full-precision and 8-bit quantized deep neural\nnetworks (DNNs), namely ElasticZO and ElasticZO-INT8. ElasticZO lies in the\nmiddle between pure ZO- and pure BP-based approaches, and is based on the idea\nto employ BP for the last few layers and ZO for the remaining layers.\nElasticZO-INT8 achieves integer arithmetic-only ZO-based training for the first\ntime, by incorporating a novel method for computing quantized ZO gradients from\ninteger cross-entropy loss values. Experimental results on the classification\ndatasets show that ElasticZO effectively addresses the slow convergence of\nvanilla ZO and shrinks the accuracy gap to BP-based training. Compared to\nvanilla ZO, ElasticZO achieves 5.2-9.5% higher accuracy with only 0.072-1.7%\nmemory overhead, and can handle fine-tuning tasks as well as full training.\nElasticZO-INT8 further reduces the memory usage and training time by 1.46-1.60x\nand 1.38-1.42x without compromising the accuracy. These results demonstrate a\nbetter tradeoff between accuracy and training cost compared to pure ZO- and\nBP-based approaches, and also highlight the potential of ZO optimization in\non-device learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zeroth-order (ZO) optimization is being recognized as a simple yet powerful\nalternative to standard backpropagation (BP)-based training. Notably, ZO\noptimization allows for training with only forward passes and (almost) the same\nmemory as inference, making it well-suited for edge devices with limited\ncomputing and memory resources. In this paper, we propose ZO-based on-device\nlearning (ODL) methods for full-precision and 8-bit quantized deep neural\nnetworks (DNNs), namely ElasticZO and ElasticZO-INT8. ElasticZO lies in the\nmiddle between pure ZO- and pure BP-based approaches, and is based on the idea\nto employ BP for the last few layers and ZO for the remaining layers.\nElasticZO-INT8 achieves integer arithmetic-only ZO-based training for the first\ntime, by incorporating a novel method for computing quantized ZO gradients from\ninteger cross-entropy loss values. Experimental results on the classification\ndatasets show that ElasticZO effectively addresses the slow convergence of\nvanilla ZO and shrinks the accuracy gap to BP-based training. Compared to\nvanilla ZO, ElasticZO achieves 5.2-9.5% higher accuracy with only 0.072-1.7%\nmemory overhead, and can handle fine-tuning tasks as well as full training.\nElasticZO-INT8 further reduces the memory usage and training time by 1.46-1.60x\nand 1.38-1.42x without compromising the accuracy. These results demonstrate a\nbetter tradeoff between accuracy and training cost compared to pure ZO- and\nBP-based approaches, and also highlight the potential of ZO optimization in\non-device learning."
                },
                "authors": [
                    {
                        "name": "Keisuke Sugiura"
                    },
                    {
                        "name": "Hiroki Matsutani"
                    }
                ],
                "author_detail": {
                    "name": "Hiroki Matsutani"
                },
                "author": "Hiroki Matsutani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04287v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04287v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04285v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04285v1",
                "updated": "2025-01-08T05:17:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    5,
                    17,
                    9,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T05:17:09Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    5,
                    17,
                    9,
                    2,
                    8,
                    0
                ],
                "title": "Separate Source Channel Coding Is Still What You Need: An LLM-based\n  Rethinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Separate Source Channel Coding Is Still What You Need: An LLM-based\n  Rethinking"
                },
                "summary": "Along with the proliferating research interest in Semantic Communication\n(SemCom), Joint Source Channel Coding (JSCC) has dominated the attention due to\nthe widely assumed existence in efficiently delivering information semantics.\n%has emerged as a pivotal area of research, aiming to enhance the efficiency\nand reliability of information transmission through deep learning-based\nmethods. Nevertheless, this paper challenges the conventional JSCC paradigm,\nand advocates for adoption of Separate Source Channel Coding (SSCC) to enjoy\nthe underlying more degree of freedom for optimization. We demonstrate that\nSSCC, after leveraging the strengths of Large Language Model (LLM) for source\ncoding and Error Correction Code Transformer (ECCT) complemented for channel\ndecoding, offers superior performance over JSCC. Our proposed framework also\neffectively highlights the compatibility challenges between SemCom approaches\nand digital communication systems, particularly concerning the resource costs\nassociated with the transmission of high precision floating point numbers.\nThrough comprehensive evaluations, we establish that empowered by LLM-based\ncompression and ECCT-enhanced error correction, SSCC remains a viable and\neffective solution for modern communication systems. In other words, separate\nsource and channel coding is still what we need!",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Along with the proliferating research interest in Semantic Communication\n(SemCom), Joint Source Channel Coding (JSCC) has dominated the attention due to\nthe widely assumed existence in efficiently delivering information semantics.\n%has emerged as a pivotal area of research, aiming to enhance the efficiency\nand reliability of information transmission through deep learning-based\nmethods. Nevertheless, this paper challenges the conventional JSCC paradigm,\nand advocates for adoption of Separate Source Channel Coding (SSCC) to enjoy\nthe underlying more degree of freedom for optimization. We demonstrate that\nSSCC, after leveraging the strengths of Large Language Model (LLM) for source\ncoding and Error Correction Code Transformer (ECCT) complemented for channel\ndecoding, offers superior performance over JSCC. Our proposed framework also\neffectively highlights the compatibility challenges between SemCom approaches\nand digital communication systems, particularly concerning the resource costs\nassociated with the transmission of high precision floating point numbers.\nThrough comprehensive evaluations, we establish that empowered by LLM-based\ncompression and ECCT-enhanced error correction, SSCC remains a viable and\neffective solution for modern communication systems. In other words, separate\nsource and channel coding is still what we need!"
                },
                "authors": [
                    {
                        "name": "Tianqi Ren"
                    },
                    {
                        "name": "Rongpeng Li"
                    },
                    {
                        "name": "Ming-min Zhao"
                    },
                    {
                        "name": "Xianfu Chen"
                    },
                    {
                        "name": "Guangyi Liu"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Zhifeng Zhao"
                    },
                    {
                        "name": "Honggang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Honggang Zhang"
                },
                "author": "Honggang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04285v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04285v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04284v1",
                "updated": "2025-01-08T05:15:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    5,
                    15,
                    43,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T05:15:43Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    5,
                    15,
                    43,
                    2,
                    8,
                    0
                ],
                "title": "ContextMRI: Enhancing Compressed Sensing MRI through Metadata\n  Conditioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextMRI: Enhancing Compressed Sensing MRI through Metadata\n  Conditioning"
                },
                "summary": "Compressed sensing MRI seeks to accelerate MRI acquisition processes by\nsampling fewer k-space measurements and then reconstructing the missing data\nalgorithmically. The success of these approaches often relies on strong priors\nor learned statistical models. While recent diffusion model-based priors have\nshown great potential, previous methods typically ignore clinically available\nmetadata (e.g. patient demographics, imaging parameters, slice-specific\ninformation). In practice, metadata contains meaningful cues about the anatomy\nand acquisition protocol, suggesting it could further constrain the\nreconstruction problem. In this work, we propose ContextMRI, a text-conditioned\ndiffusion model for MRI that integrates granular metadata into the\nreconstruction process. We train a pixel-space diffusion model directly on\nminimally processed, complex-valued MRI images. During inference, metadata is\nconverted into a structured text prompt and fed to the model via CLIP text\nembeddings. By conditioning the prior on metadata, we unlock more accurate\nreconstructions and show consistent gains across multiple datasets,\nacceleration factors, and undersampling patterns. Our experiments demonstrate\nthat increasing the fidelity of metadata, ranging from slice location and\ncontrast to patient age, sex, and pathology, systematically boosts\nreconstruction performance. This work highlights the untapped potential of\nleveraging clinical context for inverse problems and opens a new direction for\nmetadata-driven MRI reconstruction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressed sensing MRI seeks to accelerate MRI acquisition processes by\nsampling fewer k-space measurements and then reconstructing the missing data\nalgorithmically. The success of these approaches often relies on strong priors\nor learned statistical models. While recent diffusion model-based priors have\nshown great potential, previous methods typically ignore clinically available\nmetadata (e.g. patient demographics, imaging parameters, slice-specific\ninformation). In practice, metadata contains meaningful cues about the anatomy\nand acquisition protocol, suggesting it could further constrain the\nreconstruction problem. In this work, we propose ContextMRI, a text-conditioned\ndiffusion model for MRI that integrates granular metadata into the\nreconstruction process. We train a pixel-space diffusion model directly on\nminimally processed, complex-valued MRI images. During inference, metadata is\nconverted into a structured text prompt and fed to the model via CLIP text\nembeddings. By conditioning the prior on metadata, we unlock more accurate\nreconstructions and show consistent gains across multiple datasets,\nacceleration factors, and undersampling patterns. Our experiments demonstrate\nthat increasing the fidelity of metadata, ranging from slice location and\ncontrast to patient age, sex, and pathology, systematically boosts\nreconstruction performance. This work highlights the untapped potential of\nleveraging clinical context for inverse problems and opens a new direction for\nmetadata-driven MRI reconstruction."
                },
                "authors": [
                    {
                        "name": "Hyungjin Chung"
                    },
                    {
                        "name": "Dohun Lee"
                    },
                    {
                        "name": "Zihui Wu"
                    },
                    {
                        "name": "Byung-Hoon Kim"
                    },
                    {
                        "name": "Katherine L. Bouman"
                    },
                    {
                        "name": "Jong Chul Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jong Chul Ye"
                },
                "author": "Jong Chul Ye",
                "arxiv_comment": "29 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04277v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04277v1",
                "updated": "2025-01-08T04:55:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    4,
                    55,
                    59,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T04:55:59Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    4,
                    55,
                    59,
                    2,
                    8,
                    0
                ],
                "title": "Exploring the Expertise of Large Language Models in Materials Science\n  and Metallurgical Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Expertise of Large Language Models in Materials Science\n  and Metallurgical Engineering"
                },
                "summary": "The integration of artificial intelligence into various domains is rapidly\nincreasing, with Large Language Models (LLMs) becoming more prevalent in\nnumerous applications. This work is included in an overall project which aims\nto train an LLM specifically in the field of materials science. To assess the\nimpact of this specialized training, it is essential to establish the baseline\nperformance of existing LLMs in materials science. In this study, we evaluated\n15 different LLMs using the MaScQA question answering (Q&A) benchmark. This\nbenchmark comprises questions from the Graduate Aptitude Test in Engineering\n(GATE), tailored to test models' capabilities in answering questions related to\nmaterials science and metallurgical engineering. Our results indicate that\nclosed-source LLMs, such as Claude-3.5-Sonnet and GPT-4, perform the best with\nan overall accuracy of ~84%, while the open-source models, Llama3-70b and\nPhi3-14b, top at ~56% and ~43%, respectively. These findings provide a baseline\nfor the raw capabilities of LLMs on Q&A tasks applied to materials science, and\nemphasize the substantial improvement that could be brought to open-source\nmodels via prompt engineering and fine-tuning strategies. We anticipate that\nthis work could push the adoption of LLMs as valuable assistants in materials\nscience, demonstrating their utility in this specialized domain and related\nsub-domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of artificial intelligence into various domains is rapidly\nincreasing, with Large Language Models (LLMs) becoming more prevalent in\nnumerous applications. This work is included in an overall project which aims\nto train an LLM specifically in the field of materials science. To assess the\nimpact of this specialized training, it is essential to establish the baseline\nperformance of existing LLMs in materials science. In this study, we evaluated\n15 different LLMs using the MaScQA question answering (Q&A) benchmark. This\nbenchmark comprises questions from the Graduate Aptitude Test in Engineering\n(GATE), tailored to test models' capabilities in answering questions related to\nmaterials science and metallurgical engineering. Our results indicate that\nclosed-source LLMs, such as Claude-3.5-Sonnet and GPT-4, perform the best with\nan overall accuracy of ~84%, while the open-source models, Llama3-70b and\nPhi3-14b, top at ~56% and ~43%, respectively. These findings provide a baseline\nfor the raw capabilities of LLMs on Q&A tasks applied to materials science, and\nemphasize the substantial improvement that could be brought to open-source\nmodels via prompt engineering and fine-tuning strategies. We anticipate that\nthis work could push the adoption of LLMs as valuable assistants in materials\nscience, demonstrating their utility in this specialized domain and related\nsub-domains."
                },
                "authors": [
                    {
                        "name": "Christophe Bajan"
                    },
                    {
                        "name": "Guillaume Lambard"
                    }
                ],
                "author_detail": {
                    "name": "Guillaume Lambard"
                },
                "author": "Guillaume Lambard",
                "arxiv_comment": "13 pages, 6 figures, 5 tables, accepted in Digital Discovery",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04277v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03295v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03295v2",
                "updated": "2025-01-08T04:50:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    4,
                    50,
                    1,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-06T11:43:29Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    11,
                    43,
                    29,
                    0,
                    6,
                    0
                ],
                "title": "A Soft Sensor Method with Uncertainty-Awareness and Self-Explanation\n  Based on Large Language Models Enhanced by Domain Knowledge Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Soft Sensor Method with Uncertainty-Awareness and Self-Explanation\n  Based on Large Language Models Enhanced by Domain Knowledge Retrieval"
                },
                "summary": "Data-driven soft sensors are crucial in predicting key performance indicators\nin industrial systems. However, current methods predominantly rely on the\nsupervised learning paradigms of parameter updating, which inherently faces\nchallenges such as high development costs, poor robustness, training\ninstability, and lack of interpretability. Recently, large language models\n(LLMs) have demonstrated significant potential across various domains, notably\nthrough In-Context Learning (ICL), which enables high-performance task\nexecution with minimal input-label demonstrations and no prior training. This\npaper aims to replace supervised learning with the emerging ICL paradigm for\nsoft sensor modeling to address existing challenges and explore new avenues for\nadvancement. To achieve this, we propose a novel framework called the Few-shot\nUncertainty-aware and self-Explaining Soft Sensor (LLM-FUESS), which includes\nthe Zero-shot Auxiliary Variable Selector (LLM-ZAVS) and the Uncertainty-aware\nFew-shot Soft Sensor (LLM-UFSS). The LLM-ZAVS retrieves from the Industrial\nKnowledge Vector Storage to enhance LLMs' domain-specific knowledge, enabling\nzero-shot auxiliary variable selection. In the LLM-UFSS, we utilize text-based\ncontext demonstrations of structured data to prompt LLMs to execute ICL for\npredicting and propose a context sample retrieval augmentation strategy to\nimprove performance. Additionally, we explored LLMs' AIGC and probabilistic\ncharacteristics to propose self-explanation and uncertainty quantification\nmethods for constructing a trustworthy soft sensor. Extensive experiments\ndemonstrate that our method achieved state-of-the-art predictive performance,\nstrong robustness, and flexibility, effectively mitigates training instability\nfound in traditional methods. To the best of our knowledge, this is the first\nwork to establish soft sensor utilizing LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven soft sensors are crucial in predicting key performance indicators\nin industrial systems. However, current methods predominantly rely on the\nsupervised learning paradigms of parameter updating, which inherently faces\nchallenges such as high development costs, poor robustness, training\ninstability, and lack of interpretability. Recently, large language models\n(LLMs) have demonstrated significant potential across various domains, notably\nthrough In-Context Learning (ICL), which enables high-performance task\nexecution with minimal input-label demonstrations and no prior training. This\npaper aims to replace supervised learning with the emerging ICL paradigm for\nsoft sensor modeling to address existing challenges and explore new avenues for\nadvancement. To achieve this, we propose a novel framework called the Few-shot\nUncertainty-aware and self-Explaining Soft Sensor (LLM-FUESS), which includes\nthe Zero-shot Auxiliary Variable Selector (LLM-ZAVS) and the Uncertainty-aware\nFew-shot Soft Sensor (LLM-UFSS). The LLM-ZAVS retrieves from the Industrial\nKnowledge Vector Storage to enhance LLMs' domain-specific knowledge, enabling\nzero-shot auxiliary variable selection. In the LLM-UFSS, we utilize text-based\ncontext demonstrations of structured data to prompt LLMs to execute ICL for\npredicting and propose a context sample retrieval augmentation strategy to\nimprove performance. Additionally, we explored LLMs' AIGC and probabilistic\ncharacteristics to propose self-explanation and uncertainty quantification\nmethods for constructing a trustworthy soft sensor. Extensive experiments\ndemonstrate that our method achieved state-of-the-art predictive performance,\nstrong robustness, and flexibility, effectively mitigates training instability\nfound in traditional methods. To the best of our knowledge, this is the first\nwork to establish soft sensor utilizing LLMs."
                },
                "authors": [
                    {
                        "name": "Shuo Tong"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Runyuan Guo"
                    },
                    {
                        "name": "Wenqing Wang"
                    },
                    {
                        "name": "Xueqiong Tian"
                    },
                    {
                        "name": "Lingyun Wei"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Huayong Wu"
                    },
                    {
                        "name": "Ding Liu"
                    },
                    {
                        "name": "Youmin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Youmin Zhang"
                },
                "author": "Youmin Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03295v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03295v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04266v1",
                "updated": "2025-01-08T04:19:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    4,
                    19,
                    57,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T04:19:57Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    4,
                    19,
                    57,
                    2,
                    8,
                    0
                ],
                "title": "Scaling Large Language Model Training on Frontier with Low-Bandwidth\n  Partitioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Large Language Model Training on Frontier with Low-Bandwidth\n  Partitioning"
                },
                "summary": "Scaling up Large Language Model(LLM) training involves fitting a tremendous\namount of training parameters across a limited number of workers. However,\nmethods like ZeRO-3 that drastically reduce GPU memory pressure often incur\nheavy communication to ensure global synchronization and consistency.\nEstablished efforts such as ZeRO++ use secondary partitions to avoid inter-node\ncommunications, given that intra-node GPU-GPU transfer generally has more\nbandwidth and lower latency than inter-node connections. However, as more\ncapable infrastructure like Frontier, equipped with AMD GPUs, emerged with\nimpressive computing capability, there is a need for investigations on the\nhardware topology and to develop targeted strategies to improve training\nefficiency. In this work, we propose a collection of communication and\noptimization strategies for ZeRO++ to reduce communication costs and improve\nmemory utilization. In this paper, we propose a 3-level hierarchical\npartitioning specifically for the current Top-1 supercomputing cluster,\nFrontier, which aims at leveraging various bandwidths across layers of\ncommunications (GCD-GCD, GPU-GPU, and inter-node) to reduce communication\noverhead. For a 20B GPT model, we observe a 1.71x increase in TFLOPS per GPU\nwhen compared with ZeRO++ up to 384 GCDs and a scaling efficiency of 0.94 for\nup to 384 GCDs. To the best of our knowledge, our work is also the first effort\nto efficiently optimize LLM workloads on Frontier AMD GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling up Large Language Model(LLM) training involves fitting a tremendous\namount of training parameters across a limited number of workers. However,\nmethods like ZeRO-3 that drastically reduce GPU memory pressure often incur\nheavy communication to ensure global synchronization and consistency.\nEstablished efforts such as ZeRO++ use secondary partitions to avoid inter-node\ncommunications, given that intra-node GPU-GPU transfer generally has more\nbandwidth and lower latency than inter-node connections. However, as more\ncapable infrastructure like Frontier, equipped with AMD GPUs, emerged with\nimpressive computing capability, there is a need for investigations on the\nhardware topology and to develop targeted strategies to improve training\nefficiency. In this work, we propose a collection of communication and\noptimization strategies for ZeRO++ to reduce communication costs and improve\nmemory utilization. In this paper, we propose a 3-level hierarchical\npartitioning specifically for the current Top-1 supercomputing cluster,\nFrontier, which aims at leveraging various bandwidths across layers of\ncommunications (GCD-GCD, GPU-GPU, and inter-node) to reduce communication\noverhead. For a 20B GPT model, we observe a 1.71x increase in TFLOPS per GPU\nwhen compared with ZeRO++ up to 384 GCDs and a scaling efficiency of 0.94 for\nup to 384 GCDs. To the best of our knowledge, our work is also the first effort\nto efficiently optimize LLM workloads on Frontier AMD GPUs."
                },
                "authors": [
                    {
                        "name": "Lang Xu"
                    },
                    {
                        "name": "Quentin Anthony"
                    },
                    {
                        "name": "Jacob Hatef"
                    },
                    {
                        "name": "Aamir Shafi"
                    },
                    {
                        "name": "Hari Subramoni"
                    },
                    {
                        "name": "Dhabaleswar K."
                    },
                    {
                        "name": "Panda"
                    }
                ],
                "author_detail": {
                    "name": "Panda"
                },
                "arxiv_affiliation": "DK",
                "author": "Panda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14795v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14795v5",
                "updated": "2025-01-08T03:56:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    3,
                    56,
                    26,
                    2,
                    8,
                    0
                ],
                "published": "2024-04-23T07:19:20Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    7,
                    19,
                    20,
                    1,
                    114,
                    0
                ],
                "title": "Watch Out for Your Guidance on Generation! Exploring Conditional\n  Backdoor Attacks against Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watch Out for Your Guidance on Generation! Exploring Conditional\n  Backdoor Attacks against Large Language Models"
                },
                "summary": "Mainstream backdoor attacks on large language models (LLMs) typically set a\nfixed trigger in the input instance and specific responses for triggered\nqueries. However, the fixed trigger setting (e.g., unusual words) may be easily\ndetected by human detection, limiting the effectiveness and practicality in\nreal-world scenarios. To enhance the stealthiness of backdoor activation, we\npresent a new poisoning paradigm against LLMs triggered by specifying\ngeneration conditions, which are commonly adopted strategies by users during\nmodel inference. The poisoned model performs normally for output under\nnormal/other generation conditions, while becomes harmful for output under\ntarget generation conditions. To achieve this objective, we introduce BrieFool,\nan efficient attack framework. It leverages the characteristics of generation\nconditions by efficient instruction sampling and poisoning data generation,\nthereby influencing the behavior of LLMs under target conditions. Our attack\ncan be generally divided into two types with different targets: Safety\nunalignment attack and Ability degradation attack. Our extensive experiments\ndemonstrate that BrieFool is effective across safety domains and ability\ndomains, achieving higher success rates than baseline methods, with 94.3 % on\nGPT-3.5-turbo",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mainstream backdoor attacks on large language models (LLMs) typically set a\nfixed trigger in the input instance and specific responses for triggered\nqueries. However, the fixed trigger setting (e.g., unusual words) may be easily\ndetected by human detection, limiting the effectiveness and practicality in\nreal-world scenarios. To enhance the stealthiness of backdoor activation, we\npresent a new poisoning paradigm against LLMs triggered by specifying\ngeneration conditions, which are commonly adopted strategies by users during\nmodel inference. The poisoned model performs normally for output under\nnormal/other generation conditions, while becomes harmful for output under\ntarget generation conditions. To achieve this objective, we introduce BrieFool,\nan efficient attack framework. It leverages the characteristics of generation\nconditions by efficient instruction sampling and poisoning data generation,\nthereby influencing the behavior of LLMs under target conditions. Our attack\ncan be generally divided into two types with different targets: Safety\nunalignment attack and Ability degradation attack. Our extensive experiments\ndemonstrate that BrieFool is effective across safety domains and ability\ndomains, achieving higher success rates than baseline methods, with 94.3 % on\nGPT-3.5-turbo"
                },
                "authors": [
                    {
                        "name": "Jiaming He"
                    },
                    {
                        "name": "Wenbo Jiang"
                    },
                    {
                        "name": "Guanyu Hou"
                    },
                    {
                        "name": "Wenshu Fan"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Hongwei Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongwei Li"
                },
                "author": "Hongwei Li",
                "arxiv_comment": "The paper has been accepted to AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.14795v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14795v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03271v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03271v2",
                "updated": "2025-01-08T03:51:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    3,
                    51,
                    59,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-05T00:08:52Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    0,
                    8,
                    52,
                    6,
                    5,
                    0
                ],
                "title": "DPO Kernels: A Semantically-Aware, Kernel-Enhanced, and Divergence-Rich\n  Paradigm for Direct Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DPO Kernels: A Semantically-Aware, Kernel-Enhanced, and Divergence-Rich\n  Paradigm for Direct Preference Optimization"
                },
                "summary": "The rapid rise of large language models (LLMs) has unlocked many applications\nbut also underscores the challenge of aligning them with diverse values and\npreferences. Direct Preference Optimization (DPO) is central to alignment but\nconstrained by fixed divergences and limited feature transformations. We\npropose DPO-Kernels, which integrates kernel methods to address these issues\nthrough four key contributions: (i) Kernelized Representations with polynomial,\nRBF, Mahalanobis, and spectral kernels for richer transformations, plus a\nhybrid loss combining embedding-based and probability-based objectives; (ii)\nDivergence Alternatives (Jensen-Shannon, Hellinger, Renyi, Bhattacharyya,\nWasserstein, and f-divergences) for greater stability; (iii) Data-Driven\nSelection metrics that automatically choose the best kernel-divergence pair;\nand (iv) a Hierarchical Mixture of Kernels for both local precision and global\nmodeling. Evaluations on 12 datasets demonstrate state-of-the-art performance\nin factuality, safety, reasoning, and instruction following. Grounded in\nHeavy-Tailed Self-Regularization, DPO-Kernels maintains robust generalization\nfor LLMs, offering a comprehensive resource for further alignment research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid rise of large language models (LLMs) has unlocked many applications\nbut also underscores the challenge of aligning them with diverse values and\npreferences. Direct Preference Optimization (DPO) is central to alignment but\nconstrained by fixed divergences and limited feature transformations. We\npropose DPO-Kernels, which integrates kernel methods to address these issues\nthrough four key contributions: (i) Kernelized Representations with polynomial,\nRBF, Mahalanobis, and spectral kernels for richer transformations, plus a\nhybrid loss combining embedding-based and probability-based objectives; (ii)\nDivergence Alternatives (Jensen-Shannon, Hellinger, Renyi, Bhattacharyya,\nWasserstein, and f-divergences) for greater stability; (iii) Data-Driven\nSelection metrics that automatically choose the best kernel-divergence pair;\nand (iv) a Hierarchical Mixture of Kernels for both local precision and global\nmodeling. Evaluations on 12 datasets demonstrate state-of-the-art performance\nin factuality, safety, reasoning, and instruction following. Grounded in\nHeavy-Tailed Self-Regularization, DPO-Kernels maintains robust generalization\nfor LLMs, offering a comprehensive resource for further alignment research."
                },
                "authors": [
                    {
                        "name": "Amitava Das"
                    },
                    {
                        "name": "Suranjana Trivedy"
                    },
                    {
                        "name": "Danush Khanna"
                    },
                    {
                        "name": "Rajarshi Roy"
                    },
                    {
                        "name": "Gurpreet Singh"
                    },
                    {
                        "name": "Basab Ghosh"
                    },
                    {
                        "name": "Yaswanth Narsupalli"
                    },
                    {
                        "name": "Vinija Jain"
                    },
                    {
                        "name": "Vasu Sharma"
                    },
                    {
                        "name": "Aishwarya Naresh Reganti"
                    },
                    {
                        "name": "Aman Chadha"
                    }
                ],
                "author_detail": {
                    "name": "Aman Chadha"
                },
                "author": "Aman Chadha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03271v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03271v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T45",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04259v1",
                "updated": "2025-01-08T03:50:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    3,
                    50,
                    15,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T03:50:15Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    3,
                    50,
                    15,
                    2,
                    8,
                    0
                ],
                "title": "Stable Derivative Free Gaussian Mixture Variational Inference for\n  Bayesian Inverse Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable Derivative Free Gaussian Mixture Variational Inference for\n  Bayesian Inverse Problems"
                },
                "summary": "This paper is concerned with the approximation of probability distributions\nknown up to normalization constants, with a focus on Bayesian inference for\nlarge-scale inverse problems in scientific computing. In this context, key\nchallenges include costly repeated evaluations of forward models,\nmultimodality, and inaccessible gradients for the forward model. To address\nthem, we develop a variational inference framework that combines Fisher-Rao\nnatural gradient with specialized quadrature rules to enable derivative free\nupdates of Gaussian mixture variational families. The resulting method, termed\nDerivative Free Gaussian Mixture Variational Inference (DF-GMVI), guarantees\ncovariance positivity and affine invariance, offering a stable and efficient\nframework for approximating complex posterior distributions. The effectiveness\nof DF-GMVI is demonstrated through numerical experiments on challenging\nscenarios, including distributions with multiple modes, infinitely many modes,\nand curved modes in spaces with up to hundreds of dimensions. The method's\npracticality is further demonstrated in a large-scale application, where it\nsuccessfully recovers the initial conditions of the Navier-Stokes equations\nfrom solution data at positive times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper is concerned with the approximation of probability distributions\nknown up to normalization constants, with a focus on Bayesian inference for\nlarge-scale inverse problems in scientific computing. In this context, key\nchallenges include costly repeated evaluations of forward models,\nmultimodality, and inaccessible gradients for the forward model. To address\nthem, we develop a variational inference framework that combines Fisher-Rao\nnatural gradient with specialized quadrature rules to enable derivative free\nupdates of Gaussian mixture variational families. The resulting method, termed\nDerivative Free Gaussian Mixture Variational Inference (DF-GMVI), guarantees\ncovariance positivity and affine invariance, offering a stable and efficient\nframework for approximating complex posterior distributions. The effectiveness\nof DF-GMVI is demonstrated through numerical experiments on challenging\nscenarios, including distributions with multiple modes, infinitely many modes,\nand curved modes in spaces with up to hundreds of dimensions. The method's\npracticality is further demonstrated in a large-scale application, where it\nsuccessfully recovers the initial conditions of the Navier-Stokes equations\nfrom solution data at positive times."
                },
                "authors": [
                    {
                        "name": "Baojun Che"
                    },
                    {
                        "name": "Yifan Chen"
                    },
                    {
                        "name": "Zhenghao Huan"
                    },
                    {
                        "name": "Daniel Zhengyu Huang"
                    },
                    {
                        "name": "Weijie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weijie Wang"
                },
                "author": "Weijie Wang",
                "arxiv_comment": "25 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04249v1",
                "updated": "2025-01-08T03:15:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    3,
                    15,
                    10,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T03:15:10Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    3,
                    15,
                    10,
                    2,
                    8,
                    0
                ],
                "title": "IOLBENCH: Benchmarking LLMs on Linguistic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IOLBENCH: Benchmarking LLMs on Linguistic Reasoning"
                },
                "summary": "Despite the remarkable advancements and widespread applications of deep\nneural networks, their ability to perform reasoning tasks remains limited,\nparticularly in domains requiring structured, abstract thought. In this paper,\nwe investigate the linguistic reasoning capabilities of state-of-the-art large\nlanguage models (LLMs) by introducing IOLBENCH, a novel benchmark derived from\nInternational Linguistics Olympiad (IOL) problems. This dataset encompasses\ndiverse problems testing syntax, morphology, phonology, and semantics, all\ncarefully designed to be self-contained and independent of external knowledge.\nThese tasks challenge models to engage in metacognitive linguistic reasoning,\nrequiring the deduction of linguistic rules and patterns from minimal examples.\nThrough extensive benchmarking of leading LLMs, we find that even the most\nadvanced models struggle to handle the intricacies of linguistic complexity,\nparticularly in areas demanding compositional generalization and rule\nabstraction. Our analysis highlights both the strengths and persistent\nlimitations of current models in linguistic problem-solving, offering valuable\ninsights into their reasoning capabilities. By introducing IOLBENCH, we aim to\nfoster further research into developing models capable of human-like reasoning,\nwith broader implications for the fields of computational linguistics and\nartificial intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable advancements and widespread applications of deep\nneural networks, their ability to perform reasoning tasks remains limited,\nparticularly in domains requiring structured, abstract thought. In this paper,\nwe investigate the linguistic reasoning capabilities of state-of-the-art large\nlanguage models (LLMs) by introducing IOLBENCH, a novel benchmark derived from\nInternational Linguistics Olympiad (IOL) problems. This dataset encompasses\ndiverse problems testing syntax, morphology, phonology, and semantics, all\ncarefully designed to be self-contained and independent of external knowledge.\nThese tasks challenge models to engage in metacognitive linguistic reasoning,\nrequiring the deduction of linguistic rules and patterns from minimal examples.\nThrough extensive benchmarking of leading LLMs, we find that even the most\nadvanced models struggle to handle the intricacies of linguistic complexity,\nparticularly in areas demanding compositional generalization and rule\nabstraction. Our analysis highlights both the strengths and persistent\nlimitations of current models in linguistic problem-solving, offering valuable\ninsights into their reasoning capabilities. By introducing IOLBENCH, we aim to\nfoster further research into developing models capable of human-like reasoning,\nwith broader implications for the fields of computational linguistics and\nartificial intelligence."
                },
                "authors": [
                    {
                        "name": "Satyam Goyal"
                    },
                    {
                        "name": "Soham Dan"
                    }
                ],
                "author_detail": {
                    "name": "Soham Dan"
                },
                "author": "Soham Dan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16950v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16950v4",
                "updated": "2025-01-08T03:14:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    3,
                    14,
                    4,
                    2,
                    8,
                    0
                ],
                "published": "2024-03-25T17:11:28Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    17,
                    11,
                    28,
                    0,
                    85,
                    0
                ],
                "title": "Aligning with Human Judgement: The Role of Pairwise Large Language Model\n  Evaluators in Preference Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning with Human Judgement: The Role of Pairwise Large Language Model\n  Evaluators in Preference Aggregation"
                },
                "summary": "Large Language Models (LLMs) have demonstrated promising capabilities as\nautomatic evaluators in assessing the quality of generated natural language.\nHowever, LLMs still exhibit biases in evaluation and often struggle to generate\ncoherent evaluations that align with human assessments. In this work, we first\nconduct a systematic study of the misalignment between LLM evaluators and human\njudgement, revealing that existing calibration methods aimed at mitigating\nbiases are insufficient for effectively aligning LLM evaluators. Inspired by\nthe use of preference data in RLHF, we formulate the evaluation as a ranking\nproblem and introduce Pairwise-preference Search (PairS), an uncertainty-guided\nsearch method that employs LLMs to conduct pairwise comparisons and efficiently\nranks candidate texts. PairS achieves state-of-the-art performance on\nrepresentative evaluation tasks and demonstrates significant improvements over\ndirect scoring. Furthermore, we provide insights into the role of pairwise\npreference in quantifying the transitivity of LLMs and demonstrate how PairS\nbenefits from calibration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated promising capabilities as\nautomatic evaluators in assessing the quality of generated natural language.\nHowever, LLMs still exhibit biases in evaluation and often struggle to generate\ncoherent evaluations that align with human assessments. In this work, we first\nconduct a systematic study of the misalignment between LLM evaluators and human\njudgement, revealing that existing calibration methods aimed at mitigating\nbiases are insufficient for effectively aligning LLM evaluators. Inspired by\nthe use of preference data in RLHF, we formulate the evaluation as a ranking\nproblem and introduce Pairwise-preference Search (PairS), an uncertainty-guided\nsearch method that employs LLMs to conduct pairwise comparisons and efficiently\nranks candidate texts. PairS achieves state-of-the-art performance on\nrepresentative evaluation tasks and demonstrates significant improvements over\ndirect scoring. Furthermore, we provide insights into the role of pairwise\npreference in quantifying the transitivity of LLMs and demonstrate how PairS\nbenefits from calibration."
                },
                "authors": [
                    {
                        "name": "Yinhong Liu"
                    },
                    {
                        "name": "Han Zhou"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Ehsan Shareghi"
                    },
                    {
                        "name": "Ivan Vulić"
                    },
                    {
                        "name": "Anna Korhonen"
                    },
                    {
                        "name": "Nigel Collier"
                    }
                ],
                "author_detail": {
                    "name": "Nigel Collier"
                },
                "author": "Nigel Collier",
                "arxiv_comment": "This paper has been accepted by COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.16950v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16950v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.07594v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.07594v3",
                "updated": "2025-01-08T02:33:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    2,
                    33,
                    37,
                    2,
                    8,
                    0
                ],
                "published": "2023-11-10T09:51:24Z",
                "published_parsed": [
                    2023,
                    11,
                    10,
                    9,
                    51,
                    24,
                    4,
                    314,
                    0
                ],
                "title": "How to Bridge the Gap between Modalities: Survey on Multimodal Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Bridge the Gap between Modalities: Survey on Multimodal Large\n  Language Model"
                },
                "summary": "We explore Multimodal Large Language Models (MLLMs), which integrate LLMs\nlike GPT-4 to handle multimodal data, including text, images, audio, and more.\nMLLMs demonstrate capabilities such as generating image captions and answering\nimage-based questions, bridging the gap towards real-world human-computer\ninteractions and hinting at a potential pathway to artificial general\nintelligence. However, MLLMs still face challenges in addressing the semantic\ngap in multimodal data, which may lead to erroneous outputs, posing potential\nrisks to society. Selecting the appropriate modality alignment method is\ncrucial, as improper methods might require more parameters without significant\nperformance improvements. This paper aims to explore modality alignment methods\nfor LLMs and their current capabilities. Implementing effective modality\nalignment can help LLMs address environmental issues and enhance accessibility.\nThe study surveys existing modality alignment methods for MLLMs, categorizing\nthem into four groups: (1) Multimodal Converter, which transforms data into a\nformat that LLMs can understand; (2) Multimodal Perceiver, which improves how\nLLMs percieve different types of data; (3) Tool Learning, which leverages\nexternal tools to convert data into a common format, usually text; and (4)\nData-Driven Method, which teaches LLMs to understand specific data types within\ndatasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore Multimodal Large Language Models (MLLMs), which integrate LLMs\nlike GPT-4 to handle multimodal data, including text, images, audio, and more.\nMLLMs demonstrate capabilities such as generating image captions and answering\nimage-based questions, bridging the gap towards real-world human-computer\ninteractions and hinting at a potential pathway to artificial general\nintelligence. However, MLLMs still face challenges in addressing the semantic\ngap in multimodal data, which may lead to erroneous outputs, posing potential\nrisks to society. Selecting the appropriate modality alignment method is\ncrucial, as improper methods might require more parameters without significant\nperformance improvements. This paper aims to explore modality alignment methods\nfor LLMs and their current capabilities. Implementing effective modality\nalignment can help LLMs address environmental issues and enhance accessibility.\nThe study surveys existing modality alignment methods for MLLMs, categorizing\nthem into four groups: (1) Multimodal Converter, which transforms data into a\nformat that LLMs can understand; (2) Multimodal Perceiver, which improves how\nLLMs percieve different types of data; (3) Tool Learning, which leverages\nexternal tools to convert data into a common format, usually text; and (4)\nData-Driven Method, which teaches LLMs to understand specific data types within\ndatasets."
                },
                "authors": [
                    {
                        "name": "Shezheng Song"
                    },
                    {
                        "name": "Xiaopeng Li"
                    },
                    {
                        "name": "Shasha Li"
                    },
                    {
                        "name": "Shan Zhao"
                    },
                    {
                        "name": "Jie Yu"
                    },
                    {
                        "name": "Jun Ma"
                    },
                    {
                        "name": "Xiaoguang Mao"
                    },
                    {
                        "name": "Weimin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weimin Zhang"
                },
                "author": "Weimin Zhang",
                "arxiv_comment": "Accepted by TKDE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.07594v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.07594v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04234v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04234v1",
                "updated": "2025-01-08T02:17:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    2,
                    17,
                    34,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T02:17:34Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    2,
                    17,
                    34,
                    2,
                    8,
                    0
                ],
                "title": "Statistical Uncertainty Quantification for Aggregate Performance Metrics\n  in Machine Learning Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Uncertainty Quantification for Aggregate Performance Metrics\n  in Machine Learning Benchmarks"
                },
                "summary": "Modern artificial intelligence is supported by machine learning models (e.g.,\nfoundation models) that are pretrained on a massive data corpus and then\nadapted to solve a variety of downstream tasks. To summarize performance across\nmultiple tasks, evaluation metrics are often aggregated into a summary metric,\ne.g., average accuracy across 10 question-answering tasks. When aggregating\nevaluation metrics, it is useful to incorporate uncertainty in the aggregate\nmetric in order to gain a more realistic understanding of model performance.\nOur objective in this work is to demonstrate how statistical methodology can be\nused for quantifying uncertainty in metrics that have been aggregated across\nmultiple tasks. The methods we emphasize are bootstrapping, Bayesian\nhierarchical (i.e., multilevel) modeling, and the visualization of task\nweightings that consider standard errors. These techniques reveal insights such\nas the dominance of a specific model for certain types of tasks despite an\noverall poor performance. We use a popular ML benchmark, the Visual Task\nAdaptation Benchmark (VTAB), to demonstrate the usefulness of our approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern artificial intelligence is supported by machine learning models (e.g.,\nfoundation models) that are pretrained on a massive data corpus and then\nadapted to solve a variety of downstream tasks. To summarize performance across\nmultiple tasks, evaluation metrics are often aggregated into a summary metric,\ne.g., average accuracy across 10 question-answering tasks. When aggregating\nevaluation metrics, it is useful to incorporate uncertainty in the aggregate\nmetric in order to gain a more realistic understanding of model performance.\nOur objective in this work is to demonstrate how statistical methodology can be\nused for quantifying uncertainty in metrics that have been aggregated across\nmultiple tasks. The methods we emphasize are bootstrapping, Bayesian\nhierarchical (i.e., multilevel) modeling, and the visualization of task\nweightings that consider standard errors. These techniques reveal insights such\nas the dominance of a specific model for certain types of tasks despite an\noverall poor performance. We use a popular ML benchmark, the Visual Task\nAdaptation Benchmark (VTAB), to demonstrate the usefulness of our approaches."
                },
                "authors": [
                    {
                        "name": "Rachel Longjohn"
                    },
                    {
                        "name": "Giri Gopalan"
                    },
                    {
                        "name": "Emily Casleton"
                    }
                ],
                "author_detail": {
                    "name": "Emily Casleton"
                },
                "author": "Emily Casleton",
                "arxiv_comment": "LA-UR-24-25289; presented at the Workshop on Statistical Frontiers in\n  LLMs and Foundation Models at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04234v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.04695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04695v1",
                "updated": "2025-01-08T18:58:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    58,
                    22,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T18:58:22Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    58,
                    22,
                    2,
                    8,
                    0
                ],
                "title": "Re-ranking the Context for Multimodal Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-ranking the Context for Multimodal Retrieval Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating external knowledge to generate a response within a context with\nimproved accuracy and reduced hallucinations. However, multi-modal RAG systems\nface unique challenges: (i) the retrieval process may select irrelevant entries\nto user query (e.g., images, documents), and (ii) vision-language models or\nmulti-modal language models like GPT-4o may hallucinate when processing these\nentries to generate RAG output. In this paper, we aim to address the first\nchallenge, i.e, improving the selection of relevant context from the\nknowledge-base in retrieval phase of the multi-modal RAG. Specifically, we\nleverage the relevancy score (RS) measure designed in our previous work for\nevaluating the RAG performance to select more relevant entries in retrieval\nprocess. The retrieval based on embeddings, say CLIP-based embedding, and\ncosine similarity usually perform poorly particularly for multi-modal data. We\nshow that by using a more advanced relevancy measure, one can enhance the\nretrieval process by selecting more relevant pieces from the knowledge-base and\neliminate the irrelevant pieces from the context by adaptively selecting\nup-to-$k$ entries instead of fixed number of entries. Our evaluation using COCO\ndataset demonstrates significant enhancement in selecting relevant context and\naccuracy of the generated response.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating external knowledge to generate a response within a context with\nimproved accuracy and reduced hallucinations. However, multi-modal RAG systems\nface unique challenges: (i) the retrieval process may select irrelevant entries\nto user query (e.g., images, documents), and (ii) vision-language models or\nmulti-modal language models like GPT-4o may hallucinate when processing these\nentries to generate RAG output. In this paper, we aim to address the first\nchallenge, i.e, improving the selection of relevant context from the\nknowledge-base in retrieval phase of the multi-modal RAG. Specifically, we\nleverage the relevancy score (RS) measure designed in our previous work for\nevaluating the RAG performance to select more relevant entries in retrieval\nprocess. The retrieval based on embeddings, say CLIP-based embedding, and\ncosine similarity usually perform poorly particularly for multi-modal data. We\nshow that by using a more advanced relevancy measure, one can enhance the\nretrieval process by selecting more relevant pieces from the knowledge-base and\neliminate the irrelevant pieces from the context by adaptively selecting\nup-to-$k$ entries instead of fixed number of entries. Our evaluation using COCO\ndataset demonstrates significant enhancement in selecting relevant context and\naccuracy of the generated response."
                },
                "authors": [
                    {
                        "name": "Matin Mortaheb"
                    },
                    {
                        "name": "Mohammad A. Amir Khojastepour"
                    },
                    {
                        "name": "Srimat T. Chakradhar"
                    },
                    {
                        "name": "Sennur Ulukus"
                    }
                ],
                "author_detail": {
                    "name": "Sennur Ulukus"
                },
                "author": "Sennur Ulukus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04694v1",
                "updated": "2025-01-08T18:58:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    58,
                    15,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T18:58:15Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    58,
                    15,
                    2,
                    8,
                    0
                ],
                "title": "EpiCoder: Encompassing Diversity and Complexity in Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EpiCoder: Encompassing Diversity and Complexity in Code Generation"
                },
                "summary": "Effective instruction tuning is indispensable for optimizing code LLMs,\naligning model behavior with user expectations and enhancing model performance\nin real-world applications. However, most existing methods focus on code\nsnippets, which are limited to specific functionalities and rigid structures,\nrestricting the complexity and diversity of the synthesized data. To address\nthese limitations, we introduce a novel feature tree-based synthesis framework\ninspired by Abstract Syntax Trees (AST). Unlike AST, which captures syntactic\nstructure of code, our framework models semantic relationships between code\nelements, enabling the generation of more nuanced and diverse data. The feature\ntree is constructed from raw data and refined iteratively to increase the\nquantity and diversity of the extracted features. This process enables the\nidentification of more complex patterns and relationships within the code. By\nsampling subtrees with controlled depth and breadth, our framework allows\nprecise adjustments to the complexity of the generated code, supporting a wide\nrange of tasks from simple function-level operations to intricate multi-file\nscenarios. We fine-tuned widely-used base models to create the EpiCoder series,\nachieving state-of-the-art performance at both the function and file levels\nacross multiple benchmarks. Notably, empirical evidence indicates that our\napproach shows significant potential in synthesizing highly complex\nrepository-level code data. Further analysis elucidates the merits of this\napproach by rigorously assessing data complexity and diversity through software\nengineering principles and LLM-as-a-judge method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective instruction tuning is indispensable for optimizing code LLMs,\naligning model behavior with user expectations and enhancing model performance\nin real-world applications. However, most existing methods focus on code\nsnippets, which are limited to specific functionalities and rigid structures,\nrestricting the complexity and diversity of the synthesized data. To address\nthese limitations, we introduce a novel feature tree-based synthesis framework\ninspired by Abstract Syntax Trees (AST). Unlike AST, which captures syntactic\nstructure of code, our framework models semantic relationships between code\nelements, enabling the generation of more nuanced and diverse data. The feature\ntree is constructed from raw data and refined iteratively to increase the\nquantity and diversity of the extracted features. This process enables the\nidentification of more complex patterns and relationships within the code. By\nsampling subtrees with controlled depth and breadth, our framework allows\nprecise adjustments to the complexity of the generated code, supporting a wide\nrange of tasks from simple function-level operations to intricate multi-file\nscenarios. We fine-tuned widely-used base models to create the EpiCoder series,\nachieving state-of-the-art performance at both the function and file levels\nacross multiple benchmarks. Notably, empirical evidence indicates that our\napproach shows significant potential in synthesizing highly complex\nrepository-level code data. Further analysis elucidates the merits of this\napproach by rigorously assessing data complexity and diversity through software\nengineering principles and LLM-as-a-judge method."
                },
                "authors": [
                    {
                        "name": "Yaoxiang Wang"
                    },
                    {
                        "name": "Haoling Li"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Jie Wu"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Wenxiang Hu"
                    },
                    {
                        "name": "Zhongxin Guo"
                    },
                    {
                        "name": "Yangyu Huang"
                    },
                    {
                        "name": "Ying Xin"
                    },
                    {
                        "name": "Yujiu Yang"
                    },
                    {
                        "name": "Jinsong Su"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Scarlett Li"
                    }
                ],
                "author_detail": {
                    "name": "Scarlett Li"
                },
                "author": "Scarlett Li",
                "arxiv_comment": "40 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04686v1",
                "updated": "2025-01-08T18:49:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    49,
                    41,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T18:49:41Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    49,
                    41,
                    2,
                    8,
                    0
                ],
                "title": "URSA: Understanding and Verifying Chain-of-thought Reasoning in\n  Multimodal Mathematics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "URSA: Understanding and Verifying Chain-of-thought Reasoning in\n  Multimodal Mathematics"
                },
                "summary": "Chain-of-thought (CoT) reasoning has been widely applied in the mathematical\nreasoning of Large Language Models (LLMs). Recently, the introduction of\nderivative process supervision on CoT trajectories has sparked discussions on\nenhancing scaling capabilities during test time, thereby boosting the potential\nof these models. However, in multimodal mathematical reasoning, the scarcity of\nhigh-quality CoT training data has hindered existing models from achieving\nhigh-precision CoT reasoning and has limited the realization of reasoning\npotential during test time. In this work, we propose a three-module synthesis\nstrategy that integrates CoT distillation, trajectory-format rewriting, and\nformat unification. It results in a high-quality CoT reasoning instruction\nfine-tuning dataset in multimodal mathematics, MMathCoT-1M. We comprehensively\nvalidate the state-of-the-art (SOTA) performance of the trained URSA-7B model\non multiple multimodal mathematical benchmarks. For test-time scaling, we\nintroduce a data synthesis strategy that automatically generates process\nannotation datasets, known as DualMath-1.1M, focusing on both interpretation\nand logic. By further training URSA-7B on DualMath-1.1M, we transition from CoT\nreasoning capabilities to robust supervision abilities. The trained URSA-RM-7B\nacts as a verifier, effectively enhancing the performance of URSA-7B at test\ntime. URSA-RM-7B also demonstrates excellent out-of-distribution (OOD)\nverifying capabilities, showcasing its generalization. Model weights, training\ndata and code will be open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-thought (CoT) reasoning has been widely applied in the mathematical\nreasoning of Large Language Models (LLMs). Recently, the introduction of\nderivative process supervision on CoT trajectories has sparked discussions on\nenhancing scaling capabilities during test time, thereby boosting the potential\nof these models. However, in multimodal mathematical reasoning, the scarcity of\nhigh-quality CoT training data has hindered existing models from achieving\nhigh-precision CoT reasoning and has limited the realization of reasoning\npotential during test time. In this work, we propose a three-module synthesis\nstrategy that integrates CoT distillation, trajectory-format rewriting, and\nformat unification. It results in a high-quality CoT reasoning instruction\nfine-tuning dataset in multimodal mathematics, MMathCoT-1M. We comprehensively\nvalidate the state-of-the-art (SOTA) performance of the trained URSA-7B model\non multiple multimodal mathematical benchmarks. For test-time scaling, we\nintroduce a data synthesis strategy that automatically generates process\nannotation datasets, known as DualMath-1.1M, focusing on both interpretation\nand logic. By further training URSA-7B on DualMath-1.1M, we transition from CoT\nreasoning capabilities to robust supervision abilities. The trained URSA-RM-7B\nacts as a verifier, effectively enhancing the performance of URSA-7B at test\ntime. URSA-RM-7B also demonstrates excellent out-of-distribution (OOD)\nverifying capabilities, showcasing its generalization. Model weights, training\ndata and code will be open-sourced."
                },
                "authors": [
                    {
                        "name": "Ruilin Luo"
                    },
                    {
                        "name": "Zhuofan Zheng"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Yiyao Yu"
                    },
                    {
                        "name": "Xinzhe Ni"
                    },
                    {
                        "name": "Zicheng Lin"
                    },
                    {
                        "name": "Jin Zeng"
                    },
                    {
                        "name": "Yujiu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yujiu Yang"
                },
                "author": "Yujiu Yang",
                "arxiv_comment": "27 pages, 10 tables, 17 figures. The training data has been released.\n  The code and model are currently undergoing internal review. They will be\n  made available soon. Project url: https://ursa-math.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04682v1",
                "updated": "2025-01-08T18:42:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    42,
                    48,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T18:42:48Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    42,
                    48,
                    2,
                    8,
                    0
                ],
                "title": "Towards System 2 Reasoning in LLMs: Learning How to Think With Meta\n  Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards System 2 Reasoning in LLMs: Learning How to Think With Meta\n  Chain-of-Thought"
                },
                "summary": "We propose a novel framework, Meta Chain-of-Thought (Meta-CoT), which extends\ntraditional Chain-of-Thought (CoT) by explicitly modeling the underlying\nreasoning required to arrive at a particular CoT. We present empirical evidence\nfrom state-of-the-art models exhibiting behaviors consistent with in-context\nsearch, and explore methods for producing Meta-CoT via process supervision,\nsynthetic data generation, and search algorithms. Finally, we outline a\nconcrete pipeline for training a model to produce Meta-CoTs, incorporating\ninstruction tuning with linearized search traces and reinforcement learning\npost-training. Finally, we discuss open research questions, including scaling\nlaws, verifier roles, and the potential for discovering novel reasoning\nalgorithms. This work provides a theoretical and practical roadmap to enable\nMeta-CoT in LLMs, paving the way for more powerful and human-like reasoning in\nartificial intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel framework, Meta Chain-of-Thought (Meta-CoT), which extends\ntraditional Chain-of-Thought (CoT) by explicitly modeling the underlying\nreasoning required to arrive at a particular CoT. We present empirical evidence\nfrom state-of-the-art models exhibiting behaviors consistent with in-context\nsearch, and explore methods for producing Meta-CoT via process supervision,\nsynthetic data generation, and search algorithms. Finally, we outline a\nconcrete pipeline for training a model to produce Meta-CoTs, incorporating\ninstruction tuning with linearized search traces and reinforcement learning\npost-training. Finally, we discuss open research questions, including scaling\nlaws, verifier roles, and the potential for discovering novel reasoning\nalgorithms. This work provides a theoretical and practical roadmap to enable\nMeta-CoT in LLMs, paving the way for more powerful and human-like reasoning in\nartificial intelligence."
                },
                "authors": [
                    {
                        "name": "Violet Xiang"
                    },
                    {
                        "name": "Charlie Snell"
                    },
                    {
                        "name": "Kanishk Gandhi"
                    },
                    {
                        "name": "Alon Albalak"
                    },
                    {
                        "name": "Anikait Singh"
                    },
                    {
                        "name": "Chase Blagden"
                    },
                    {
                        "name": "Duy Phung"
                    },
                    {
                        "name": "Rafael Rafailov"
                    },
                    {
                        "name": "Nathan Lile"
                    },
                    {
                        "name": "Dakota Mahan"
                    },
                    {
                        "name": "Louis Castricato"
                    },
                    {
                        "name": "Jan-Philipp Franken"
                    },
                    {
                        "name": "Nick Haber"
                    },
                    {
                        "name": "Chelsea Finn"
                    }
                ],
                "author_detail": {
                    "name": "Chelsea Finn"
                },
                "author": "Chelsea Finn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04675v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04675v1",
                "updated": "2025-01-08T18:33:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    33,
                    17,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T18:33:17Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    33,
                    17,
                    2,
                    8,
                    0
                ],
                "title": "Enhancing Financial VQA in Vision Language Models using Intermediate\n  Structured Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Financial VQA in Vision Language Models using Intermediate\n  Structured Representations"
                },
                "summary": "Chart interpretation is crucial for visual data analysis, but accurately\nextracting information from charts poses significant challenges for automated\nmodels. This study investigates the fine-tuning of DEPLOT, a modality\nconversion module that translates the image of a plot or chart to a linearized\ntable, on a custom dataset of 50,000 bar charts. The dataset comprises simple,\nstacked, and grouped bar charts, targeting the unique structural features of\nthese visualizations. The finetuned DEPLOT model is evaluated against its base\nversion using a test set of 1,000 images and two metrics: Relative Mapping\nSimilarity (RMS), which measures categorical mapping accuracy, and Relative\nNumber Set Similarity (RNSS), which evaluates numerical interpretation\naccuracy. To further explore the reasoning capabilities of large language\nmodels (LLMs), we curate an additional set of 100 bar chart images paired with\nquestion answer sets. Our findings demonstrate that providing a structured\nintermediate table alongside the image significantly enhances LLM reasoning\nperformance compared to direct image queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chart interpretation is crucial for visual data analysis, but accurately\nextracting information from charts poses significant challenges for automated\nmodels. This study investigates the fine-tuning of DEPLOT, a modality\nconversion module that translates the image of a plot or chart to a linearized\ntable, on a custom dataset of 50,000 bar charts. The dataset comprises simple,\nstacked, and grouped bar charts, targeting the unique structural features of\nthese visualizations. The finetuned DEPLOT model is evaluated against its base\nversion using a test set of 1,000 images and two metrics: Relative Mapping\nSimilarity (RMS), which measures categorical mapping accuracy, and Relative\nNumber Set Similarity (RNSS), which evaluates numerical interpretation\naccuracy. To further explore the reasoning capabilities of large language\nmodels (LLMs), we curate an additional set of 100 bar chart images paired with\nquestion answer sets. Our findings demonstrate that providing a structured\nintermediate table alongside the image significantly enhances LLM reasoning\nperformance compared to direct image queries."
                },
                "authors": [
                    {
                        "name": "Archita Srivastava"
                    },
                    {
                        "name": "Abhas Kumar"
                    },
                    {
                        "name": "Rajesh Kumar"
                    },
                    {
                        "name": "Prabhakar Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Prabhakar Srinivasan"
                },
                "author": "Prabhakar Srinivasan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04675v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04675v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04670v1",
                "updated": "2025-01-08T18:30:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    30,
                    53,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T18:30:53Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    30,
                    53,
                    2,
                    8,
                    0
                ],
                "title": "Are They the Same? Exploring Visual Correspondence Shortcomings of\n  Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are They the Same? Exploring Visual Correspondence Shortcomings of\n  Multimodal LLMs"
                },
                "summary": "Recent advancements in multimodal models have shown a strong ability in\nvisual perception, reasoning abilities, and vision-language understanding.\nHowever, studies on visual matching ability are missing, where finding the\nvisual correspondence of objects is essential in vision research. Our research\nreveals that the matching capabilities in recent multimodal LLMs (MLLMs) still\nexhibit systematic shortcomings, even with current strong MLLMs models, GPT-4o.\nIn particular, we construct a Multimodal Visual Matching (MMVM) benchmark to\nfairly benchmark over 30 different MLLMs. The MMVM benchmark is built from 15\nopen-source datasets and Internet videos with manual annotation. We categorize\nthe data samples of MMVM benchmark into eight aspects based on the required\ncues and capabilities to more comprehensively evaluate and analyze current\nMLLMs. In addition, we have designed an automatic annotation pipeline to\ngenerate the MMVM SFT dataset, including 220K visual matching data with\nreasoning annotation. Finally, we present CoLVA, a novel contrastive MLLM with\ntwo novel technical designs: fine-grained vision expert with object-level\ncontrastive learning and instruction augmentation strategy. CoLVA achieves\n51.06\\% overall accuracy (OA) on the MMVM benchmark, surpassing GPT-4o and\nbaseline by 8.41\\% and 23.58\\% OA, respectively. The results show the\neffectiveness of our MMVM SFT dataset and our novel technical designs. Code,\nbenchmark, dataset, and models are available at\nhttps://github.com/zhouyiks/CoLVA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in multimodal models have shown a strong ability in\nvisual perception, reasoning abilities, and vision-language understanding.\nHowever, studies on visual matching ability are missing, where finding the\nvisual correspondence of objects is essential in vision research. Our research\nreveals that the matching capabilities in recent multimodal LLMs (MLLMs) still\nexhibit systematic shortcomings, even with current strong MLLMs models, GPT-4o.\nIn particular, we construct a Multimodal Visual Matching (MMVM) benchmark to\nfairly benchmark over 30 different MLLMs. The MMVM benchmark is built from 15\nopen-source datasets and Internet videos with manual annotation. We categorize\nthe data samples of MMVM benchmark into eight aspects based on the required\ncues and capabilities to more comprehensively evaluate and analyze current\nMLLMs. In addition, we have designed an automatic annotation pipeline to\ngenerate the MMVM SFT dataset, including 220K visual matching data with\nreasoning annotation. Finally, we present CoLVA, a novel contrastive MLLM with\ntwo novel technical designs: fine-grained vision expert with object-level\ncontrastive learning and instruction augmentation strategy. CoLVA achieves\n51.06\\% overall accuracy (OA) on the MMVM benchmark, surpassing GPT-4o and\nbaseline by 8.41\\% and 23.58\\% OA, respectively. The results show the\neffectiveness of our MMVM SFT dataset and our novel technical designs. Code,\nbenchmark, dataset, and models are available at\nhttps://github.com/zhouyiks/CoLVA."
                },
                "authors": [
                    {
                        "name": "Yikang Zhou"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Shilin Xu"
                    },
                    {
                        "name": "Shihao Chen"
                    },
                    {
                        "name": "Qianyu Zhou"
                    },
                    {
                        "name": "Yunhai Tong"
                    },
                    {
                        "name": "Shunping Ji"
                    },
                    {
                        "name": "Jiangning Zhang"
                    },
                    {
                        "name": "Xiangtai Li"
                    },
                    {
                        "name": "Lu Qi"
                    }
                ],
                "author_detail": {
                    "name": "Lu Qi"
                },
                "author": "Lu Qi",
                "arxiv_comment": "project page: https://zhouyiks.github.io/projects/CoLVA/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04661v1",
                "updated": "2025-01-08T18:15:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    15,
                    10,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T18:15:10Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    15,
                    10,
                    2,
                    8,
                    0
                ],
                "title": "Assessing Language Comprehension in Large Language Models Using\n  Construction Grammar",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Language Comprehension in Large Language Models Using\n  Construction Grammar"
                },
                "summary": "Large Language Models, despite their significant capabilities, are known to\nfail in surprising and unpredictable ways. Evaluating their true\n`understanding' of language is particularly challenging due to the extensive\nweb-scale data they are trained on. Therefore, we construct an evaluation to\nsystematically assess natural language understanding (NLU) in LLMs by\nleveraging Construction Grammar (CxG), which provides insights into the meaning\ncaptured by linguistic elements known as constructions (Cxns). CxG is\nwell-suited for this purpose because provides a theoretical basis to construct\ntargeted evaluation sets. These datasets are carefully constructed to include\nexamples which are unlikely to appear in pre-training data, yet intuitive and\neasy for humans to understand, enabling a more targeted and reliable\nassessment. Our experiments focus on downstream natural language inference and\nreasoning tasks by comparing LLMs' understanding of the underlying meanings\ncommunicated through 8 unique Cxns with that of humans. The results show that\nwhile LLMs demonstrate some knowledge of constructional information, even the\nlatest models including GPT-o1 struggle with abstract meanings conveyed by\nthese Cxns, as demonstrated in cases where test sentences are dissimilar to\ntheir pre-training data. We argue that such cases provide a more accurate test\nof true language understanding, highlighting key limitations in LLMs' semantic\ncapabilities. We make our novel dataset and associated experimental data\nincluding prompts and model responses publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models, despite their significant capabilities, are known to\nfail in surprising and unpredictable ways. Evaluating their true\n`understanding' of language is particularly challenging due to the extensive\nweb-scale data they are trained on. Therefore, we construct an evaluation to\nsystematically assess natural language understanding (NLU) in LLMs by\nleveraging Construction Grammar (CxG), which provides insights into the meaning\ncaptured by linguistic elements known as constructions (Cxns). CxG is\nwell-suited for this purpose because provides a theoretical basis to construct\ntargeted evaluation sets. These datasets are carefully constructed to include\nexamples which are unlikely to appear in pre-training data, yet intuitive and\neasy for humans to understand, enabling a more targeted and reliable\nassessment. Our experiments focus on downstream natural language inference and\nreasoning tasks by comparing LLMs' understanding of the underlying meanings\ncommunicated through 8 unique Cxns with that of humans. The results show that\nwhile LLMs demonstrate some knowledge of constructional information, even the\nlatest models including GPT-o1 struggle with abstract meanings conveyed by\nthese Cxns, as demonstrated in cases where test sentences are dissimilar to\ntheir pre-training data. We argue that such cases provide a more accurate test\nof true language understanding, highlighting key limitations in LLMs' semantic\ncapabilities. We make our novel dataset and associated experimental data\nincluding prompts and model responses publicly available."
                },
                "authors": [
                    {
                        "name": "Wesley Scivetti"
                    },
                    {
                        "name": "Melissa Torgbi"
                    },
                    {
                        "name": "Austin Blodgett"
                    },
                    {
                        "name": "Mollie Shichman"
                    },
                    {
                        "name": "Taylor Hudson"
                    },
                    {
                        "name": "Claire Bonial"
                    },
                    {
                        "name": "Harish Tayyar Madabushi"
                    }
                ],
                "author_detail": {
                    "name": "Harish Tayyar Madabushi"
                },
                "author": "Harish Tayyar Madabushi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04652v1",
                "updated": "2025-01-08T18:05:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    5,
                    30,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T18:05:30Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    5,
                    30,
                    2,
                    8,
                    0
                ],
                "title": "Multi-task retriever fine-tuning for domain-specific and efficient RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-task retriever fine-tuning for domain-specific and efficient RAG"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has become ubiquitous when deploying\nLarge Language Models (LLMs), as it can address typical limitations such as\ngenerating hallucinated or outdated information. However, when building\nreal-world RAG applications, practical issues arise. First, the retrieved\ninformation is generally domain-specific. Since it is computationally expensive\nto fine-tune LLMs, it is more feasible to fine-tune the retriever to improve\nthe quality of the data included in the LLM input. Second, as more applications\nare deployed in the same real-world system, one cannot afford to deploy\nseparate retrievers. Moreover, these RAG applications normally retrieve\ndifferent kinds of data. Our solution is to instruction fine-tune a small\nretriever encoder on a variety of domain-specific tasks to allow us to deploy\none encoder that can serve many use cases, thereby achieving low-cost,\nscalability, and speed. We show how this encoder generalizes to out-of-domain\nsettings as well as to an unseen retrieval task on real-world enterprise use\ncases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has become ubiquitous when deploying\nLarge Language Models (LLMs), as it can address typical limitations such as\ngenerating hallucinated or outdated information. However, when building\nreal-world RAG applications, practical issues arise. First, the retrieved\ninformation is generally domain-specific. Since it is computationally expensive\nto fine-tune LLMs, it is more feasible to fine-tune the retriever to improve\nthe quality of the data included in the LLM input. Second, as more applications\nare deployed in the same real-world system, one cannot afford to deploy\nseparate retrievers. Moreover, these RAG applications normally retrieve\ndifferent kinds of data. Our solution is to instruction fine-tune a small\nretriever encoder on a variety of domain-specific tasks to allow us to deploy\none encoder that can serve many use cases, thereby achieving low-cost,\nscalability, and speed. We show how this encoder generalizes to out-of-domain\nsettings as well as to an unseen retrieval task on real-world enterprise use\ncases."
                },
                "authors": [
                    {
                        "name": "Patrice Béchard"
                    },
                    {
                        "name": "Orlando Marquez Ayala"
                    }
                ],
                "author_detail": {
                    "name": "Orlando Marquez Ayala"
                },
                "author": "Orlando Marquez Ayala",
                "arxiv_comment": "9 pages, 2 figures. Submitted to NAACL 2025 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04648v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04648v1",
                "updated": "2025-01-08T18:01:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    1,
                    49,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T18:01:49Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    1,
                    49,
                    2,
                    8,
                    0
                ],
                "title": "FlairGPT: Repurposing LLMs for Interior Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlairGPT: Repurposing LLMs for Interior Designs"
                },
                "summary": "Interior design involves the careful selection and arrangement of objects to\ncreate an aesthetically pleasing, functional, and harmonized space that aligns\nwith the client's design brief. This task is particularly challenging, as a\nsuccessful design must not only incorporate all the necessary objects in a\ncohesive style, but also ensure they are arranged in a way that maximizes\naccessibility, while adhering to a variety of affordability and usage\nconsiderations. Data-driven solutions have been proposed, but these are\ntypically room- or domain-specific and lack explainability in their design\ndesign considerations used in producing the final layout. In this paper, we\ninvestigate if large language models (LLMs) can be directly utilized for\ninterior design. While we find that LLMs are not yet capable of generating\ncomplete layouts, they can be effectively leveraged in a structured manner,\ninspired by the workflow of interior designers. By systematically probing LLMs,\nwe can reliably generate a list of objects along with relevant constraints that\nguide their placement. We translate this information into a design layout\ngraph, which is then solved using an off-the-shelf constrained optimization\nsetup to generate the final layouts. We benchmark our algorithm in various\ndesign configurations against existing LLM-based methods and human designs, and\nevaluate the results using a variety of quantitative and qualitative metrics\nalong with user studies. In summary, we demonstrate that LLMs, when used in a\nstructured manner, can effectively generate diverse high-quality layouts,\nmaking them a viable solution for creating large-scale virtual scenes. Project\nwebpage at https://flairgpt.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interior design involves the careful selection and arrangement of objects to\ncreate an aesthetically pleasing, functional, and harmonized space that aligns\nwith the client's design brief. This task is particularly challenging, as a\nsuccessful design must not only incorporate all the necessary objects in a\ncohesive style, but also ensure they are arranged in a way that maximizes\naccessibility, while adhering to a variety of affordability and usage\nconsiderations. Data-driven solutions have been proposed, but these are\ntypically room- or domain-specific and lack explainability in their design\ndesign considerations used in producing the final layout. In this paper, we\ninvestigate if large language models (LLMs) can be directly utilized for\ninterior design. While we find that LLMs are not yet capable of generating\ncomplete layouts, they can be effectively leveraged in a structured manner,\ninspired by the workflow of interior designers. By systematically probing LLMs,\nwe can reliably generate a list of objects along with relevant constraints that\nguide their placement. We translate this information into a design layout\ngraph, which is then solved using an off-the-shelf constrained optimization\nsetup to generate the final layouts. We benchmark our algorithm in various\ndesign configurations against existing LLM-based methods and human designs, and\nevaluate the results using a variety of quantitative and qualitative metrics\nalong with user studies. In summary, we demonstrate that LLMs, when used in a\nstructured manner, can effectively generate diverse high-quality layouts,\nmaking them a viable solution for creating large-scale virtual scenes. Project\nwebpage at https://flairgpt.github.io/"
                },
                "authors": [
                    {
                        "name": "Gabrielle Littlefair"
                    },
                    {
                        "name": "Niladri Shekhar Dutt"
                    },
                    {
                        "name": "Niloy J. Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Niloy J. Mitra"
                },
                "author": "Niloy J. Mitra",
                "arxiv_comment": "Accepted at EUROGRAPHICS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04648v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04648v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03800v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03800v2",
                "updated": "2025-01-08T17:44:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    17,
                    44,
                    11,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-07T14:06:57Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    14,
                    6,
                    57,
                    1,
                    7,
                    0
                ],
                "title": "MADation: Face Morphing Attack Detection with Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MADation: Face Morphing Attack Detection with Foundation Models"
                },
                "summary": "Despite the considerable performance improvements of face recognition\nalgorithms in recent years, the same scientific advances responsible for this\nprogress can also be used to create efficient ways to attack them, posing a\nthreat to their secure deployment. Morphing attack detection (MAD) systems aim\nto detect a specific type of threat, morphing attacks, at an early stage,\npreventing them from being considered for verification in critical processes.\nFoundation models (FM) learn from extensive amounts of unlabeled data,\nachieving remarkable zero-shot generalization to unseen domains. Although this\ngeneralization capacity might be weak when dealing with domain-specific\ndownstream tasks such as MAD, FMs can easily adapt to these settings while\nretaining the built-in knowledge acquired during pre-training. In this work, we\nrecognize the potential of FMs to perform well in the MAD task when properly\nadapted to its specificities. To this end, we adapt FM CLIP architectures with\nLoRA weights while simultaneously training a classification header. The\nproposed framework, MADation surpasses our alternative FM and transformer-based\nframeworks and constitutes the first adaption of FMs to the MAD task. MADation\npresents competitive results with current MAD solutions in the literature and\neven surpasses them in several evaluation scenarios. To encourage\nreproducibility and facilitate further research in MAD, we publicly release the\nimplementation of MADation at https: //github.com/gurayozgur/MADation",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the considerable performance improvements of face recognition\nalgorithms in recent years, the same scientific advances responsible for this\nprogress can also be used to create efficient ways to attack them, posing a\nthreat to their secure deployment. Morphing attack detection (MAD) systems aim\nto detect a specific type of threat, morphing attacks, at an early stage,\npreventing them from being considered for verification in critical processes.\nFoundation models (FM) learn from extensive amounts of unlabeled data,\nachieving remarkable zero-shot generalization to unseen domains. Although this\ngeneralization capacity might be weak when dealing with domain-specific\ndownstream tasks such as MAD, FMs can easily adapt to these settings while\nretaining the built-in knowledge acquired during pre-training. In this work, we\nrecognize the potential of FMs to perform well in the MAD task when properly\nadapted to its specificities. To this end, we adapt FM CLIP architectures with\nLoRA weights while simultaneously training a classification header. The\nproposed framework, MADation surpasses our alternative FM and transformer-based\nframeworks and constitutes the first adaption of FMs to the MAD task. MADation\npresents competitive results with current MAD solutions in the literature and\neven surpasses them in several evaluation scenarios. To encourage\nreproducibility and facilitate further research in MAD, we publicly release the\nimplementation of MADation at https: //github.com/gurayozgur/MADation"
                },
                "authors": [
                    {
                        "name": "Eduarda Caldeira"
                    },
                    {
                        "name": "Guray Ozgur"
                    },
                    {
                        "name": "Tahar Chettaoui"
                    },
                    {
                        "name": "Marija Ivanovska"
                    },
                    {
                        "name": "Peter Peer"
                    },
                    {
                        "name": "Fadi Boutros"
                    },
                    {
                        "name": "Vitomir Struc"
                    },
                    {
                        "name": "Naser Damer"
                    }
                ],
                "author_detail": {
                    "name": "Naser Damer"
                },
                "author": "Naser Damer",
                "arxiv_comment": "Accepted at WACV 2025 workshops",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03800v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03800v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15861v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15861v2",
                "updated": "2025-01-08T17:41:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    17,
                    41,
                    51,
                    2,
                    8,
                    0
                ],
                "published": "2024-09-24T08:33:41Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    33,
                    41,
                    1,
                    268,
                    0
                ],
                "title": "A Zero-Shot Open-Vocabulary Pipeline for Dialogue Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Zero-Shot Open-Vocabulary Pipeline for Dialogue Understanding"
                },
                "summary": "Dialogue State Tracking (DST) is crucial for understanding user needs and\nexecuting appropriate system actions in task-oriented dialogues. Majority of\nexisting DST methods are designed to work within predefined ontologies and\nassume the availability of gold domain labels, struggling with adapting to new\nslots values. While Large Language Models (LLMs)-based systems show promising\nzero-shot DST performance, they either require extensive computational\nresources or they underperform existing fully-trained systems, limiting their\npracticality. To address these limitations, we propose a zero-shot,\nopen-vocabulary system that integrates domain classification and DST in a\nsingle pipeline. Our approach includes reformulating DST as a\nquestion-answering task for less capable models and employing self-refining\nprompts for more adaptable ones. Our system does not rely on fixed slot values\ndefined in the ontology allowing the system to adapt dynamically. We compare\nour approach with existing SOTA, and show that it provides up to 20% better\nJoint Goal Accuracy (JGA) over previous methods on datasets like Multi-WOZ 2.1,\nwith up to 90% fewer requests to the LLM API.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialogue State Tracking (DST) is crucial for understanding user needs and\nexecuting appropriate system actions in task-oriented dialogues. Majority of\nexisting DST methods are designed to work within predefined ontologies and\nassume the availability of gold domain labels, struggling with adapting to new\nslots values. While Large Language Models (LLMs)-based systems show promising\nzero-shot DST performance, they either require extensive computational\nresources or they underperform existing fully-trained systems, limiting their\npracticality. To address these limitations, we propose a zero-shot,\nopen-vocabulary system that integrates domain classification and DST in a\nsingle pipeline. Our approach includes reformulating DST as a\nquestion-answering task for less capable models and employing self-refining\nprompts for more adaptable ones. Our system does not rely on fixed slot values\ndefined in the ontology allowing the system to adapt dynamically. We compare\nour approach with existing SOTA, and show that it provides up to 20% better\nJoint Goal Accuracy (JGA) over previous methods on datasets like Multi-WOZ 2.1,\nwith up to 90% fewer requests to the LLM API."
                },
                "authors": [
                    {
                        "name": "Abdulfattah Safa"
                    },
                    {
                        "name": "Gözde Gül Şahin"
                    }
                ],
                "author_detail": {
                    "name": "Gözde Gül Şahin"
                },
                "author": "Gözde Gül Şahin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15861v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15861v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04635v1",
                "updated": "2025-01-08T17:29:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    17,
                    29,
                    46,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T17:29:46Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    17,
                    29,
                    46,
                    2,
                    8,
                    0
                ],
                "title": "Knowledge Retrieval Based on Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Retrieval Based on Generative AI"
                },
                "summary": "This study develops a question-answering system based on Retrieval-Augmented\nGeneration (RAG) using Chinese Wikipedia and Lawbank as retrieval sources.\nUsing TTQA and TMMLU+ as evaluation datasets, the system employs BGE-M3 for\ndense vector retrieval to obtain highly relevant search results and\nBGE-reranker to reorder these results based on query relevance. The most\npertinent retrieval outcomes serve as reference knowledge for a Large Language\nModel (LLM), enhancing its ability to answer questions and establishing a\nknowledge retrieval system grounded in generative AI.\n  The system's effectiveness is assessed through a two-stage evaluation:\nautomatic and assisted performance evaluations. The automatic evaluation\ncalculates accuracy by comparing the model's auto-generated labels with ground\ntruth answers, measuring performance under standardized conditions without\nhuman intervention. The assisted performance evaluation involves 20\nfinance-related multiple-choice questions answered by 20 participants without\nfinancial backgrounds. Initially, participants answer independently. Later,\nthey receive system-generated reference information to assist in answering,\nexamining whether the system improves accuracy when assistance is provided.\n  The main contributions of this research are: (1) Enhanced LLM Capability: By\nintegrating BGE-M3 and BGE-reranker, the system retrieves and reorders highly\nrelevant results, reduces hallucinations, and dynamically accesses authorized\nor public knowledge sources. (2) Improved Data Privacy: A customized RAG\narchitecture enables local operation of the LLM, eliminating the need to send\nprivate data to external servers. This approach enhances data security, reduces\nreliance on commercial services, lowers operational costs, and mitigates\nprivacy risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study develops a question-answering system based on Retrieval-Augmented\nGeneration (RAG) using Chinese Wikipedia and Lawbank as retrieval sources.\nUsing TTQA and TMMLU+ as evaluation datasets, the system employs BGE-M3 for\ndense vector retrieval to obtain highly relevant search results and\nBGE-reranker to reorder these results based on query relevance. The most\npertinent retrieval outcomes serve as reference knowledge for a Large Language\nModel (LLM), enhancing its ability to answer questions and establishing a\nknowledge retrieval system grounded in generative AI.\n  The system's effectiveness is assessed through a two-stage evaluation:\nautomatic and assisted performance evaluations. The automatic evaluation\ncalculates accuracy by comparing the model's auto-generated labels with ground\ntruth answers, measuring performance under standardized conditions without\nhuman intervention. The assisted performance evaluation involves 20\nfinance-related multiple-choice questions answered by 20 participants without\nfinancial backgrounds. Initially, participants answer independently. Later,\nthey receive system-generated reference information to assist in answering,\nexamining whether the system improves accuracy when assistance is provided.\n  The main contributions of this research are: (1) Enhanced LLM Capability: By\nintegrating BGE-M3 and BGE-reranker, the system retrieves and reorders highly\nrelevant results, reduces hallucinations, and dynamically accesses authorized\nor public knowledge sources. (2) Improved Data Privacy: A customized RAG\narchitecture enables local operation of the LLM, eliminating the need to send\nprivate data to external servers. This approach enhances data security, reduces\nreliance on commercial services, lowers operational costs, and mitigates\nprivacy risks."
                },
                "authors": [
                    {
                        "name": "Te-Lun Yang"
                    },
                    {
                        "name": "Jyi-Shane Liu"
                    },
                    {
                        "name": "Yuen-Hsien Tseng"
                    },
                    {
                        "name": "Jyh-Shing Roger Jang"
                    }
                ],
                "author_detail": {
                    "name": "Jyh-Shing Roger Jang"
                },
                "author": "Jyh-Shing Roger Jang",
                "arxiv_comment": "8 pages, 13 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04185v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04185v4",
                "updated": "2025-01-08T17:11:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    17,
                    11,
                    53,
                    2,
                    8,
                    0
                ],
                "published": "2024-07-04T23:26:56Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    23,
                    26,
                    56,
                    3,
                    186,
                    0
                ],
                "title": "HAF-RM: A Hybrid Alignment Framework for Reward Model Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HAF-RM: A Hybrid Alignment Framework for Reward Model Training"
                },
                "summary": "The reward model has become increasingly important in alignment, assessment,\nand data construction for large language models (LLMs). Most existing\nresearchers focus on enhancing reward models through data improvements,\nfollowing the conventional training framework for reward models that directly\noptimizes the predicted rewards. In this paper, we propose a hybrid alignment\nframework HaF-RM for reward model training by introducing an additional\nconstraint on token-level policy probabilities in addition to the reward score.\nIt can simultaneously supervise the internal preference model at the token\nlevel and optimize the mapping layer of the reward model at the sequence level.\nExperiment results on five datasets sufficiently show the validity and\neffectiveness of our proposed hybrid framework for training a high-quality\nreward model. By decoupling the reward modeling procedure and incorporating\nhybrid supervision, our HaF-RM framework offers a principled and effective\napproach to enhancing the performance and alignment of reward models, a\ncritical component in the responsible development of powerful language models.\nWe release our code at https://haf-rm.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reward model has become increasingly important in alignment, assessment,\nand data construction for large language models (LLMs). Most existing\nresearchers focus on enhancing reward models through data improvements,\nfollowing the conventional training framework for reward models that directly\noptimizes the predicted rewards. In this paper, we propose a hybrid alignment\nframework HaF-RM for reward model training by introducing an additional\nconstraint on token-level policy probabilities in addition to the reward score.\nIt can simultaneously supervise the internal preference model at the token\nlevel and optimize the mapping layer of the reward model at the sequence level.\nExperiment results on five datasets sufficiently show the validity and\neffectiveness of our proposed hybrid framework for training a high-quality\nreward model. By decoupling the reward modeling procedure and incorporating\nhybrid supervision, our HaF-RM framework offers a principled and effective\napproach to enhancing the performance and alignment of reward models, a\ncritical component in the responsible development of powerful language models.\nWe release our code at https://haf-rm.github.io."
                },
                "authors": [
                    {
                        "name": "Shujun Liu"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    },
                    {
                        "name": "Yuhang Lai"
                    },
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Shengbin Yue"
                    },
                    {
                        "name": "Zengfeng Huang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04185v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04185v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03304v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03304v2",
                "updated": "2025-01-08T16:41:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    16,
                    41,
                    3,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-06T16:04:56Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    16,
                    4,
                    56,
                    0,
                    6,
                    0
                ],
                "title": "LiLMaps: Learnable Implicit Language Maps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiLMaps: Learnable Implicit Language Maps"
                },
                "summary": "One of the current trends in robotics is to employ large language models\n(LLMs) to provide non-predefined command execution and natural human-robot\ninteraction. It is useful to have an environment map together with its language\nrepresentation, which can be further utilized by LLMs. Such a comprehensive\nscene representation enables numerous ways of interaction with the map for\nautonomously operating robots. In this work, we present an approach that\nenhances incremental implicit mapping through the integration of\nvision-language features. Specifically, we (i) propose a decoder optimization\ntechnique for implicit language maps which can be used when new objects appear\non the scene, and (ii) address the problem of inconsistent vision-language\npredictions between different viewing positions. Our experiments demonstrate\nthe effectiveness of LiLMaps and solid improvements in performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the current trends in robotics is to employ large language models\n(LLMs) to provide non-predefined command execution and natural human-robot\ninteraction. It is useful to have an environment map together with its language\nrepresentation, which can be further utilized by LLMs. Such a comprehensive\nscene representation enables numerous ways of interaction with the map for\nautonomously operating robots. In this work, we present an approach that\nenhances incremental implicit mapping through the integration of\nvision-language features. Specifically, we (i) propose a decoder optimization\ntechnique for implicit language maps which can be used when new objects appear\non the scene, and (ii) address the problem of inconsistent vision-language\npredictions between different viewing positions. Our experiments demonstrate\nthe effectiveness of LiLMaps and solid improvements in performance."
                },
                "authors": [
                    {
                        "name": "Evgenii Kruzhkov"
                    },
                    {
                        "name": "Sven Behnke"
                    }
                ],
                "author_detail": {
                    "name": "Sven Behnke"
                },
                "author": "Sven Behnke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03304v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03304v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.17464v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.17464v3",
                "updated": "2025-01-08T16:27:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    16,
                    27,
                    29,
                    2,
                    8,
                    0
                ],
                "published": "2024-01-30T21:53:30Z",
                "published_parsed": [
                    2024,
                    1,
                    30,
                    21,
                    53,
                    30,
                    1,
                    30,
                    0
                ],
                "title": "Efficient Tool Use with Chain-of-Abstraction Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Tool Use with Chain-of-Abstraction Reasoning"
                },
                "summary": "To achieve faithful reasoning that aligns with human expectations, large\nlanguage models (LLMs) need to ground their reasoning to real-world knowledge\n(e.g., web facts, math and physical rules). Tools help LLMs access this\nexternal knowledge, but there remains challenges for fine-tuning LLM agents\n(e.g., Toolformer) to invoke tools in multi-step reasoning problems, where\ninter-connected tool calls require holistic and efficient tool usage planning.\n  In this work, we propose a new method for LLMs to better leverage tools in\nmulti-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to\nfirst decode reasoning chains with abstract placeholders, and then call domain\ntools to reify each reasoning chain by filling in specific knowledge. This\nplanning with abstract chains enables LLMs to learn more general reasoning\nstrategies, which are robust to shifts of domain knowledge (e.g., math results)\nrelevant to different reasoning questions. It also allows LLMs to perform\ndecoding and calling of external tools in parallel, which avoids the inference\ndelay caused by waiting for tool responses. In mathematical reasoning and Wiki\nQA domains, we show that our method consistently outperforms previous\nchain-of-thought and tool-augmented baselines on both in-distribution and\nout-of-distribution test sets, with an average ~6% absolute QA accuracy\nimprovement. LLM agents trained with our method also show more efficient tool\nuse, with inference speed being on average ~1.4x faster than baseline\ntool-augmented LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To achieve faithful reasoning that aligns with human expectations, large\nlanguage models (LLMs) need to ground their reasoning to real-world knowledge\n(e.g., web facts, math and physical rules). Tools help LLMs access this\nexternal knowledge, but there remains challenges for fine-tuning LLM agents\n(e.g., Toolformer) to invoke tools in multi-step reasoning problems, where\ninter-connected tool calls require holistic and efficient tool usage planning.\n  In this work, we propose a new method for LLMs to better leverage tools in\nmulti-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to\nfirst decode reasoning chains with abstract placeholders, and then call domain\ntools to reify each reasoning chain by filling in specific knowledge. This\nplanning with abstract chains enables LLMs to learn more general reasoning\nstrategies, which are robust to shifts of domain knowledge (e.g., math results)\nrelevant to different reasoning questions. It also allows LLMs to perform\ndecoding and calling of external tools in parallel, which avoids the inference\ndelay caused by waiting for tool responses. In mathematical reasoning and Wiki\nQA domains, we show that our method consistently outperforms previous\nchain-of-thought and tool-augmented baselines on both in-distribution and\nout-of-distribution test sets, with an average ~6% absolute QA accuracy\nimprovement. LLM agents trained with our method also show more efficient tool\nuse, with inference speed being on average ~1.4x faster than baseline\ntool-augmented LLMs."
                },
                "authors": [
                    {
                        "name": "Silin Gao"
                    },
                    {
                        "name": "Jane Dwivedi-Yu"
                    },
                    {
                        "name": "Ping Yu"
                    },
                    {
                        "name": "Xiaoqing Ellen Tan"
                    },
                    {
                        "name": "Ramakanth Pasunuru"
                    },
                    {
                        "name": "Olga Golovneva"
                    },
                    {
                        "name": "Koustuv Sinha"
                    },
                    {
                        "name": "Asli Celikyilmaz"
                    },
                    {
                        "name": "Antoine Bosselut"
                    },
                    {
                        "name": "Tianlu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Tianlu Wang"
                },
                "author": "Tianlu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.17464v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.17464v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00957v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00957v3",
                "updated": "2025-01-08T16:07:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    16,
                    7,
                    35,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-01T21:23:22Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    21,
                    23,
                    22,
                    2,
                    1,
                    0
                ],
                "title": "Generative AI and LLMs in Industry: A text-mining Analysis and Critical\n  Evaluation of Guidelines and Policy Statements Across Fourteen Industrial\n  Sectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI and LLMs in Industry: A text-mining Analysis and Critical\n  Evaluation of Guidelines and Policy Statements Across Fourteen Industrial\n  Sectors"
                },
                "summary": "The rise of Generative AI (GAI) and Large Language Models (LLMs) has\ntransformed industrial landscapes, offering unprecedented opportunities for\nefficiency and innovation while raising critical ethical, regulatory, and\noperational challenges. This study conducts a text-based analysis of 160\nguidelines and policy statements across fourteen industrial sectors, utilizing\nsystematic methods and text-mining techniques to evaluate the governance of\nthese technologies. By examining global directives, industry practices, and\nsector-specific policies, the paper highlights the complexities of balancing\ninnovation with ethical accountability and equitable access. The findings\nprovide actionable insights and recommendations for fostering responsible,\ntransparent, and safe integration of GAI and LLMs in diverse industry contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Generative AI (GAI) and Large Language Models (LLMs) has\ntransformed industrial landscapes, offering unprecedented opportunities for\nefficiency and innovation while raising critical ethical, regulatory, and\noperational challenges. This study conducts a text-based analysis of 160\nguidelines and policy statements across fourteen industrial sectors, utilizing\nsystematic methods and text-mining techniques to evaluate the governance of\nthese technologies. By examining global directives, industry practices, and\nsector-specific policies, the paper highlights the complexities of balancing\ninnovation with ethical accountability and equitable access. The findings\nprovide actionable insights and recommendations for fostering responsible,\ntransparent, and safe integration of GAI and LLMs in diverse industry contexts."
                },
                "authors": [
                    {
                        "name": "Junfeng Jiao"
                    },
                    {
                        "name": "Saleh Afroogh"
                    },
                    {
                        "name": "Kevin Chen"
                    },
                    {
                        "name": "David Atkinson"
                    },
                    {
                        "name": "Amit Dhurandhar"
                    }
                ],
                "author_detail": {
                    "name": "Amit Dhurandhar"
                },
                "author": "Amit Dhurandhar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00957v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00957v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04572v1",
                "updated": "2025-01-08T15:42:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    15,
                    42,
                    41,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T15:42:41Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    15,
                    42,
                    41,
                    2,
                    8,
                    0
                ],
                "title": "Regret Analysis: a control perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regret Analysis: a control perspective"
                },
                "summary": "Online learning and model reference adaptive control have many interesting\nintersections. One area where they differ however is in how the algorithms are\nanalyzed and what objective or metric is used to discriminate \"good\" algorithms\nfrom \"bad\" algorithms. In adaptive control there are usually two objectives: 1)\nprove that all time varying parameters/states of the system are bounded, and 2)\nthat the instantaneous error between the adaptively controlled system and a\nreference system converges to zero over time (or at least a compact set). For\nonline learning the performance of algorithms is often characterized by the\nregret the algorithm incurs. Regret is defined as the cumulative loss (cost)\nover time from the online algorithm minus the cumulative loss (cost) of the\nsingle optimal fixed parameter choice in hindsight. Another significant\ndifference between the two areas of research is with regard to the assumptions\nmade in order to obtain said results. Adaptive control makes assumptions about\nthe input-output properties of the control problem and derives solutions for a\nfixed error model or optimization task. In the online learning literature\nresults are derived for classes of loss functions (i.e. convex) while a priori\nassuming that all time varying parameters are bounded, which for many\noptimization tasks is not unrealistic, but is a non starter in control\napplications. In this work we discuss these differences in detail through the\nregret based analysis of gradient descent for convex functions and the control\nbased analysis of a streaming regression problem. We close with a discussion\nabout the newly defined paradigm of online adaptive control and ask the\nfollowing question \"Are regret optimal control strategies deployable?\"",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online learning and model reference adaptive control have many interesting\nintersections. One area where they differ however is in how the algorithms are\nanalyzed and what objective or metric is used to discriminate \"good\" algorithms\nfrom \"bad\" algorithms. In adaptive control there are usually two objectives: 1)\nprove that all time varying parameters/states of the system are bounded, and 2)\nthat the instantaneous error between the adaptively controlled system and a\nreference system converges to zero over time (or at least a compact set). For\nonline learning the performance of algorithms is often characterized by the\nregret the algorithm incurs. Regret is defined as the cumulative loss (cost)\nover time from the online algorithm minus the cumulative loss (cost) of the\nsingle optimal fixed parameter choice in hindsight. Another significant\ndifference between the two areas of research is with regard to the assumptions\nmade in order to obtain said results. Adaptive control makes assumptions about\nthe input-output properties of the control problem and derives solutions for a\nfixed error model or optimization task. In the online learning literature\nresults are derived for classes of loss functions (i.e. convex) while a priori\nassuming that all time varying parameters are bounded, which for many\noptimization tasks is not unrealistic, but is a non starter in control\napplications. In this work we discuss these differences in detail through the\nregret based analysis of gradient descent for convex functions and the control\nbased analysis of a streaming regression problem. We close with a discussion\nabout the newly defined paradigm of online adaptive control and ask the\nfollowing question \"Are regret optimal control strategies deployable?\""
                },
                "authors": [
                    {
                        "name": "Travis E. Gibson"
                    },
                    {
                        "name": "Sawal Acharya"
                    }
                ],
                "author_detail": {
                    "name": "Sawal Acharya"
                },
                "author": "Sawal Acharya",
                "arxiv_comment": "10 pages no figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18367v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18367v3",
                "updated": "2025-01-08T15:30:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    15,
                    30,
                    11,
                    2,
                    8,
                    0
                ],
                "published": "2024-12-24T11:50:18Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    11,
                    50,
                    18,
                    1,
                    359,
                    0
                ],
                "title": "Towards Global AI Inclusivity: A Large-Scale Multilingual Terminology\n  Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Global AI Inclusivity: A Large-Scale Multilingual Terminology\n  Dataset"
                },
                "summary": "The field of machine translation has achieved significant advancements, yet\ndomain-specific terminology translation, particularly in AI, remains\nchallenging. We introduced GIST, a large-scale multilingual AI terminology\ndataset containing 5K terms extracted from top AI conference papers spanning\n2000 to 2023. The terms were translated into Arabic, Chinese, French, Japanese,\nand Russian using a hybrid framework that combines LLMs for extraction with\nhuman expertise for translation. The dataset's quality was benchmarked against\nexisting resources, demonstrating superior translation accuracy through\ncrowdsourced evaluation. GIST was integrated into translation workflows using\npost-translation refinement methods that required no retraining, where LLM\nprompting consistently improved BLEU and COMET scores. A web demonstration on\nthe ACL Anthology platform highlights its practical application, showcasing\nimproved accessibility for non-English speakers. This work aims to address\ncritical gaps in AI terminology resources and fosters global inclusivity and\ncollaboration in AI research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of machine translation has achieved significant advancements, yet\ndomain-specific terminology translation, particularly in AI, remains\nchallenging. We introduced GIST, a large-scale multilingual AI terminology\ndataset containing 5K terms extracted from top AI conference papers spanning\n2000 to 2023. The terms were translated into Arabic, Chinese, French, Japanese,\nand Russian using a hybrid framework that combines LLMs for extraction with\nhuman expertise for translation. The dataset's quality was benchmarked against\nexisting resources, demonstrating superior translation accuracy through\ncrowdsourced evaluation. GIST was integrated into translation workflows using\npost-translation refinement methods that required no retraining, where LLM\nprompting consistently improved BLEU and COMET scores. A web demonstration on\nthe ACL Anthology platform highlights its practical application, showcasing\nimproved accessibility for non-English speakers. This work aims to address\ncritical gaps in AI terminology resources and fosters global inclusivity and\ncollaboration in AI research."
                },
                "authors": [
                    {
                        "name": "Jiarui Liu"
                    },
                    {
                        "name": "Iman Ouzzani"
                    },
                    {
                        "name": "Wenkai Li"
                    },
                    {
                        "name": "Lechen Zhang"
                    },
                    {
                        "name": "Tianyue Ou"
                    },
                    {
                        "name": "Houda Bouamor"
                    },
                    {
                        "name": "Zhijing Jin"
                    },
                    {
                        "name": "Mona Diab"
                    }
                ],
                "author_detail": {
                    "name": "Mona Diab"
                },
                "author": "Mona Diab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18367v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18367v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18205v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18205v4",
                "updated": "2025-01-08T15:18:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    15,
                    18,
                    15,
                    2,
                    8,
                    0
                ],
                "published": "2024-02-28T09:51:55Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    9,
                    51,
                    55,
                    2,
                    59,
                    0
                ],
                "title": "Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging"
                },
                "summary": "Logs produced by extensive software systems are integral to monitoring system\nbehaviors. Advanced log analysis facilitates the detection, alerting, and\ndiagnosis of system faults. Log parsing, which entails transforming raw log\nmessages into structured templates, constitutes a critical phase in the\nautomation of log analytics. Existing log parsers fail to identify the correct\ntemplates due to reliance on human-made rules. Besides, These methods focus on\nstatistical features while ignoring semantic information in log messages. To\naddress these challenges, we introduce a cutting-edge \\textbf{L}og parsing\nframework with \\textbf{E}ntropy sampling and Chain-of-Thought \\textbf{M}erging\n(Lemur). Specifically, to discard the tedious manual rules. We propose a novel\nsampling method inspired by information entropy, which efficiently clusters\ntypical logs. Furthermore, to enhance the merging of log templates, we design a\nchain-of-thought method for large language models (LLMs). LLMs exhibit\nexceptional semantic comprehension, deftly distinguishing between parameters\nand invariant tokens. We have conducted experiments on large-scale public\ndatasets. Extensive evaluation demonstrates that Lemur achieves the\nstate-of-the-art performance and impressive efficiency. The Code is available\nat https://github.com/zwpride/lemur.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logs produced by extensive software systems are integral to monitoring system\nbehaviors. Advanced log analysis facilitates the detection, alerting, and\ndiagnosis of system faults. Log parsing, which entails transforming raw log\nmessages into structured templates, constitutes a critical phase in the\nautomation of log analytics. Existing log parsers fail to identify the correct\ntemplates due to reliance on human-made rules. Besides, These methods focus on\nstatistical features while ignoring semantic information in log messages. To\naddress these challenges, we introduce a cutting-edge \\textbf{L}og parsing\nframework with \\textbf{E}ntropy sampling and Chain-of-Thought \\textbf{M}erging\n(Lemur). Specifically, to discard the tedious manual rules. We propose a novel\nsampling method inspired by information entropy, which efficiently clusters\ntypical logs. Furthermore, to enhance the merging of log templates, we design a\nchain-of-thought method for large language models (LLMs). LLMs exhibit\nexceptional semantic comprehension, deftly distinguishing between parameters\nand invariant tokens. We have conducted experiments on large-scale public\ndatasets. Extensive evaluation demonstrates that Lemur achieves the\nstate-of-the-art performance and impressive efficiency. The Code is available\nat https://github.com/zwpride/lemur."
                },
                "authors": [
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Hongcheng Guo"
                    },
                    {
                        "name": "Anjie Le"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Zhoujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhoujun Li"
                },
                "author": "Zhoujun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18205v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18205v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04557v1",
                "updated": "2025-01-08T15:07:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    15,
                    7,
                    18,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T15:07:18Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    15,
                    7,
                    18,
                    2,
                    8,
                    0
                ],
                "title": "Satellite-Terrestrial Routing or Inter-Satellite Routing? A Stochastic\n  Geometry Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satellite-Terrestrial Routing or Inter-Satellite Routing? A Stochastic\n  Geometry Perspective"
                },
                "summary": "The design and comparison of satellite-terrestrial routing (STR) and\ninter-satellite routing (ISR) in low Earth orbit satellite constellations is a\nwidely discussed topic. The signal propagation distance under STR is generally\nlonger than that under ISR, resulting in greater path loss. The global\ndeployment of gateways introduces additional costs for STR. In contrast,\ntransmissions under ISR rely on the energy of satellites, which could be more\ncostly. Additionally, ISLs require more complex communication protocol design,\nextra hardware support, and increased computational power. To maximize energy\nefficiency, we propose two optimal routing relay selection algorithms for ISR\nand STR, respectively. Furthermore, we derive the analytical expressions for\nthe routing availability probability and energy efficiency, quantifying the\nperformance of the algorithms. The analyses enable us to assess the performance\nof the proposed algorithms against existing methods through numerical results,\ncompare the performance of STR and ISR, and provide useful insights for\nconstellation design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The design and comparison of satellite-terrestrial routing (STR) and\ninter-satellite routing (ISR) in low Earth orbit satellite constellations is a\nwidely discussed topic. The signal propagation distance under STR is generally\nlonger than that under ISR, resulting in greater path loss. The global\ndeployment of gateways introduces additional costs for STR. In contrast,\ntransmissions under ISR rely on the energy of satellites, which could be more\ncostly. Additionally, ISLs require more complex communication protocol design,\nextra hardware support, and increased computational power. To maximize energy\nefficiency, we propose two optimal routing relay selection algorithms for ISR\nand STR, respectively. Furthermore, we derive the analytical expressions for\nthe routing availability probability and energy efficiency, quantifying the\nperformance of the algorithms. The analyses enable us to assess the performance\nof the proposed algorithms against existing methods through numerical results,\ncompare the performance of STR and ISR, and provide useful insights for\nconstellation design."
                },
                "authors": [
                    {
                        "name": "Ruibo Wang"
                    },
                    {
                        "name": "Mustafa A. Kishk"
                    },
                    {
                        "name": "Howard H. Yang"
                    },
                    {
                        "name": "Mohamed-Slim Alouini"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed-Slim Alouini"
                },
                "author": "Mohamed-Slim Alouini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04547v1",
                "updated": "2025-01-08T14:51:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    14,
                    51,
                    36,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T14:51:36Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    14,
                    51,
                    36,
                    2,
                    8,
                    0
                ],
                "title": "Medical artificial intelligence toolbox (MAIT): an explainable machine\n  learning framework for binary classification, survival modelling, and\n  regression analyses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical artificial intelligence toolbox (MAIT): an explainable machine\n  learning framework for binary classification, survival modelling, and\n  regression analyses"
                },
                "summary": "While machine learning offers diverse techniques suitable for exploring\nvarious medical research questions, a cohesive synergistic framework can\nfacilitate the integration and understanding of new approaches within unified\nmodel development and interpretation. We therefore introduce the Medical\nArtificial Intelligence Toolbox (MAIT), an explainable, open-source Python\npipeline for developing and evaluating binary classification, regression, and\nsurvival models on tabular datasets. MAIT addresses key challenges (e.g., high\ndimensionality, class imbalance, mixed variable types, and missingness) while\npromoting transparency in reporting (TRIPOD+AI compliant). Offering automated\nconfigurations for beginners and customizable source code for experts, MAIT\nstreamlines two primary use cases: Discovery (feature importance via unified\nscoring, e.g., SHapley Additive exPlanations - SHAP) and Prediction (model\ndevelopment and deployment with optimized solutions). Moreover, MAIT proposes\nnew techniques including fine-tuning of probability threshold in binary\nclassification, translation of cumulative hazard curves to binary\nclassification, enhanced visualizations for model interpretation for mixed data\ntypes, and handling censoring through semi-supervised learning, to adapt to a\nwide set of data constraints and study designs. We provide detailed tutorials\non GitHub, using four open-access data sets, to demonstrate how MAIT can be\nused to improve implementation and interpretation of ML models in medical\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While machine learning offers diverse techniques suitable for exploring\nvarious medical research questions, a cohesive synergistic framework can\nfacilitate the integration and understanding of new approaches within unified\nmodel development and interpretation. We therefore introduce the Medical\nArtificial Intelligence Toolbox (MAIT), an explainable, open-source Python\npipeline for developing and evaluating binary classification, regression, and\nsurvival models on tabular datasets. MAIT addresses key challenges (e.g., high\ndimensionality, class imbalance, mixed variable types, and missingness) while\npromoting transparency in reporting (TRIPOD+AI compliant). Offering automated\nconfigurations for beginners and customizable source code for experts, MAIT\nstreamlines two primary use cases: Discovery (feature importance via unified\nscoring, e.g., SHapley Additive exPlanations - SHAP) and Prediction (model\ndevelopment and deployment with optimized solutions). Moreover, MAIT proposes\nnew techniques including fine-tuning of probability threshold in binary\nclassification, translation of cumulative hazard curves to binary\nclassification, enhanced visualizations for model interpretation for mixed data\ntypes, and handling censoring through semi-supervised learning, to adapt to a\nwide set of data constraints and study designs. We provide detailed tutorials\non GitHub, using four open-access data sets, to demonstrate how MAIT can be\nused to improve implementation and interpretation of ML models in medical\nresearch."
                },
                "authors": [
                    {
                        "name": "Ramtin Zargari Marandi"
                    },
                    {
                        "name": "Anne Svane Frahm"
                    },
                    {
                        "name": "Jens Lundgren"
                    },
                    {
                        "name": "Daniel Dawson Murray"
                    },
                    {
                        "name": "Maja Milojevic"
                    }
                ],
                "author_detail": {
                    "name": "Maja Milojevic"
                },
                "author": "Maja Milojevic",
                "arxiv_comment": "14 pages, 2 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04543v1",
                "updated": "2025-01-08T14:46:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    14,
                    46,
                    37,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T14:46:37Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    14,
                    46,
                    37,
                    2,
                    8,
                    0
                ],
                "title": "The Impostor is Among Us: Can Large Language Models Capture the\n  Complexity of Human Personas?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impostor is Among Us: Can Large Language Models Capture the\n  Complexity of Human Personas?"
                },
                "summary": "Large Language Models (LLMs) created new opportunities for generating\npersonas, which are expected to streamline and accelerate the human-centered\ndesign process. Yet, AI-generated personas may not accurately represent actual\nuser experiences, as they can miss contextual and emotional insights critical\nto understanding real users' needs and behaviors. This paper examines the\ndifferences in how users perceive personas created by LLMs compared to those\ncrafted by humans regarding their credibility for design. We gathered ten\nhuman-crafted personas developed by HCI experts according to relevant\nattributes established in related work. Then, we systematically generated ten\npersonas and compared them with human-crafted ones in a survey. The results\nshowed that participants differentiated between human-created and AI-generated\npersonas, with the latter being perceived as more informative and consistent.\nHowever, participants noted that the AI-generated personas tended to follow\nstereotypes, highlighting the need for a greater emphasis on diversity when\nutilizing LLMs for persona creation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) created new opportunities for generating\npersonas, which are expected to streamline and accelerate the human-centered\ndesign process. Yet, AI-generated personas may not accurately represent actual\nuser experiences, as they can miss contextual and emotional insights critical\nto understanding real users' needs and behaviors. This paper examines the\ndifferences in how users perceive personas created by LLMs compared to those\ncrafted by humans regarding their credibility for design. We gathered ten\nhuman-crafted personas developed by HCI experts according to relevant\nattributes established in related work. Then, we systematically generated ten\npersonas and compared them with human-crafted ones in a survey. The results\nshowed that participants differentiated between human-created and AI-generated\npersonas, with the latter being perceived as more informative and consistent.\nHowever, participants noted that the AI-generated personas tended to follow\nstereotypes, highlighting the need for a greater emphasis on diversity when\nutilizing LLMs for persona creation."
                },
                "authors": [
                    {
                        "name": "Christopher Lazik"
                    },
                    {
                        "name": "Christopher Katins"
                    },
                    {
                        "name": "Charlotte Kauter"
                    },
                    {
                        "name": "Jonas Jakob"
                    },
                    {
                        "name": "Caroline Jay"
                    },
                    {
                        "name": "Lars Grunske"
                    },
                    {
                        "name": "Thomas Kosch"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Kosch"
                },
                "author": "Thomas Kosch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13375v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13375v2",
                "updated": "2025-01-08T14:41:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    14,
                    41,
                    4,
                    2,
                    8,
                    0
                ],
                "published": "2024-12-17T23:18:06Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    23,
                    18,
                    6,
                    1,
                    352,
                    0
                ],
                "title": "Extending LLMs to New Languages: A Case Study of Llama and Persian\n  Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending LLMs to New Languages: A Case Study of Llama and Persian\n  Adaptation"
                },
                "summary": "Large language models (LLMs) have made great progress in classification and\ntext generation tasks. However, they are mainly trained on English data and\noften struggle with low-resource languages. In this study, we explore adding a\nnew language, i.e., Persian, to Llama (a model with a limited understanding of\nPersian) using parameter-efficient fine-tuning. We employ a multi-stage\napproach involving pretraining on monolingual Persian data, aligning\nrepresentations through bilingual pretraining and instruction datasets, and\ninstruction-tuning with task-specific datasets. We evaluate the model's\nperformance at each stage on generation and classification tasks. Our findings\nsuggest that incorporating the Persian language, through bilingual data\nalignment, can enhance classification accuracy for Persian tasks, with no\nadverse impact and sometimes even improvements on English tasks. Additionally,\nthe results highlight the model's initial strength as a critical factor when\nworking with limited training data, with cross-lingual alignment offering\nminimal benefits for the low-resource language. Knowledge transfer from English\nto Persian has a marginal effect, primarily benefiting simple classification\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made great progress in classification and\ntext generation tasks. However, they are mainly trained on English data and\noften struggle with low-resource languages. In this study, we explore adding a\nnew language, i.e., Persian, to Llama (a model with a limited understanding of\nPersian) using parameter-efficient fine-tuning. We employ a multi-stage\napproach involving pretraining on monolingual Persian data, aligning\nrepresentations through bilingual pretraining and instruction datasets, and\ninstruction-tuning with task-specific datasets. We evaluate the model's\nperformance at each stage on generation and classification tasks. Our findings\nsuggest that incorporating the Persian language, through bilingual data\nalignment, can enhance classification accuracy for Persian tasks, with no\nadverse impact and sometimes even improvements on English tasks. Additionally,\nthe results highlight the model's initial strength as a critical factor when\nworking with limited training data, with cross-lingual alignment offering\nminimal benefits for the low-resource language. Knowledge transfer from English\nto Persian has a marginal effect, primarily benefiting simple classification\ntasks."
                },
                "authors": [
                    {
                        "name": "Samin Mahdizadeh Sani"
                    },
                    {
                        "name": "Pouya Sadeghi"
                    },
                    {
                        "name": "Thuy-Trang Vu"
                    },
                    {
                        "name": "Yadollah Yaghoobzadeh"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    }
                ],
                "author_detail": {
                    "name": "Gholamreza Haffari"
                },
                "author": "Gholamreza Haffari",
                "arxiv_comment": "accepted at COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13375v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13375v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00599v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00599v2",
                "updated": "2025-01-08T14:38:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    14,
                    38,
                    30,
                    2,
                    8,
                    0
                ],
                "published": "2024-12-31T18:56:46Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    18,
                    56,
                    46,
                    1,
                    366,
                    0
                ],
                "title": "VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with\n  Video LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with\n  Video LLM"
                },
                "summary": "Video Large Language Models (Video LLMs) have recently exhibited remarkable\ncapabilities in general video understanding. However, they mainly focus on\nholistic comprehension and struggle with capturing fine-grained spatial and\ntemporal details. Besides, the lack of high-quality object-level video\ninstruction data and a comprehensive benchmark further hinders their\nadvancements. To tackle these challenges, we introduce the VideoRefer Suite to\nempower Video LLM for finer-level spatial-temporal video understanding, i.e.,\nenabling perception and reasoning on any objects throughout the video.\nSpecially, we thoroughly develop VideoRefer Suite across three essential\naspects: dataset, model, and benchmark. Firstly, we introduce a multi-agent\ndata engine to meticulously curate a large-scale, high-quality object-level\nvideo instruction dataset, termed VideoRefer-700K. Next, we present the\nVideoRefer model, which equips a versatile spatial-temporal object encoder to\ncapture precise regional and sequential representations. Finally, we\nmeticulously create a VideoRefer-Bench to comprehensively assess the\nspatial-temporal understanding capability of a Video LLM, evaluating it across\nvarious aspects. Extensive experiments and analyses demonstrate that our\nVideoRefer model not only achieves promising performance on video referring\nbenchmarks but also facilitates general video understanding capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (Video LLMs) have recently exhibited remarkable\ncapabilities in general video understanding. However, they mainly focus on\nholistic comprehension and struggle with capturing fine-grained spatial and\ntemporal details. Besides, the lack of high-quality object-level video\ninstruction data and a comprehensive benchmark further hinders their\nadvancements. To tackle these challenges, we introduce the VideoRefer Suite to\nempower Video LLM for finer-level spatial-temporal video understanding, i.e.,\nenabling perception and reasoning on any objects throughout the video.\nSpecially, we thoroughly develop VideoRefer Suite across three essential\naspects: dataset, model, and benchmark. Firstly, we introduce a multi-agent\ndata engine to meticulously curate a large-scale, high-quality object-level\nvideo instruction dataset, termed VideoRefer-700K. Next, we present the\nVideoRefer model, which equips a versatile spatial-temporal object encoder to\ncapture precise regional and sequential representations. Finally, we\nmeticulously create a VideoRefer-Bench to comprehensively assess the\nspatial-temporal understanding capability of a Video LLM, evaluating it across\nvarious aspects. Extensive experiments and analyses demonstrate that our\nVideoRefer model not only achieves promising performance on video referring\nbenchmarks but also facilitates general video understanding capabilities."
                },
                "authors": [
                    {
                        "name": "Yuqian Yuan"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Wentong Li"
                    },
                    {
                        "name": "Zesen Cheng"
                    },
                    {
                        "name": "Boqiang Zhang"
                    },
                    {
                        "name": "Long Li"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Deli Zhao"
                    },
                    {
                        "name": "Wenqiao Zhang"
                    },
                    {
                        "name": "Yueting Zhuang"
                    },
                    {
                        "name": "Jianke Zhu"
                    },
                    {
                        "name": "Lidong Bing"
                    }
                ],
                "author_detail": {
                    "name": "Lidong Bing"
                },
                "author": "Lidong Bing",
                "arxiv_comment": "17 pages, 14 figures, technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00599v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00599v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04519v1",
                "updated": "2025-01-08T14:12:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    14,
                    12,
                    57,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T14:12:57Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    14,
                    12,
                    57,
                    2,
                    8,
                    0
                ],
                "title": "rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep\n  Thinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep\n  Thinking"
                },
                "summary": "We present rStar-Math to demonstrate that small language models (SLMs) can\nrival or even surpass the math reasoning capability of OpenAI o1, without\ndistillation from superior models. rStar-Math achieves this by exercising \"deep\nthinking\" through Monte Carlo Tree Search (MCTS), where a math policy SLM\nperforms test-time search guided by an SLM-based process reward model.\nrStar-Math introduces three innovations to tackle the challenges in training\nthe two SLMs: (1) a novel code-augmented CoT data sythesis method, which\nperforms extensive MCTS rollouts to generate step-by-step verified reasoning\ntrajectories used to train the policy SLM; (2) a novel process reward model\ntraining method that avoids na\\\"ive step-level score annotation, yielding a\nmore effective process preference model (PPM); (3) a self-evolution recipe in\nwhich the policy SLM and PPM are built from scratch and iteratively evolved to\nimprove reasoning capabilities. Through 4 rounds of self-evolution with\nmillions of synthesized solutions for 747k math problems, rStar-Math boosts\nSLMs' math reasoning to state-of-the-art levels. On the MATH benchmark, it\nimproves Qwen2.5-Math-7B from 58.8% to 90.0% and Phi3-mini-3.8B from 41.4% to\n86.4%, surpassing o1-preview by +4.5% and +0.9%. On the USA Math Olympiad\n(AIME), rStar-Math solves an average of 53.3% (8/15) of problems, ranking among\nthe top 20% the brightest high school math students. Code and data will be\navailable at https://github.com/microsoft/rStar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present rStar-Math to demonstrate that small language models (SLMs) can\nrival or even surpass the math reasoning capability of OpenAI o1, without\ndistillation from superior models. rStar-Math achieves this by exercising \"deep\nthinking\" through Monte Carlo Tree Search (MCTS), where a math policy SLM\nperforms test-time search guided by an SLM-based process reward model.\nrStar-Math introduces three innovations to tackle the challenges in training\nthe two SLMs: (1) a novel code-augmented CoT data sythesis method, which\nperforms extensive MCTS rollouts to generate step-by-step verified reasoning\ntrajectories used to train the policy SLM; (2) a novel process reward model\ntraining method that avoids na\\\"ive step-level score annotation, yielding a\nmore effective process preference model (PPM); (3) a self-evolution recipe in\nwhich the policy SLM and PPM are built from scratch and iteratively evolved to\nimprove reasoning capabilities. Through 4 rounds of self-evolution with\nmillions of synthesized solutions for 747k math problems, rStar-Math boosts\nSLMs' math reasoning to state-of-the-art levels. On the MATH benchmark, it\nimproves Qwen2.5-Math-7B from 58.8% to 90.0% and Phi3-mini-3.8B from 41.4% to\n86.4%, surpassing o1-preview by +4.5% and +0.9%. On the USA Math Olympiad\n(AIME), rStar-Math solves an average of 53.3% (8/15) of problems, ranking among\nthe top 20% the brightest high school math students. Code and data will be\navailable at https://github.com/microsoft/rStar."
                },
                "authors": [
                    {
                        "name": "Xinyu Guan"
                    },
                    {
                        "name": "Li Lyna Zhang"
                    },
                    {
                        "name": "Yifei Liu"
                    },
                    {
                        "name": "Ning Shang"
                    },
                    {
                        "name": "Youran Sun"
                    },
                    {
                        "name": "Yi Zhu"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01587v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01587v2",
                "updated": "2025-01-08T14:12:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    14,
                    12,
                    45,
                    2,
                    8,
                    0
                ],
                "published": "2024-04-02T02:29:41Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    2,
                    29,
                    41,
                    1,
                    93,
                    0
                ],
                "title": "TSCM: A Teacher-Student Model for Vision Place Recognition Using\n  Cross-Metric Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TSCM: A Teacher-Student Model for Vision Place Recognition Using\n  Cross-Metric Knowledge Distillation"
                },
                "summary": "Visual place recognition (VPR) plays a pivotal role in autonomous exploration\nand navigation of mobile robots within complex outdoor environments. While\ncost-effective and easily deployed, camera sensors are sensitive to lighting\nand weather changes, and even slight image alterations can greatly affect VPR\nefficiency and precision. Existing methods overcome this by exploiting powerful\nyet large networks, leading to significant consumption of computational\nresources. In this paper, we propose a high-performance teacher and lightweight\nstudent distillation framework called TSCM. It exploits our devised\ncross-metric knowledge distillation to narrow the performance gap between the\nteacher and student models, maintaining superior performance while enabling\nminimal computational load during deployment. We conduct comprehensive\nevaluations on large-scale datasets, namely Pittsburgh30k and Pittsburgh250k.\nExperimental results demonstrate the superiority of our method over baseline\nmodels in terms of recognition accuracy and model parameter efficiency.\nMoreover, our ablation studies show that the proposed knowledge distillation\ntechnique surpasses other counterparts. The code of our method has been\nreleased at https://github.com/nubot-nudt/TSCM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual place recognition (VPR) plays a pivotal role in autonomous exploration\nand navigation of mobile robots within complex outdoor environments. While\ncost-effective and easily deployed, camera sensors are sensitive to lighting\nand weather changes, and even slight image alterations can greatly affect VPR\nefficiency and precision. Existing methods overcome this by exploiting powerful\nyet large networks, leading to significant consumption of computational\nresources. In this paper, we propose a high-performance teacher and lightweight\nstudent distillation framework called TSCM. It exploits our devised\ncross-metric knowledge distillation to narrow the performance gap between the\nteacher and student models, maintaining superior performance while enabling\nminimal computational load during deployment. We conduct comprehensive\nevaluations on large-scale datasets, namely Pittsburgh30k and Pittsburgh250k.\nExperimental results demonstrate the superiority of our method over baseline\nmodels in terms of recognition accuracy and model parameter efficiency.\nMoreover, our ablation studies show that the proposed knowledge distillation\ntechnique surpasses other counterparts. The code of our method has been\nreleased at https://github.com/nubot-nudt/TSCM."
                },
                "authors": [
                    {
                        "name": "Yehui Shen"
                    },
                    {
                        "name": "Mingmin Liu"
                    },
                    {
                        "name": "Huimin Lu"
                    },
                    {
                        "name": "Xieyuanli Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xieyuanli Chen"
                },
                "author": "Xieyuanli Chen",
                "arxiv_comment": "Accepted to ICRA 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01587v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01587v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13867v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13867v2",
                "updated": "2025-01-08T14:08:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    14,
                    8,
                    11,
                    2,
                    8,
                    0
                ],
                "published": "2024-05-22T17:48:17Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    17,
                    48,
                    17,
                    2,
                    143,
                    0
                ],
                "title": "Scaling-laws for Large Time-series Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-laws for Large Time-series Models"
                },
                "summary": "Scaling laws for large language models (LLMs) have provided useful guidance\nin training ever larger models for predictable performance gains. Time series\nforecasting shares a similar sequential structure to language, and is amenable\nto large-scale transformer architectures. Here we show that foundational\ndecoder-only time series transformer models exhibit analogous scaling-behavior\nto LLMs, with architectural details (aspect ratio and number of heads) having a\nminimal effect over broad ranges. We assemble a large corpus of heterogenous\ntime series data on which to train, and establish for the first time power-law\nscaling with parameter count, dataset size, and training compute, spanning five\norders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling laws for large language models (LLMs) have provided useful guidance\nin training ever larger models for predictable performance gains. Time series\nforecasting shares a similar sequential structure to language, and is amenable\nto large-scale transformer architectures. Here we show that foundational\ndecoder-only time series transformer models exhibit analogous scaling-behavior\nto LLMs, with architectural details (aspect ratio and number of heads) having a\nminimal effect over broad ranges. We assemble a large corpus of heterogenous\ntime series data on which to train, and establish for the first time power-law\nscaling with parameter count, dataset size, and training compute, spanning five\norders of magnitude."
                },
                "authors": [
                    {
                        "name": "Thomas D. P. Edwards"
                    },
                    {
                        "name": "James Alvey"
                    },
                    {
                        "name": "Justin Alsing"
                    },
                    {
                        "name": "Nam H. Nguyen"
                    },
                    {
                        "name": "Benjamin D. Wandelt"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin D. Wandelt"
                },
                "author": "Benjamin D. Wandelt",
                "arxiv_comment": "4 main pages (16 total), 4 figures; Accepted for oral presentation in\n  Time Series in the Age of Large Models (TSALM) Workshop at Neurips 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13867v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13867v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04510v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04510v1",
                "updated": "2025-01-08T13:56:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    13,
                    56,
                    17,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T13:56:17Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    13,
                    56,
                    17,
                    2,
                    8,
                    0
                ],
                "title": "CGP-Tuning: Structure-Aware Soft Prompt Tuning for Code Vulnerability\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CGP-Tuning: Structure-Aware Soft Prompt Tuning for Code Vulnerability\n  Detection"
                },
                "summary": "Large language models (LLMs) have been proposed as powerful tools for\ndetecting software vulnerabilities, where task-specific fine-tuning is\ntypically employed to provide vulnerability-specific knowledge to the LLMs for\nthis purpose. However, traditional full-parameter fine-tuning is inefficient\nfor modern, complex LLMs, which contain billions of parameters.\n  Soft prompt tuning has been suggested as a more efficient alternative for\nfine-tuning LLMs in general cases. However, pure soft prompt tuning treats\nsource code as plain text, losing structural information inherent in source\ncode. Meanwhile, graph-enhanced soft prompt tuning methods, which aim to\naddress this issue, are unable to preserve the rich semantic information within\ncode graphs, as they are primarily designed for general graph-related tasks and\nfocus more on adjacency information. They also fail to ensure computational\nefficiency while accounting for graph-text interactions.\n  This paper, therefore, introduces a new code graph-enhanced, structure-aware\nsoft prompt tuning method for vulnerability detection, referred to as\nCGP-Tuning. It employs innovative type-aware embeddings to capture the rich\nsemantic information within code graphs, along with a novel and efficient\ncross-modal alignment module that achieves linear computational cost while\nincorporating graph-text interactions. The proposed CGP-Tuning is evaluated on\nthe latest DiverseVul dataset and the most recent open-source code LLMs,\nCodeLlama and CodeGemma. Experimental results demonstrate that CGP-Tuning\noutperforms the best state-of-the-art method by an average of 3.5 percentage\npoints in accuracy, without compromising its vulnerability detection\ncapabilities for long source code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been proposed as powerful tools for\ndetecting software vulnerabilities, where task-specific fine-tuning is\ntypically employed to provide vulnerability-specific knowledge to the LLMs for\nthis purpose. However, traditional full-parameter fine-tuning is inefficient\nfor modern, complex LLMs, which contain billions of parameters.\n  Soft prompt tuning has been suggested as a more efficient alternative for\nfine-tuning LLMs in general cases. However, pure soft prompt tuning treats\nsource code as plain text, losing structural information inherent in source\ncode. Meanwhile, graph-enhanced soft prompt tuning methods, which aim to\naddress this issue, are unable to preserve the rich semantic information within\ncode graphs, as they are primarily designed for general graph-related tasks and\nfocus more on adjacency information. They also fail to ensure computational\nefficiency while accounting for graph-text interactions.\n  This paper, therefore, introduces a new code graph-enhanced, structure-aware\nsoft prompt tuning method for vulnerability detection, referred to as\nCGP-Tuning. It employs innovative type-aware embeddings to capture the rich\nsemantic information within code graphs, along with a novel and efficient\ncross-modal alignment module that achieves linear computational cost while\nincorporating graph-text interactions. The proposed CGP-Tuning is evaluated on\nthe latest DiverseVul dataset and the most recent open-source code LLMs,\nCodeLlama and CodeGemma. Experimental results demonstrate that CGP-Tuning\noutperforms the best state-of-the-art method by an average of 3.5 percentage\npoints in accuracy, without compromising its vulnerability detection\ncapabilities for long source code."
                },
                "authors": [
                    {
                        "name": "Ruijun Feng"
                    },
                    {
                        "name": "Hammond Pearce"
                    },
                    {
                        "name": "Pietro Liguori"
                    },
                    {
                        "name": "Yulei Sui"
                    }
                ],
                "author_detail": {
                    "name": "Yulei Sui"
                },
                "author": "Yulei Sui",
                "arxiv_comment": "14 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04510v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00481v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00481v5",
                "updated": "2025-01-08T13:42:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    13,
                    42,
                    18,
                    2,
                    8,
                    0
                ],
                "published": "2024-08-31T15:26:57Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    15,
                    26,
                    57,
                    5,
                    244,
                    0
                ],
                "title": "DCIM-AVSR : Efficient Audio-Visual Speech Recognition via Dual Conformer\n  Interaction Module",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DCIM-AVSR : Efficient Audio-Visual Speech Recognition via Dual Conformer\n  Interaction Module"
                },
                "summary": "Speech recognition is the technology that enables machines to interpret and\nprocess human speech, converting spoken language into text or commands. This\ntechnology is essential for applications such as virtual assistants,\ntranscription services, and communication tools. The Audio-Visual Speech\nRecognition (AVSR) model enhances traditional speech recognition, particularly\nin noisy environments, by incorporating visual modalities like lip movements\nand facial expressions. While traditional AVSR models trained on large-scale\ndatasets with numerous parameters can achieve remarkable accuracy, often\nsurpassing human performance, they also come with high training costs and\ndeployment challenges. To address these issues, we introduce an efficient AVSR\nmodel that reduces the number of parameters through the integration of a Dual\nConformer Interaction Module (DCIM). In addition, we propose a pre-training\nmethod that further optimizes model performance by selectively updating\nparameters, leading to significant improvements in efficiency. Unlike\nconventional models that require the system to independently learn the\nhierarchical relationship between audio and visual modalities, our approach\nincorporates this distinction directly into the model architecture. This design\nenhances both efficiency and performance, resulting in a more practical and\neffective solution for AVSR tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech recognition is the technology that enables machines to interpret and\nprocess human speech, converting spoken language into text or commands. This\ntechnology is essential for applications such as virtual assistants,\ntranscription services, and communication tools. The Audio-Visual Speech\nRecognition (AVSR) model enhances traditional speech recognition, particularly\nin noisy environments, by incorporating visual modalities like lip movements\nand facial expressions. While traditional AVSR models trained on large-scale\ndatasets with numerous parameters can achieve remarkable accuracy, often\nsurpassing human performance, they also come with high training costs and\ndeployment challenges. To address these issues, we introduce an efficient AVSR\nmodel that reduces the number of parameters through the integration of a Dual\nConformer Interaction Module (DCIM). In addition, we propose a pre-training\nmethod that further optimizes model performance by selectively updating\nparameters, leading to significant improvements in efficiency. Unlike\nconventional models that require the system to independently learn the\nhierarchical relationship between audio and visual modalities, our approach\nincorporates this distinction directly into the model architecture. This design\nenhances both efficiency and performance, resulting in a more practical and\neffective solution for AVSR tasks."
                },
                "authors": [
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Haotian Jiang"
                    },
                    {
                        "name": "Haolin Huang"
                    },
                    {
                        "name": "Yu Fang"
                    },
                    {
                        "name": "Mengjie Xu"
                    },
                    {
                        "name": "Qian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qian Wang"
                },
                "author": "Qian Wang",
                "arxiv_comment": "Accepted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00481v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00481v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04487v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04487v1",
                "updated": "2025-01-08T13:14:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    13,
                    14,
                    5,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T13:14:05Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    13,
                    14,
                    5,
                    2,
                    8,
                    0
                ],
                "title": "Integrating remote sensing data assimilation, deep learning and large\n  language model for interactive wheat breeding yield prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating remote sensing data assimilation, deep learning and large\n  language model for interactive wheat breeding yield prediction"
                },
                "summary": "Yield is one of the core goals of crop breeding. By predicting the potential\nyield of different breeding materials, breeders can screen these materials at\nvarious growth stages to select the best performing. Based on unmanned aerial\nvehicle remote sensing technology, high-throughput crop phenotyping data in\nbreeding areas is collected to provide data support for the breeding decisions\nof breeders. However, the accuracy of current yield predictions still requires\nimprovement, and the usability and user-friendliness of yield forecasting tools\nremain suboptimal. To address these challenges, this study introduces a hybrid\nmethod and tool for crop yield prediction, designed to allow breeders to\ninteractively and accurately predict wheat yield by chatting with a large\nlanguage model (LLM). First, the newly designed data assimilation algorithm is\nused to assimilate the leaf area index into the WOFOST model. Then, selected\noutputs from the assimilation process, along with remote sensing inversion\nresults, are used to drive the time-series temporal fusion transformer model\nfor wheat yield prediction. Finally, based on this hybrid method and leveraging\nan LLM with retrieval augmented generation technology, we developed an\ninteractive yield prediction Web tool that is user-friendly and supports\nsustainable data updates. This tool integrates multi-source data to assist\nbreeding decision-making. This study aims to accelerate the identification of\nhigh-yield materials in the breeding process, enhance breeding efficiency, and\nenable more scientific and smart breeding decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yield is one of the core goals of crop breeding. By predicting the potential\nyield of different breeding materials, breeders can screen these materials at\nvarious growth stages to select the best performing. Based on unmanned aerial\nvehicle remote sensing technology, high-throughput crop phenotyping data in\nbreeding areas is collected to provide data support for the breeding decisions\nof breeders. However, the accuracy of current yield predictions still requires\nimprovement, and the usability and user-friendliness of yield forecasting tools\nremain suboptimal. To address these challenges, this study introduces a hybrid\nmethod and tool for crop yield prediction, designed to allow breeders to\ninteractively and accurately predict wheat yield by chatting with a large\nlanguage model (LLM). First, the newly designed data assimilation algorithm is\nused to assimilate the leaf area index into the WOFOST model. Then, selected\noutputs from the assimilation process, along with remote sensing inversion\nresults, are used to drive the time-series temporal fusion transformer model\nfor wheat yield prediction. Finally, based on this hybrid method and leveraging\nan LLM with retrieval augmented generation technology, we developed an\ninteractive yield prediction Web tool that is user-friendly and supports\nsustainable data updates. This tool integrates multi-source data to assist\nbreeding decision-making. This study aims to accelerate the identification of\nhigh-yield materials in the breeding process, enhance breeding efficiency, and\nenable more scientific and smart breeding decisions."
                },
                "authors": [
                    {
                        "name": "Guofeng Yang"
                    },
                    {
                        "name": "Nanfei Jin"
                    },
                    {
                        "name": "Wenjie Ai"
                    },
                    {
                        "name": "Zhonghua Zheng"
                    },
                    {
                        "name": "Yuhong He"
                    },
                    {
                        "name": "Yong He"
                    }
                ],
                "author_detail": {
                    "name": "Yong He"
                },
                "author": "Yong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04487v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06159v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06159v2",
                "updated": "2025-01-08T13:06:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    13,
                    6,
                    27,
                    2,
                    8,
                    0
                ],
                "published": "2024-11-09T12:06:40Z",
                "published_parsed": [
                    2024,
                    11,
                    9,
                    12,
                    6,
                    40,
                    5,
                    314,
                    0
                ],
                "title": "Mixture of Knowledge Minigraph Agents for Literature Review Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Knowledge Minigraph Agents for Literature Review Generation"
                },
                "summary": "Literature reviews play a crucial role in scientific research for\nunderstanding the current state of research, identifying gaps, and guiding\nfuture studies on specific topics. However, the process of conducting a\ncomprehensive literature review is yet time-consuming. This paper proposes a\nnovel framework, collaborative knowledge minigraph agents (CKMAs), to automate\nscholarly literature reviews. A novel prompt-based algorithm, the knowledge\nminigraph construction agent (KMCA), is designed to identify relations between\nconcepts from academic literature and automatically constructs knowledge\nminigraphs. By leveraging the capabilities of large language models on\nconstructed knowledge minigraphs, the multiple path summarization agent (MPSA)\nefficiently organizes concepts and relations from different viewpoints to\ngenerate literature review paragraphs. We evaluate CKMAs on three benchmark\ndatasets. Experimental results show the effectiveness of the proposed method,\nfurther revealing promising applications of LLMs in scientific research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Literature reviews play a crucial role in scientific research for\nunderstanding the current state of research, identifying gaps, and guiding\nfuture studies on specific topics. However, the process of conducting a\ncomprehensive literature review is yet time-consuming. This paper proposes a\nnovel framework, collaborative knowledge minigraph agents (CKMAs), to automate\nscholarly literature reviews. A novel prompt-based algorithm, the knowledge\nminigraph construction agent (KMCA), is designed to identify relations between\nconcepts from academic literature and automatically constructs knowledge\nminigraphs. By leveraging the capabilities of large language models on\nconstructed knowledge minigraphs, the multiple path summarization agent (MPSA)\nefficiently organizes concepts and relations from different viewpoints to\ngenerate literature review paragraphs. We evaluate CKMAs on three benchmark\ndatasets. Experimental results show the effectiveness of the proposed method,\nfurther revealing promising applications of LLMs in scientific research."
                },
                "authors": [
                    {
                        "name": "Zhi Zhang"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Sheng-hua Zhong"
                    },
                    {
                        "name": "Gong Chen"
                    },
                    {
                        "name": "Yu Yang"
                    },
                    {
                        "name": "Jiannong Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jiannong Cao"
                },
                "author": "Jiannong Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06159v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06159v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04473v1",
                "updated": "2025-01-08T12:54:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    12,
                    54,
                    5,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T12:54:05Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    12,
                    54,
                    5,
                    2,
                    8,
                    0
                ],
                "title": "When LLMs Struggle: Reference-less Translation Evaluation for\n  Low-resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When LLMs Struggle: Reference-less Translation Evaluation for\n  Low-resource Languages"
                },
                "summary": "This paper investigates the reference-less evaluation of machine translation\nfor low-resource language pairs, known as quality estimation (QE).\nSegment-level QE is a challenging cross-lingual language understanding task\nthat provides a quality score (0-100) to the translated output. We\ncomprehensively evaluate large language models (LLMs) in zero/few-shot\nscenarios and perform instruction fine-tuning using a novel prompt based on\nannotation guidelines. Our results indicate that prompt-based approaches are\noutperformed by the encoder-based fine-tuned QE models. Our error analysis\nreveals tokenization issues, along with errors due to transliteration and named\nentities, and argues for refinement in LLM pre-training for cross-lingual\ntasks. We release the data, and models trained publicly for further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the reference-less evaluation of machine translation\nfor low-resource language pairs, known as quality estimation (QE).\nSegment-level QE is a challenging cross-lingual language understanding task\nthat provides a quality score (0-100) to the translated output. We\ncomprehensively evaluate large language models (LLMs) in zero/few-shot\nscenarios and perform instruction fine-tuning using a novel prompt based on\nannotation guidelines. Our results indicate that prompt-based approaches are\noutperformed by the encoder-based fine-tuned QE models. Our error analysis\nreveals tokenization issues, along with errors due to transliteration and named\nentities, and argues for refinement in LLM pre-training for cross-lingual\ntasks. We release the data, and models trained publicly for further research."
                },
                "authors": [
                    {
                        "name": "Archchana Sindhujan"
                    },
                    {
                        "name": "Diptesh Kanojia"
                    },
                    {
                        "name": "Constantin Orasan"
                    },
                    {
                        "name": "Shenbin Qian"
                    }
                ],
                "author_detail": {
                    "name": "Shenbin Qian"
                },
                "author": "Shenbin Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04464v1",
                "updated": "2025-01-08T12:37:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    12,
                    37,
                    31,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T12:37:31Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    12,
                    37,
                    31,
                    2,
                    8,
                    0
                ],
                "title": "Assessing the Acceptance of a Mid-Air Gesture Syntax for Smart Space\n  Interaction: An Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Acceptance of a Mid-Air Gesture Syntax for Smart Space\n  Interaction: An Empirical Study"
                },
                "summary": "This article explores the use of a location-aware mid-air gesture-based\ncommand triplet syntax to interact with a smart space. The syntax, inspired by\nhuman language, is built as a vocative case with an imperative structure. In a\nsentence like 'Light, please switch on', the object being activated is invoked\nvia making a gesture that mimics its initial letter/acronym (vocative,\ncoincident with the sentence's elliptical subject). A geometrical or\ndirectional gesture then identifies the action (imperative verb) and may\ninclude an object feature or a second object with which to network\n(complement), which also represented by the initial or acronym letter.\nTechnically, an interpreter relying on a trainable multidevice gesture\nrecognition layer makes the pair/triplet syntax decoding possible. The\nrecognition layer works on acceleration and position input signals from\ngraspable (smartphone) and free-hand devices (smartwatch and external depth\ncameras), as well as a specific compiler. On a specific deployment at a Living\nLab facility, the syntax has been instantiated via the use of a lexicon derived\nfrom English (with respect to the initial letters and acronyms). A\nwithin-subject analysis with twelve users has enabled the analysis of the\nsyntax acceptance (in terms of usability, gesture agreement for actions over\nobjects, and social acceptance) and technology preference of the gesture syntax\nwithin its three device implementations (graspable, wearable, and device-free\nones). Participants express consensus regarding the simplicity of learning the\nsyntax and its potential effectiveness in managing smart resources. Socially,\nparticipants favoured the Watch for outdoor activities and the Phone for home\nand work settings, underscoring the importance of social context in technology\ndesign. The Phone emerged as the preferred option for gesture recognition due\nto its efficiency and familiarity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article explores the use of a location-aware mid-air gesture-based\ncommand triplet syntax to interact with a smart space. The syntax, inspired by\nhuman language, is built as a vocative case with an imperative structure. In a\nsentence like 'Light, please switch on', the object being activated is invoked\nvia making a gesture that mimics its initial letter/acronym (vocative,\ncoincident with the sentence's elliptical subject). A geometrical or\ndirectional gesture then identifies the action (imperative verb) and may\ninclude an object feature or a second object with which to network\n(complement), which also represented by the initial or acronym letter.\nTechnically, an interpreter relying on a trainable multidevice gesture\nrecognition layer makes the pair/triplet syntax decoding possible. The\nrecognition layer works on acceleration and position input signals from\ngraspable (smartphone) and free-hand devices (smartwatch and external depth\ncameras), as well as a specific compiler. On a specific deployment at a Living\nLab facility, the syntax has been instantiated via the use of a lexicon derived\nfrom English (with respect to the initial letters and acronyms). A\nwithin-subject analysis with twelve users has enabled the analysis of the\nsyntax acceptance (in terms of usability, gesture agreement for actions over\nobjects, and social acceptance) and technology preference of the gesture syntax\nwithin its three device implementations (graspable, wearable, and device-free\nones). Participants express consensus regarding the simplicity of learning the\nsyntax and its potential effectiveness in managing smart resources. Socially,\nparticipants favoured the Watch for outdoor activities and the Phone for home\nand work settings, underscoring the importance of social context in technology\ndesign. The Phone emerged as the preferred option for gesture recognition due\nto its efficiency and familiarity."
                },
                "authors": [
                    {
                        "name": "Ana M. Bernardos"
                    },
                    {
                        "name": "Xian Wang"
                    },
                    {
                        "name": "Luca Bergesio"
                    },
                    {
                        "name": "Juan A. Besada"
                    },
                    {
                        "name": "José R. Casar"
                    }
                ],
                "author_detail": {
                    "name": "José R. Casar"
                },
                "author": "José R. Casar",
                "arxiv_doi": "10.3390/jsan13020025",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3390/jsan13020025",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.04464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "J. Sens. Actuator Netw. 2024, 13(2), 25",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04455v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04455v1",
                "updated": "2025-01-08T12:18:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    12,
                    18,
                    11,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T12:18:11Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    12,
                    18,
                    11,
                    2,
                    8,
                    0
                ],
                "title": "Hidden Entity Detection from GitHub Leveraging Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hidden Entity Detection from GitHub Leveraging Large Language Models"
                },
                "summary": "Named entity recognition is an important task when constructing knowledge\nbases from unstructured data sources. Whereas entity detection methods mostly\nrely on extensive training data, Large Language Models (LLMs) have paved the\nway towards approaches that rely on zero-shot learning (ZSL) or few-shot\nlearning (FSL) by taking advantage of the capabilities LLMs acquired during\npretraining. Specifically, in very specialized scenarios where large-scale\ntraining data is not available, ZSL / FSL opens new opportunities. This paper\nfollows this recent trend and investigates the potential of leveraging Large\nLanguage Models (LLMs) in such scenarios to automatically detect datasets and\nsoftware within textual content from GitHub repositories. While existing\nmethods focused solely on named entities, this study aims to broaden the scope\nby incorporating resources such as repositories and online hubs where entities\nare also represented by URLs. The study explores different FSL prompt learning\napproaches to enhance the LLMs' ability to identify dataset and software\nmentions within repository texts. Through analyses of LLM effectiveness and\nlearning strategies, this paper offers insights into the potential of advanced\nlanguage models for automated entity detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named entity recognition is an important task when constructing knowledge\nbases from unstructured data sources. Whereas entity detection methods mostly\nrely on extensive training data, Large Language Models (LLMs) have paved the\nway towards approaches that rely on zero-shot learning (ZSL) or few-shot\nlearning (FSL) by taking advantage of the capabilities LLMs acquired during\npretraining. Specifically, in very specialized scenarios where large-scale\ntraining data is not available, ZSL / FSL opens new opportunities. This paper\nfollows this recent trend and investigates the potential of leveraging Large\nLanguage Models (LLMs) in such scenarios to automatically detect datasets and\nsoftware within textual content from GitHub repositories. While existing\nmethods focused solely on named entities, this study aims to broaden the scope\nby incorporating resources such as repositories and online hubs where entities\nare also represented by URLs. The study explores different FSL prompt learning\napproaches to enhance the LLMs' ability to identify dataset and software\nmentions within repository texts. Through analyses of LLM effectiveness and\nlearning strategies, this paper offers insights into the potential of advanced\nlanguage models for automated entity detection."
                },
                "authors": [
                    {
                        "name": "Lu Gan"
                    },
                    {
                        "name": "Martin Blum"
                    },
                    {
                        "name": "Danilo Dessi"
                    },
                    {
                        "name": "Brigitte Mathiak"
                    },
                    {
                        "name": "Ralf Schenkel"
                    },
                    {
                        "name": "Stefan Dietze"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Dietze"
                },
                "author": "Stefan Dietze",
                "arxiv_comment": "accepted by KDD2024 workshop DL4KG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04455v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04455v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04437v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04437v1",
                "updated": "2025-01-08T11:37:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    11,
                    37,
                    35,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T11:37:35Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    11,
                    37,
                    35,
                    2,
                    8,
                    0
                ],
                "title": "Integrating LLMs with ITS: Recent Advances, Potentials, Challenges, and\n  Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating LLMs with ITS: Recent Advances, Potentials, Challenges, and\n  Future Directions"
                },
                "summary": "Intelligent Transportation Systems (ITS) are crucial for the development and\noperation of smart cities, addressing key challenges in efficiency,\nproductivity, and environmental sustainability. This paper comprehensively\nreviews the transformative potential of Large Language Models (LLMs) in\noptimizing ITS. Initially, we provide an extensive overview of ITS,\nhighlighting its components, operational principles, and overall effectiveness.\nWe then delve into the theoretical background of various LLM techniques, such\nas GPT, T5, CTRL, and BERT, elucidating their relevance to ITS applications.\nFollowing this, we examine the wide-ranging applications of LLMs within ITS,\nincluding traffic flow prediction, vehicle detection and classification,\nautonomous driving, traffic sign recognition, and pedestrian detection. Our\nanalysis reveals how these advanced models can significantly enhance traffic\nmanagement and safety. Finally, we explore the challenges and limitations LLMs\nface in ITS, such as data availability, computational constraints, and ethical\nconsiderations. We also present several future research directions and\npotential innovations to address these challenges. This paper aims to guide\nresearchers and practitioners through the complexities and opportunities of\nintegrating LLMs in ITS, offering a roadmap to create more efficient,\nsustainable, and responsive next-generation transportation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent Transportation Systems (ITS) are crucial for the development and\noperation of smart cities, addressing key challenges in efficiency,\nproductivity, and environmental sustainability. This paper comprehensively\nreviews the transformative potential of Large Language Models (LLMs) in\noptimizing ITS. Initially, we provide an extensive overview of ITS,\nhighlighting its components, operational principles, and overall effectiveness.\nWe then delve into the theoretical background of various LLM techniques, such\nas GPT, T5, CTRL, and BERT, elucidating their relevance to ITS applications.\nFollowing this, we examine the wide-ranging applications of LLMs within ITS,\nincluding traffic flow prediction, vehicle detection and classification,\nautonomous driving, traffic sign recognition, and pedestrian detection. Our\nanalysis reveals how these advanced models can significantly enhance traffic\nmanagement and safety. Finally, we explore the challenges and limitations LLMs\nface in ITS, such as data availability, computational constraints, and ethical\nconsiderations. We also present several future research directions and\npotential innovations to address these challenges. This paper aims to guide\nresearchers and practitioners through the complexities and opportunities of\nintegrating LLMs in ITS, offering a roadmap to create more efficient,\nsustainable, and responsive next-generation transportation systems."
                },
                "authors": [
                    {
                        "name": "Doaa Mahmud"
                    },
                    {
                        "name": "Hadeel Hajmohamed"
                    },
                    {
                        "name": "Shamma Almentheri"
                    },
                    {
                        "name": "Shamma Alqaydi"
                    },
                    {
                        "name": "Lameya Aldhaheri"
                    },
                    {
                        "name": "Ruhul Amin Khalil"
                    },
                    {
                        "name": "Nasir Saeed"
                    }
                ],
                "author_detail": {
                    "name": "Nasir Saeed"
                },
                "author": "Nasir Saeed",
                "arxiv_comment": "Accepted for publication in IEEE Transactions on Intelligent\n  Transportation Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04437v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04437v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04436v1",
                "updated": "2025-01-08T11:37:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    11,
                    37,
                    6,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T11:37:06Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    11,
                    37,
                    6,
                    2,
                    8,
                    0
                ],
                "title": "Federated Fine-Tuning of LLMs: Framework Comparison and Research\n  Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Fine-Tuning of LLMs: Framework Comparison and Research\n  Directions"
                },
                "summary": "Federated learning (FL) provides a privacy-preserving solution for\nfine-tuning pre-trained large language models (LLMs) using distributed private\ndatasets, enabling task-specific adaptation while preserving data privacy.\nHowever, fine-tuning the extensive parameters in LLMs is particularly\nchallenging in resource-constrained federated scenarios due to the significant\ncommunication and computational costs. To gain a deeper understanding of how\nthese challenges can be addressed, this article conducts a comparative analysis\nthree advanced federated LLM (FedLLM) frameworks that integrate knowledge\ndistillation (KD) and split learning (SL) to mitigate these issues: 1) FedLLMs,\nwhere clients upload model parameters or gradients to enable straightforward\nand effective fine-tuning; 2) KD-FedLLMs, which leverage KD for efficient\nknowledge sharing via logits; and 3) Split-FedLLMs, which split the LLMs into\ntwo parts, with one part executed on the client and the other one on the\nserver, to balance the computational load. Each framework is evaluated based on\nkey performance metrics, including model accuracy, communication overhead, and\nclient-side computational load, offering insights into their effectiveness for\nvarious federated fine-tuning scenarios. Through this analysis, we identify\nframework-specific optimization opportunities to enhance the efficiency of\nFedLLMs and discuss broader research directions, highlighting open\nopportunities to better adapt FedLLMs for real-world applications. A use case\nis presented to demonstrate the performance comparison of these three\nframeworks under varying configurations and settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) provides a privacy-preserving solution for\nfine-tuning pre-trained large language models (LLMs) using distributed private\ndatasets, enabling task-specific adaptation while preserving data privacy.\nHowever, fine-tuning the extensive parameters in LLMs is particularly\nchallenging in resource-constrained federated scenarios due to the significant\ncommunication and computational costs. To gain a deeper understanding of how\nthese challenges can be addressed, this article conducts a comparative analysis\nthree advanced federated LLM (FedLLM) frameworks that integrate knowledge\ndistillation (KD) and split learning (SL) to mitigate these issues: 1) FedLLMs,\nwhere clients upload model parameters or gradients to enable straightforward\nand effective fine-tuning; 2) KD-FedLLMs, which leverage KD for efficient\nknowledge sharing via logits; and 3) Split-FedLLMs, which split the LLMs into\ntwo parts, with one part executed on the client and the other one on the\nserver, to balance the computational load. Each framework is evaluated based on\nkey performance metrics, including model accuracy, communication overhead, and\nclient-side computational load, offering insights into their effectiveness for\nvarious federated fine-tuning scenarios. Through this analysis, we identify\nframework-specific optimization opportunities to enhance the efficiency of\nFedLLMs and discuss broader research directions, highlighting open\nopportunities to better adapt FedLLMs for real-world applications. A use case\nis presented to demonstrate the performance comparison of these three\nframeworks under varying configurations and settings."
                },
                "authors": [
                    {
                        "name": "Na Yan"
                    },
                    {
                        "name": "Yang Su"
                    },
                    {
                        "name": "Yansha Deng"
                    },
                    {
                        "name": "Robert Schober"
                    }
                ],
                "author_detail": {
                    "name": "Robert Schober"
                },
                "author": "Robert Schober",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11189v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11189v2",
                "updated": "2025-01-08T11:24:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    11,
                    24,
                    17,
                    2,
                    8,
                    0
                ],
                "published": "2024-12-15T13:48:39Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    13,
                    48,
                    39,
                    6,
                    350,
                    0
                ],
                "title": "Leveraging Large Language Models for Active Merchant Non-player\n  Characters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Active Merchant Non-player\n  Characters"
                },
                "summary": "We highlight two significant issues leading to the passivity of current\nmerchant non-player characters (NPCs): pricing and communication. While\nimmersive interactions have been a focus, negotiations between merchant NPCs\nand players on item prices have not received sufficient attention. First, we\ndefine passive pricing as the limited ability of merchants to modify predefined\nitem prices. Second, passive communication means that merchants can only\ninteract with players in a scripted manner. To tackle these issues and create\nan active merchant NPC, we propose a merchant framework based on large language\nmodels (LLMs), called MART, which consists of an appraiser module and a\nnegotiator module. We conducted two experiments to guide game developers in\nselecting appropriate implementations by comparing different training methods\nand LLM sizes. Our findings indicate that finetuning methods, such as\nsupervised finetuning (SFT) and knowledge distillation (KD), are effective in\nusing smaller LLMs to implement active merchant NPCs. Additionally, we found\nthree irregular cases arising from the responses of LLMs. We expect our\nfindings to guide developers in using LLMs for developing active merchant NPCs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We highlight two significant issues leading to the passivity of current\nmerchant non-player characters (NPCs): pricing and communication. While\nimmersive interactions have been a focus, negotiations between merchant NPCs\nand players on item prices have not received sufficient attention. First, we\ndefine passive pricing as the limited ability of merchants to modify predefined\nitem prices. Second, passive communication means that merchants can only\ninteract with players in a scripted manner. To tackle these issues and create\nan active merchant NPC, we propose a merchant framework based on large language\nmodels (LLMs), called MART, which consists of an appraiser module and a\nnegotiator module. We conducted two experiments to guide game developers in\nselecting appropriate implementations by comparing different training methods\nand LLM sizes. Our findings indicate that finetuning methods, such as\nsupervised finetuning (SFT) and knowledge distillation (KD), are effective in\nusing smaller LLMs to implement active merchant NPCs. Additionally, we found\nthree irregular cases arising from the responses of LLMs. We expect our\nfindings to guide developers in using LLMs for developing active merchant NPCs."
                },
                "authors": [
                    {
                        "name": "Byungjun Kim"
                    },
                    {
                        "name": "Minju Kim"
                    },
                    {
                        "name": "Dayeon Seo"
                    },
                    {
                        "name": "Bugeun Kim"
                    }
                ],
                "author_detail": {
                    "name": "Bugeun Kim"
                },
                "author": "Bugeun Kim",
                "arxiv_comment": "Under review / Modified the links to code and dataset",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11189v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11189v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10587v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10587v3",
                "updated": "2025-01-08T11:21:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    11,
                    21,
                    12,
                    2,
                    8,
                    0
                ],
                "published": "2024-05-17T07:22:02Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    7,
                    22,
                    2,
                    4,
                    138,
                    0
                ],
                "title": "RDRec: Rationale Distillation for LLM-based Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RDRec: Rationale Distillation for LLM-based Recommendation"
                },
                "summary": "Large language model (LLM)-based recommender models that bridge users and\nitems through textual prompts for effective semantic reasoning have gained\nconsiderable attention. However, few methods consider the underlying rationales\nbehind interactions, such as user preferences and item attributes, limiting the\nreasoning capability of LLMs for recommendations. This paper proposes a\nrationale distillation recommender (RDRec), a compact model designed to learn\nrationales generated by a larger language model (LM). By leveraging rationales\nfrom reviews related to users and items, RDRec remarkably specifies their\nprofiles for recommendations. Experiments show that RDRec achieves\nstate-of-the-art (SOTA) performance in both top-N and sequential\nrecommendations. Our source code is released at\nhttps://github.com/WangXFng/RDRec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM)-based recommender models that bridge users and\nitems through textual prompts for effective semantic reasoning have gained\nconsiderable attention. However, few methods consider the underlying rationales\nbehind interactions, such as user preferences and item attributes, limiting the\nreasoning capability of LLMs for recommendations. This paper proposes a\nrationale distillation recommender (RDRec), a compact model designed to learn\nrationales generated by a larger language model (LM). By leveraging rationales\nfrom reviews related to users and items, RDRec remarkably specifies their\nprofiles for recommendations. Experiments show that RDRec achieves\nstate-of-the-art (SOTA) performance in both top-N and sequential\nrecommendations. Our source code is released at\nhttps://github.com/WangXFng/RDRec."
                },
                "authors": [
                    {
                        "name": "Xinfeng Wang"
                    },
                    {
                        "name": "Jin Cui"
                    },
                    {
                        "name": "Yoshimi Suzuki"
                    },
                    {
                        "name": "Fumiyo Fukumoto"
                    }
                ],
                "author_detail": {
                    "name": "Fumiyo Fukumoto"
                },
                "author": "Fumiyo Fukumoto",
                "arxiv_comment": "10 pages. Accepted to ACL 2024 Main as a short paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10587v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10587v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04425v1",
                "updated": "2025-01-08T11:18:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    11,
                    18,
                    36,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T11:18:36Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    11,
                    18,
                    36,
                    2,
                    8,
                    0
                ],
                "title": "End-to-End Bangla AI for Solving Math Olympiad Problem Benchmark:\n  Leveraging Large Language Model Using Integrated Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End Bangla AI for Solving Math Olympiad Problem Benchmark:\n  Leveraging Large Language Model Using Integrated Approach"
                },
                "summary": "This work introduces systematic approach for enhancing large language models\n(LLMs) to address Bangla AI mathematical challenges. Through the assessment of\ndiverse LLM configurations, fine-tuning with specific datasets, and the\nimplementation of Retrieval-Augmented Generation (RAG), we enhanced the model's\nreasoning precision in a multilingual setting. Crucial discoveries indicate\nthat customized prompting, dataset augmentation, and iterative reasoning\nimprove the model's efficiency regarding Olympiad-level mathematical\nchallenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work introduces systematic approach for enhancing large language models\n(LLMs) to address Bangla AI mathematical challenges. Through the assessment of\ndiverse LLM configurations, fine-tuning with specific datasets, and the\nimplementation of Retrieval-Augmented Generation (RAG), we enhanced the model's\nreasoning precision in a multilingual setting. Crucial discoveries indicate\nthat customized prompting, dataset augmentation, and iterative reasoning\nimprove the model's efficiency regarding Olympiad-level mathematical\nchallenges."
                },
                "authors": [
                    {
                        "name": "H. M. Shadman Tabib"
                    },
                    {
                        "name": "Jaber Ahmed Deedar"
                    }
                ],
                "author_detail": {
                    "name": "Jaber Ahmed Deedar"
                },
                "author": "Jaber Ahmed Deedar",
                "arxiv_doi": "10.5121/ijnlc.2024.13604",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5121/ijnlc.2024.13604",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.04425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IJNLC,vol:13, Issue:5/6, page 49-59,2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14821v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14821v2",
                "updated": "2025-01-08T10:39:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    39,
                    19,
                    2,
                    8,
                    0
                ],
                "published": "2023-09-26T10:39:59Z",
                "published_parsed": [
                    2023,
                    9,
                    26,
                    10,
                    39,
                    59,
                    1,
                    269,
                    0
                ],
                "title": "Shattering the Ephemeral Storage Cost Barrier for Data-Intensive\n  Serverless Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shattering the Ephemeral Storage Cost Barrier for Data-Intensive\n  Serverless Workflows"
                },
                "summary": "Serverless computing is a popular cloud deployment paradigm where developers\nimplement applications as workflows of functions that invoke each other. Cloud\nproviders automatically scale function instances on demand and forward workflow\nrequests to appropriate instances. However, current serverless clouds lack\nefficient cross-function data transfer, limiting the execution of\ndata-intensive applications. Functions often rely on third-party services like\nAWS S3, AWS ElastiCache, or multi-tier solutions for intermediate data\ntransfers, which introduces inefficiencies.\n  We demonstrate that such through-storage transfers make data-intensive\ndeployments economically impractical, with storage costs comprising more than\n24-99% of the total serverless bill. To address this, we introduce Zipline, a\nfast, API-preserving data communication method for serverless platforms.\nZipline enables direct function-to-function transfers, where the sender\nfunction buffers payloads in memory and sends a reference to the receiver. The\nreceiver retrieves the data directly from the sender's memory, guided by the\nload balancer and autoscaler. Zipline integrates seamlessly with existing\nautoscaling, maintains invocation semantics, and eliminates the costs and\noverheads of intermediate services. We prototype Zipline in vHive/Knative on\nAWS EC2 nodes, demonstrating significant improvements. Zipline reduces costs\nand enhances latency and bandwidth compared to AWS S3 (the lowest-cost\nsolution) and ElastiCache (the highest-performance solution). On real-world\napplications, Zipline lowers costs by 2-5x and reduces execution times by\n1.3-3.4x versus S3. Compared to ElastiCache, Zipline achieves 17-772x cost\nreductions while improving performance by 2-5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing is a popular cloud deployment paradigm where developers\nimplement applications as workflows of functions that invoke each other. Cloud\nproviders automatically scale function instances on demand and forward workflow\nrequests to appropriate instances. However, current serverless clouds lack\nefficient cross-function data transfer, limiting the execution of\ndata-intensive applications. Functions often rely on third-party services like\nAWS S3, AWS ElastiCache, or multi-tier solutions for intermediate data\ntransfers, which introduces inefficiencies.\n  We demonstrate that such through-storage transfers make data-intensive\ndeployments economically impractical, with storage costs comprising more than\n24-99% of the total serverless bill. To address this, we introduce Zipline, a\nfast, API-preserving data communication method for serverless platforms.\nZipline enables direct function-to-function transfers, where the sender\nfunction buffers payloads in memory and sends a reference to the receiver. The\nreceiver retrieves the data directly from the sender's memory, guided by the\nload balancer and autoscaler. Zipline integrates seamlessly with existing\nautoscaling, maintains invocation semantics, and eliminates the costs and\noverheads of intermediate services. We prototype Zipline in vHive/Knative on\nAWS EC2 nodes, demonstrating significant improvements. Zipline reduces costs\nand enhances latency and bandwidth compared to AWS S3 (the lowest-cost\nsolution) and ElastiCache (the highest-performance solution). On real-world\napplications, Zipline lowers costs by 2-5x and reduces execution times by\n1.3-3.4x versus S3. Compared to ElastiCache, Zipline achieves 17-772x cost\nreductions while improving performance by 2-5%."
                },
                "authors": [
                    {
                        "name": "Dmitrii Ustiugov"
                    },
                    {
                        "name": "Shyam Jesalpura"
                    },
                    {
                        "name": "Mert Bora Alper"
                    },
                    {
                        "name": "Michal Baczun"
                    },
                    {
                        "name": "Rustem Feyzkhanov"
                    },
                    {
                        "name": "Edouard Bugnion"
                    },
                    {
                        "name": "Boris Grot"
                    },
                    {
                        "name": "Marios Kogias"
                    }
                ],
                "author_detail": {
                    "name": "Marios Kogias"
                },
                "author": "Marios Kogias",
                "arxiv_comment": "added cost reduction details",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.14821v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14821v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03535v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03535v2",
                "updated": "2025-01-08T10:34:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    34,
                    54,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-07T05:15:46Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    5,
                    15,
                    46,
                    1,
                    7,
                    0
                ],
                "title": "SenseRAG: Constructing Environmental Knowledge Bases with Proactive\n  Querying for LLM-Based Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SenseRAG: Constructing Environmental Knowledge Bases with Proactive\n  Querying for LLM-Based Autonomous Driving"
                },
                "summary": "This study addresses the critical need for enhanced situational awareness in\nautonomous driving (AD) by leveraging the contextual reasoning capabilities of\nlarge language models (LLMs). Unlike traditional perception systems that rely\non rigid, label-based annotations, it integrates real-time, multimodal sensor\ndata into a unified, LLMs-readable knowledge base, enabling LLMs to dynamically\nunderstand and respond to complex driving environments. To overcome the\ninherent latency and modality limitations of LLMs, a proactive\nRetrieval-Augmented Generation (RAG) is designed for AD, combined with a\nchain-of-thought prompting mechanism, ensuring rapid and context-rich\nunderstanding. Experimental results using real-world Vehicle-to-everything\n(V2X) datasets demonstrate significant improvements in perception and\nprediction performance, highlighting the potential of this framework to enhance\nsafety, adaptability, and decision-making in next-generation AD systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study addresses the critical need for enhanced situational awareness in\nautonomous driving (AD) by leveraging the contextual reasoning capabilities of\nlarge language models (LLMs). Unlike traditional perception systems that rely\non rigid, label-based annotations, it integrates real-time, multimodal sensor\ndata into a unified, LLMs-readable knowledge base, enabling LLMs to dynamically\nunderstand and respond to complex driving environments. To overcome the\ninherent latency and modality limitations of LLMs, a proactive\nRetrieval-Augmented Generation (RAG) is designed for AD, combined with a\nchain-of-thought prompting mechanism, ensuring rapid and context-rich\nunderstanding. Experimental results using real-world Vehicle-to-everything\n(V2X) datasets demonstrate significant improvements in perception and\nprediction performance, highlighting the potential of this framework to enhance\nsafety, adaptability, and decision-making in next-generation AD systems."
                },
                "authors": [
                    {
                        "name": "Xuewen Luo"
                    },
                    {
                        "name": "Fan Ding"
                    },
                    {
                        "name": "Fengze Yang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Junnyong Loo"
                    },
                    {
                        "name": "Hwa Hui Tew"
                    },
                    {
                        "name": "Chenxi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chenxi Liu"
                },
                "author": "Chenxi Liu",
                "arxiv_comment": "This paper has been accepted for presentation at WACV Workshop LLMAD\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03535v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03535v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04393v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04393v1",
                "updated": "2025-01-08T10:10:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    10,
                    29,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T10:10:29Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    10,
                    29,
                    2,
                    8,
                    0
                ],
                "title": "SEO: Stochastic Experience Optimization for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEO: Stochastic Experience Optimization for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) can benefit from useful experiences to improve\ntheir performance on specific tasks. However, finding helpful experiences for\ndifferent LLMs is not obvious, since it is unclear what experiences suit\nspecific LLMs. Previous studies intended to automatically find useful\nexperiences using LLMs, while it is difficult to ensure the effectiveness of\nthe obtained experience. In this paper, we propose Stochastic Experience\nOptimization (SEO), an iterative approach that finds optimized model-specific\nexperience without modifying model parameters through experience update in\nnatural language. In SEO, we propose a stochastic validation method to ensure\nthe update direction of experience, avoiding unavailing updates. Experimental\nresults on three tasks for three LLMs demonstrate that experiences optimized by\nSEO can achieve consistently improved performance. Further analysis indicates\nthat SEO-optimized experience can generalize to out-of-distribution data,\nboosting the performance of LLMs on similar tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can benefit from useful experiences to improve\ntheir performance on specific tasks. However, finding helpful experiences for\ndifferent LLMs is not obvious, since it is unclear what experiences suit\nspecific LLMs. Previous studies intended to automatically find useful\nexperiences using LLMs, while it is difficult to ensure the effectiveness of\nthe obtained experience. In this paper, we propose Stochastic Experience\nOptimization (SEO), an iterative approach that finds optimized model-specific\nexperience without modifying model parameters through experience update in\nnatural language. In SEO, we propose a stochastic validation method to ensure\nthe update direction of experience, avoiding unavailing updates. Experimental\nresults on three tasks for three LLMs demonstrate that experiences optimized by\nSEO can achieve consistently improved performance. Further analysis indicates\nthat SEO-optimized experience can generalize to out-of-distribution data,\nboosting the performance of LLMs on similar tasks."
                },
                "authors": [
                    {
                        "name": "Jitao Xu"
                    },
                    {
                        "name": "Hongyun Zhou"
                    },
                    {
                        "name": "Lei Shen"
                    },
                    {
                        "name": "Conghui Zhu"
                    },
                    {
                        "name": "Jin Huang"
                    },
                    {
                        "name": "Yitao Duan"
                    }
                ],
                "author_detail": {
                    "name": "Yitao Duan"
                },
                "author": "Yitao Duan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04393v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15209v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15209v3",
                "updated": "2025-01-08T09:29:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    9,
                    29,
                    10,
                    2,
                    8,
                    0
                ],
                "published": "2024-03-22T13:50:27Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    13,
                    50,
                    27,
                    4,
                    82,
                    0
                ],
                "title": "MSCoTDet: Language-driven Multi-modal Fusion for Improved Multispectral\n  Pedestrian Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSCoTDet: Language-driven Multi-modal Fusion for Improved Multispectral\n  Pedestrian Detection"
                },
                "summary": "Multispectral pedestrian detection is attractive for around-the-clock\napplications due to the complementary information between RGB and thermal\nmodalities. However, current models often fail to detect pedestrians in certain\ncases (e.g., thermal-obscured pedestrians), particularly due to the modality\nbias learned from statistically biased datasets. In this paper, we investigate\nhow to mitigate modality bias in multispectral pedestrian detection using Large\nLanguage Models (LLMs). Accordingly, we design a Multispectral Chain-of-Thought\n(MSCoT) prompting strategy, which prompts the LLM to perform multispectral\npedestrian detection. Moreover, we propose a novel Multispectral\nChain-of-Thought Detection (MSCoTDet) framework that integrates MSCoT prompting\ninto multispectral pedestrian detection. To this end, we design a\nLanguage-driven Multi-modal Fusion (LMF) strategy that enables fusing the\noutputs of MSCoT prompting with the detection results of vision-based\nmultispectral pedestrian detection models. Extensive experiments validate that\nMSCoTDet effectively mitigates modality biases and improves multispectral\npedestrian detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multispectral pedestrian detection is attractive for around-the-clock\napplications due to the complementary information between RGB and thermal\nmodalities. However, current models often fail to detect pedestrians in certain\ncases (e.g., thermal-obscured pedestrians), particularly due to the modality\nbias learned from statistically biased datasets. In this paper, we investigate\nhow to mitigate modality bias in multispectral pedestrian detection using Large\nLanguage Models (LLMs). Accordingly, we design a Multispectral Chain-of-Thought\n(MSCoT) prompting strategy, which prompts the LLM to perform multispectral\npedestrian detection. Moreover, we propose a novel Multispectral\nChain-of-Thought Detection (MSCoTDet) framework that integrates MSCoT prompting\ninto multispectral pedestrian detection. To this end, we design a\nLanguage-driven Multi-modal Fusion (LMF) strategy that enables fusing the\noutputs of MSCoT prompting with the detection results of vision-based\nmultispectral pedestrian detection models. Extensive experiments validate that\nMSCoTDet effectively mitigates modality biases and improves multispectral\npedestrian detection."
                },
                "authors": [
                    {
                        "name": "Taeheon Kim"
                    },
                    {
                        "name": "Sangyun Chung"
                    },
                    {
                        "name": "Damin Yeom"
                    },
                    {
                        "name": "Youngjoon Yu"
                    },
                    {
                        "name": "Hak Gu Kim"
                    },
                    {
                        "name": "Yong Man Ro"
                    }
                ],
                "author_detail": {
                    "name": "Yong Man Ro"
                },
                "author": "Yong Man Ro",
                "arxiv_doi": "10.1109/TCSVT.2024.3524645",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TCSVT.2024.3524645",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.15209v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15209v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "IEEE Transactions on Circuits and Systems for Video Technology\n  (TCSVT)",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15267v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15267v2",
                "updated": "2025-01-08T09:18:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    9,
                    18,
                    5,
                    2,
                    8,
                    0
                ],
                "published": "2024-12-17T05:04:57Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    4,
                    57,
                    1,
                    352,
                    0
                ],
                "title": "Toxicity Detection towards Adaptability to Changing Perturbations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toxicity Detection towards Adaptability to Changing Perturbations"
                },
                "summary": "Toxicity detection is crucial for maintaining the peace of the society. While\nexisting methods perform well on normal toxic contents or those generated by\nspecific perturbation methods, they are vulnerable to evolving perturbation\npatterns. However, in real-world scenarios, malicious users tend to create new\nperturbation patterns for fooling the detectors. For example, some users may\ncircumvent the detector of large language models (LLMs) by adding `I am a\nscientist' at the beginning of the prompt. In this paper, we introduce a novel\nproblem, i.e., continual learning jailbreak perturbation patterns, into the\ntoxicity detection field. To tackle this problem, we first construct a new\ndataset generated by 9 types of perturbation patterns, 7 of them are summarized\nfrom prior work and 2 of them are developed by us. We then systematically\nvalidate the vulnerability of current methods on this new perturbation\npattern-aware dataset via both the zero-shot and fine tuned cross-pattern\ndetection. Upon this, we present the domain incremental learning paradigm and\nthe corresponding benchmark to ensure the detector's robustness to dynamically\nemerging types of perturbed toxic text. Our code and dataset are provided in\nthe appendix and will be publicly available at GitHub, by which we wish to\noffer new research opportunities for the security-relevant communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toxicity detection is crucial for maintaining the peace of the society. While\nexisting methods perform well on normal toxic contents or those generated by\nspecific perturbation methods, they are vulnerable to evolving perturbation\npatterns. However, in real-world scenarios, malicious users tend to create new\nperturbation patterns for fooling the detectors. For example, some users may\ncircumvent the detector of large language models (LLMs) by adding `I am a\nscientist' at the beginning of the prompt. In this paper, we introduce a novel\nproblem, i.e., continual learning jailbreak perturbation patterns, into the\ntoxicity detection field. To tackle this problem, we first construct a new\ndataset generated by 9 types of perturbation patterns, 7 of them are summarized\nfrom prior work and 2 of them are developed by us. We then systematically\nvalidate the vulnerability of current methods on this new perturbation\npattern-aware dataset via both the zero-shot and fine tuned cross-pattern\ndetection. Upon this, we present the domain incremental learning paradigm and\nthe corresponding benchmark to ensure the detector's robustness to dynamically\nemerging types of perturbed toxic text. Our code and dataset are provided in\nthe appendix and will be publicly available at GitHub, by which we wish to\noffer new research opportunities for the security-relevant communities."
                },
                "authors": [
                    {
                        "name": "Hankun Kang"
                    },
                    {
                        "name": "Jianhao Chen"
                    },
                    {
                        "name": "Yongqi Li"
                    },
                    {
                        "name": "Xin Miao"
                    },
                    {
                        "name": "Mayi Xu"
                    },
                    {
                        "name": "Ming Zhong"
                    },
                    {
                        "name": "Yuanyuan Zhu"
                    },
                    {
                        "name": "Tieyun Qian"
                    }
                ],
                "author_detail": {
                    "name": "Tieyun Qian"
                },
                "author": "Tieyun Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15267v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15267v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04366v1",
                "updated": "2025-01-08T09:08:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    9,
                    8,
                    24,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T09:08:24Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    9,
                    8,
                    24,
                    2,
                    8,
                    0
                ],
                "title": "DispFormer: Pretrained Transformer for Flexible Dispersion Curve\n  Inversion from Global Synthesis to Regional Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DispFormer: Pretrained Transformer for Flexible Dispersion Curve\n  Inversion from Global Synthesis to Regional Applications"
                },
                "summary": "Surface wave dispersion curve inversion is essential for estimating\nsubsurface Shear-wave velocity ($v_s$), yet traditional methods often struggle\nto balance computational efficiency with inversion accuracy. While deep\nlearning approaches show promise, previous studies typically require large\namounts of labeled data and struggle with real-world datasets that have varying\nperiod ranges, missing data, and low signal-to-noise ratios. This study\nproposes DispFormer, a transformer-based neural network for inverting the $v_s$\nprofile from Rayleigh-wave phase and group dispersion curves. DispFormer\nprocesses dispersion data at each period independently, thereby allowing it to\nhandle data of varying lengths without requiring network modifications or\nalignment between training and testing data. The performance is demonstrated by\npre-training it on a global synthetic dataset and testing it on two regional\nsynthetic datasets using zero-shot and few-shot strategies. Results indicate\nthat zero-shot DispFormer, even without any labeled data, produces inversion\nprofiles that match well with the ground truth, providing a deployable initial\nmodel generator to assist traditional methods. When labeled data is available,\nfew-shot DispFormer outperforms traditional methods with only a small number of\nlabels. Furthermore, real-world tests indicate that DispFormer effectively\nhandles varying length data, and yields lower data residuals than reference\nmodels. These findings demonstrate that DispFormer provides a robust foundation\nmodel for dispersion curve inversion and is a promising approach for broader\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surface wave dispersion curve inversion is essential for estimating\nsubsurface Shear-wave velocity ($v_s$), yet traditional methods often struggle\nto balance computational efficiency with inversion accuracy. While deep\nlearning approaches show promise, previous studies typically require large\namounts of labeled data and struggle with real-world datasets that have varying\nperiod ranges, missing data, and low signal-to-noise ratios. This study\nproposes DispFormer, a transformer-based neural network for inverting the $v_s$\nprofile from Rayleigh-wave phase and group dispersion curves. DispFormer\nprocesses dispersion data at each period independently, thereby allowing it to\nhandle data of varying lengths without requiring network modifications or\nalignment between training and testing data. The performance is demonstrated by\npre-training it on a global synthetic dataset and testing it on two regional\nsynthetic datasets using zero-shot and few-shot strategies. Results indicate\nthat zero-shot DispFormer, even without any labeled data, produces inversion\nprofiles that match well with the ground truth, providing a deployable initial\nmodel generator to assist traditional methods. When labeled data is available,\nfew-shot DispFormer outperforms traditional methods with only a small number of\nlabels. Furthermore, real-world tests indicate that DispFormer effectively\nhandles varying length data, and yields lower data residuals than reference\nmodels. These findings demonstrate that DispFormer provides a robust foundation\nmodel for dispersion curve inversion and is a promising approach for broader\napplications."
                },
                "authors": [
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Bao Deng"
                    },
                    {
                        "name": "Rui Su"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Wanli Ouyang"
                    }
                ],
                "author_detail": {
                    "name": "Wanli Ouyang"
                },
                "author": "Wanli Ouyang",
                "arxiv_comment": "11 pages, 11 figures, related codes and data are available at\n  https://github.com/liufeng2317/DispFormer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04341v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04341v1",
                "updated": "2025-01-08T08:26:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    8,
                    26,
                    56,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T08:26:56Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    8,
                    26,
                    56,
                    2,
                    8,
                    0
                ],
                "title": "Understanding Before Reasoning: Enhancing Chain-of-Thought with\n  Iterative Summarization Pre-Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Before Reasoning: Enhancing Chain-of-Thought with\n  Iterative Summarization Pre-Prompting"
                },
                "summary": "Chain-of-Thought (CoT) Prompting is a dominant paradigm in Large Language\nModels (LLMs) to enhance complex reasoning. It guides LLMs to present\nmulti-step reasoning, rather than generating the final answer directly.\nHowever, CoT encounters difficulties when key information required for\nreasoning is implicit or missing. This occurs because CoT emphasizes the\nsequence of reasoning steps while overlooking the early extraction of essential\ninformation. We propose a pre-prompting method called Iterative Summarization\nPre-Prompting (ISP^2) to refine LLM reasoning when key information is not\nexplicitly provided. First, entities and their corresponding descriptions are\nextracted to form potential key information pairs. Next, we use a reliability\nrating to assess these pairs, then merge the two lowest-ranked pairs into a new\nentity description. This process is repeated until a unique key information\npair is obtained. Finally, that pair, along with the original question, is fed\ninto LLMs to produce the answer. Extensive experiments demonstrate a 7.1%\nimprovement compared to existing methods. Unlike traditional prompting, ISP^2\nadopts an inductive approach with pre-prompting, offering flexible integration\ninto diverse reasoning frameworks. The code is available at\nhttps://github.com/zdhgreat/ISP-2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) Prompting is a dominant paradigm in Large Language\nModels (LLMs) to enhance complex reasoning. It guides LLMs to present\nmulti-step reasoning, rather than generating the final answer directly.\nHowever, CoT encounters difficulties when key information required for\nreasoning is implicit or missing. This occurs because CoT emphasizes the\nsequence of reasoning steps while overlooking the early extraction of essential\ninformation. We propose a pre-prompting method called Iterative Summarization\nPre-Prompting (ISP^2) to refine LLM reasoning when key information is not\nexplicitly provided. First, entities and their corresponding descriptions are\nextracted to form potential key information pairs. Next, we use a reliability\nrating to assess these pairs, then merge the two lowest-ranked pairs into a new\nentity description. This process is repeated until a unique key information\npair is obtained. Finally, that pair, along with the original question, is fed\ninto LLMs to produce the answer. Extensive experiments demonstrate a 7.1%\nimprovement compared to existing methods. Unlike traditional prompting, ISP^2\nadopts an inductive approach with pre-prompting, offering flexible integration\ninto diverse reasoning frameworks. The code is available at\nhttps://github.com/zdhgreat/ISP-2."
                },
                "authors": [
                    {
                        "name": "Dong-Hai Zhu"
                    },
                    {
                        "name": "Yu-Jie Xiong"
                    },
                    {
                        "name": "Jia-Chen Zhang"
                    },
                    {
                        "name": "Xi-Jiong Xie"
                    },
                    {
                        "name": "Chun-Ming Xia"
                    }
                ],
                "author_detail": {
                    "name": "Chun-Ming Xia"
                },
                "author": "Chun-Ming Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04341v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04341v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04336v1",
                "updated": "2025-01-08T08:15:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    8,
                    15,
                    29,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T08:15:29Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    8,
                    15,
                    29,
                    2,
                    8,
                    0
                ],
                "title": "Building a Mind Palace: Structuring Environment-Grounded Semantic Graphs\n  for Effective Long Video Analysis with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building a Mind Palace: Structuring Environment-Grounded Semantic Graphs\n  for Effective Long Video Analysis with LLMs"
                },
                "summary": "Long-form video understanding with Large Vision Language Models is challenged\nby the need to analyze temporally dispersed yet spatially concentrated key\nmoments within limited context windows. In this work, we introduce\nVideoMindPalace, a new framework inspired by the \"Mind Palace\", which organizes\ncritical video moments into a topologically structured semantic graph.\nVideoMindPalace organizes key information through (i) hand-object tracking and\ninteraction, (ii) clustered activity zones representing specific areas of\nrecurring activities, and (iii) environment layout mapping, allowing natural\nlanguage parsing by LLMs to provide grounded insights on spatio-temporal and 3D\ncontext. In addition, we propose the Video MindPalace Benchmark (VMB), to\nassess human-like reasoning, including spatial localization, temporal\nreasoning, and layout-aware sequential understanding. Evaluated on VMB and\nestablished video QA datasets, including EgoSchema, NExT-QA, IntentQA, and the\nActive Memories Benchmark, VideoMindPalace demonstrates notable gains in\nspatio-temporal coherence and human-aligned reasoning, advancing long-form\nvideo analysis capabilities in VLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-form video understanding with Large Vision Language Models is challenged\nby the need to analyze temporally dispersed yet spatially concentrated key\nmoments within limited context windows. In this work, we introduce\nVideoMindPalace, a new framework inspired by the \"Mind Palace\", which organizes\ncritical video moments into a topologically structured semantic graph.\nVideoMindPalace organizes key information through (i) hand-object tracking and\ninteraction, (ii) clustered activity zones representing specific areas of\nrecurring activities, and (iii) environment layout mapping, allowing natural\nlanguage parsing by LLMs to provide grounded insights on spatio-temporal and 3D\ncontext. In addition, we propose the Video MindPalace Benchmark (VMB), to\nassess human-like reasoning, including spatial localization, temporal\nreasoning, and layout-aware sequential understanding. Evaluated on VMB and\nestablished video QA datasets, including EgoSchema, NExT-QA, IntentQA, and the\nActive Memories Benchmark, VideoMindPalace demonstrates notable gains in\nspatio-temporal coherence and human-aligned reasoning, advancing long-form\nvideo analysis capabilities in VLMs."
                },
                "authors": [
                    {
                        "name": "Zeyi Huang"
                    },
                    {
                        "name": "Yuyang Ji"
                    },
                    {
                        "name": "Xiaofang Wang"
                    },
                    {
                        "name": "Nikhil Mehta"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Donghyun Lee"
                    },
                    {
                        "name": "Sigmund Vanvalkenburgh"
                    },
                    {
                        "name": "Shengxin Zha"
                    },
                    {
                        "name": "Bolin Lai"
                    },
                    {
                        "name": "Licheng Yu"
                    },
                    {
                        "name": "Ning Zhang"
                    },
                    {
                        "name": "Yong Jae Lee"
                    },
                    {
                        "name": "Miao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Miao Liu"
                },
                "author": "Miao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04327v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04327v1",
                "updated": "2025-01-08T07:59:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    59,
                    40,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T07:59:40Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    59,
                    40,
                    2,
                    8,
                    0
                ],
                "title": "Machine Learning Enhanced Quantum State Tomography on FPGA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Learning Enhanced Quantum State Tomography on FPGA"
                },
                "summary": "Machine learning techniques have opened new avenues for real-time quantum\nstate tomography (QST). In this work, we demonstrate the deployment of machine\nlearning-based QST onto edge devices, specifically utilizing field programmable\ngate arrays (FPGAs). This implementation is realized using the {\\it Vitis AI\nIntegrated Development Environment} provided by AMD\\textsuperscript\n\\textregistered~Inc. Compared to the Graphics Processing Unit (GPU)-based\nmachine learning QST, our FPGA-based one reduces the average inference time by\nan order of magnitude, from 38 ms to 2.94 ms, but only sacrifices the average\nfidelity about $1\\% $ reduction (from 0.99 to 0.98). The FPGA-based QST offers\na highly efficient and precise tool for diagnosing quantum states, marking a\nsignificant advancement in the practical applications for quantum information\nprocessing and quantum sensing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning techniques have opened new avenues for real-time quantum\nstate tomography (QST). In this work, we demonstrate the deployment of machine\nlearning-based QST onto edge devices, specifically utilizing field programmable\ngate arrays (FPGAs). This implementation is realized using the {\\it Vitis AI\nIntegrated Development Environment} provided by AMD\\textsuperscript\n\\textregistered~Inc. Compared to the Graphics Processing Unit (GPU)-based\nmachine learning QST, our FPGA-based one reduces the average inference time by\nan order of magnitude, from 38 ms to 2.94 ms, but only sacrifices the average\nfidelity about $1\\% $ reduction (from 0.99 to 0.98). The FPGA-based QST offers\na highly efficient and precise tool for diagnosing quantum states, marking a\nsignificant advancement in the practical applications for quantum information\nprocessing and quantum sensing."
                },
                "authors": [
                    {
                        "name": "Hsun-Chung Wu"
                    },
                    {
                        "name": "Hsien-Yi Hsieh"
                    },
                    {
                        "name": "Zhi-Kai Xu"
                    },
                    {
                        "name": "Hua Li Chen"
                    },
                    {
                        "name": "Zi-Hao Shi"
                    },
                    {
                        "name": "Po-Han Wang"
                    },
                    {
                        "name": "Popo Yang"
                    },
                    {
                        "name": "Ole Steuernagel"
                    },
                    {
                        "name": "Chien-Ming Wu"
                    },
                    {
                        "name": "Ray-Kuang Lee"
                    }
                ],
                "author_detail": {
                    "name": "Ray-Kuang Lee"
                },
                "author": "Ray-Kuang Lee",
                "arxiv_comment": "6 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04327v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04327v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14978v2",
                "updated": "2025-01-08T07:53:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    53,
                    15,
                    2,
                    8,
                    0
                ],
                "published": "2024-09-23T12:57:24Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    12,
                    57,
                    24,
                    0,
                    267,
                    0
                ],
                "title": "TS-HTFA: Advancing Time Series Forecasting via Hierarchical Text-Free\n  Alignment with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TS-HTFA: Advancing Time Series Forecasting via Hierarchical Text-Free\n  Alignment with Large Language Models"
                },
                "summary": "Given the significant potential of large language models (LLMs) in sequence\nmodeling, emerging studies have begun applying them to time-series forecasting.\nDespite notable progress, existing methods still face two critical challenges:\n1) their reliance on large amounts of paired text data, limiting the model\napplicability, and 2) a substantial modality gap between text and time series,\nleading to insufficient alignment and suboptimal performance. In this paper, we\nintroduce \\textbf{H}ierarchical \\textbf{T}ext-\\textbf{F}ree \\textbf{A}lignment\n(\\textbf{TS-HTFA}), a novel method that leverages hierarchical alignment to\nfully exploit the representation capacity of LLMs while eliminating the\ndependence on text data. Specifically, we replace paired text data with\nadaptive virtual text based on QR decomposition word embeddings and learnable\nprompt. Furthermore, we establish comprehensive cross-modal alignment at three\nlevels: input, feature, and output. Extensive experiments on multiple\ntime-series benchmarks demonstrate that HTFA achieves state-of-the-art\nperformance, significantly improving prediction accuracy and generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given the significant potential of large language models (LLMs) in sequence\nmodeling, emerging studies have begun applying them to time-series forecasting.\nDespite notable progress, existing methods still face two critical challenges:\n1) their reliance on large amounts of paired text data, limiting the model\napplicability, and 2) a substantial modality gap between text and time series,\nleading to insufficient alignment and suboptimal performance. In this paper, we\nintroduce \\textbf{H}ierarchical \\textbf{T}ext-\\textbf{F}ree \\textbf{A}lignment\n(\\textbf{TS-HTFA}), a novel method that leverages hierarchical alignment to\nfully exploit the representation capacity of LLMs while eliminating the\ndependence on text data. Specifically, we replace paired text data with\nadaptive virtual text based on QR decomposition word embeddings and learnable\nprompt. Furthermore, we establish comprehensive cross-modal alignment at three\nlevels: input, feature, and output. Extensive experiments on multiple\ntime-series benchmarks demonstrate that HTFA achieves state-of-the-art\nperformance, significantly improving prediction accuracy and generalization."
                },
                "authors": [
                    {
                        "name": "Pengfei Wang"
                    },
                    {
                        "name": "Huanran Zheng"
                    },
                    {
                        "name": "Qi'ao Xu"
                    },
                    {
                        "name": "Silong Dai"
                    },
                    {
                        "name": "Yiqiao Wang"
                    },
                    {
                        "name": "Wenjing Yue"
                    },
                    {
                        "name": "Wei Zhu"
                    },
                    {
                        "name": "Tianwen Qian"
                    },
                    {
                        "name": "Xiaoling Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoling Wang"
                },
                "author": "Xiaoling Wang",
                "arxiv_comment": "19 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04323v1",
                "updated": "2025-01-08T07:47:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    47,
                    43,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T07:47:43Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    47,
                    43,
                    2,
                    8,
                    0
                ],
                "title": "Navigating the Designs of Privacy-Preserving Fine-tuning for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating the Designs of Privacy-Preserving Fine-tuning for Large\n  Language Models"
                },
                "summary": "Instruction tuning has proven effective in enhancing Large Language Models'\n(LLMs) performance on downstream tasks. However, real-world fine-tuning faces\ninherent conflicts between model providers' intellectual property protection,\nclients' data privacy requirements, and tuning costs. While recent approaches\nlike split learning and offsite tuning demonstrate promising architectures for\nprivacy-preserving fine-tuning, there is a gap in systematically addressing the\nmultidimensional trade-offs required for diverse real-world deployments. We\npropose several indicative evaluation metrics to guide design trade-offs for\nprivacy-preserving fine-tuning and a series of example designs, collectively\nnamed GuardedTuning; they result from novel combinations of system\narchitectures with adapted privacy-enhancement methods and emerging computation\ntechniques. Each design represents distinct trade-offs across model utility,\nprivacy guarantees, and costs. Experimental results demonstrate that these\ndesigns protect against data reconstruction attacks while maintaining\ncompetitive fine-tuning performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning has proven effective in enhancing Large Language Models'\n(LLMs) performance on downstream tasks. However, real-world fine-tuning faces\ninherent conflicts between model providers' intellectual property protection,\nclients' data privacy requirements, and tuning costs. While recent approaches\nlike split learning and offsite tuning demonstrate promising architectures for\nprivacy-preserving fine-tuning, there is a gap in systematically addressing the\nmultidimensional trade-offs required for diverse real-world deployments. We\npropose several indicative evaluation metrics to guide design trade-offs for\nprivacy-preserving fine-tuning and a series of example designs, collectively\nnamed GuardedTuning; they result from novel combinations of system\narchitectures with adapted privacy-enhancement methods and emerging computation\ntechniques. Each design represents distinct trade-offs across model utility,\nprivacy guarantees, and costs. Experimental results demonstrate that these\ndesigns protect against data reconstruction attacks while maintaining\ncompetitive fine-tuning performance."
                },
                "authors": [
                    {
                        "name": "Shi Haonan"
                    },
                    {
                        "name": "Ouyang Tu"
                    },
                    {
                        "name": "Wang An"
                    }
                ],
                "author_detail": {
                    "name": "Wang An"
                },
                "author": "Wang An",
                "arxiv_comment": "4 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04316v1",
                "updated": "2025-01-08T07:28:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    28,
                    10,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T07:28:10Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    28,
                    10,
                    2,
                    8,
                    0
                ],
                "title": "Who Does the Giant Number Pile Like Best: Analyzing Fairness in Hiring\n  Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who Does the Giant Number Pile Like Best: Analyzing Fairness in Hiring\n  Contexts"
                },
                "summary": "Large language models (LLMs) are increasingly being deployed in high-stakes\napplications like hiring, yet their potential for unfair decision-making and\noutcomes remains understudied, particularly in generative settings. In this\nwork, we examine the fairness of LLM-based hiring systems through two\nreal-world tasks: resume summarization and retrieval. By constructing a\nsynthetic resume dataset and curating job postings, we investigate whether\nmodel behavior differs across demographic groups and is sensitive to\ndemographic perturbations. Our findings reveal that race-based differences\nappear in approximately 10% of generated summaries, while gender-based\ndifferences occur in only 1%. In the retrieval setting, all evaluated models\ndisplay non-uniform selection patterns across demographic groups and exhibit\nhigh sensitivity to both gender and race-based perturbations. Surprisingly,\nretrieval models demonstrate comparable sensitivity to non-demographic changes,\nsuggesting that fairness issues may stem, in part, from general brittleness\nissues. Overall, our results indicate that LLM-based hiring systems, especially\nat the retrieval stage, can exhibit notable biases that lead to discriminatory\noutcomes in real-world contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being deployed in high-stakes\napplications like hiring, yet their potential for unfair decision-making and\noutcomes remains understudied, particularly in generative settings. In this\nwork, we examine the fairness of LLM-based hiring systems through two\nreal-world tasks: resume summarization and retrieval. By constructing a\nsynthetic resume dataset and curating job postings, we investigate whether\nmodel behavior differs across demographic groups and is sensitive to\ndemographic perturbations. Our findings reveal that race-based differences\nappear in approximately 10% of generated summaries, while gender-based\ndifferences occur in only 1%. In the retrieval setting, all evaluated models\ndisplay non-uniform selection patterns across demographic groups and exhibit\nhigh sensitivity to both gender and race-based perturbations. Surprisingly,\nretrieval models demonstrate comparable sensitivity to non-demographic changes,\nsuggesting that fairness issues may stem, in part, from general brittleness\nissues. Overall, our results indicate that LLM-based hiring systems, especially\nat the retrieval stage, can exhibit notable biases that lead to discriminatory\noutcomes in real-world contexts."
                },
                "authors": [
                    {
                        "name": "Preethi Seshadri"
                    },
                    {
                        "name": "Seraphina Goldfarb-Tarrant"
                    }
                ],
                "author_detail": {
                    "name": "Seraphina Goldfarb-Tarrant"
                },
                "author": "Seraphina Goldfarb-Tarrant",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07464v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07464v2",
                "updated": "2025-01-08T07:25:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    25,
                    55,
                    2,
                    8,
                    0
                ],
                "published": "2024-11-12T00:57:30Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    0,
                    57,
                    30,
                    1,
                    317,
                    0
                ],
                "title": "BudgetMLAgent: A Cost-Effective LLM Multi-Agent system for Automating\n  Machine Learning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BudgetMLAgent: A Cost-Effective LLM Multi-Agent system for Automating\n  Machine Learning Tasks"
                },
                "summary": "Large Language Models (LLMs) excel in diverse applications including\ngeneration of code snippets, but often struggle with generating code for\ncomplex Machine Learning (ML) tasks. Although existing LLM single-agent based\nsystems give varying performance depending on the task complexity, they purely\nrely on larger and expensive models such as GPT-4. Our investigation reveals\nthat no-cost and low-cost models such as Gemini-Pro, Mixtral and CodeLlama\nperform far worse than GPT-4 in a single-agent setting. With the motivation of\ndeveloping a cost-efficient LLM based solution for solving ML tasks, we propose\nan LLM Multi-Agent based system which leverages combination of experts using\nprofiling, efficient retrieval of past observations, LLM cascades, and\nask-the-expert calls. Through empirical analysis on ML engineering tasks in the\nMLAgentBench benchmark, we demonstrate the effectiveness of our system, using\nno-cost models, namely Gemini as the base LLM, paired with GPT-4 in cascade and\nexpert to serve occasional ask-the-expert calls for planning. With 94.2\\%\nreduction in the cost (from \\$0.931 per run cost averaged over all tasks for\nGPT-4 single agent system to \\$0.054), our system is able to yield better\naverage success rate of 32.95\\% as compared to GPT-4 single-agent system\nyielding 22.72\\% success rate averaged over all the tasks of MLAgentBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in diverse applications including\ngeneration of code snippets, but often struggle with generating code for\ncomplex Machine Learning (ML) tasks. Although existing LLM single-agent based\nsystems give varying performance depending on the task complexity, they purely\nrely on larger and expensive models such as GPT-4. Our investigation reveals\nthat no-cost and low-cost models such as Gemini-Pro, Mixtral and CodeLlama\nperform far worse than GPT-4 in a single-agent setting. With the motivation of\ndeveloping a cost-efficient LLM based solution for solving ML tasks, we propose\nan LLM Multi-Agent based system which leverages combination of experts using\nprofiling, efficient retrieval of past observations, LLM cascades, and\nask-the-expert calls. Through empirical analysis on ML engineering tasks in the\nMLAgentBench benchmark, we demonstrate the effectiveness of our system, using\nno-cost models, namely Gemini as the base LLM, paired with GPT-4 in cascade and\nexpert to serve occasional ask-the-expert calls for planning. With 94.2\\%\nreduction in the cost (from \\$0.931 per run cost averaged over all tasks for\nGPT-4 single agent system to \\$0.054), our system is able to yield better\naverage success rate of 32.95\\% as compared to GPT-4 single-agent system\nyielding 22.72\\% success rate averaged over all the tasks of MLAgentBench."
                },
                "authors": [
                    {
                        "name": "Shubham Gandhi"
                    },
                    {
                        "name": "Manasi Patwardhan"
                    },
                    {
                        "name": "Lovekesh Vig"
                    },
                    {
                        "name": "Gautam Shroff"
                    }
                ],
                "author_detail": {
                    "name": "Gautam Shroff"
                },
                "author": "Gautam Shroff",
                "arxiv_comment": "Presented at AIMLSystems '24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07464v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07464v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T42",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1; I.2.2; I.2.5; I.2.7; I.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14418v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14418v3",
                "updated": "2025-01-08T07:23:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    23,
                    56,
                    2,
                    8,
                    0
                ],
                "published": "2024-08-26T17:04:00Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    4,
                    0,
                    0,
                    239,
                    0
                ],
                "title": "MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR\n  Errors with LLM-generated Synthetic Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR\n  Errors with LLM-generated Synthetic Dialogues"
                },
                "summary": "Automatic Speech Recognition (ASR) systems are pivotal in transcribing speech\ninto text, yet the errors they introduce can significantly degrade the\nperformance of downstream tasks like summarization. This issue is particularly\npronounced in clinical dialogue summarization, a low-resource domain where\nsupervised data for fine-tuning is scarce, necessitating the use of ASR models\nas black-box solutions. Employing conventional data augmentation for enhancing\nthe noise robustness of summarization models is not feasible either due to the\nunavailability of sufficient medical dialogue audio recordings and\ncorresponding ASR transcripts. To address this challenge, we propose MEDSAGE,\nan approach for generating synthetic samples for data augmentation using Large\nLanguage Models (LLMs). Specifically, we leverage the in-context learning\ncapabilities of LLMs and instruct them to generate ASR-like errors based on a\nfew available medical dialogue examples with audio recordings. Experimental\nresults show that LLMs can effectively model ASR noise, and incorporating this\nnoisy data into the training process significantly improves the robustness and\naccuracy of medical dialogue summarization systems. This approach addresses the\nchallenges of noisy ASR outputs in critical applications, offering a robust\nsolution to enhance the reliability of clinical dialogue summarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Speech Recognition (ASR) systems are pivotal in transcribing speech\ninto text, yet the errors they introduce can significantly degrade the\nperformance of downstream tasks like summarization. This issue is particularly\npronounced in clinical dialogue summarization, a low-resource domain where\nsupervised data for fine-tuning is scarce, necessitating the use of ASR models\nas black-box solutions. Employing conventional data augmentation for enhancing\nthe noise robustness of summarization models is not feasible either due to the\nunavailability of sufficient medical dialogue audio recordings and\ncorresponding ASR transcripts. To address this challenge, we propose MEDSAGE,\nan approach for generating synthetic samples for data augmentation using Large\nLanguage Models (LLMs). Specifically, we leverage the in-context learning\ncapabilities of LLMs and instruct them to generate ASR-like errors based on a\nfew available medical dialogue examples with audio recordings. Experimental\nresults show that LLMs can effectively model ASR noise, and incorporating this\nnoisy data into the training process significantly improves the robustness and\naccuracy of medical dialogue summarization systems. This approach addresses the\nchallenges of noisy ASR outputs in critical applications, offering a robust\nsolution to enhance the reliability of clinical dialogue summarization."
                },
                "authors": [
                    {
                        "name": "Kuluhan Binici"
                    },
                    {
                        "name": "Abhinav Ramesh Kashyap"
                    },
                    {
                        "name": "Viktor Schlegel"
                    },
                    {
                        "name": "Andy T. Liu"
                    },
                    {
                        "name": "Vijay Prakash Dwivedi"
                    },
                    {
                        "name": "Thanh-Tung Nguyen"
                    },
                    {
                        "name": "Xiaoxue Gao"
                    },
                    {
                        "name": "Nancy F. Chen"
                    },
                    {
                        "name": "Stefan Winkler"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Winkler"
                },
                "author": "Stefan Winkler",
                "arxiv_comment": "Accepted by the Thirty-Ninth AAAI Conference on Artificial\n  Intelligence (AAAI-25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14418v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14418v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11102v2",
                "updated": "2025-01-08T07:22:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    22,
                    51,
                    2,
                    8,
                    0
                ],
                "published": "2024-12-15T07:49:31Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    7,
                    49,
                    31,
                    6,
                    350,
                    0
                ],
                "title": "Empowering LLMs to Understand and Generate Complex Vector Graphics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering LLMs to Understand and Generate Complex Vector Graphics"
                },
                "summary": "The unprecedented advancements in Large Language Models (LLMs) have\nprofoundly impacted natural language processing but have yet to fully embrace\nthe realm of scalable vector graphics (SVG) generation. While LLMs encode\npartial knowledge of SVG data from web pages during training, recent findings\nsuggest that semantically ambiguous and tokenized representations within LLMs\nmay result in hallucinations in vector primitive predictions. Additionally, LLM\ntraining typically lacks modeling and understanding of the rendering sequence\nof vector paths, which can lead to occlusion between output vector primitives.\nIn this paper, we present LLM4SVG, an initial yet substantial step toward\nbridging this gap by enabling LLMs to better understand and generate vector\ngraphics. LLM4SVG facilitates a deeper understanding of SVG components through\nlearnable semantic tokens, which precisely encode these tokens and their\ncorresponding properties to generate semantically aligned SVG outputs. Using a\nseries of learnable semantic tokens, a structured dataset for instruction\nfollowing is developed to support comprehension and generation across two\nprimary tasks. Our method introduces a modular architecture to existing large\nlanguage models, integrating semantic tags, vector instruction encoders,\nfine-tuned commands, and powerful LLMs to tightly combine geometric,\nappearance, and language information. To overcome the scarcity of SVG-text\ninstruction data, we developed an automated data generation pipeline that\ncollected a massive dataset of more than 250k SVG data and 580k SVG-text\ninstructions, which facilitated the adoption of the two-stage training strategy\npopular in LLM development. By exploring various training strategies, we\ndeveloped LLM4SVG, which significantly moves beyond optimized rendering-based\napproaches and language-model-based baselines to achieve remarkable results in\nhuman evaluation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The unprecedented advancements in Large Language Models (LLMs) have\nprofoundly impacted natural language processing but have yet to fully embrace\nthe realm of scalable vector graphics (SVG) generation. While LLMs encode\npartial knowledge of SVG data from web pages during training, recent findings\nsuggest that semantically ambiguous and tokenized representations within LLMs\nmay result in hallucinations in vector primitive predictions. Additionally, LLM\ntraining typically lacks modeling and understanding of the rendering sequence\nof vector paths, which can lead to occlusion between output vector primitives.\nIn this paper, we present LLM4SVG, an initial yet substantial step toward\nbridging this gap by enabling LLMs to better understand and generate vector\ngraphics. LLM4SVG facilitates a deeper understanding of SVG components through\nlearnable semantic tokens, which precisely encode these tokens and their\ncorresponding properties to generate semantically aligned SVG outputs. Using a\nseries of learnable semantic tokens, a structured dataset for instruction\nfollowing is developed to support comprehension and generation across two\nprimary tasks. Our method introduces a modular architecture to existing large\nlanguage models, integrating semantic tags, vector instruction encoders,\nfine-tuned commands, and powerful LLMs to tightly combine geometric,\nappearance, and language information. To overcome the scarcity of SVG-text\ninstruction data, we developed an automated data generation pipeline that\ncollected a massive dataset of more than 250k SVG data and 580k SVG-text\ninstructions, which facilitated the adoption of the two-stage training strategy\npopular in LLM development. By exploring various training strategies, we\ndeveloped LLM4SVG, which significantly moves beyond optimized rendering-based\napproaches and language-model-based baselines to achieve remarkable results in\nhuman evaluation tasks."
                },
                "authors": [
                    {
                        "name": "Ximing Xing"
                    },
                    {
                        "name": "Juncheng Hu"
                    },
                    {
                        "name": "Guotao Liang"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Dong Xu"
                    },
                    {
                        "name": "Qian Yu"
                    }
                ],
                "author_detail": {
                    "name": "Qian Yu"
                },
                "author": "Qian Yu",
                "arxiv_comment": "Project Page: https://ximinng.github.io/LLM4SVGProject/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16040v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16040v2",
                "updated": "2025-01-08T07:21:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    21,
                    15,
                    2,
                    8,
                    0
                ],
                "published": "2024-07-22T20:34:00Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    20,
                    34,
                    0,
                    0,
                    204,
                    0
                ],
                "title": "Generalizing Teacher Networks for Effective Knowledge Distillation\n  Across Student Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalizing Teacher Networks for Effective Knowledge Distillation\n  Across Student Architectures"
                },
                "summary": "Knowledge distillation (KD) is a model compression method that entails\ntraining a compact student model to emulate the performance of a more complex\nteacher model. However, the architectural capacity gap between the two models\nlimits the effectiveness of knowledge transfer. Addressing this issue, previous\nworks focused on customizing teacher-student pairs to improve compatibility, a\ncomputationally expensive process that needs to be repeated every time either\nmodel changes. Hence, these methods are impractical when a teacher model has to\nbe compressed into different student models for deployment on multiple hardware\ndevices with distinct resource constraints. In this work, we propose Generic\nTeacher Network (GTN), a one-off KD-aware training to create a generic teacher\ncapable of effectively transferring knowledge to any student model sampled from\na given finite pool of architectures. To this end, we represent the student\npool as a weight-sharing supernet and condition our generic teacher to align\nwith the capacities of various student architectures sampled from this\nsupernet. Experimental evaluation shows that our method both improves overall\nKD effectiveness and amortizes the minimal additional training cost of the\ngeneric teacher across students in the pool.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation (KD) is a model compression method that entails\ntraining a compact student model to emulate the performance of a more complex\nteacher model. However, the architectural capacity gap between the two models\nlimits the effectiveness of knowledge transfer. Addressing this issue, previous\nworks focused on customizing teacher-student pairs to improve compatibility, a\ncomputationally expensive process that needs to be repeated every time either\nmodel changes. Hence, these methods are impractical when a teacher model has to\nbe compressed into different student models for deployment on multiple hardware\ndevices with distinct resource constraints. In this work, we propose Generic\nTeacher Network (GTN), a one-off KD-aware training to create a generic teacher\ncapable of effectively transferring knowledge to any student model sampled from\na given finite pool of architectures. To this end, we represent the student\npool as a weight-sharing supernet and condition our generic teacher to align\nwith the capacities of various student architectures sampled from this\nsupernet. Experimental evaluation shows that our method both improves overall\nKD effectiveness and amortizes the minimal additional training cost of the\ngeneric teacher across students in the pool."
                },
                "authors": [
                    {
                        "name": "Kuluhan Binici"
                    },
                    {
                        "name": "Weiming Wu"
                    },
                    {
                        "name": "Tulika Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Tulika Mitra"
                },
                "author": "Tulika Mitra",
                "arxiv_comment": "British Machine Vision Conference (BMVC 24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16040v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16040v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04315v1",
                "updated": "2025-01-08T07:13:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    13,
                    52,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T07:13:52Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    13,
                    52,
                    2,
                    8,
                    0
                ],
                "title": "RoRA: Efficient Fine-Tuning of LLM with Reliability Optimization for\n  Rank Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoRA: Efficient Fine-Tuning of LLM with Reliability Optimization for\n  Rank Adaptation"
                },
                "summary": "Fine-tuning helps large language models (LLM) recover degraded information\nand enhance task performance.Although Low-Rank Adaptation (LoRA) is widely used\nand effective for fine-tuning, we have observed that its scaling factor can\nlimit or even reduce performance as the rank size increases. To address this\nissue, we propose RoRA (Rank-adaptive Reliability Optimization), a simple yet\neffective method for optimizing LoRA's scaling factor. By replacing $\\alpha/r$\nwith $\\alpha/\\sqrt{r}$, RoRA ensures improved performance as rank size\nincreases. Moreover, RoRA enhances low-rank adaptation in fine-tuning\nuncompressed models and excels in the more challenging task of accuracy\nrecovery when fine-tuning pruned models. Extensive experiments demonstrate the\neffectiveness of RoRA in fine-tuning both uncompressed and pruned models. RoRA\nsurpasses the state-of-the-art (SOTA) in average accuracy and robustness on\nLLaMA-7B/13B, LLaMA2-7B, and LLaMA3-8B, specifically outperforming LoRA and\nDoRA by 6.5% and 2.9% on LLaMA-7B, respectively. In pruned model fine-tuning,\nRoRA shows significant advantages; for SHEARED-LLAMA-1.3, a LLaMA-7B with 81.4%\npruning, RoRA achieves 5.7% higher average accuracy than LoRA and 3.9% higher\nthan DoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning helps large language models (LLM) recover degraded information\nand enhance task performance.Although Low-Rank Adaptation (LoRA) is widely used\nand effective for fine-tuning, we have observed that its scaling factor can\nlimit or even reduce performance as the rank size increases. To address this\nissue, we propose RoRA (Rank-adaptive Reliability Optimization), a simple yet\neffective method for optimizing LoRA's scaling factor. By replacing $\\alpha/r$\nwith $\\alpha/\\sqrt{r}$, RoRA ensures improved performance as rank size\nincreases. Moreover, RoRA enhances low-rank adaptation in fine-tuning\nuncompressed models and excels in the more challenging task of accuracy\nrecovery when fine-tuning pruned models. Extensive experiments demonstrate the\neffectiveness of RoRA in fine-tuning both uncompressed and pruned models. RoRA\nsurpasses the state-of-the-art (SOTA) in average accuracy and robustness on\nLLaMA-7B/13B, LLaMA2-7B, and LLaMA3-8B, specifically outperforming LoRA and\nDoRA by 6.5% and 2.9% on LLaMA-7B, respectively. In pruned model fine-tuning,\nRoRA shows significant advantages; for SHEARED-LLAMA-1.3, a LLaMA-7B with 81.4%\npruning, RoRA achieves 5.7% higher average accuracy than LoRA and 3.9% higher\nthan DoRA."
                },
                "authors": [
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Zhenglun Kong"
                    },
                    {
                        "name": "Peiyan Dong"
                    },
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Geng Yuan"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Wenbin Zhang"
                    },
                    {
                        "name": "Xue Lin"
                    },
                    {
                        "name": "Dong Huang"
                    },
                    {
                        "name": "Yanzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanzhi Wang"
                },
                "author": "Yanzhi Wang",
                "arxiv_comment": "ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04312v1",
                "updated": "2025-01-08T07:07:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    7,
                    22,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T07:07:22Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    7,
                    22,
                    2,
                    8,
                    0
                ],
                "title": "Your Fix Is My Exploit: Enabling Comprehensive DL Library API Fuzzing\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Your Fix Is My Exploit: Enabling Comprehensive DL Library API Fuzzing\n  with Large Language Models"
                },
                "summary": "Deep learning (DL) libraries, widely used in AI applications, often contain\nvulnerabilities like buffer overflows and use-after-free errors. Traditional\nfuzzing struggles with the complexity and API diversity of DL libraries such as\nTensorFlow and PyTorch, which feature over 1,000 APIs. Testing all these APIs\nis challenging due to complex inputs and varied usage patterns. While large\nlanguage models (LLMs) show promise in code understanding and generation,\nexisting LLM-based fuzzers lack deep knowledge of API edge cases and struggle\nwith test input generation. To address this, we propose DFUZZ, an LLM-driven\nfuzzing approach for DL libraries. DFUZZ leverages two insights: (1) LLMs can\nreason about error-triggering edge cases from API code and apply this knowledge\nto untested APIs, and (2) LLMs can accurately synthesize test programs to\nautomate API testing. By providing LLMs with a \"white-box view\" of APIs, DFUZZ\nenhances reasoning and generation for comprehensive fuzzing. Experimental\nresults show that DFUZZ outperforms state-of-the-art fuzzers in API coverage\nfor TensorFlow and PyTorch, uncovering 37 bugs, with 8 fixed and 19 under\ndeveloper investigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning (DL) libraries, widely used in AI applications, often contain\nvulnerabilities like buffer overflows and use-after-free errors. Traditional\nfuzzing struggles with the complexity and API diversity of DL libraries such as\nTensorFlow and PyTorch, which feature over 1,000 APIs. Testing all these APIs\nis challenging due to complex inputs and varied usage patterns. While large\nlanguage models (LLMs) show promise in code understanding and generation,\nexisting LLM-based fuzzers lack deep knowledge of API edge cases and struggle\nwith test input generation. To address this, we propose DFUZZ, an LLM-driven\nfuzzing approach for DL libraries. DFUZZ leverages two insights: (1) LLMs can\nreason about error-triggering edge cases from API code and apply this knowledge\nto untested APIs, and (2) LLMs can accurately synthesize test programs to\nautomate API testing. By providing LLMs with a \"white-box view\" of APIs, DFUZZ\nenhances reasoning and generation for comprehensive fuzzing. Experimental\nresults show that DFUZZ outperforms state-of-the-art fuzzers in API coverage\nfor TensorFlow and PyTorch, uncovering 37 bugs, with 8 fixed and 19 under\ndeveloper investigation."
                },
                "authors": [
                    {
                        "name": "Kunpeng Zhang"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Jitao Han"
                    },
                    {
                        "name": "Xiaogang Zhu"
                    },
                    {
                        "name": "Xian Li"
                    },
                    {
                        "name": "Shaohua Wang"
                    },
                    {
                        "name": "Sheng Wen"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Wen"
                },
                "author": "Sheng Wen",
                "arxiv_journal_ref": "2025 IEEE/ACM 47th International Conference on Software\n  Engineering (ICSE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13720v2",
                "updated": "2025-01-08T07:03:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    3,
                    42,
                    2,
                    8,
                    0
                ],
                "published": "2024-12-18T11:00:58Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    11,
                    0,
                    58,
                    2,
                    353,
                    0
                ],
                "title": "Federated Learning and RAG Integration: A Scalable Approach for Medical\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning and RAG Integration: A Scalable Approach for Medical\n  Large Language Models"
                },
                "summary": "This study analyzes the performance of domain-specific Large Language Models\n(LLMs) for the medical field by integrating Retrieval-Augmented Generation\n(RAG) systems within a federated learning framework. Leveraging the inherent\nadvantages of federated learning, such as preserving data privacy and enabling\ndistributed computation, this research explores the integration of RAG systems\nwith models trained under varying client configurations to optimize\nperformance. Experimental results demonstrate that the federated learning-based\nmodels integrated with RAG systems consistently outperform their non-integrated\ncounterparts across all evaluation metrics. This study highlights the potential\nof combining federated learning and RAG systems for developing domain-specific\nLLMs in the medical field, providing a scalable and privacy-preserving solution\nfor enhancing text generation capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study analyzes the performance of domain-specific Large Language Models\n(LLMs) for the medical field by integrating Retrieval-Augmented Generation\n(RAG) systems within a federated learning framework. Leveraging the inherent\nadvantages of federated learning, such as preserving data privacy and enabling\ndistributed computation, this research explores the integration of RAG systems\nwith models trained under varying client configurations to optimize\nperformance. Experimental results demonstrate that the federated learning-based\nmodels integrated with RAG systems consistently outperform their non-integrated\ncounterparts across all evaluation metrics. This study highlights the potential\nof combining federated learning and RAG systems for developing domain-specific\nLLMs in the medical field, providing a scalable and privacy-preserving solution\nfor enhancing text generation capabilities."
                },
                "authors": [
                    {
                        "name": "Jincheol Jung"
                    },
                    {
                        "name": "Hongju Jeong"
                    },
                    {
                        "name": "Eui-Nam Huh"
                    }
                ],
                "author_detail": {
                    "name": "Eui-Nam Huh"
                },
                "author": "Eui-Nam Huh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04306v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04306v1",
                "updated": "2025-01-08T06:44:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    6,
                    44,
                    2,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T06:44:02Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    6,
                    44,
                    2,
                    2,
                    8,
                    0
                ],
                "title": "LLM4SR: A Survey on Large Language Models for Scientific Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4SR: A Survey on Large Language Models for Scientific Research"
                },
                "summary": "In recent years, the rapid advancement of Large Language Models (LLMs) has\ntransformed the landscape of scientific research, offering unprecedented\nsupport across various stages of the research cycle. This paper presents the\nfirst systematic survey dedicated to exploring how LLMs are revolutionizing the\nscientific research process. We analyze the unique roles LLMs play across four\ncritical stages of research: hypothesis discovery, experiment planning and\nimplementation, scientific writing, and peer reviewing. Our review\ncomprehensively showcases the task-specific methodologies and evaluation\nbenchmarks. By identifying current challenges and proposing future research\ndirections, this survey not only highlights the transformative potential of\nLLMs, but also aims to inspire and guide researchers and practitioners in\nleveraging LLMs to advance scientific inquiry. Resources are available at the\nfollowing repository: https://github.com/du-nlp-lab/LLM4SR",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the rapid advancement of Large Language Models (LLMs) has\ntransformed the landscape of scientific research, offering unprecedented\nsupport across various stages of the research cycle. This paper presents the\nfirst systematic survey dedicated to exploring how LLMs are revolutionizing the\nscientific research process. We analyze the unique roles LLMs play across four\ncritical stages of research: hypothesis discovery, experiment planning and\nimplementation, scientific writing, and peer reviewing. Our review\ncomprehensively showcases the task-specific methodologies and evaluation\nbenchmarks. By identifying current challenges and proposing future research\ndirections, this survey not only highlights the transformative potential of\nLLMs, but also aims to inspire and guide researchers and practitioners in\nleveraging LLMs to advance scientific inquiry. Resources are available at the\nfollowing repository: https://github.com/du-nlp-lab/LLM4SR"
                },
                "authors": [
                    {
                        "name": "Ziming Luo"
                    },
                    {
                        "name": "Zonglin Yang"
                    },
                    {
                        "name": "Zexin Xu"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Xinya Du"
                    }
                ],
                "author_detail": {
                    "name": "Xinya Du"
                },
                "author": "Xinya Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04306v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04306v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.00398v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.00398v3",
                "updated": "2025-01-08T06:35:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    6,
                    35,
                    7,
                    2,
                    8,
                    0
                ],
                "published": "2023-06-01T07:00:07Z",
                "published_parsed": [
                    2023,
                    6,
                    1,
                    7,
                    0,
                    7,
                    3,
                    152,
                    0
                ],
                "title": "Preference-grounded Token-level Guidance for Language Model Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference-grounded Token-level Guidance for Language Model Fine-tuning"
                },
                "summary": "Aligning language models (LMs) with preferences is an important problem in\nnatural language generation. A key challenge is that preferences are typically\nprovided at the sequence level while LM training and generation both occur at\nthe token level. There is, therefore, a granularity mismatch between the\npreference and the LM training losses, which may complicate the learning\nproblem. In this paper, we address this issue by developing an alternate\ntraining process, where we iterate between grounding the sequence-level\npreference into token-level training guidance, and improving the LM with the\nlearned guidance. For guidance learning, we design a framework that extends the\npairwise-preference learning in imitation learning to both variable-length LM\ngeneration and the utilization of the preference among multiple generations.\nFor LM training, based on the amount of supervised data, we present two\nminimalist learning objectives that utilize the learned guidance. In\nexperiments, our method performs competitively on two distinct representative\nLM tasks -- discrete-prompt generation and text summarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning language models (LMs) with preferences is an important problem in\nnatural language generation. A key challenge is that preferences are typically\nprovided at the sequence level while LM training and generation both occur at\nthe token level. There is, therefore, a granularity mismatch between the\npreference and the LM training losses, which may complicate the learning\nproblem. In this paper, we address this issue by developing an alternate\ntraining process, where we iterate between grounding the sequence-level\npreference into token-level training guidance, and improving the LM with the\nlearned guidance. For guidance learning, we design a framework that extends the\npairwise-preference learning in imitation learning to both variable-length LM\ngeneration and the utilization of the preference among multiple generations.\nFor LM training, based on the amount of supervised data, we present two\nminimalist learning objectives that utilize the learned guidance. In\nexperiments, our method performs competitively on two distinct representative\nLM tasks -- discrete-prompt generation and text summarization."
                },
                "authors": [
                    {
                        "name": "Shentao Yang"
                    },
                    {
                        "name": "Shujian Zhang"
                    },
                    {
                        "name": "Congying Xia"
                    },
                    {
                        "name": "Yihao Feng"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Mingyuan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Mingyuan Zhou"
                },
                "author": "Mingyuan Zhou",
                "arxiv_comment": "v2: 37th Conference on Neural Information Processing Systems (NeurIPS\n  2023); v3: update on scaling up to PPO + LLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.00398v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.00398v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04285v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04285v1",
                "updated": "2025-01-08T05:17:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    5,
                    17,
                    9,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T05:17:09Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    5,
                    17,
                    9,
                    2,
                    8,
                    0
                ],
                "title": "Separate Source Channel Coding Is Still What You Need: An LLM-based\n  Rethinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Separate Source Channel Coding Is Still What You Need: An LLM-based\n  Rethinking"
                },
                "summary": "Along with the proliferating research interest in Semantic Communication\n(SemCom), Joint Source Channel Coding (JSCC) has dominated the attention due to\nthe widely assumed existence in efficiently delivering information semantics.\n%has emerged as a pivotal area of research, aiming to enhance the efficiency\nand reliability of information transmission through deep learning-based\nmethods. Nevertheless, this paper challenges the conventional JSCC paradigm,\nand advocates for adoption of Separate Source Channel Coding (SSCC) to enjoy\nthe underlying more degree of freedom for optimization. We demonstrate that\nSSCC, after leveraging the strengths of Large Language Model (LLM) for source\ncoding and Error Correction Code Transformer (ECCT) complemented for channel\ndecoding, offers superior performance over JSCC. Our proposed framework also\neffectively highlights the compatibility challenges between SemCom approaches\nand digital communication systems, particularly concerning the resource costs\nassociated with the transmission of high precision floating point numbers.\nThrough comprehensive evaluations, we establish that empowered by LLM-based\ncompression and ECCT-enhanced error correction, SSCC remains a viable and\neffective solution for modern communication systems. In other words, separate\nsource and channel coding is still what we need!",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Along with the proliferating research interest in Semantic Communication\n(SemCom), Joint Source Channel Coding (JSCC) has dominated the attention due to\nthe widely assumed existence in efficiently delivering information semantics.\n%has emerged as a pivotal area of research, aiming to enhance the efficiency\nand reliability of information transmission through deep learning-based\nmethods. Nevertheless, this paper challenges the conventional JSCC paradigm,\nand advocates for adoption of Separate Source Channel Coding (SSCC) to enjoy\nthe underlying more degree of freedom for optimization. We demonstrate that\nSSCC, after leveraging the strengths of Large Language Model (LLM) for source\ncoding and Error Correction Code Transformer (ECCT) complemented for channel\ndecoding, offers superior performance over JSCC. Our proposed framework also\neffectively highlights the compatibility challenges between SemCom approaches\nand digital communication systems, particularly concerning the resource costs\nassociated with the transmission of high precision floating point numbers.\nThrough comprehensive evaluations, we establish that empowered by LLM-based\ncompression and ECCT-enhanced error correction, SSCC remains a viable and\neffective solution for modern communication systems. In other words, separate\nsource and channel coding is still what we need!"
                },
                "authors": [
                    {
                        "name": "Tianqi Ren"
                    },
                    {
                        "name": "Rongpeng Li"
                    },
                    {
                        "name": "Ming-min Zhao"
                    },
                    {
                        "name": "Xianfu Chen"
                    },
                    {
                        "name": "Guangyi Liu"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Zhifeng Zhao"
                    },
                    {
                        "name": "Honggang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Honggang Zhang"
                },
                "author": "Honggang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04285v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04285v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04277v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04277v1",
                "updated": "2025-01-08T04:55:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    4,
                    55,
                    59,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T04:55:59Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    4,
                    55,
                    59,
                    2,
                    8,
                    0
                ],
                "title": "Exploring the Expertise of Large Language Models in Materials Science\n  and Metallurgical Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Expertise of Large Language Models in Materials Science\n  and Metallurgical Engineering"
                },
                "summary": "The integration of artificial intelligence into various domains is rapidly\nincreasing, with Large Language Models (LLMs) becoming more prevalent in\nnumerous applications. This work is included in an overall project which aims\nto train an LLM specifically in the field of materials science. To assess the\nimpact of this specialized training, it is essential to establish the baseline\nperformance of existing LLMs in materials science. In this study, we evaluated\n15 different LLMs using the MaScQA question answering (Q&A) benchmark. This\nbenchmark comprises questions from the Graduate Aptitude Test in Engineering\n(GATE), tailored to test models' capabilities in answering questions related to\nmaterials science and metallurgical engineering. Our results indicate that\nclosed-source LLMs, such as Claude-3.5-Sonnet and GPT-4, perform the best with\nan overall accuracy of ~84%, while the open-source models, Llama3-70b and\nPhi3-14b, top at ~56% and ~43%, respectively. These findings provide a baseline\nfor the raw capabilities of LLMs on Q&A tasks applied to materials science, and\nemphasize the substantial improvement that could be brought to open-source\nmodels via prompt engineering and fine-tuning strategies. We anticipate that\nthis work could push the adoption of LLMs as valuable assistants in materials\nscience, demonstrating their utility in this specialized domain and related\nsub-domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of artificial intelligence into various domains is rapidly\nincreasing, with Large Language Models (LLMs) becoming more prevalent in\nnumerous applications. This work is included in an overall project which aims\nto train an LLM specifically in the field of materials science. To assess the\nimpact of this specialized training, it is essential to establish the baseline\nperformance of existing LLMs in materials science. In this study, we evaluated\n15 different LLMs using the MaScQA question answering (Q&A) benchmark. This\nbenchmark comprises questions from the Graduate Aptitude Test in Engineering\n(GATE), tailored to test models' capabilities in answering questions related to\nmaterials science and metallurgical engineering. Our results indicate that\nclosed-source LLMs, such as Claude-3.5-Sonnet and GPT-4, perform the best with\nan overall accuracy of ~84%, while the open-source models, Llama3-70b and\nPhi3-14b, top at ~56% and ~43%, respectively. These findings provide a baseline\nfor the raw capabilities of LLMs on Q&A tasks applied to materials science, and\nemphasize the substantial improvement that could be brought to open-source\nmodels via prompt engineering and fine-tuning strategies. We anticipate that\nthis work could push the adoption of LLMs as valuable assistants in materials\nscience, demonstrating their utility in this specialized domain and related\nsub-domains."
                },
                "authors": [
                    {
                        "name": "Christophe Bajan"
                    },
                    {
                        "name": "Guillaume Lambard"
                    }
                ],
                "author_detail": {
                    "name": "Guillaume Lambard"
                },
                "author": "Guillaume Lambard",
                "arxiv_comment": "13 pages, 6 figures, 5 tables, accepted in Digital Discovery",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04277v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03295v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03295v2",
                "updated": "2025-01-08T04:50:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    4,
                    50,
                    1,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-06T11:43:29Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    11,
                    43,
                    29,
                    0,
                    6,
                    0
                ],
                "title": "A Soft Sensor Method with Uncertainty-Awareness and Self-Explanation\n  Based on Large Language Models Enhanced by Domain Knowledge Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Soft Sensor Method with Uncertainty-Awareness and Self-Explanation\n  Based on Large Language Models Enhanced by Domain Knowledge Retrieval"
                },
                "summary": "Data-driven soft sensors are crucial in predicting key performance indicators\nin industrial systems. However, current methods predominantly rely on the\nsupervised learning paradigms of parameter updating, which inherently faces\nchallenges such as high development costs, poor robustness, training\ninstability, and lack of interpretability. Recently, large language models\n(LLMs) have demonstrated significant potential across various domains, notably\nthrough In-Context Learning (ICL), which enables high-performance task\nexecution with minimal input-label demonstrations and no prior training. This\npaper aims to replace supervised learning with the emerging ICL paradigm for\nsoft sensor modeling to address existing challenges and explore new avenues for\nadvancement. To achieve this, we propose a novel framework called the Few-shot\nUncertainty-aware and self-Explaining Soft Sensor (LLM-FUESS), which includes\nthe Zero-shot Auxiliary Variable Selector (LLM-ZAVS) and the Uncertainty-aware\nFew-shot Soft Sensor (LLM-UFSS). The LLM-ZAVS retrieves from the Industrial\nKnowledge Vector Storage to enhance LLMs' domain-specific knowledge, enabling\nzero-shot auxiliary variable selection. In the LLM-UFSS, we utilize text-based\ncontext demonstrations of structured data to prompt LLMs to execute ICL for\npredicting and propose a context sample retrieval augmentation strategy to\nimprove performance. Additionally, we explored LLMs' AIGC and probabilistic\ncharacteristics to propose self-explanation and uncertainty quantification\nmethods for constructing a trustworthy soft sensor. Extensive experiments\ndemonstrate that our method achieved state-of-the-art predictive performance,\nstrong robustness, and flexibility, effectively mitigates training instability\nfound in traditional methods. To the best of our knowledge, this is the first\nwork to establish soft sensor utilizing LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven soft sensors are crucial in predicting key performance indicators\nin industrial systems. However, current methods predominantly rely on the\nsupervised learning paradigms of parameter updating, which inherently faces\nchallenges such as high development costs, poor robustness, training\ninstability, and lack of interpretability. Recently, large language models\n(LLMs) have demonstrated significant potential across various domains, notably\nthrough In-Context Learning (ICL), which enables high-performance task\nexecution with minimal input-label demonstrations and no prior training. This\npaper aims to replace supervised learning with the emerging ICL paradigm for\nsoft sensor modeling to address existing challenges and explore new avenues for\nadvancement. To achieve this, we propose a novel framework called the Few-shot\nUncertainty-aware and self-Explaining Soft Sensor (LLM-FUESS), which includes\nthe Zero-shot Auxiliary Variable Selector (LLM-ZAVS) and the Uncertainty-aware\nFew-shot Soft Sensor (LLM-UFSS). The LLM-ZAVS retrieves from the Industrial\nKnowledge Vector Storage to enhance LLMs' domain-specific knowledge, enabling\nzero-shot auxiliary variable selection. In the LLM-UFSS, we utilize text-based\ncontext demonstrations of structured data to prompt LLMs to execute ICL for\npredicting and propose a context sample retrieval augmentation strategy to\nimprove performance. Additionally, we explored LLMs' AIGC and probabilistic\ncharacteristics to propose self-explanation and uncertainty quantification\nmethods for constructing a trustworthy soft sensor. Extensive experiments\ndemonstrate that our method achieved state-of-the-art predictive performance,\nstrong robustness, and flexibility, effectively mitigates training instability\nfound in traditional methods. To the best of our knowledge, this is the first\nwork to establish soft sensor utilizing LLMs."
                },
                "authors": [
                    {
                        "name": "Shuo Tong"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Runyuan Guo"
                    },
                    {
                        "name": "Wenqing Wang"
                    },
                    {
                        "name": "Xueqiong Tian"
                    },
                    {
                        "name": "Lingyun Wei"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Huayong Wu"
                    },
                    {
                        "name": "Ding Liu"
                    },
                    {
                        "name": "Youmin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Youmin Zhang"
                },
                "author": "Youmin Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03295v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03295v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04266v1",
                "updated": "2025-01-08T04:19:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    4,
                    19,
                    57,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T04:19:57Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    4,
                    19,
                    57,
                    2,
                    8,
                    0
                ],
                "title": "Scaling Large Language Model Training on Frontier with Low-Bandwidth\n  Partitioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Large Language Model Training on Frontier with Low-Bandwidth\n  Partitioning"
                },
                "summary": "Scaling up Large Language Model(LLM) training involves fitting a tremendous\namount of training parameters across a limited number of workers. However,\nmethods like ZeRO-3 that drastically reduce GPU memory pressure often incur\nheavy communication to ensure global synchronization and consistency.\nEstablished efforts such as ZeRO++ use secondary partitions to avoid inter-node\ncommunications, given that intra-node GPU-GPU transfer generally has more\nbandwidth and lower latency than inter-node connections. However, as more\ncapable infrastructure like Frontier, equipped with AMD GPUs, emerged with\nimpressive computing capability, there is a need for investigations on the\nhardware topology and to develop targeted strategies to improve training\nefficiency. In this work, we propose a collection of communication and\noptimization strategies for ZeRO++ to reduce communication costs and improve\nmemory utilization. In this paper, we propose a 3-level hierarchical\npartitioning specifically for the current Top-1 supercomputing cluster,\nFrontier, which aims at leveraging various bandwidths across layers of\ncommunications (GCD-GCD, GPU-GPU, and inter-node) to reduce communication\noverhead. For a 20B GPT model, we observe a 1.71x increase in TFLOPS per GPU\nwhen compared with ZeRO++ up to 384 GCDs and a scaling efficiency of 0.94 for\nup to 384 GCDs. To the best of our knowledge, our work is also the first effort\nto efficiently optimize LLM workloads on Frontier AMD GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling up Large Language Model(LLM) training involves fitting a tremendous\namount of training parameters across a limited number of workers. However,\nmethods like ZeRO-3 that drastically reduce GPU memory pressure often incur\nheavy communication to ensure global synchronization and consistency.\nEstablished efforts such as ZeRO++ use secondary partitions to avoid inter-node\ncommunications, given that intra-node GPU-GPU transfer generally has more\nbandwidth and lower latency than inter-node connections. However, as more\ncapable infrastructure like Frontier, equipped with AMD GPUs, emerged with\nimpressive computing capability, there is a need for investigations on the\nhardware topology and to develop targeted strategies to improve training\nefficiency. In this work, we propose a collection of communication and\noptimization strategies for ZeRO++ to reduce communication costs and improve\nmemory utilization. In this paper, we propose a 3-level hierarchical\npartitioning specifically for the current Top-1 supercomputing cluster,\nFrontier, which aims at leveraging various bandwidths across layers of\ncommunications (GCD-GCD, GPU-GPU, and inter-node) to reduce communication\noverhead. For a 20B GPT model, we observe a 1.71x increase in TFLOPS per GPU\nwhen compared with ZeRO++ up to 384 GCDs and a scaling efficiency of 0.94 for\nup to 384 GCDs. To the best of our knowledge, our work is also the first effort\nto efficiently optimize LLM workloads on Frontier AMD GPUs."
                },
                "authors": [
                    {
                        "name": "Lang Xu"
                    },
                    {
                        "name": "Quentin Anthony"
                    },
                    {
                        "name": "Jacob Hatef"
                    },
                    {
                        "name": "Aamir Shafi"
                    },
                    {
                        "name": "Hari Subramoni"
                    },
                    {
                        "name": "Dhabaleswar K."
                    },
                    {
                        "name": "Panda"
                    }
                ],
                "author_detail": {
                    "name": "Panda"
                },
                "arxiv_affiliation": "DK",
                "author": "Panda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14795v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14795v5",
                "updated": "2025-01-08T03:56:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    3,
                    56,
                    26,
                    2,
                    8,
                    0
                ],
                "published": "2024-04-23T07:19:20Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    7,
                    19,
                    20,
                    1,
                    114,
                    0
                ],
                "title": "Watch Out for Your Guidance on Generation! Exploring Conditional\n  Backdoor Attacks against Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watch Out for Your Guidance on Generation! Exploring Conditional\n  Backdoor Attacks against Large Language Models"
                },
                "summary": "Mainstream backdoor attacks on large language models (LLMs) typically set a\nfixed trigger in the input instance and specific responses for triggered\nqueries. However, the fixed trigger setting (e.g., unusual words) may be easily\ndetected by human detection, limiting the effectiveness and practicality in\nreal-world scenarios. To enhance the stealthiness of backdoor activation, we\npresent a new poisoning paradigm against LLMs triggered by specifying\ngeneration conditions, which are commonly adopted strategies by users during\nmodel inference. The poisoned model performs normally for output under\nnormal/other generation conditions, while becomes harmful for output under\ntarget generation conditions. To achieve this objective, we introduce BrieFool,\nan efficient attack framework. It leverages the characteristics of generation\nconditions by efficient instruction sampling and poisoning data generation,\nthereby influencing the behavior of LLMs under target conditions. Our attack\ncan be generally divided into two types with different targets: Safety\nunalignment attack and Ability degradation attack. Our extensive experiments\ndemonstrate that BrieFool is effective across safety domains and ability\ndomains, achieving higher success rates than baseline methods, with 94.3 % on\nGPT-3.5-turbo",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mainstream backdoor attacks on large language models (LLMs) typically set a\nfixed trigger in the input instance and specific responses for triggered\nqueries. However, the fixed trigger setting (e.g., unusual words) may be easily\ndetected by human detection, limiting the effectiveness and practicality in\nreal-world scenarios. To enhance the stealthiness of backdoor activation, we\npresent a new poisoning paradigm against LLMs triggered by specifying\ngeneration conditions, which are commonly adopted strategies by users during\nmodel inference. The poisoned model performs normally for output under\nnormal/other generation conditions, while becomes harmful for output under\ntarget generation conditions. To achieve this objective, we introduce BrieFool,\nan efficient attack framework. It leverages the characteristics of generation\nconditions by efficient instruction sampling and poisoning data generation,\nthereby influencing the behavior of LLMs under target conditions. Our attack\ncan be generally divided into two types with different targets: Safety\nunalignment attack and Ability degradation attack. Our extensive experiments\ndemonstrate that BrieFool is effective across safety domains and ability\ndomains, achieving higher success rates than baseline methods, with 94.3 % on\nGPT-3.5-turbo"
                },
                "authors": [
                    {
                        "name": "Jiaming He"
                    },
                    {
                        "name": "Wenbo Jiang"
                    },
                    {
                        "name": "Guanyu Hou"
                    },
                    {
                        "name": "Wenshu Fan"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Hongwei Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongwei Li"
                },
                "author": "Hongwei Li",
                "arxiv_comment": "The paper has been accepted to AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.14795v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14795v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03271v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03271v2",
                "updated": "2025-01-08T03:51:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    3,
                    51,
                    59,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-05T00:08:52Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    0,
                    8,
                    52,
                    6,
                    5,
                    0
                ],
                "title": "DPO Kernels: A Semantically-Aware, Kernel-Enhanced, and Divergence-Rich\n  Paradigm for Direct Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DPO Kernels: A Semantically-Aware, Kernel-Enhanced, and Divergence-Rich\n  Paradigm for Direct Preference Optimization"
                },
                "summary": "The rapid rise of large language models (LLMs) has unlocked many applications\nbut also underscores the challenge of aligning them with diverse values and\npreferences. Direct Preference Optimization (DPO) is central to alignment but\nconstrained by fixed divergences and limited feature transformations. We\npropose DPO-Kernels, which integrates kernel methods to address these issues\nthrough four key contributions: (i) Kernelized Representations with polynomial,\nRBF, Mahalanobis, and spectral kernels for richer transformations, plus a\nhybrid loss combining embedding-based and probability-based objectives; (ii)\nDivergence Alternatives (Jensen-Shannon, Hellinger, Renyi, Bhattacharyya,\nWasserstein, and f-divergences) for greater stability; (iii) Data-Driven\nSelection metrics that automatically choose the best kernel-divergence pair;\nand (iv) a Hierarchical Mixture of Kernels for both local precision and global\nmodeling. Evaluations on 12 datasets demonstrate state-of-the-art performance\nin factuality, safety, reasoning, and instruction following. Grounded in\nHeavy-Tailed Self-Regularization, DPO-Kernels maintains robust generalization\nfor LLMs, offering a comprehensive resource for further alignment research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid rise of large language models (LLMs) has unlocked many applications\nbut also underscores the challenge of aligning them with diverse values and\npreferences. Direct Preference Optimization (DPO) is central to alignment but\nconstrained by fixed divergences and limited feature transformations. We\npropose DPO-Kernels, which integrates kernel methods to address these issues\nthrough four key contributions: (i) Kernelized Representations with polynomial,\nRBF, Mahalanobis, and spectral kernels for richer transformations, plus a\nhybrid loss combining embedding-based and probability-based objectives; (ii)\nDivergence Alternatives (Jensen-Shannon, Hellinger, Renyi, Bhattacharyya,\nWasserstein, and f-divergences) for greater stability; (iii) Data-Driven\nSelection metrics that automatically choose the best kernel-divergence pair;\nand (iv) a Hierarchical Mixture of Kernels for both local precision and global\nmodeling. Evaluations on 12 datasets demonstrate state-of-the-art performance\nin factuality, safety, reasoning, and instruction following. Grounded in\nHeavy-Tailed Self-Regularization, DPO-Kernels maintains robust generalization\nfor LLMs, offering a comprehensive resource for further alignment research."
                },
                "authors": [
                    {
                        "name": "Amitava Das"
                    },
                    {
                        "name": "Suranjana Trivedy"
                    },
                    {
                        "name": "Danush Khanna"
                    },
                    {
                        "name": "Rajarshi Roy"
                    },
                    {
                        "name": "Gurpreet Singh"
                    },
                    {
                        "name": "Basab Ghosh"
                    },
                    {
                        "name": "Yaswanth Narsupalli"
                    },
                    {
                        "name": "Vinija Jain"
                    },
                    {
                        "name": "Vasu Sharma"
                    },
                    {
                        "name": "Aishwarya Naresh Reganti"
                    },
                    {
                        "name": "Aman Chadha"
                    }
                ],
                "author_detail": {
                    "name": "Aman Chadha"
                },
                "author": "Aman Chadha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03271v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03271v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T45",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04249v1",
                "updated": "2025-01-08T03:15:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    3,
                    15,
                    10,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T03:15:10Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    3,
                    15,
                    10,
                    2,
                    8,
                    0
                ],
                "title": "IOLBENCH: Benchmarking LLMs on Linguistic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IOLBENCH: Benchmarking LLMs on Linguistic Reasoning"
                },
                "summary": "Despite the remarkable advancements and widespread applications of deep\nneural networks, their ability to perform reasoning tasks remains limited,\nparticularly in domains requiring structured, abstract thought. In this paper,\nwe investigate the linguistic reasoning capabilities of state-of-the-art large\nlanguage models (LLMs) by introducing IOLBENCH, a novel benchmark derived from\nInternational Linguistics Olympiad (IOL) problems. This dataset encompasses\ndiverse problems testing syntax, morphology, phonology, and semantics, all\ncarefully designed to be self-contained and independent of external knowledge.\nThese tasks challenge models to engage in metacognitive linguistic reasoning,\nrequiring the deduction of linguistic rules and patterns from minimal examples.\nThrough extensive benchmarking of leading LLMs, we find that even the most\nadvanced models struggle to handle the intricacies of linguistic complexity,\nparticularly in areas demanding compositional generalization and rule\nabstraction. Our analysis highlights both the strengths and persistent\nlimitations of current models in linguistic problem-solving, offering valuable\ninsights into their reasoning capabilities. By introducing IOLBENCH, we aim to\nfoster further research into developing models capable of human-like reasoning,\nwith broader implications for the fields of computational linguistics and\nartificial intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable advancements and widespread applications of deep\nneural networks, their ability to perform reasoning tasks remains limited,\nparticularly in domains requiring structured, abstract thought. In this paper,\nwe investigate the linguistic reasoning capabilities of state-of-the-art large\nlanguage models (LLMs) by introducing IOLBENCH, a novel benchmark derived from\nInternational Linguistics Olympiad (IOL) problems. This dataset encompasses\ndiverse problems testing syntax, morphology, phonology, and semantics, all\ncarefully designed to be self-contained and independent of external knowledge.\nThese tasks challenge models to engage in metacognitive linguistic reasoning,\nrequiring the deduction of linguistic rules and patterns from minimal examples.\nThrough extensive benchmarking of leading LLMs, we find that even the most\nadvanced models struggle to handle the intricacies of linguistic complexity,\nparticularly in areas demanding compositional generalization and rule\nabstraction. Our analysis highlights both the strengths and persistent\nlimitations of current models in linguistic problem-solving, offering valuable\ninsights into their reasoning capabilities. By introducing IOLBENCH, we aim to\nfoster further research into developing models capable of human-like reasoning,\nwith broader implications for the fields of computational linguistics and\nartificial intelligence."
                },
                "authors": [
                    {
                        "name": "Satyam Goyal"
                    },
                    {
                        "name": "Soham Dan"
                    }
                ],
                "author_detail": {
                    "name": "Soham Dan"
                },
                "author": "Soham Dan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16950v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16950v4",
                "updated": "2025-01-08T03:14:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    3,
                    14,
                    4,
                    2,
                    8,
                    0
                ],
                "published": "2024-03-25T17:11:28Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    17,
                    11,
                    28,
                    0,
                    85,
                    0
                ],
                "title": "Aligning with Human Judgement: The Role of Pairwise Large Language Model\n  Evaluators in Preference Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning with Human Judgement: The Role of Pairwise Large Language Model\n  Evaluators in Preference Aggregation"
                },
                "summary": "Large Language Models (LLMs) have demonstrated promising capabilities as\nautomatic evaluators in assessing the quality of generated natural language.\nHowever, LLMs still exhibit biases in evaluation and often struggle to generate\ncoherent evaluations that align with human assessments. In this work, we first\nconduct a systematic study of the misalignment between LLM evaluators and human\njudgement, revealing that existing calibration methods aimed at mitigating\nbiases are insufficient for effectively aligning LLM evaluators. Inspired by\nthe use of preference data in RLHF, we formulate the evaluation as a ranking\nproblem and introduce Pairwise-preference Search (PairS), an uncertainty-guided\nsearch method that employs LLMs to conduct pairwise comparisons and efficiently\nranks candidate texts. PairS achieves state-of-the-art performance on\nrepresentative evaluation tasks and demonstrates significant improvements over\ndirect scoring. Furthermore, we provide insights into the role of pairwise\npreference in quantifying the transitivity of LLMs and demonstrate how PairS\nbenefits from calibration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated promising capabilities as\nautomatic evaluators in assessing the quality of generated natural language.\nHowever, LLMs still exhibit biases in evaluation and often struggle to generate\ncoherent evaluations that align with human assessments. In this work, we first\nconduct a systematic study of the misalignment between LLM evaluators and human\njudgement, revealing that existing calibration methods aimed at mitigating\nbiases are insufficient for effectively aligning LLM evaluators. Inspired by\nthe use of preference data in RLHF, we formulate the evaluation as a ranking\nproblem and introduce Pairwise-preference Search (PairS), an uncertainty-guided\nsearch method that employs LLMs to conduct pairwise comparisons and efficiently\nranks candidate texts. PairS achieves state-of-the-art performance on\nrepresentative evaluation tasks and demonstrates significant improvements over\ndirect scoring. Furthermore, we provide insights into the role of pairwise\npreference in quantifying the transitivity of LLMs and demonstrate how PairS\nbenefits from calibration."
                },
                "authors": [
                    {
                        "name": "Yinhong Liu"
                    },
                    {
                        "name": "Han Zhou"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Ehsan Shareghi"
                    },
                    {
                        "name": "Ivan Vulić"
                    },
                    {
                        "name": "Anna Korhonen"
                    },
                    {
                        "name": "Nigel Collier"
                    }
                ],
                "author_detail": {
                    "name": "Nigel Collier"
                },
                "author": "Nigel Collier",
                "arxiv_comment": "This paper has been accepted by COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.16950v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16950v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.07594v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.07594v3",
                "updated": "2025-01-08T02:33:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    2,
                    33,
                    37,
                    2,
                    8,
                    0
                ],
                "published": "2023-11-10T09:51:24Z",
                "published_parsed": [
                    2023,
                    11,
                    10,
                    9,
                    51,
                    24,
                    4,
                    314,
                    0
                ],
                "title": "How to Bridge the Gap between Modalities: Survey on Multimodal Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Bridge the Gap between Modalities: Survey on Multimodal Large\n  Language Model"
                },
                "summary": "We explore Multimodal Large Language Models (MLLMs), which integrate LLMs\nlike GPT-4 to handle multimodal data, including text, images, audio, and more.\nMLLMs demonstrate capabilities such as generating image captions and answering\nimage-based questions, bridging the gap towards real-world human-computer\ninteractions and hinting at a potential pathway to artificial general\nintelligence. However, MLLMs still face challenges in addressing the semantic\ngap in multimodal data, which may lead to erroneous outputs, posing potential\nrisks to society. Selecting the appropriate modality alignment method is\ncrucial, as improper methods might require more parameters without significant\nperformance improvements. This paper aims to explore modality alignment methods\nfor LLMs and their current capabilities. Implementing effective modality\nalignment can help LLMs address environmental issues and enhance accessibility.\nThe study surveys existing modality alignment methods for MLLMs, categorizing\nthem into four groups: (1) Multimodal Converter, which transforms data into a\nformat that LLMs can understand; (2) Multimodal Perceiver, which improves how\nLLMs percieve different types of data; (3) Tool Learning, which leverages\nexternal tools to convert data into a common format, usually text; and (4)\nData-Driven Method, which teaches LLMs to understand specific data types within\ndatasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore Multimodal Large Language Models (MLLMs), which integrate LLMs\nlike GPT-4 to handle multimodal data, including text, images, audio, and more.\nMLLMs demonstrate capabilities such as generating image captions and answering\nimage-based questions, bridging the gap towards real-world human-computer\ninteractions and hinting at a potential pathway to artificial general\nintelligence. However, MLLMs still face challenges in addressing the semantic\ngap in multimodal data, which may lead to erroneous outputs, posing potential\nrisks to society. Selecting the appropriate modality alignment method is\ncrucial, as improper methods might require more parameters without significant\nperformance improvements. This paper aims to explore modality alignment methods\nfor LLMs and their current capabilities. Implementing effective modality\nalignment can help LLMs address environmental issues and enhance accessibility.\nThe study surveys existing modality alignment methods for MLLMs, categorizing\nthem into four groups: (1) Multimodal Converter, which transforms data into a\nformat that LLMs can understand; (2) Multimodal Perceiver, which improves how\nLLMs percieve different types of data; (3) Tool Learning, which leverages\nexternal tools to convert data into a common format, usually text; and (4)\nData-Driven Method, which teaches LLMs to understand specific data types within\ndatasets."
                },
                "authors": [
                    {
                        "name": "Shezheng Song"
                    },
                    {
                        "name": "Xiaopeng Li"
                    },
                    {
                        "name": "Shasha Li"
                    },
                    {
                        "name": "Shan Zhao"
                    },
                    {
                        "name": "Jie Yu"
                    },
                    {
                        "name": "Jun Ma"
                    },
                    {
                        "name": "Xiaoguang Mao"
                    },
                    {
                        "name": "Weimin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weimin Zhang"
                },
                "author": "Weimin Zhang",
                "arxiv_comment": "Accepted by TKDE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.07594v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.07594v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04239v1",
                "updated": "2025-01-08T02:32:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    2,
                    32,
                    48,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T02:32:48Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    2,
                    32,
                    48,
                    2,
                    8,
                    0
                ],
                "title": "Dynamic Localisation of Spatial-Temporal Graph Neural Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Localisation of Spatial-Temporal Graph Neural Network"
                },
                "summary": "Spatial-temporal data, fundamental to many intelligent applications, reveals\ndependencies indicating causal links between present measurements at specific\nlocations and historical data at the same or other locations. Within this\ncontext, adaptive spatial-temporal graph neural networks (ASTGNNs) have emerged\nas valuable tools for modelling these dependencies, especially through a\ndata-driven approach rather than pre-defined spatial graphs. While this\napproach offers higher accuracy, it presents increased computational demands.\nAddressing this challenge, this paper delves into the concept of localisation\nwithin ASTGNNs, introducing an innovative perspective that spatial dependencies\nshould be dynamically evolving over time. We introduce \\textit{DynAGS}, a\nlocalised ASTGNN framework aimed at maximising efficiency and accuracy in\ndistributed deployment. This framework integrates dynamic localisation,\ntime-evolving spatial graphs, and personalised localisation, all orchestrated\naround the Dynamic Graph Generator, a light-weighted central module leveraging\ncross attention. The central module can integrate historical information in a\nnode-independent manner to enhance the feature representation of nodes at the\ncurrent moment. This improved feature representation is then used to generate a\ndynamic sparse graph without the need for costly data exchanges, and it\nsupports personalised localisation. Performance assessments across two core\nASTGNN architectures and nine real-world datasets from various applications\nreveal that \\textit{DynAGS} outshines current benchmarks, underscoring that the\ndynamic modelling of spatial dependencies can drastically improve model\nexpressibility, flexibility, and system efficiency, especially in distributed\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial-temporal data, fundamental to many intelligent applications, reveals\ndependencies indicating causal links between present measurements at specific\nlocations and historical data at the same or other locations. Within this\ncontext, adaptive spatial-temporal graph neural networks (ASTGNNs) have emerged\nas valuable tools for modelling these dependencies, especially through a\ndata-driven approach rather than pre-defined spatial graphs. While this\napproach offers higher accuracy, it presents increased computational demands.\nAddressing this challenge, this paper delves into the concept of localisation\nwithin ASTGNNs, introducing an innovative perspective that spatial dependencies\nshould be dynamically evolving over time. We introduce \\textit{DynAGS}, a\nlocalised ASTGNN framework aimed at maximising efficiency and accuracy in\ndistributed deployment. This framework integrates dynamic localisation,\ntime-evolving spatial graphs, and personalised localisation, all orchestrated\naround the Dynamic Graph Generator, a light-weighted central module leveraging\ncross attention. The central module can integrate historical information in a\nnode-independent manner to enhance the feature representation of nodes at the\ncurrent moment. This improved feature representation is then used to generate a\ndynamic sparse graph without the need for costly data exchanges, and it\nsupports personalised localisation. Performance assessments across two core\nASTGNN architectures and nine real-world datasets from various applications\nreveal that \\textit{DynAGS} outshines current benchmarks, underscoring that the\ndynamic modelling of spatial dependencies can drastically improve model\nexpressibility, flexibility, and system efficiency, especially in distributed\nsettings."
                },
                "authors": [
                    {
                        "name": "Wenying Duan"
                    },
                    {
                        "name": "Shujun Guo"
                    },
                    {
                        "name": "Wei huang"
                    },
                    {
                        "name": "Hong Rao"
                    },
                    {
                        "name": "Xiaoxi He"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxi He"
                },
                "author": "Xiaoxi He",
                "arxiv_comment": "This paper was accepted by KDD'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04234v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04234v1",
                "updated": "2025-01-08T02:17:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    2,
                    17,
                    34,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T02:17:34Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    2,
                    17,
                    34,
                    2,
                    8,
                    0
                ],
                "title": "Statistical Uncertainty Quantification for Aggregate Performance Metrics\n  in Machine Learning Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Uncertainty Quantification for Aggregate Performance Metrics\n  in Machine Learning Benchmarks"
                },
                "summary": "Modern artificial intelligence is supported by machine learning models (e.g.,\nfoundation models) that are pretrained on a massive data corpus and then\nadapted to solve a variety of downstream tasks. To summarize performance across\nmultiple tasks, evaluation metrics are often aggregated into a summary metric,\ne.g., average accuracy across 10 question-answering tasks. When aggregating\nevaluation metrics, it is useful to incorporate uncertainty in the aggregate\nmetric in order to gain a more realistic understanding of model performance.\nOur objective in this work is to demonstrate how statistical methodology can be\nused for quantifying uncertainty in metrics that have been aggregated across\nmultiple tasks. The methods we emphasize are bootstrapping, Bayesian\nhierarchical (i.e., multilevel) modeling, and the visualization of task\nweightings that consider standard errors. These techniques reveal insights such\nas the dominance of a specific model for certain types of tasks despite an\noverall poor performance. We use a popular ML benchmark, the Visual Task\nAdaptation Benchmark (VTAB), to demonstrate the usefulness of our approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern artificial intelligence is supported by machine learning models (e.g.,\nfoundation models) that are pretrained on a massive data corpus and then\nadapted to solve a variety of downstream tasks. To summarize performance across\nmultiple tasks, evaluation metrics are often aggregated into a summary metric,\ne.g., average accuracy across 10 question-answering tasks. When aggregating\nevaluation metrics, it is useful to incorporate uncertainty in the aggregate\nmetric in order to gain a more realistic understanding of model performance.\nOur objective in this work is to demonstrate how statistical methodology can be\nused for quantifying uncertainty in metrics that have been aggregated across\nmultiple tasks. The methods we emphasize are bootstrapping, Bayesian\nhierarchical (i.e., multilevel) modeling, and the visualization of task\nweightings that consider standard errors. These techniques reveal insights such\nas the dominance of a specific model for certain types of tasks despite an\noverall poor performance. We use a popular ML benchmark, the Visual Task\nAdaptation Benchmark (VTAB), to demonstrate the usefulness of our approaches."
                },
                "authors": [
                    {
                        "name": "Rachel Longjohn"
                    },
                    {
                        "name": "Giri Gopalan"
                    },
                    {
                        "name": "Emily Casleton"
                    }
                ],
                "author_detail": {
                    "name": "Emily Casleton"
                },
                "author": "Emily Casleton",
                "arxiv_comment": "LA-UR-24-25289; presented at the Workshop on Statistical Frontiers in\n  LLMs and Foundation Models at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04234v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13586v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13586v2",
                "updated": "2025-01-08T02:09:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    2,
                    9,
                    15,
                    2,
                    8,
                    0
                ],
                "published": "2024-08-24T14:14:32Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    14,
                    14,
                    32,
                    5,
                    237,
                    0
                ],
                "title": "Balancing Diversity and Risk in LLM Sampling: How to Select Your Method\n  and Parameter for Open-Ended Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Balancing Diversity and Risk in LLM Sampling: How to Select Your Method\n  and Parameter for Open-Ended Text Generation"
                },
                "summary": "Sampling-based decoding strategies have been widely adopted for Large\nLanguage Models (LLMs) in numerous applications, targeting a balance between\ndiversity and quality via temperature tuning and tail truncation. Considering\nthe strong dependency of the candidate next tokens on different prefixes,\nrecent studies propose to adaptively truncate the tail of LLMs' predicted\ndistribution. Although improved results have been reported with these methods\non open-ended text generation tasks, the results are highly dependent on the\ncurated parameters and the limited exemplar text. In this paper, we propose a\nsystematic way to estimate the capacity of a truncation sampling method by\nconsidering the trade-off between diversity and risk at each decoding step,\nbased on our collected prefix tree which preserves the context of a full\nsentence. Our work offers a comprehensive comparison of existing truncation\nsampling methods and serves as a practical user guideline for their parameter\nselection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sampling-based decoding strategies have been widely adopted for Large\nLanguage Models (LLMs) in numerous applications, targeting a balance between\ndiversity and quality via temperature tuning and tail truncation. Considering\nthe strong dependency of the candidate next tokens on different prefixes,\nrecent studies propose to adaptively truncate the tail of LLMs' predicted\ndistribution. Although improved results have been reported with these methods\non open-ended text generation tasks, the results are highly dependent on the\ncurated parameters and the limited exemplar text. In this paper, we propose a\nsystematic way to estimate the capacity of a truncation sampling method by\nconsidering the trade-off between diversity and risk at each decoding step,\nbased on our collected prefix tree which preserves the context of a full\nsentence. Our work offers a comprehensive comparison of existing truncation\nsampling methods and serves as a practical user guideline for their parameter\nselection."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhou"
                    },
                    {
                        "name": "Margret Keuper"
                    },
                    {
                        "name": "Mario Fritz"
                    }
                ],
                "author_detail": {
                    "name": "Mario Fritz"
                },
                "author": "Mario Fritz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13586v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13586v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04227v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04227v1",
                "updated": "2025-01-08T01:58:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    1,
                    58,
                    42,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T01:58:42Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    1,
                    58,
                    42,
                    2,
                    8,
                    0
                ],
                "title": "Agent Laboratory: Using LLM Agents as Research Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent Laboratory: Using LLM Agents as Research Assistants"
                },
                "summary": "Historically, scientific discovery has been a lengthy and costly process,\ndemanding substantial time and resources from initial conception to final\nresults. To accelerate scientific discovery, reduce research costs, and improve\nresearch quality, we introduce Agent Laboratory, an autonomous LLM-based\nframework capable of completing the entire research process. This framework\naccepts a human-provided research idea and progresses through three\nstages--literature review, experimentation, and report writing to produce\ncomprehensive research outputs, including a code repository and a research\nreport, while enabling users to provide feedback and guidance at each stage. We\ndeploy Agent Laboratory with various state-of-the-art LLMs and invite multiple\nresearchers to assess its quality by participating in a survey, providing human\nfeedback to guide the research process, and then evaluate the final paper. We\nfound that: (1) Agent Laboratory driven by o1-preview generates the best\nresearch outcomes; (2) The generated machine learning code is able to achieve\nstate-of-the-art performance compared to existing methods; (3) Human\ninvolvement, providing feedback at each stage, significantly improves the\noverall quality of research; (4) Agent Laboratory significantly reduces\nresearch expenses, achieving an 84% decrease compared to previous autonomous\nresearch methods. We hope Agent Laboratory enables researchers to allocate more\neffort toward creative ideation rather than low-level coding and writing,\nultimately accelerating scientific discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Historically, scientific discovery has been a lengthy and costly process,\ndemanding substantial time and resources from initial conception to final\nresults. To accelerate scientific discovery, reduce research costs, and improve\nresearch quality, we introduce Agent Laboratory, an autonomous LLM-based\nframework capable of completing the entire research process. This framework\naccepts a human-provided research idea and progresses through three\nstages--literature review, experimentation, and report writing to produce\ncomprehensive research outputs, including a code repository and a research\nreport, while enabling users to provide feedback and guidance at each stage. We\ndeploy Agent Laboratory with various state-of-the-art LLMs and invite multiple\nresearchers to assess its quality by participating in a survey, providing human\nfeedback to guide the research process, and then evaluate the final paper. We\nfound that: (1) Agent Laboratory driven by o1-preview generates the best\nresearch outcomes; (2) The generated machine learning code is able to achieve\nstate-of-the-art performance compared to existing methods; (3) Human\ninvolvement, providing feedback at each stage, significantly improves the\noverall quality of research; (4) Agent Laboratory significantly reduces\nresearch expenses, achieving an 84% decrease compared to previous autonomous\nresearch methods. We hope Agent Laboratory enables researchers to allocate more\neffort toward creative ideation rather than low-level coding and writing,\nultimately accelerating scientific discovery."
                },
                "authors": [
                    {
                        "name": "Samuel Schmidgall"
                    },
                    {
                        "name": "Yusheng Su"
                    },
                    {
                        "name": "Ze Wang"
                    },
                    {
                        "name": "Ximeng Sun"
                    },
                    {
                        "name": "Jialian Wu"
                    },
                    {
                        "name": "Xiaodong Yu"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04227v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04227v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00790v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00790v2",
                "updated": "2025-01-07T23:43:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    23,
                    43,
                    9,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-01T10:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    10,
                    0,
                    49,
                    2,
                    1,
                    0
                ],
                "title": "LENS-XAI: Redefining Lightweight and Explainable Network Security\n  through Knowledge Distillation and Variational Autoencoders for Scalable\n  Intrusion Detection in Cybersecurity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LENS-XAI: Redefining Lightweight and Explainable Network Security\n  through Knowledge Distillation and Variational Autoencoders for Scalable\n  Intrusion Detection in Cybersecurity"
                },
                "summary": "The rapid proliferation of Industrial Internet of Things (IIoT) systems\nnecessitates advanced, interpretable, and scalable intrusion detection systems\n(IDS) to combat emerging cyber threats. Traditional IDS face challenges such as\nhigh computational demands, limited explainability, and inflexibility against\nevolving attack patterns. To address these limitations, this study introduces\nthe Lightweight Explainable Network Security framework (LENS-XAI), which\ncombines robust intrusion detection with enhanced interpretability and\nscalability. LENS-XAI integrates knowledge distillation, variational\nautoencoder models, and attribution-based explainability techniques to achieve\nhigh detection accuracy and transparency in decision-making. By leveraging a\ntraining set comprising 10% of the available data, the framework optimizes\ncomputational efficiency without sacrificing performance. Experimental\nevaluation on four benchmark datasets: Edge-IIoTset, UKM-IDS20, CTU-13, and\nNSL-KDD, demonstrates the framework's superior performance, achieving detection\naccuracies of 95.34%, 99.92%, 98.42%, and 99.34%, respectively. Additionally,\nthe framework excels in reducing false positives and adapting to complex attack\nscenarios, outperforming existing state-of-the-art methods. Key strengths of\nLENS-XAI include its lightweight design, suitable for resource-constrained\nenvironments, and its scalability across diverse IIoT and cybersecurity\ncontexts. Moreover, the explainability module enhances trust and transparency,\ncritical for practical deployment in dynamic and sensitive applications. This\nresearch contributes significantly to advancing IDS by addressing computational\nefficiency, feature interpretability, and real-world applicability. Future work\ncould focus on extending the framework to ensemble AI systems for distributed\nenvironments, further enhancing its robustness and adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of Industrial Internet of Things (IIoT) systems\nnecessitates advanced, interpretable, and scalable intrusion detection systems\n(IDS) to combat emerging cyber threats. Traditional IDS face challenges such as\nhigh computational demands, limited explainability, and inflexibility against\nevolving attack patterns. To address these limitations, this study introduces\nthe Lightweight Explainable Network Security framework (LENS-XAI), which\ncombines robust intrusion detection with enhanced interpretability and\nscalability. LENS-XAI integrates knowledge distillation, variational\nautoencoder models, and attribution-based explainability techniques to achieve\nhigh detection accuracy and transparency in decision-making. By leveraging a\ntraining set comprising 10% of the available data, the framework optimizes\ncomputational efficiency without sacrificing performance. Experimental\nevaluation on four benchmark datasets: Edge-IIoTset, UKM-IDS20, CTU-13, and\nNSL-KDD, demonstrates the framework's superior performance, achieving detection\naccuracies of 95.34%, 99.92%, 98.42%, and 99.34%, respectively. Additionally,\nthe framework excels in reducing false positives and adapting to complex attack\nscenarios, outperforming existing state-of-the-art methods. Key strengths of\nLENS-XAI include its lightweight design, suitable for resource-constrained\nenvironments, and its scalability across diverse IIoT and cybersecurity\ncontexts. Moreover, the explainability module enhances trust and transparency,\ncritical for practical deployment in dynamic and sensitive applications. This\nresearch contributes significantly to advancing IDS by addressing computational\nefficiency, feature interpretability, and real-world applicability. Future work\ncould focus on extending the framework to ensemble AI systems for distributed\nenvironments, further enhancing its robustness and adaptability."
                },
                "authors": [
                    {
                        "name": "Muhammet Anil Yagiz"
                    },
                    {
                        "name": "Polat Goktas"
                    }
                ],
                "author_detail": {
                    "name": "Polat Goktas"
                },
                "author": "Polat Goktas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00790v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00790v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15823v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15823v3",
                "updated": "2025-01-07T22:43:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    22,
                    43,
                    28,
                    1,
                    7,
                    0
                ],
                "published": "2024-06-22T11:46:04Z",
                "published_parsed": [
                    2024,
                    6,
                    22,
                    11,
                    46,
                    4,
                    5,
                    174,
                    0
                ],
                "title": "CaT-BENCH: Benchmarking Language Model Understanding of Causal and\n  Temporal Dependencies in Plans",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaT-BENCH: Benchmarking Language Model Understanding of Causal and\n  Temporal Dependencies in Plans"
                },
                "summary": "Understanding the abilities of LLMs to reason about natural language plans,\nsuch as instructional text and recipes, is critical to reliably using them in\ndecision-making systems. A fundamental aspect of plans is the temporal order in\nwhich their steps needs to be executed, which reflects the underlying causal\ndependencies between them. We introduce CaT-Bench, a benchmark of Step Order\nPrediction questions, which test whether a step must necessarily occur before\nor after another in cooking recipe plans. We use this to evaluate how well\nfrontier LLMs understand causal and temporal dependencies. We find that SOTA\nLLMs are underwhelming (best zero-shot is only 0.59 in F1), and are biased\ntowards predicting dependence more often, perhaps relying on temporal order of\nsteps as a heuristic. While prompting for explanations and using few-shot\nexamples improve performance, the best F1 result is only 0.73. Further, human\nevaluation of explanations along with answer correctness show that, on average,\nhumans do not agree with model reasoning. Surprisingly, we also find that\nexplaining after answering leads to better performance than normal\nchain-of-thought prompting, and LLM answers are not consistent across questions\nabout the same step pairs. Overall, results show that LLMs' ability to detect\ndependence between steps has significant room for improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the abilities of LLMs to reason about natural language plans,\nsuch as instructional text and recipes, is critical to reliably using them in\ndecision-making systems. A fundamental aspect of plans is the temporal order in\nwhich their steps needs to be executed, which reflects the underlying causal\ndependencies between them. We introduce CaT-Bench, a benchmark of Step Order\nPrediction questions, which test whether a step must necessarily occur before\nor after another in cooking recipe plans. We use this to evaluate how well\nfrontier LLMs understand causal and temporal dependencies. We find that SOTA\nLLMs are underwhelming (best zero-shot is only 0.59 in F1), and are biased\ntowards predicting dependence more often, perhaps relying on temporal order of\nsteps as a heuristic. While prompting for explanations and using few-shot\nexamples improve performance, the best F1 result is only 0.73. Further, human\nevaluation of explanations along with answer correctness show that, on average,\nhumans do not agree with model reasoning. Surprisingly, we also find that\nexplaining after answering leads to better performance than normal\nchain-of-thought prompting, and LLM answers are not consistent across questions\nabout the same step pairs. Overall, results show that LLMs' ability to detect\ndependence between steps has significant room for improvement."
                },
                "authors": [
                    {
                        "name": "Yash Kumar Lal"
                    },
                    {
                        "name": "Vanya Cohen"
                    },
                    {
                        "name": "Nathanael Chambers"
                    },
                    {
                        "name": "Niranjan Balasubramanian"
                    },
                    {
                        "name": "Raymond Mooney"
                    }
                ],
                "author_detail": {
                    "name": "Raymond Mooney"
                },
                "author": "Raymond Mooney",
                "arxiv_comment": "Accepted to EMNLP 2024 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15823v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15823v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04167v1",
                "updated": "2025-01-07T22:29:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    22,
                    29,
                    8,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T22:29:08Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    22,
                    29,
                    8,
                    1,
                    7,
                    0
                ],
                "title": "Reasoning-Enhanced Self-Training for Long-Form Personalized Text\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning-Enhanced Self-Training for Long-Form Personalized Text\n  Generation"
                },
                "summary": "Personalized text generation requires a unique ability of large language\nmodels (LLMs) to learn from context that they often do not encounter during\ntheir standard training. One way to encourage LLMs to better use personalized\ncontext for generating outputs that better align with the user's expectations\nis to instruct them to reason over the user's past preferences, background\nknowledge, or writing style. To achieve this, we propose Reasoning-Enhanced\nSelf-Training for Personalized Text Generation (REST-PG), a framework that\ntrains LLMs to reason over personal data during response generation. REST-PG\nfirst generates reasoning paths to train the LLM's reasoning abilities and then\nemploys Expectation-Maximization Reinforced Self-Training to iteratively train\nthe LLM based on its own high-reward outputs. We evaluate REST-PG on the\nLongLaMP benchmark, consisting of four diverse personalized long-form text\ngeneration tasks. Our experiments demonstrate that REST-PG achieves significant\nimprovements over state-of-the-art baselines, with an average relative\nperformance gain of 14.5% on the benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized text generation requires a unique ability of large language\nmodels (LLMs) to learn from context that they often do not encounter during\ntheir standard training. One way to encourage LLMs to better use personalized\ncontext for generating outputs that better align with the user's expectations\nis to instruct them to reason over the user's past preferences, background\nknowledge, or writing style. To achieve this, we propose Reasoning-Enhanced\nSelf-Training for Personalized Text Generation (REST-PG), a framework that\ntrains LLMs to reason over personal data during response generation. REST-PG\nfirst generates reasoning paths to train the LLM's reasoning abilities and then\nemploys Expectation-Maximization Reinforced Self-Training to iteratively train\nthe LLM based on its own high-reward outputs. We evaluate REST-PG on the\nLongLaMP benchmark, consisting of four diverse personalized long-form text\ngeneration tasks. Our experiments demonstrate that REST-PG achieves significant\nimprovements over state-of-the-art baselines, with an average relative\nperformance gain of 14.5% on the benchmark."
                },
                "authors": [
                    {
                        "name": "Alireza Salemi"
                    },
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Mingyang Zhang"
                    },
                    {
                        "name": "Qiaozhu Mei"
                    },
                    {
                        "name": "Weize Kong"
                    },
                    {
                        "name": "Tao Chen"
                    },
                    {
                        "name": "Zhuowan Li"
                    },
                    {
                        "name": "Michael Bendersky"
                    },
                    {
                        "name": "Hamed Zamani"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Zamani"
                },
                "author": "Hamed Zamani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04164v1",
                "updated": "2025-01-07T22:23:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    22,
                    23,
                    20,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T22:23:20Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    22,
                    23,
                    20,
                    1,
                    7,
                    0
                ],
                "title": "Holographic Metasurface-Based Beamforming for Multi-Altitude LEO\n  Satellite Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Holographic Metasurface-Based Beamforming for Multi-Altitude LEO\n  Satellite Networks"
                },
                "summary": "Low Earth Orbit (LEO) satellite networks are capable of improving the global\nInternet service coverage. In this context, we propose a hybrid beamforming\ndesign for holographic metasurface based terrestrial users in multi-altitude\nLEO satellite networks. Firstly, the holographic beamformer is optimized by\nmaximizing the downlink channel gain from the serving satellite to the\nterrestrial user. Then, the digital beamformer is designed by conceiving a\nminimum mean square error (MMSE) based detection algorithm for mitigating the\ninterference arriving from other satellites. To dispense with excessive\noverhead of full channel state information (CSI) acquisition of all satellites,\nwe propose a low-complexity MMSE beamforming algorithm that only relies on the\ndistribution of the LEO satellite constellation harnessing stochastic geometry,\nwhich can achieve comparable throughput to that of the algorithm based on the\nfull CSI in the case of a dense LEO satellite deployment. Furthermore, it\noutperforms the maximum ratio combining (MRC) algorithm, thanks to its\ninter-satellite interference mitigation capacity. The simulation results show\nthat our proposed holographic metasurface based hybrid beamforming architecture\nis capable of outperforming the state-of-the-art antenna array architecture in\nterms of its throughput, given the same physical size of the transceivers.\nMoreover, we demonstrate that the beamforming performance attained can be\nsubstantially improved by taking into account the mutual coupling effect,\nimposed by the dense placement of the holographic metasurface elements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low Earth Orbit (LEO) satellite networks are capable of improving the global\nInternet service coverage. In this context, we propose a hybrid beamforming\ndesign for holographic metasurface based terrestrial users in multi-altitude\nLEO satellite networks. Firstly, the holographic beamformer is optimized by\nmaximizing the downlink channel gain from the serving satellite to the\nterrestrial user. Then, the digital beamformer is designed by conceiving a\nminimum mean square error (MMSE) based detection algorithm for mitigating the\ninterference arriving from other satellites. To dispense with excessive\noverhead of full channel state information (CSI) acquisition of all satellites,\nwe propose a low-complexity MMSE beamforming algorithm that only relies on the\ndistribution of the LEO satellite constellation harnessing stochastic geometry,\nwhich can achieve comparable throughput to that of the algorithm based on the\nfull CSI in the case of a dense LEO satellite deployment. Furthermore, it\noutperforms the maximum ratio combining (MRC) algorithm, thanks to its\ninter-satellite interference mitigation capacity. The simulation results show\nthat our proposed holographic metasurface based hybrid beamforming architecture\nis capable of outperforming the state-of-the-art antenna array architecture in\nterms of its throughput, given the same physical size of the transceivers.\nMoreover, we demonstrate that the beamforming performance attained can be\nsubstantially improved by taking into account the mutual coupling effect,\nimposed by the dense placement of the holographic metasurface elements."
                },
                "authors": [
                    {
                        "name": "Qingchao Li"
                    },
                    {
                        "name": "Mohammed El-Hajjar"
                    },
                    {
                        "name": "Kaijun Cao"
                    },
                    {
                        "name": "Chao Xu"
                    },
                    {
                        "name": "Harald Haas"
                    },
                    {
                        "name": "Lajos Hanzo"
                    }
                ],
                "author_detail": {
                    "name": "Lajos Hanzo"
                },
                "author": "Lajos Hanzo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02086v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02086v2",
                "updated": "2025-01-07T22:03:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    22,
                    3,
                    4,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-03T20:19:14Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    20,
                    19,
                    14,
                    4,
                    3,
                    0
                ],
                "title": "Instruction-Following Pruning for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-Following Pruning for Large Language Models"
                },
                "summary": "With the rapid scaling of large language models (LLMs), structured pruning\nhas become a widely used technique to learn efficient, smaller models from\nlarger ones, delivering superior performance compared to training similarly\nsized models from scratch. In this paper, we move beyond the traditional static\npruning approach of determining a fixed pruning mask for a model, and propose a\ndynamic approach to structured pruning. In our method, the pruning mask is\ninput-dependent and adapts dynamically based on the information described in a\nuser instruction. Our approach, termed \"instruction-following pruning\",\nintroduces a sparse mask predictor that takes the user instruction as input and\ndynamically selects the most relevant model parameters for the given task. To\nidentify and activate effective parameters, we jointly optimize the sparse mask\npredictor and the LLM, leveraging both instruction-following data and the\npre-training corpus. Experimental results demonstrate the effectiveness of our\napproach on a wide range of evaluation benchmarks. For example, our 3B\nactivated model improves over the 3B dense model by 5-8 points of absolute\nmargin on domains such as math and coding, and rivals the performance of a 9B\nmodel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid scaling of large language models (LLMs), structured pruning\nhas become a widely used technique to learn efficient, smaller models from\nlarger ones, delivering superior performance compared to training similarly\nsized models from scratch. In this paper, we move beyond the traditional static\npruning approach of determining a fixed pruning mask for a model, and propose a\ndynamic approach to structured pruning. In our method, the pruning mask is\ninput-dependent and adapts dynamically based on the information described in a\nuser instruction. Our approach, termed \"instruction-following pruning\",\nintroduces a sparse mask predictor that takes the user instruction as input and\ndynamically selects the most relevant model parameters for the given task. To\nidentify and activate effective parameters, we jointly optimize the sparse mask\npredictor and the LLM, leveraging both instruction-following data and the\npre-training corpus. Experimental results demonstrate the effectiveness of our\napproach on a wide range of evaluation benchmarks. For example, our 3B\nactivated model improves over the 3B dense model by 5-8 points of absolute\nmargin on domains such as math and coding, and rivals the performance of a 9B\nmodel."
                },
                "authors": [
                    {
                        "name": "Bairu Hou"
                    },
                    {
                        "name": "Qibin Chen"
                    },
                    {
                        "name": "Jianyu Wang"
                    },
                    {
                        "name": "Guoli Yin"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Nan Du"
                    },
                    {
                        "name": "Ruoming Pang"
                    },
                    {
                        "name": "Shiyu Chang"
                    },
                    {
                        "name": "Tao Lei"
                    }
                ],
                "author_detail": {
                    "name": "Tao Lei"
                },
                "author": "Tao Lei",
                "arxiv_comment": "13 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02086v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02086v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04156v1",
                "updated": "2025-01-07T21:57:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    21,
                    57,
                    51,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T21:57:51Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    21,
                    57,
                    51,
                    1,
                    7,
                    0
                ],
                "title": "AdaptiveCoPilot: Design and Testing of a NeuroAdaptive LLM Cockpit\n  Guidance System in both Novice and Expert Pilots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaptiveCoPilot: Design and Testing of a NeuroAdaptive LLM Cockpit\n  Guidance System in both Novice and Expert Pilots"
                },
                "summary": "Pilots operating modern cockpits often face high cognitive demands due to\ncomplex interfaces and multitasking requirements, which can lead to overload\nand decreased performance. This study introduces AdaptiveCoPilot, a\nneuroadaptive guidance system that adapts visual, auditory, and textual cues in\nreal time based on the pilot's cognitive workload, measured via functional\nNear-Infrared Spectroscopy (fNIRS). A formative study with expert pilots (N=3)\nidentified adaptive rules for modality switching and information load\nadjustments during preflight tasks. These insights informed the design of\nAdaptiveCoPilot, which integrates cognitive state assessments, behavioral data,\nand adaptive strategies within a context-aware Large Language Model (LLM). The\nsystem was evaluated in a virtual reality (VR) simulated cockpit with licensed\npilots (N=8), comparing its performance against baseline and random feedback\nconditions. The results indicate that the pilots using AdaptiveCoPilot\nexhibited higher rates of optimal cognitive load states on the facets of\nworking memory and perception, along with reduced task completion times. Based\non the formative study, experimental findings, qualitative interviews, we\npropose a set of strategies for future development of neuroadaptive pilot\nguidance systems and highlight the potential of neuroadaptive systems to\nenhance pilot performance and safety in aviation environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pilots operating modern cockpits often face high cognitive demands due to\ncomplex interfaces and multitasking requirements, which can lead to overload\nand decreased performance. This study introduces AdaptiveCoPilot, a\nneuroadaptive guidance system that adapts visual, auditory, and textual cues in\nreal time based on the pilot's cognitive workload, measured via functional\nNear-Infrared Spectroscopy (fNIRS). A formative study with expert pilots (N=3)\nidentified adaptive rules for modality switching and information load\nadjustments during preflight tasks. These insights informed the design of\nAdaptiveCoPilot, which integrates cognitive state assessments, behavioral data,\nand adaptive strategies within a context-aware Large Language Model (LLM). The\nsystem was evaluated in a virtual reality (VR) simulated cockpit with licensed\npilots (N=8), comparing its performance against baseline and random feedback\nconditions. The results indicate that the pilots using AdaptiveCoPilot\nexhibited higher rates of optimal cognitive load states on the facets of\nworking memory and perception, along with reduced task completion times. Based\non the formative study, experimental findings, qualitative interviews, we\npropose a set of strategies for future development of neuroadaptive pilot\nguidance systems and highlight the potential of neuroadaptive systems to\nenhance pilot performance and safety in aviation environments."
                },
                "authors": [
                    {
                        "name": "Shaoyue Wen"
                    },
                    {
                        "name": "Michael Middleton"
                    },
                    {
                        "name": "Songming Ping"
                    },
                    {
                        "name": "Nayan N Chawla"
                    },
                    {
                        "name": "Guande Wu"
                    },
                    {
                        "name": "Bradley S Feest"
                    },
                    {
                        "name": "Chihab Nadri"
                    },
                    {
                        "name": "Yunmei Liu"
                    },
                    {
                        "name": "David Kaber"
                    },
                    {
                        "name": "Maryam Zahabi"
                    },
                    {
                        "name": "Ryan P. McMahan"
                    },
                    {
                        "name": "Sonia Castelo"
                    },
                    {
                        "name": "Ryan Mckendrick"
                    },
                    {
                        "name": "Jing Qian"
                    },
                    {
                        "name": "Claudio Silva"
                    }
                ],
                "author_detail": {
                    "name": "Claudio Silva"
                },
                "author": "Claudio Silva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.1.2; I.2.1; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.17218v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.17218v2",
                "updated": "2025-01-07T21:57:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    21,
                    57,
                    38,
                    1,
                    7,
                    0
                ],
                "published": "2024-03-25T21:47:36Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    21,
                    47,
                    36,
                    0,
                    85,
                    0
                ],
                "title": "To Err is Machine: Vulnerability Detection Challenges LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Err is Machine: Vulnerability Detection Challenges LLM Reasoning"
                },
                "summary": "In this paper, we present a challenging code reasoning task: vulnerability\ndetection. Large Language Models (LLMs) have shown promising results in\nnatural-language and math reasoning, but state-of-the-art (SOTA) models\nreported only 54.5% Balanced Accuracy in our vulnerability detection\nevaluation, even those models pre-trained on large amounts of source code. Our\nerror analysis on LLM responses shows that the models struggle to reason about\nthe code semantics relevant to identifying vulnerabilities, especially subtle\nsemantic differences caused by small textual changes. We explored prominent\nmodels and training settings to understand their effects on vulnerability\ndetection performance -- including better prompts, larger models, more\npre-training data, and fine-tuning -- but none led to significant improvements.\nThis raises the question of whether simply scaling training data and model size\nwill allow us to \"solve\" complex code reasoning tasks like vulnerability\ndetection, or if a fundamental shift in modeling and training techniques is\nrequired. We also explored adding domain knowledge to prompts; although it\nhelped certain models understand some code semantics, vulnerability detection\nrequires multi-step reasoning, and these models still failed in steps, such as\nreasoning about variable relations. Our results suggest that new models, new\ntraining methods, or more execution-specific pretraining data may be needed to\nconquer vulnerability detection. We speculate that auto-regressive pre-training\non source code may not effectively extract code semantics, especially on the\ncurrent pretraining mixtures, in which execution data is scarce. Success on\nvulnerability detection as a code reasoning task can benefit many areas of\nsoftware engineering such as debugging, test input generation, and program\nrepair. Our code and data are available at\nhttps://doi.org/10.6084/m9.figshare.27368025.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a challenging code reasoning task: vulnerability\ndetection. Large Language Models (LLMs) have shown promising results in\nnatural-language and math reasoning, but state-of-the-art (SOTA) models\nreported only 54.5% Balanced Accuracy in our vulnerability detection\nevaluation, even those models pre-trained on large amounts of source code. Our\nerror analysis on LLM responses shows that the models struggle to reason about\nthe code semantics relevant to identifying vulnerabilities, especially subtle\nsemantic differences caused by small textual changes. We explored prominent\nmodels and training settings to understand their effects on vulnerability\ndetection performance -- including better prompts, larger models, more\npre-training data, and fine-tuning -- but none led to significant improvements.\nThis raises the question of whether simply scaling training data and model size\nwill allow us to \"solve\" complex code reasoning tasks like vulnerability\ndetection, or if a fundamental shift in modeling and training techniques is\nrequired. We also explored adding domain knowledge to prompts; although it\nhelped certain models understand some code semantics, vulnerability detection\nrequires multi-step reasoning, and these models still failed in steps, such as\nreasoning about variable relations. Our results suggest that new models, new\ntraining methods, or more execution-specific pretraining data may be needed to\nconquer vulnerability detection. We speculate that auto-regressive pre-training\non source code may not effectively extract code semantics, especially on the\ncurrent pretraining mixtures, in which execution data is scarce. Success on\nvulnerability detection as a code reasoning task can benefit many areas of\nsoftware engineering such as debugging, test input generation, and program\nrepair. Our code and data are available at\nhttps://doi.org/10.6084/m9.figshare.27368025."
                },
                "authors": [
                    {
                        "name": "Benjamin Steenhoek"
                    },
                    {
                        "name": "Md Mahbubur Rahman"
                    },
                    {
                        "name": "Monoshi Kumar Roy"
                    },
                    {
                        "name": "Mirza Sanjida Alam"
                    },
                    {
                        "name": "Hengbo Tong"
                    },
                    {
                        "name": "Swarna Das"
                    },
                    {
                        "name": "Earl T. Barr"
                    },
                    {
                        "name": "Wei Le"
                    }
                ],
                "author_detail": {
                    "name": "Wei Le"
                },
                "author": "Wei Le",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.17218v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.17218v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02825v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02825v2",
                "updated": "2025-01-07T21:51:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    21,
                    51,
                    30,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-06T07:57:51Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    7,
                    57,
                    51,
                    0,
                    6,
                    0
                ],
                "title": "Randomly Sampled Language Reasoning Problems Reveal Limits of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomly Sampled Language Reasoning Problems Reveal Limits of LLMs"
                },
                "summary": "Can LLMs pick up language structure from examples? Evidence in prior work\nseems to indicate yes, as pretrained models repeatedly demonstrate the ability\nto adapt to new language structures and vocabularies. However, this line of\nresearch typically considers languages that are present within common\npretraining datasets, or otherwise share notable similarities with these seen\nlanguages. In contrast, in this work we attempt to measure models' language\nunderstanding capacity while circumventing the risk of dataset recall. We\nparameterize large families of language tasks recognized by deterministic\nfinite automata (DFAs), and can thus sample novel language reasoning problems\nto fairly evaulate LLMs regardless of training data. We find that, even in the\nstrikingly simple setting of 3-state DFAs, LLMs underperform unparameterized\nngram models on both language recognition and synthesis tasks. These results\nsuggest that LLMs struggle to match the ability of basic language models in\nrecognizing and reasoning over languages that are sufficiently distinct from\nthe ones they see at training time, underscoring the distinction between\nlearning individual languages and possessing a general theory of language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs pick up language structure from examples? Evidence in prior work\nseems to indicate yes, as pretrained models repeatedly demonstrate the ability\nto adapt to new language structures and vocabularies. However, this line of\nresearch typically considers languages that are present within common\npretraining datasets, or otherwise share notable similarities with these seen\nlanguages. In contrast, in this work we attempt to measure models' language\nunderstanding capacity while circumventing the risk of dataset recall. We\nparameterize large families of language tasks recognized by deterministic\nfinite automata (DFAs), and can thus sample novel language reasoning problems\nto fairly evaulate LLMs regardless of training data. We find that, even in the\nstrikingly simple setting of 3-state DFAs, LLMs underperform unparameterized\nngram models on both language recognition and synthesis tasks. These results\nsuggest that LLMs struggle to match the ability of basic language models in\nrecognizing and reasoning over languages that are sufficiently distinct from\nthe ones they see at training time, underscoring the distinction between\nlearning individual languages and possessing a general theory of language."
                },
                "authors": [
                    {
                        "name": "Kavi Gupta"
                    },
                    {
                        "name": "Kate Sanders"
                    },
                    {
                        "name": "Armando Solar-Lezama"
                    }
                ],
                "author_detail": {
                    "name": "Armando Solar-Lezama"
                },
                "author": "Armando Solar-Lezama",
                "arxiv_comment": "8 pages, 3 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02825v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02825v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04149v1",
                "updated": "2025-01-07T21:33:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    21,
                    33,
                    42,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T21:33:42Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    21,
                    33,
                    42,
                    1,
                    7,
                    0
                ],
                "title": "Comparison of STR and EMLSR Performance in Wi-Fi 7 MLO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparison of STR and EMLSR Performance in Wi-Fi 7 MLO"
                },
                "summary": "This project compares the performance of simultaneous transmit and receive\n(STR) and enhanced multi-link single radio (EMLSR) within Multi-Link Operation\n(MLO) in Wi-Fi 7 networks. Using the ns-3 simulator, we evaluate both\ntechniques under various scenarios, including changes in modulation coding\nscheme (MCS), bandwidth, link quality, and interference levels. Key performance\nmetrics such as latency, throughput, and energy efficiency are analyzed to\ndetermine the trade-offs between STR and EMLSR. The results demonstrate that\nSTR achieves higher throughput and lower latency due to dual-link utilization,\nmaking it suitable for high-load environments. In contrast, EMLSR balances\nenergy efficiency with responsiveness, making it advantageous for\npower-sensitive applications. This analysis provides insights into the\nstrengths and limitations of STR and EMLSR, guiding optimal deployment\nstrategies for future Wi-Fi 7 networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This project compares the performance of simultaneous transmit and receive\n(STR) and enhanced multi-link single radio (EMLSR) within Multi-Link Operation\n(MLO) in Wi-Fi 7 networks. Using the ns-3 simulator, we evaluate both\ntechniques under various scenarios, including changes in modulation coding\nscheme (MCS), bandwidth, link quality, and interference levels. Key performance\nmetrics such as latency, throughput, and energy efficiency are analyzed to\ndetermine the trade-offs between STR and EMLSR. The results demonstrate that\nSTR achieves higher throughput and lower latency due to dual-link utilization,\nmaking it suitable for high-load environments. In contrast, EMLSR balances\nenergy efficiency with responsiveness, making it advantageous for\npower-sensitive applications. This analysis provides insights into the\nstrengths and limitations of STR and EMLSR, guiding optimal deployment\nstrategies for future Wi-Fi 7 networks."
                },
                "authors": [
                    {
                        "name": "Aishwarya Choorakuzhiyil"
                    },
                    {
                        "name": "Kevin Ho"
                    },
                    {
                        "name": "Sara Reyes"
                    }
                ],
                "author_detail": {
                    "name": "Sara Reyes"
                },
                "author": "Sara Reyes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17044v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17044v3",
                "updated": "2025-01-07T21:29:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    21,
                    29,
                    45,
                    1,
                    7,
                    0
                ],
                "published": "2024-05-27T11:00:51Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    11,
                    0,
                    51,
                    0,
                    148,
                    0
                ],
                "title": "Interesting Scientific Idea Generation using Knowledge Graphs and LLMs:\n  Evaluations with 100 Research Group Leaders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interesting Scientific Idea Generation using Knowledge Graphs and LLMs:\n  Evaluations with 100 Research Group Leaders"
                },
                "summary": "The rapid growth of scientific literature makes it challenging for\nresearchers to identify novel and impactful ideas, especially across\ndisciplines. Modern artificial intelligence (AI) systems offer new approaches,\npotentially inspiring ideas not conceived by humans alone. But how compelling\nare these AI-generated ideas, and how can we improve their quality? Here, we\nintroduce SciMuse, which uses 58 million research papers and a large-language\nmodel to generate research ideas. We conduct a large-scale evaluation in which\nover 100 research group leaders -- from natural sciences to humanities --\nranked more than 4,400 personalized ideas based on their interest. This data\nallows us to predict research interest using (1) supervised neural networks\ntrained on human evaluations, and (2) unsupervised zero-shot ranking with\nlarge-language models. Our results demonstrate how future systems can help\ngenerating compelling research ideas and foster unforeseen interdisciplinary\ncollaborations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of scientific literature makes it challenging for\nresearchers to identify novel and impactful ideas, especially across\ndisciplines. Modern artificial intelligence (AI) systems offer new approaches,\npotentially inspiring ideas not conceived by humans alone. But how compelling\nare these AI-generated ideas, and how can we improve their quality? Here, we\nintroduce SciMuse, which uses 58 million research papers and a large-language\nmodel to generate research ideas. We conduct a large-scale evaluation in which\nover 100 research group leaders -- from natural sciences to humanities --\nranked more than 4,400 personalized ideas based on their interest. This data\nallows us to predict research interest using (1) supervised neural networks\ntrained on human evaluations, and (2) unsupervised zero-shot ranking with\nlarge-language models. Our results demonstrate how future systems can help\ngenerating compelling research ideas and foster unforeseen interdisciplinary\ncollaborations."
                },
                "authors": [
                    {
                        "name": "Xuemei Gu"
                    },
                    {
                        "name": "Mario Krenn"
                    }
                ],
                "author_detail": {
                    "name": "Mario Krenn"
                },
                "author": "Mario Krenn",
                "arxiv_comment": "8 pages; 4 figures; Appendix: 6 pages, 5 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17044v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17044v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04138v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04138v1",
                "updated": "2025-01-07T20:57:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    20,
                    57,
                    59,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T20:57:59Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    20,
                    57,
                    59,
                    1,
                    7,
                    0
                ],
                "title": "\"Yeah Right!\" -- Do LLMs Exhibit Multimodal Feature Transfer?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Yeah Right!\" -- Do LLMs Exhibit Multimodal Feature Transfer?"
                },
                "summary": "Human communication is a multifaceted and multimodal skill. Communication\nrequires an understanding of both the surface-level textual content and the\nconnotative intent of a piece of communication. In humans, learning to go\nbeyond the surface level starts by learning communicative intent in speech.\nOnce humans acquire these skills in spoken communication, they transfer those\nskills to written communication. In this paper, we assess the ability of\nspeech+text models and text models trained with special emphasis on\nhuman-to-human conversations to make this multimodal transfer of skill. We\nspecifically test these models on their ability to detect covert deceptive\ncommunication. We find that with no special prompting speech+text LLMs have an\nadvantage over unimodal LLMs in performing this task. Likewise, we find that\nhuman-to-human conversation-trained LLMs are also advantaged in this skill.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human communication is a multifaceted and multimodal skill. Communication\nrequires an understanding of both the surface-level textual content and the\nconnotative intent of a piece of communication. In humans, learning to go\nbeyond the surface level starts by learning communicative intent in speech.\nOnce humans acquire these skills in spoken communication, they transfer those\nskills to written communication. In this paper, we assess the ability of\nspeech+text models and text models trained with special emphasis on\nhuman-to-human conversations to make this multimodal transfer of skill. We\nspecifically test these models on their ability to detect covert deceptive\ncommunication. We find that with no special prompting speech+text LLMs have an\nadvantage over unimodal LLMs in performing this task. Likewise, we find that\nhuman-to-human conversation-trained LLMs are also advantaged in this skill."
                },
                "authors": [
                    {
                        "name": "Benjamin Reichman"
                    },
                    {
                        "name": "Kartik Talamadupula"
                    }
                ],
                "author_detail": {
                    "name": "Kartik Talamadupula"
                },
                "author": "Kartik Talamadupula",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04138v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00530v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00530v2",
                "updated": "2025-01-07T20:36:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    20,
                    36,
                    35,
                    1,
                    7,
                    0
                ],
                "published": "2024-03-31T02:05:40Z",
                "published_parsed": [
                    2024,
                    3,
                    31,
                    2,
                    5,
                    40,
                    6,
                    91,
                    0
                ],
                "title": "Comparing Bad Apples to Good Oranges: Aligning Large Language Models via\n  Joint Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing Bad Apples to Good Oranges: Aligning Large Language Models via\n  Joint Preference Optimization"
                },
                "summary": "A common technique for aligning large language models (LLMs) relies on\nacquiring human preferences by comparing multiple generations conditioned on a\nfixed context. This method, however, relies solely on pairwise comparisons,\nwhere the generations are evaluated within an identical context. While\neffective to such conditional preferences often fail to encompass the nuanced\nand multidimensional nature of human preferences. In this work, we revisit the\ntraditional paradigm of preference acquisition and propose a new axis based on\neliciting preferences jointly over the instruction-response pairs. Unlike prior\npreference optimizations, which are designed for conditional ranking protocols\n(e.g., DPO), we propose Joint Preference Optimization (JPO), a new preference\noptimization objective that upweights the joint probability of the chosen\ninstruction-response pair over the rejected instruction-response pair.\nInterestingly, LLMs trained with joint instruction-response preference data\nusing JPO outperform LLM trained with DPO by $5.2\\%$ and $3.3\\%$ win-rate for\nsummarization and open-ended dialogue datasets, respectively. Our findings\nreveal that joint preferences over instruction and response pairs can\nsignificantly enhance the alignment of LLMs by tapping into a broader spectrum\nof human preference elicitation. The data and code is available at\nhttps://github.com/Hritikbansal/dove.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A common technique for aligning large language models (LLMs) relies on\nacquiring human preferences by comparing multiple generations conditioned on a\nfixed context. This method, however, relies solely on pairwise comparisons,\nwhere the generations are evaluated within an identical context. While\neffective to such conditional preferences often fail to encompass the nuanced\nand multidimensional nature of human preferences. In this work, we revisit the\ntraditional paradigm of preference acquisition and propose a new axis based on\neliciting preferences jointly over the instruction-response pairs. Unlike prior\npreference optimizations, which are designed for conditional ranking protocols\n(e.g., DPO), we propose Joint Preference Optimization (JPO), a new preference\noptimization objective that upweights the joint probability of the chosen\ninstruction-response pair over the rejected instruction-response pair.\nInterestingly, LLMs trained with joint instruction-response preference data\nusing JPO outperform LLM trained with DPO by $5.2\\%$ and $3.3\\%$ win-rate for\nsummarization and open-ended dialogue datasets, respectively. Our findings\nreveal that joint preferences over instruction and response pairs can\nsignificantly enhance the alignment of LLMs by tapping into a broader spectrum\nof human preference elicitation. The data and code is available at\nhttps://github.com/Hritikbansal/dove."
                },
                "authors": [
                    {
                        "name": "Hritik Bansal"
                    },
                    {
                        "name": "Ashima Suvarna"
                    },
                    {
                        "name": "Gantavya Bhatt"
                    },
                    {
                        "name": "Nanyun Peng"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Aditya Grover"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Grover"
                },
                "author": "Aditya Grover",
                "arxiv_comment": "22 pages, 16 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00530v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00530v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05781v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05781v3",
                "updated": "2025-01-07T20:27:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    20,
                    27,
                    9,
                    1,
                    7,
                    0
                ],
                "published": "2024-12-08T02:27:17Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    2,
                    27,
                    17,
                    6,
                    343,
                    0
                ],
                "title": "Open-Source Acceleration of Stable-Diffusion.cpp Deployable on All\n  Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-Source Acceleration of Stable-Diffusion.cpp Deployable on All\n  Devices"
                },
                "summary": "Stable diffusion plays a crucial role in generating high-quality images.\nHowever, image generation is time-consuming and memory-intensive. To address\nthis, stable-diffusion.cpp (Sdcpp) emerges as an efficient inference framework\nto accelerate the diffusion models. Although it is lightweight, the current\nimplementation of ggml_conv_2d operator in Sdcpp is suboptimal, exhibiting both\nhigh inference latency and massive memory usage. To address this, in this work,\nwe present an optimized version of Sdcpp leveraging the Winograd algorithm to\naccelerate 2D convolution operations, which is the primary bottleneck in the\npipeline. By analyzing both dependent and independent computation graphs, we\nexploit the device's locality and parallelism to achieve substantial\nperformance improvements. Our framework delivers correct end-to-end results\nacross various stable diffusion models, including SDv1.4, v1.5, v2.1, SDXL, and\nSDXL-Turbo. Our evaluation results demonstrate a speedup up to 2.76x for\nindividual convolutional layers and an inference speedup up to 4.79x for the\noverall image generation process, compared with the original Sdcpp on M1 pro.\nHomepage: https://github.com/SealAILab/stable-diffusion-cpp",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable diffusion plays a crucial role in generating high-quality images.\nHowever, image generation is time-consuming and memory-intensive. To address\nthis, stable-diffusion.cpp (Sdcpp) emerges as an efficient inference framework\nto accelerate the diffusion models. Although it is lightweight, the current\nimplementation of ggml_conv_2d operator in Sdcpp is suboptimal, exhibiting both\nhigh inference latency and massive memory usage. To address this, in this work,\nwe present an optimized version of Sdcpp leveraging the Winograd algorithm to\naccelerate 2D convolution operations, which is the primary bottleneck in the\npipeline. By analyzing both dependent and independent computation graphs, we\nexploit the device's locality and parallelism to achieve substantial\nperformance improvements. Our framework delivers correct end-to-end results\nacross various stable diffusion models, including SDv1.4, v1.5, v2.1, SDXL, and\nSDXL-Turbo. Our evaluation results demonstrate a speedup up to 2.76x for\nindividual convolutional layers and an inference speedup up to 4.79x for the\noverall image generation process, compared with the original Sdcpp on M1 pro.\nHomepage: https://github.com/SealAILab/stable-diffusion-cpp"
                },
                "authors": [
                    {
                        "name": "Jingxu Ng"
                    },
                    {
                        "name": "Cheng Lv"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Juyi Lin"
                    },
                    {
                        "name": "Minzhou Pan"
                    },
                    {
                        "name": "Yun Liang"
                    },
                    {
                        "name": "Yanzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanzhi Wang"
                },
                "author": "Yanzhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05781v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05781v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09529v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09529v2",
                "updated": "2025-01-07T20:26:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    20,
                    26,
                    34,
                    1,
                    7,
                    0
                ],
                "published": "2024-08-18T16:26:39Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    16,
                    26,
                    39,
                    6,
                    231,
                    0
                ],
                "title": "Revisiting the Graph Reasoning Ability of Large Language Models: Case\n  Studies in Translation, Connectivity and Shortest Path",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting the Graph Reasoning Ability of Large Language Models: Case\n  Studies in Translation, Connectivity and Shortest Path"
                },
                "summary": "Large Language Models (LLMs) have achieved great success in various reasoning\ntasks. In this work, we focus on the graph reasoning ability of LLMs. Although\ntheoretical studies proved that LLMs are capable of handling graph reasoning\ntasks, empirical evaluations reveal numerous failures. To deepen our\nunderstanding on this discrepancy, we revisit the ability of LLMs on three\nfundamental graph tasks: graph description translation, graph connectivity, and\nthe shortest-path problem. Our findings suggest that LLMs can fail to\nunderstand graph structures through text descriptions and exhibit varying\nperformance for all these three fundamental tasks. Meanwhile, we perform a\nreal-world investigation on knowledge graphs and make consistent observations\nwith our findings. The codes and datasets are available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved great success in various reasoning\ntasks. In this work, we focus on the graph reasoning ability of LLMs. Although\ntheoretical studies proved that LLMs are capable of handling graph reasoning\ntasks, empirical evaluations reveal numerous failures. To deepen our\nunderstanding on this discrepancy, we revisit the ability of LLMs on three\nfundamental graph tasks: graph description translation, graph connectivity, and\nthe shortest-path problem. Our findings suggest that LLMs can fail to\nunderstand graph structures through text descriptions and exhibit varying\nperformance for all these three fundamental tasks. Meanwhile, we perform a\nreal-world investigation on knowledge graphs and make consistent observations\nwith our findings. The codes and datasets are available."
                },
                "authors": [
                    {
                        "name": "Xinnan Dai"
                    },
                    {
                        "name": "Qihao Wen"
                    },
                    {
                        "name": "Yifei Shen"
                    },
                    {
                        "name": "Hongzhi Wen"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Jiliang Tang"
                    },
                    {
                        "name": "Caihua Shan"
                    }
                ],
                "author_detail": {
                    "name": "Caihua Shan"
                },
                "author": "Caihua Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09529v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09529v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08213v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08213v2",
                "updated": "2025-01-07T20:08:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    20,
                    8,
                    13,
                    1,
                    7,
                    0
                ],
                "published": "2024-07-11T06:30:46Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    6,
                    30,
                    46,
                    3,
                    193,
                    0
                ],
                "title": "PrefCLM: Enhancing Preference-based Reinforcement Learning with\n  Crowdsourced Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefCLM: Enhancing Preference-based Reinforcement Learning with\n  Crowdsourced Large Language Models"
                },
                "summary": "Preference-based reinforcement learning (PbRL) is emerging as a promising\napproach to teaching robots through human comparative feedback, sidestepping\nthe need for complex reward engineering. However, the substantial volume of\nfeedback required in existing PbRL methods often lead to reliance on synthetic\nfeedback generated by scripted teachers. This approach necessitates intricate\nreward engineering again and struggles to adapt to the nuanced preferences\nparticular to human-robot interaction (HRI) scenarios, where users may have\nunique expectations toward the same task. To address these challenges, we\nintroduce PrefCLM, a novel framework that utilizes crowdsourced large language\nmodels (LLMs) as simulated teachers in PbRL. We utilize Dempster-Shafer Theory\nto fuse individual preferences from multiple LLM agents at the score level,\nefficiently leveraging their diversity and collective intelligence. We also\nintroduce a human-in-the-loop pipeline that facilitates collective refinements\nbased on user interactive feedback. Experimental results across various general\nRL tasks show that PrefCLM achieves competitive performance compared to\ntraditional scripted teachers and excels in facilitating more more natural and\nefficient behaviors. A real-world user study (N=10) further demonstrates its\ncapability to tailor robot behaviors to individual user preferences,\nsignificantly enhancing user satisfaction in HRI scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference-based reinforcement learning (PbRL) is emerging as a promising\napproach to teaching robots through human comparative feedback, sidestepping\nthe need for complex reward engineering. However, the substantial volume of\nfeedback required in existing PbRL methods often lead to reliance on synthetic\nfeedback generated by scripted teachers. This approach necessitates intricate\nreward engineering again and struggles to adapt to the nuanced preferences\nparticular to human-robot interaction (HRI) scenarios, where users may have\nunique expectations toward the same task. To address these challenges, we\nintroduce PrefCLM, a novel framework that utilizes crowdsourced large language\nmodels (LLMs) as simulated teachers in PbRL. We utilize Dempster-Shafer Theory\nto fuse individual preferences from multiple LLM agents at the score level,\nefficiently leveraging their diversity and collective intelligence. We also\nintroduce a human-in-the-loop pipeline that facilitates collective refinements\nbased on user interactive feedback. Experimental results across various general\nRL tasks show that PrefCLM achieves competitive performance compared to\ntraditional scripted teachers and excels in facilitating more more natural and\nefficient behaviors. A real-world user study (N=10) further demonstrates its\ncapability to tailor robot behaviors to individual user preferences,\nsignificantly enhancing user satisfaction in HRI scenarios."
                },
                "authors": [
                    {
                        "name": "Ruiqi Wang"
                    },
                    {
                        "name": "Dezhong Zhao"
                    },
                    {
                        "name": "Ziqin Yuan"
                    },
                    {
                        "name": "Ike Obi"
                    },
                    {
                        "name": "Byung-Cheol Min"
                    }
                ],
                "author_detail": {
                    "name": "Byung-Cheol Min"
                },
                "author": "Byung-Cheol Min",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08213v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08213v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02063v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02063v2",
                "updated": "2025-01-07T19:12:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    19,
                    12,
                    22,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-03T19:16:36Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    19,
                    16,
                    36,
                    4,
                    3,
                    0
                ],
                "title": "AGGA: A Dataset of Academic Guidelines for Generative AI and Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGGA: A Dataset of Academic Guidelines for Generative AI and Large\n  Language Models"
                },
                "summary": "This study introduces AGGA, a dataset comprising 80 academic guidelines for\nthe use of Generative AIs (GAIs) and Large Language Models (LLMs) in academic\nsettings, meticulously collected from official university websites. The dataset\ncontains 188,674 words and serves as a valuable resource for natural language\nprocessing tasks commonly applied in requirements engineering, such as model\nsynthesis, abstraction identification, and document structure assessment.\nAdditionally, AGGA can be further annotated to function as a benchmark for\nvarious tasks, including ambiguity detection, requirements categorization, and\nthe identification of equivalent requirements. Our methodologically rigorous\napproach ensured a thorough examination, with a selection of universities that\nrepresent a diverse range of global institutions, including top-ranked\nuniversities across six continents. The dataset captures perspectives from a\nvariety of academic fields, including humanities, technology, and both public\nand private institutions, offering a broad spectrum of insights into the\nintegration of GAIs and LLMs in academia.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces AGGA, a dataset comprising 80 academic guidelines for\nthe use of Generative AIs (GAIs) and Large Language Models (LLMs) in academic\nsettings, meticulously collected from official university websites. The dataset\ncontains 188,674 words and serves as a valuable resource for natural language\nprocessing tasks commonly applied in requirements engineering, such as model\nsynthesis, abstraction identification, and document structure assessment.\nAdditionally, AGGA can be further annotated to function as a benchmark for\nvarious tasks, including ambiguity detection, requirements categorization, and\nthe identification of equivalent requirements. Our methodologically rigorous\napproach ensured a thorough examination, with a selection of universities that\nrepresent a diverse range of global institutions, including top-ranked\nuniversities across six continents. The dataset captures perspectives from a\nvariety of academic fields, including humanities, technology, and both public\nand private institutions, offering a broad spectrum of insights into the\nintegration of GAIs and LLMs in academia."
                },
                "authors": [
                    {
                        "name": "Junfeng Jiao"
                    },
                    {
                        "name": "Saleh Afroogh"
                    },
                    {
                        "name": "Kevin Chen"
                    },
                    {
                        "name": "David Atkinson"
                    },
                    {
                        "name": "Amit Dhurandhar"
                    }
                ],
                "author_detail": {
                    "name": "Amit Dhurandhar"
                },
                "author": "Amit Dhurandhar",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2406.18842,\n  arXiv:2501.00959",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02063v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02063v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04001v1",
                "updated": "2025-01-07T18:58:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    58,
                    54,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T18:58:54Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    58,
                    54,
                    1,
                    7,
                    0
                ],
                "title": "Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of\n  Images and Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of\n  Images and Videos"
                },
                "summary": "This work presents Sa2VA, the first unified model for dense grounded\nunderstanding of both images and videos. Unlike existing multi-modal large\nlanguage models, which are often limited to specific modalities and tasks,\nSa2VA supports a wide range of image and video tasks, including referring\nsegmentation and conversation, with minimal one-shot instruction tuning. Sa2VA\ncombines SAM-2, a foundation video segmentation model, with LLaVA, an advanced\nvision-language model, and unifies text, image, and video into a shared LLM\ntoken space. Using the LLM, Sa2VA generates instruction tokens that guide SAM-2\nin producing precise masks, enabling a grounded, multi-modal understanding of\nboth static and dynamic visual content. Additionally, we introduce Ref-SAV, an\nauto-labeled dataset containing over 72k object expressions in complex video\nscenes, designed to boost model performance. We also manually validate 2k video\nobjects in the Ref-SAV datasets to benchmark referring video object\nsegmentation in complex environments. Experiments show that Sa2VA achieves\nstate-of-the-art across multiple tasks, particularly in referring video object\nsegmentation, highlighting its potential for complex real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents Sa2VA, the first unified model for dense grounded\nunderstanding of both images and videos. Unlike existing multi-modal large\nlanguage models, which are often limited to specific modalities and tasks,\nSa2VA supports a wide range of image and video tasks, including referring\nsegmentation and conversation, with minimal one-shot instruction tuning. Sa2VA\ncombines SAM-2, a foundation video segmentation model, with LLaVA, an advanced\nvision-language model, and unifies text, image, and video into a shared LLM\ntoken space. Using the LLM, Sa2VA generates instruction tokens that guide SAM-2\nin producing precise masks, enabling a grounded, multi-modal understanding of\nboth static and dynamic visual content. Additionally, we introduce Ref-SAV, an\nauto-labeled dataset containing over 72k object expressions in complex video\nscenes, designed to boost model performance. We also manually validate 2k video\nobjects in the Ref-SAV datasets to benchmark referring video object\nsegmentation in complex environments. Experiments show that Sa2VA achieves\nstate-of-the-art across multiple tasks, particularly in referring video object\nsegmentation, highlighting its potential for complex real-world applications."
                },
                "authors": [
                    {
                        "name": "Haobo Yuan"
                    },
                    {
                        "name": "Xiangtai Li"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Zilong Huang"
                    },
                    {
                        "name": "Shilin Xu"
                    },
                    {
                        "name": "Shunping Ji"
                    },
                    {
                        "name": "Yunhai Tong"
                    },
                    {
                        "name": "Lu Qi"
                    },
                    {
                        "name": "Jiashi Feng"
                    },
                    {
                        "name": "Ming-Hsuan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Ming-Hsuan Yang"
                },
                "author": "Ming-Hsuan Yang",
                "arxiv_comment": "Project page: https://lxtgh.github.io/project/sa2va",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03995v1",
                "updated": "2025-01-07T18:52:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    52,
                    5,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T18:52:05Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    52,
                    5,
                    1,
                    7,
                    0
                ],
                "title": "RAG-Check: Evaluating Multimodal Retrieval Augmented Generation\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG-Check: Evaluating Multimodal Retrieval Augmented Generation\n  Performance"
                },
                "summary": "Retrieval-augmented generation (RAG) improves large language models (LLMs) by\nusing external knowledge to guide response generation, reducing hallucinations.\nHowever, RAG, particularly multi-modal RAG, can introduce new hallucination\nsources: (i) the retrieval process may select irrelevant pieces (e.g.,\ndocuments, images) as raw context from the database, and (ii) retrieved images\nare processed into text-based context via vision-language models (VLMs) or\ndirectly used by multi-modal language models (MLLMs) like GPT-4o, which may\nhallucinate. To address this, we propose a novel framework to evaluate the\nreliability of multi-modal RAG using two performance measures: (i) the\nrelevancy score (RS), assessing the relevance of retrieved entries to the\nquery, and (ii) the correctness score (CS), evaluating the accuracy of the\ngenerated response. We train RS and CS models using a ChatGPT-derived database\nand human evaluator samples. Results show that both models achieve ~88%\naccuracy on test data. Additionally, we construct a 5000-sample human-annotated\ndatabase evaluating the relevancy of retrieved pieces and the correctness of\nresponse statements. Our RS model aligns with human preferences 20% more often\nthan CLIP in retrieval, and our CS model matches human preferences ~91% of the\ntime. Finally, we assess various RAG systems' selection and generation\nperformances using RS and CS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) improves large language models (LLMs) by\nusing external knowledge to guide response generation, reducing hallucinations.\nHowever, RAG, particularly multi-modal RAG, can introduce new hallucination\nsources: (i) the retrieval process may select irrelevant pieces (e.g.,\ndocuments, images) as raw context from the database, and (ii) retrieved images\nare processed into text-based context via vision-language models (VLMs) or\ndirectly used by multi-modal language models (MLLMs) like GPT-4o, which may\nhallucinate. To address this, we propose a novel framework to evaluate the\nreliability of multi-modal RAG using two performance measures: (i) the\nrelevancy score (RS), assessing the relevance of retrieved entries to the\nquery, and (ii) the correctness score (CS), evaluating the accuracy of the\ngenerated response. We train RS and CS models using a ChatGPT-derived database\nand human evaluator samples. Results show that both models achieve ~88%\naccuracy on test data. Additionally, we construct a 5000-sample human-annotated\ndatabase evaluating the relevancy of retrieved pieces and the correctness of\nresponse statements. Our RS model aligns with human preferences 20% more often\nthan CLIP in retrieval, and our CS model matches human preferences ~91% of the\ntime. Finally, we assess various RAG systems' selection and generation\nperformances using RS and CS."
                },
                "authors": [
                    {
                        "name": "Matin Mortaheb"
                    },
                    {
                        "name": "Mohammad A. Amir Khojastepour"
                    },
                    {
                        "name": "Srimat T. Chakradhar"
                    },
                    {
                        "name": "Sennur Ulukus"
                    }
                ],
                "author_detail": {
                    "name": "Sennur Ulukus"
                },
                "author": "Sennur Ulukus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03991v1",
                "updated": "2025-01-07T18:48:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    48,
                    42,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T18:48:42Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    48,
                    42,
                    1,
                    7,
                    0
                ],
                "title": "Influences on LLM Calibration: A Study of Response Agreement, Loss\n  Functions, and Prompt Styles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Influences on LLM Calibration: A Study of Response Agreement, Loss\n  Functions, and Prompt Styles"
                },
                "summary": "Calibration, the alignment between model confidence and prediction accuracy,\nis critical for the reliable deployment of large language models (LLMs).\nExisting works neglect to measure the generalization of their methods to other\nprompt styles and different sizes of LLMs. To address this, we define a\ncontrolled experimental setting covering 12 LLMs and four prompt styles. We\nadditionally investigate if incorporating the response agreement of multiple\nLLMs and an appropriate loss function can improve calibration performance.\nConcretely, we build Calib-n, a novel framework that trains an auxiliary model\nfor confidence estimation that aggregates responses from multiple LLMs to\ncapture inter-model agreement. To optimize calibration, we integrate focal and\nAUC surrogate losses alongside binary cross-entropy. Experiments across four\ndatasets demonstrate that both response agreement and focal loss improve\ncalibration from baselines. We find that few-shot prompts are the most\neffective for auxiliary model-based methods, and auxiliary models demonstrate\nrobust calibration performance across accuracy variations, outperforming LLMs'\ninternal probabilities and verbalized confidences. These insights deepen the\nunderstanding of influence factors in LLM calibration, supporting their\nreliable deployment in diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibration, the alignment between model confidence and prediction accuracy,\nis critical for the reliable deployment of large language models (LLMs).\nExisting works neglect to measure the generalization of their methods to other\nprompt styles and different sizes of LLMs. To address this, we define a\ncontrolled experimental setting covering 12 LLMs and four prompt styles. We\nadditionally investigate if incorporating the response agreement of multiple\nLLMs and an appropriate loss function can improve calibration performance.\nConcretely, we build Calib-n, a novel framework that trains an auxiliary model\nfor confidence estimation that aggregates responses from multiple LLMs to\ncapture inter-model agreement. To optimize calibration, we integrate focal and\nAUC surrogate losses alongside binary cross-entropy. Experiments across four\ndatasets demonstrate that both response agreement and focal loss improve\ncalibration from baselines. We find that few-shot prompts are the most\neffective for auxiliary model-based methods, and auxiliary models demonstrate\nrobust calibration performance across accuracy variations, outperforming LLMs'\ninternal probabilities and verbalized confidences. These insights deepen the\nunderstanding of influence factors in LLM calibration, supporting their\nreliable deployment in diverse applications."
                },
                "authors": [
                    {
                        "name": "Yuxi Xia"
                    },
                    {
                        "name": "Pedro Henrique Luz de Araujo"
                    },
                    {
                        "name": "Klim Zaporojets"
                    },
                    {
                        "name": "Benjamin Roth"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Roth"
                },
                "author": "Benjamin Roth",
                "arxiv_comment": "24 pages, 11 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03989v1",
                "updated": "2025-01-07T18:46:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    46,
                    34,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T18:46:34Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    46,
                    34,
                    1,
                    7,
                    0
                ],
                "title": "(De)-Indexing and the Right to be Forgotten",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "(De)-Indexing and the Right to be Forgotten"
                },
                "summary": "In the digital age, the challenge of forgetfulness has emerged as a\nsignificant concern, particularly regarding the management of personal data and\nits accessibility online. The right to be forgotten (RTBF) allows individuals\nto request the removal of outdated or harmful information from public access,\nyet implementing this right poses substantial technical difficulties for search\nengines. This paper aims to introduce non-experts to the foundational concepts\nof information retrieval (IR) and de-indexing, which are critical for\nunderstanding how search engines can effectively \"forget\" certain content. We\nwill explore various IR models, including boolean, probabilistic, vector space,\nand embedding-based approaches, as well as the role of Large Language Models\n(LLMs) in enhancing data processing capabilities. By providing this overview,\nwe seek to highlight the complexities involved in balancing individual privacy\nrights with the operational challenges faced by search engines in managing\ninformation visibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the digital age, the challenge of forgetfulness has emerged as a\nsignificant concern, particularly regarding the management of personal data and\nits accessibility online. The right to be forgotten (RTBF) allows individuals\nto request the removal of outdated or harmful information from public access,\nyet implementing this right poses substantial technical difficulties for search\nengines. This paper aims to introduce non-experts to the foundational concepts\nof information retrieval (IR) and de-indexing, which are critical for\nunderstanding how search engines can effectively \"forget\" certain content. We\nwill explore various IR models, including boolean, probabilistic, vector space,\nand embedding-based approaches, as well as the role of Large Language Models\n(LLMs) in enhancing data processing capabilities. By providing this overview,\nwe seek to highlight the complexities involved in balancing individual privacy\nrights with the operational challenges faced by search engines in managing\ninformation visibility."
                },
                "authors": [
                    {
                        "name": "Salvatore Vilella"
                    },
                    {
                        "name": "Giancarlo Ruffo"
                    }
                ],
                "author_detail": {
                    "name": "Giancarlo Ruffo"
                },
                "author": "Giancarlo Ruffo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.4; H.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13510v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13510v2",
                "updated": "2025-01-07T18:16:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    16,
                    17,
                    1,
                    7,
                    0
                ],
                "published": "2024-08-24T08:12:22Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    8,
                    12,
                    22,
                    5,
                    237,
                    0
                ],
                "title": "Intelligent Router for LLM Workloads: Improving Performance Through\n  Workload-Aware Load Balancing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent Router for LLM Workloads: Improving Performance Through\n  Workload-Aware Load Balancing"
                },
                "summary": "Large Language Model (LLM) workloads have distinct prefill and decode phases\nwith different compute and memory requirements which should ideally be\naccounted for when scheduling input queries across different LLM instances in a\ncluster. However existing scheduling algorithms treat LLM workloads as\nmonolithic jobs without considering the distinct characteristics of the two\nphases in each workload. This leads to sub-optimal scheduling and increased\nresponse latency. In this work, we start by characterizing factors affecting\nthe response latency during LLM inference serving. We establish that better\nload balancing of inference requests across the available LLM instances can\nimprove the end-to-end latency to a larger extent than merely focusing on\noptimizing the instance-level scheduler. Motivated by our findings, we propose\na heuristic-guided reinforcement learning-based intelligent router for\ndata-driven and workload-aware scheduling. Our router schedules queries across\nLLM instances by leveraging a trainable response-length predictor, and a novel\nformulation for estimating the impact of mixing different workloads and\nachieves over 11% lower end-to-end latency than existing approaches on a mix of\npublic datasets and 7.8% lower end-to-end latency on real workload data with\ndiverse input and output trends from Cloud Provider X. Additionally, the\nproposed framework can also serve as a standard for benchmarking different LLM\ninference schedulers since it provides the best latency for a given model,\nhardware, and instance-level scheduler combination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) workloads have distinct prefill and decode phases\nwith different compute and memory requirements which should ideally be\naccounted for when scheduling input queries across different LLM instances in a\ncluster. However existing scheduling algorithms treat LLM workloads as\nmonolithic jobs without considering the distinct characteristics of the two\nphases in each workload. This leads to sub-optimal scheduling and increased\nresponse latency. In this work, we start by characterizing factors affecting\nthe response latency during LLM inference serving. We establish that better\nload balancing of inference requests across the available LLM instances can\nimprove the end-to-end latency to a larger extent than merely focusing on\noptimizing the instance-level scheduler. Motivated by our findings, we propose\na heuristic-guided reinforcement learning-based intelligent router for\ndata-driven and workload-aware scheduling. Our router schedules queries across\nLLM instances by leveraging a trainable response-length predictor, and a novel\nformulation for estimating the impact of mixing different workloads and\nachieves over 11% lower end-to-end latency than existing approaches on a mix of\npublic datasets and 7.8% lower end-to-end latency on real workload data with\ndiverse input and output trends from Cloud Provider X. Additionally, the\nproposed framework can also serve as a standard for benchmarking different LLM\ninference schedulers since it provides the best latency for a given model,\nhardware, and instance-level scheduler combination."
                },
                "authors": [
                    {
                        "name": "Kunal Jain"
                    },
                    {
                        "name": "Anjaly Parayil"
                    },
                    {
                        "name": "Ankur Mallick"
                    },
                    {
                        "name": "Esha Choukse"
                    },
                    {
                        "name": "Xiaoting Qin"
                    },
                    {
                        "name": "Jue Zhang"
                    },
                    {
                        "name": "Íñigo Goiri"
                    },
                    {
                        "name": "Rujia Wang"
                    },
                    {
                        "name": "Chetan Bansal"
                    },
                    {
                        "name": "Victor Rühle"
                    },
                    {
                        "name": "Anoop Kulkarni"
                    },
                    {
                        "name": "Steve Kofsky"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13510v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13510v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03968v1",
                "updated": "2025-01-07T18:06:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    6,
                    27,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T18:06:27Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    18,
                    6,
                    27,
                    1,
                    7,
                    0
                ],
                "title": "VLM-driven Behavior Tree for Context-aware Task Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLM-driven Behavior Tree for Context-aware Task Planning"
                },
                "summary": "The use of Large Language Models (LLMs) for generating Behavior Trees (BTs)\nhas recently gained attention in the robotics community, yet remains in its\nearly stages of development. In this paper, we propose a novel framework that\nleverages Vision-Language Models (VLMs) to interactively generate and edit BTs\nthat address visual conditions, enabling context-aware robot operations in\nvisually complex environments. A key feature of our approach lies in the\nconditional control through self-prompted visual conditions. Specifically, the\nVLM generates BTs with visual condition nodes, where conditions are expressed\nas free-form text. Another VLM process integrates the text into its prompt and\nevaluates the conditions against real-world images during robot execution. We\nvalidated our framework in a real-world cafe scenario, demonstrating both its\nfeasibility and limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of Large Language Models (LLMs) for generating Behavior Trees (BTs)\nhas recently gained attention in the robotics community, yet remains in its\nearly stages of development. In this paper, we propose a novel framework that\nleverages Vision-Language Models (VLMs) to interactively generate and edit BTs\nthat address visual conditions, enabling context-aware robot operations in\nvisually complex environments. A key feature of our approach lies in the\nconditional control through self-prompted visual conditions. Specifically, the\nVLM generates BTs with visual condition nodes, where conditions are expressed\nas free-form text. Another VLM process integrates the text into its prompt and\nevaluates the conditions against real-world images during robot execution. We\nvalidated our framework in a real-world cafe scenario, demonstrating both its\nfeasibility and limitations."
                },
                "authors": [
                    {
                        "name": "Naoki Wake"
                    },
                    {
                        "name": "Atsushi Kanehira"
                    },
                    {
                        "name": "Jun Takamatsu"
                    },
                    {
                        "name": "Kazuhiro Sasabuchi"
                    },
                    {
                        "name": "Katsushi Ikeuchi"
                    }
                ],
                "author_detail": {
                    "name": "Katsushi Ikeuchi"
                },
                "author": "Katsushi Ikeuchi",
                "arxiv_comment": "10 pages, 11 figures, 5 tables. Last updated on January 7th, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03957v1",
                "updated": "2025-01-07T17:37:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    37,
                    57,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T17:37:57Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    37,
                    57,
                    1,
                    7,
                    0
                ],
                "title": "Vision Language Models as Values Detectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models as Values Detectors"
                },
                "summary": "Large Language Models integrating textual and visual inputs have introduced\nnew possibilities for interpreting complex data. Despite their remarkable\nability to generate coherent and contextually relevant text based on visual\nstimuli, the alignment of these models with human perception in identifying\nrelevant elements in images requires further exploration. This paper\ninvestigates the alignment between state-of-the-art LLMs and human annotators\nin detecting elements of relevance within home environment scenarios. We\ncreated a set of twelve images depicting various domestic scenarios and\nenlisted fourteen annotators to identify the key element in each image. We then\ncompared these human responses with outputs from five different LLMs, including\nGPT-4o and four LLaVA variants. Our findings reveal a varied degree of\nalignment, with LLaVA 34B showing the highest performance but still scoring\nlow. However, an analysis of the results highlights the models' potential to\ndetect value-laden elements in images, suggesting that with improved training\nand refined prompts, LLMs could enhance applications in social robotics,\nassistive technologies, and human-computer interaction by providing deeper\ninsights and more contextually relevant responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models integrating textual and visual inputs have introduced\nnew possibilities for interpreting complex data. Despite their remarkable\nability to generate coherent and contextually relevant text based on visual\nstimuli, the alignment of these models with human perception in identifying\nrelevant elements in images requires further exploration. This paper\ninvestigates the alignment between state-of-the-art LLMs and human annotators\nin detecting elements of relevance within home environment scenarios. We\ncreated a set of twelve images depicting various domestic scenarios and\nenlisted fourteen annotators to identify the key element in each image. We then\ncompared these human responses with outputs from five different LLMs, including\nGPT-4o and four LLaVA variants. Our findings reveal a varied degree of\nalignment, with LLaVA 34B showing the highest performance but still scoring\nlow. However, an analysis of the results highlights the models' potential to\ndetect value-laden elements in images, suggesting that with improved training\nand refined prompts, LLMs could enhance applications in social robotics,\nassistive technologies, and human-computer interaction by providing deeper\ninsights and more contextually relevant responses."
                },
                "authors": [
                    {
                        "name": "Giulio Antonio Abbo"
                    },
                    {
                        "name": "Tony Belpaeme"
                    }
                ],
                "author_detail": {
                    "name": "Tony Belpaeme"
                },
                "author": "Tony Belpaeme",
                "arxiv_comment": "13 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11735v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11735v3",
                "updated": "2025-01-07T17:34:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    34,
                    4,
                    1,
                    7,
                    0
                ],
                "published": "2024-08-21T15:59:33Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    15,
                    59,
                    33,
                    2,
                    234,
                    0
                ],
                "title": "Clinical Insights: A Comprehensive Review of Language Models in Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical Insights: A Comprehensive Review of Language Models in Medicine"
                },
                "summary": "This paper explores the advancements and applications of language models in\nhealthcare, focusing on their clinical use cases. It examines the evolution\nfrom early encoder-based systems requiring extensive fine-tuning to\nstate-of-the-art large language and multimodal models capable of integrating\ntext and visual data through in-context learning. The analysis emphasizes\nlocally deployable models, which enhance data privacy and operational autonomy,\nand their applications in tasks such as text generation, classification,\ninformation extraction, and conversational systems. The paper also highlights a\nstructured organization of tasks and a tiered ethical approach, providing a\nvaluable resource for researchers and practitioners, while discussing key\nchallenges related to ethics, evaluation, and implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the advancements and applications of language models in\nhealthcare, focusing on their clinical use cases. It examines the evolution\nfrom early encoder-based systems requiring extensive fine-tuning to\nstate-of-the-art large language and multimodal models capable of integrating\ntext and visual data through in-context learning. The analysis emphasizes\nlocally deployable models, which enhance data privacy and operational autonomy,\nand their applications in tasks such as text generation, classification,\ninformation extraction, and conversational systems. The paper also highlights a\nstructured organization of tasks and a tiered ethical approach, providing a\nvaluable resource for researchers and practitioners, while discussing key\nchallenges related to ethics, evaluation, and implementation."
                },
                "authors": [
                    {
                        "name": "Nikita Neveditsin"
                    },
                    {
                        "name": "Pawan Lingras"
                    },
                    {
                        "name": "Vijay Mago"
                    }
                ],
                "author_detail": {
                    "name": "Vijay Mago"
                },
                "author": "Vijay Mago",
                "arxiv_comment": "Submitted to PLOS Digital Health, Revision 1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11735v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11735v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.08110v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.08110v6",
                "updated": "2025-01-07T17:26:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    26,
                    26,
                    1,
                    7,
                    0
                ],
                "published": "2023-01-19T15:01:00Z",
                "published_parsed": [
                    2023,
                    1,
                    19,
                    15,
                    1,
                    0,
                    3,
                    19,
                    0
                ],
                "title": "AtMan: Understanding Transformer Predictions Through Memory Efficient\n  Attention Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AtMan: Understanding Transformer Predictions Through Memory Efficient\n  Attention Manipulation"
                },
                "summary": "Generative transformer models have become increasingly complex, with large\nnumbers of parameters and the ability to process multiple input modalities.\nCurrent methods for explaining their predictions are resource-intensive. Most\ncrucially, they require prohibitively large amounts of extra memory, since they\nrely on backpropagation which allocates almost twice as much GPU memory as the\nforward pass. This makes it difficult, if not impossible, to use them in\nproduction. We present AtMan that provides explanations of generative\ntransformer models at almost no extra cost. Specifically, AtMan is a\nmodality-agnostic perturbation method that manipulates the attention mechanisms\nof transformers to produce relevance maps for the input with respect to the\noutput prediction. Instead of using backpropagation, AtMan applies a\nparallelizable token-based search method based on cosine similarity\nneighborhood in the embedding space. Our exhaustive experiments on text and\nimage-text benchmarks demonstrate that AtMan outperforms current\nstate-of-the-art gradient-based methods on several metrics while being\ncomputationally efficient. As such, AtMan is suitable for use in large model\ninference deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative transformer models have become increasingly complex, with large\nnumbers of parameters and the ability to process multiple input modalities.\nCurrent methods for explaining their predictions are resource-intensive. Most\ncrucially, they require prohibitively large amounts of extra memory, since they\nrely on backpropagation which allocates almost twice as much GPU memory as the\nforward pass. This makes it difficult, if not impossible, to use them in\nproduction. We present AtMan that provides explanations of generative\ntransformer models at almost no extra cost. Specifically, AtMan is a\nmodality-agnostic perturbation method that manipulates the attention mechanisms\nof transformers to produce relevance maps for the input with respect to the\noutput prediction. Instead of using backpropagation, AtMan applies a\nparallelizable token-based search method based on cosine similarity\nneighborhood in the embedding space. Our exhaustive experiments on text and\nimage-text benchmarks demonstrate that AtMan outperforms current\nstate-of-the-art gradient-based methods on several metrics while being\ncomputationally efficient. As such, AtMan is suitable for use in large model\ninference deployments."
                },
                "authors": [
                    {
                        "name": "Björn Deiseroth"
                    },
                    {
                        "name": "Mayukh Deb"
                    },
                    {
                        "name": "Samuel Weinbach"
                    },
                    {
                        "name": "Manuel Brack"
                    },
                    {
                        "name": "Patrick Schramowski"
                    },
                    {
                        "name": "Kristian Kersting"
                    }
                ],
                "author_detail": {
                    "name": "Kristian Kersting"
                },
                "author": "Kristian Kersting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.08110v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.08110v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03952v1",
                "updated": "2025-01-07T17:24:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    24,
                    17,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T17:24:17Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    24,
                    17,
                    1,
                    7,
                    0
                ],
                "title": "Localizing AI: Evaluating Open-Weight Language Models for Languages of\n  Baltic States",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localizing AI: Evaluating Open-Weight Language Models for Languages of\n  Baltic States"
                },
                "summary": "Although large language models (LLMs) have transformed our expectations of\nmodern language technologies, concerns over data privacy often restrict the use\nof commercially available LLMs hosted outside of EU jurisdictions. This limits\ntheir application in governmental, defence, and other data-sensitive sectors.\nIn this work, we evaluate the extent to which locally deployable open-weight\nLLMs support lesser-spoken languages such as Lithuanian, Latvian, and Estonian.\nWe examine various size and precision variants of the top-performing\nmultilingual open-weight models, Llama~3, Gemma~2, Phi, and NeMo, on machine\ntranslation, multiple-choice question answering, and free-form text generation.\nThe results indicate that while certain models like Gemma~2 perform close to\nthe top commercially available models, many LLMs struggle with these languages.\nMost surprisingly, however, we find that these models, while showing close to\nstate-of-the-art translation performance, are still prone to lexical\nhallucinations with errors in at least 1 in 20 words for all open-weight\nmultilingual LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) have transformed our expectations of\nmodern language technologies, concerns over data privacy often restrict the use\nof commercially available LLMs hosted outside of EU jurisdictions. This limits\ntheir application in governmental, defence, and other data-sensitive sectors.\nIn this work, we evaluate the extent to which locally deployable open-weight\nLLMs support lesser-spoken languages such as Lithuanian, Latvian, and Estonian.\nWe examine various size and precision variants of the top-performing\nmultilingual open-weight models, Llama~3, Gemma~2, Phi, and NeMo, on machine\ntranslation, multiple-choice question answering, and free-form text generation.\nThe results indicate that while certain models like Gemma~2 perform close to\nthe top commercially available models, many LLMs struggle with these languages.\nMost surprisingly, however, we find that these models, while showing close to\nstate-of-the-art translation performance, are still prone to lexical\nhallucinations with errors in at least 1 in 20 words for all open-weight\nmultilingual LLMs."
                },
                "authors": [
                    {
                        "name": "Jurgita Kapočiūtė-Dzikienė"
                    },
                    {
                        "name": "Toms Bergmanis"
                    },
                    {
                        "name": "Mārcis Pinnis"
                    }
                ],
                "author_detail": {
                    "name": "Mārcis Pinnis"
                },
                "author": "Mārcis Pinnis",
                "arxiv_comment": "This paper is accepted to NoDaLiDa/Baltic-HLT 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v1",
                "updated": "2025-01-07T17:00:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-González"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martín"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]