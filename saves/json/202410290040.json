[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.19355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19355v1",
                "updated": "2024-10-25T07:24:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T07:24:38Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "title": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality"
                },
                "summary": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality."
                },
                "authors": [
                    {
                        "name": "Zhengyao Lv"
                    },
                    {
                        "name": "Chenyang Si"
                    },
                    {
                        "name": "Junhao Song"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Kwan-Yee K. Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Yee K. Wong"
                },
                "author": "Kwan-Yee K. Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19274v1",
                "updated": "2024-10-25T03:01:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    1,
                    19,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T03:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    1,
                    19,
                    4,
                    299,
                    0
                ],
                "title": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference."
                },
                "authors": [
                    {
                        "name": "Tuowei Wang"
                    },
                    {
                        "name": "Ruwen Fan"
                    },
                    {
                        "name": "Minxing Huang"
                    },
                    {
                        "name": "Zixu Hao"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Youyou Lu"
                    },
                    {
                        "name": "Yaoxue Zhang"
                    },
                    {
                        "name": "Ju Ren"
                    }
                ],
                "author_detail": {
                    "name": "Ju Ren"
                },
                "author": "Ju Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19258v1",
                "updated": "2024-10-25T02:22:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T02:22:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning"
                },
                "summary": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark."
                },
                "authors": [
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "18pages,submitted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19123v1",
                "updated": "2024-10-24T19:48:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    19,
                    48,
                    51,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T19:48:51Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    19,
                    48,
                    51,
                    3,
                    298,
                    0
                ],
                "title": "Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with\n  System Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with\n  System Co-Design"
                },
                "summary": "The proliferation of large language models (LLMs) has led to the adoption of\nMixture-of-Experts (MoE) architectures that dynamically leverage specialized\nsubnetworks for improved efficiency and performance. Despite their benefits,\nMoE models face significant challenges during inference, including inefficient\nmemory management and suboptimal batching, due to misaligned design choices\nbetween the model architecture and the system policies. Furthermore, the\nconventional approach of training MoEs from scratch is increasingly prohibitive\nin terms of cost. In this paper, we propose a novel framework Read-ME that\ntransforms pre-trained dense LLMs into smaller MoE models (in contrast to\n\"upcycling\" generalist MoEs), avoiding the high costs of ground-up training.\nOur approach employs activation sparsity to extract experts. To compose\nexperts, we examine the widely-adopted layer-wise router design and show its\nredundancy, and thus we introduce the pre-gating router decoupled from the MoE\nbackbone that facilitates system-friendly pre-computing and lookahead\nscheduling, enhancing expert-aware batching and caching. Our codesign therefore\naddresses critical gaps on both the algorithmic and system fronts, establishing\na scalable and efficient alternative for LLM inference in resource-constrained\nsettings. Read-ME outperforms other popular open-source dense models of similar\nscales, achieving improvements of up to 10.1% on MMLU, and improving mean\nend-to-end latency up to 6.1%. Codes are available at:\nhttps://github.com/VITA-Group/READ-ME.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of large language models (LLMs) has led to the adoption of\nMixture-of-Experts (MoE) architectures that dynamically leverage specialized\nsubnetworks for improved efficiency and performance. Despite their benefits,\nMoE models face significant challenges during inference, including inefficient\nmemory management and suboptimal batching, due to misaligned design choices\nbetween the model architecture and the system policies. Furthermore, the\nconventional approach of training MoEs from scratch is increasingly prohibitive\nin terms of cost. In this paper, we propose a novel framework Read-ME that\ntransforms pre-trained dense LLMs into smaller MoE models (in contrast to\n\"upcycling\" generalist MoEs), avoiding the high costs of ground-up training.\nOur approach employs activation sparsity to extract experts. To compose\nexperts, we examine the widely-adopted layer-wise router design and show its\nredundancy, and thus we introduce the pre-gating router decoupled from the MoE\nbackbone that facilitates system-friendly pre-computing and lookahead\nscheduling, enhancing expert-aware batching and caching. Our codesign therefore\naddresses critical gaps on both the algorithmic and system fronts, establishing\na scalable and efficient alternative for LLM inference in resource-constrained\nsettings. Read-ME outperforms other popular open-source dense models of similar\nscales, achieving improvements of up to 10.1% on MMLU, and improving mean\nend-to-end latency up to 6.1%. Codes are available at:\nhttps://github.com/VITA-Group/READ-ME."
                },
                "authors": [
                    {
                        "name": "Ruisi Cai"
                    },
                    {
                        "name": "Yeonju Ro"
                    },
                    {
                        "name": "Geon-Woo Kim"
                    },
                    {
                        "name": "Peihao Wang"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Zhangyang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhangyang Wang"
                },
                "author": "Zhangyang Wang",
                "arxiv_comment": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15420v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15420v2",
                "updated": "2024-10-24T16:40:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    40,
                    10,
                    3,
                    298,
                    0
                ],
                "published": "2024-04-23T18:10:42Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    18,
                    10,
                    42,
                    1,
                    114,
                    0
                ],
                "title": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference"
                },
                "summary": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "João Monteiro"
                    },
                    {
                        "name": "Étienne Marcotte"
                    },
                    {
                        "name": "Pierre-André Noël"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "David Vázquez"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Christopher Pal"
                    },
                    {
                        "name": "Perouz Taslakian"
                    }
                ],
                "author_detail": {
                    "name": "Perouz Taslakian"
                },
                "author": "Perouz Taslakian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15420v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15420v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v1",
                "updated": "2024-10-24T10:36:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a Base Station (BS) associated with local cache,\nwhich in turn is connected to the backend server and users. The contents get\ncontinuously updated at the backend server, and the BS has a local copy of the\nsubset of the contents. Upon receiving a request from the user, the BS can\neither fetch a fresh version or, serve the local copy or can wait for\nadditional requests before serving. Fetching content from the BS incurs a fixed\nfetching cost, serving it locally incurs an aging cost, and for each request\nwaiting at the BS, there will be a waiting for cost per unit time. The aging\ncost relies on the freshness of the content, which is measured by a metric age\nof version (AoV). We aim to minimize the average cost subject to cache capacity\nconstraints. We pose the problem as Restless Multi-armed Bandits Problem (RMAB)\nand propose a Whittle index-based policy that performs very close to the\noptimal policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a Base Station (BS) associated with local cache,\nwhich in turn is connected to the backend server and users. The contents get\ncontinuously updated at the backend server, and the BS has a local copy of the\nsubset of the contents. Upon receiving a request from the user, the BS can\neither fetch a fresh version or, serve the local copy or can wait for\nadditional requests before serving. Fetching content from the BS incurs a fixed\nfetching cost, serving it locally incurs an aging cost, and for each request\nwaiting at the BS, there will be a waiting for cost per unit time. The aging\ncost relies on the freshness of the content, which is measured by a metric age\nof version (AoV). We aim to minimize the average cost subject to cache capacity\nconstraints. We pose the problem as Restless Multi-armed Bandits Problem (RMAB)\nand propose a Whittle index-based policy that performs very close to the\noptimal policy."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18517v1",
                "updated": "2024-10-24T08:06:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    6,
                    41,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T08:06:41Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    6,
                    41,
                    3,
                    298,
                    0
                ],
                "title": "KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing"
                },
                "summary": "The development of large language models (LLMs) has significantly expanded\nmodel sizes, resulting in substantial GPU memory requirements during inference.\nThe key and value storage of the attention map in the KV (key-value) cache\naccounts for more than 80\\% of this memory consumption. Nowadays, most existing\nKV cache compression methods focus on intra-layer compression within a single\nTransformer layer but few works consider layer-wise compression. In this paper,\nwe propose a plug-and-play method called \\textit{KVSharer}, which shares the KV\ncache between layers to achieve layer-wise compression. Rather than intuitively\nsharing based on higher similarity, we discover a counterintuitive phenomenon:\nsharing dissimilar KV caches better preserves the model performance.\nExperiments show that \\textit{KVSharer} can reduce KV cache computation by\n30\\%, thereby lowering memory consumption without significantly impacting model\nperformance and it can also achieve at least 1.3 times generation acceleration.\nAdditionally, we verify that \\textit{KVSharer} is compatible with existing\nintra-layer KV cache compression methods, and combining both can further save\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has significantly expanded\nmodel sizes, resulting in substantial GPU memory requirements during inference.\nThe key and value storage of the attention map in the KV (key-value) cache\naccounts for more than 80\\% of this memory consumption. Nowadays, most existing\nKV cache compression methods focus on intra-layer compression within a single\nTransformer layer but few works consider layer-wise compression. In this paper,\nwe propose a plug-and-play method called \\textit{KVSharer}, which shares the KV\ncache between layers to achieve layer-wise compression. Rather than intuitively\nsharing based on higher similarity, we discover a counterintuitive phenomenon:\nsharing dissimilar KV caches better preserves the model performance.\nExperiments show that \\textit{KVSharer} can reduce KV cache computation by\n30\\%, thereby lowering memory consumption without significantly impacting model\nperformance and it can also achieve at least 1.3 times generation acceleration.\nAdditionally, we verify that \\textit{KVSharer} is compatible with existing\nintra-layer KV cache compression methods, and combining both can further save\nmemory."
                },
                "authors": [
                    {
                        "name": "Yifei Yang"
                    },
                    {
                        "name": "Zouying Cao"
                    },
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Dongjie Yang"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "arxiv_comment": "Under Review by ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18441v1",
                "updated": "2024-10-24T05:29:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    29,
                    20,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T05:29:20Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    29,
                    20,
                    3,
                    298,
                    0
                ],
                "title": "The Nature of Mathematical Modeling and Probabilistic Optimization\n  Engineering in Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Nature of Mathematical Modeling and Probabilistic Optimization\n  Engineering in Generative AI"
                },
                "summary": "In this paper, we give an in-depth analysis on the mathematical problem\nformulations and the probabilistic optimization explorations for some of the\nkey components in Transformer model [33] in the field of generative AI. We\nexplore and discuss some potential further enhancement for current state of the\nart methods for some key underlying technologies of generative AI models from\nalgorithmic and probabilistic optimization perspective. In particular, we\npresent an optimal solution for sub-word encoding (SWE) based on similar\ninitial settings as that of byte-pair encoding (BPE) algorithm in [9] with\nsimilar objectives as that of WordPiece approach in [28, 31] to maximize the\nlikelihood of the training data. We also present cross entropy optimization\nmethod to optimize hyperparameters for word2vec model [17]. In addition, we\npropose a factored combination of rotary positional encoding (RoPE) [32] and\nattention with linear biases (ALiBi) [23] with a harmonic series. We also\npresent a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a\nprobability distribution over block distances in the matrix to decide which\nblock is likely to participate in a given round of attention computation while\nmaintaining the lower triangle shape of the tensor for autoregressive language\nmodels by re-shaping the tensors. Finally, we present staircase adaptive\nquantization (SAQ) of key-value (KV) cache for multi-query attention (MQA)\nbased on the framework presented in [16] to have gradual quantization\ndegradation while achieving reasonable model quality and cost savings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we give an in-depth analysis on the mathematical problem\nformulations and the probabilistic optimization explorations for some of the\nkey components in Transformer model [33] in the field of generative AI. We\nexplore and discuss some potential further enhancement for current state of the\nart methods for some key underlying technologies of generative AI models from\nalgorithmic and probabilistic optimization perspective. In particular, we\npresent an optimal solution for sub-word encoding (SWE) based on similar\ninitial settings as that of byte-pair encoding (BPE) algorithm in [9] with\nsimilar objectives as that of WordPiece approach in [28, 31] to maximize the\nlikelihood of the training data. We also present cross entropy optimization\nmethod to optimize hyperparameters for word2vec model [17]. In addition, we\npropose a factored combination of rotary positional encoding (RoPE) [32] and\nattention with linear biases (ALiBi) [23] with a harmonic series. We also\npresent a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a\nprobability distribution over block distances in the matrix to decide which\nblock is likely to participate in a given round of attention computation while\nmaintaining the lower triangle shape of the tensor for autoregressive language\nmodels by re-shaping the tensors. Finally, we present staircase adaptive\nquantization (SAQ) of key-value (KV) cache for multi-query attention (MQA)\nbased on the framework presented in [16] to have gradual quantization\ndegradation while achieving reasonable model quality and cost savings."
                },
                "authors": [
                    {
                        "name": "Fulu Li"
                    }
                ],
                "author_detail": {
                    "name": "Fulu Li"
                },
                "author": "Fulu Li",
                "arxiv_comment": "19 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18248v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18248v2",
                "updated": "2024-10-25T19:18:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    0,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-23T19:53:30Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    19,
                    53,
                    30,
                    2,
                    297,
                    0
                ],
                "title": "Fast Inference for Augmented Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Inference for Augmented Large Language Models"
                },
                "summary": "Augmented Large Language Models (LLMs) enhance the capabilities of standalone\nLLMs by integrating external data sources through API calls. In interactive LLM\napplications, efficient scheduling is crucial for maintaining low request\ncompletion times, directly impacting user engagement. However, these\naugmentations introduce scheduling challenges due to the need to manage limited\nmemory for cached information (KV caches). As a result, traditional size-based\nscheduling algorithms, such as Shortest Job First (SJF), become less effective\nat minimizing completion times. Existing work focuses only on handling requests\nduring API calls by preserving, discarding, or swapping memory without\nconsidering how to schedule requests with API calls. In this paper, we propose\nLAMPS, a novel LLM inference framework for augmented LLMs. LAMPS minimizes\nrequest completion time through a unified scheduling approach that considers\nthe total length of requests and their handling strategies during API calls.\nRecognizing that LLM inference is memory-bound, our approach ranks requests\nbased on their consumption of memory over time, which depends on both the\noutput sizes and how a request is managed during its API calls. To implement\nour scheduling, LAMPS predicts the strategy that minimizes memory waste of a\nrequest during its API calls, aligning with but improving upon existing\napproaches. We also propose starvation prevention techniques and optimizations\nto mitigate the overhead of our scheduling. We implement LAMPS on top of vLLM\nand evaluate its performance against baseline LLM inference systems,\ndemonstrating improvements in end-to-end latency by 27%-85% and reductions in\nTTFT by 4%-96% compared to the existing augmented-LLM system, with even greater\ngains over vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented Large Language Models (LLMs) enhance the capabilities of standalone\nLLMs by integrating external data sources through API calls. In interactive LLM\napplications, efficient scheduling is crucial for maintaining low request\ncompletion times, directly impacting user engagement. However, these\naugmentations introduce scheduling challenges due to the need to manage limited\nmemory for cached information (KV caches). As a result, traditional size-based\nscheduling algorithms, such as Shortest Job First (SJF), become less effective\nat minimizing completion times. Existing work focuses only on handling requests\nduring API calls by preserving, discarding, or swapping memory without\nconsidering how to schedule requests with API calls. In this paper, we propose\nLAMPS, a novel LLM inference framework for augmented LLMs. LAMPS minimizes\nrequest completion time through a unified scheduling approach that considers\nthe total length of requests and their handling strategies during API calls.\nRecognizing that LLM inference is memory-bound, our approach ranks requests\nbased on their consumption of memory over time, which depends on both the\noutput sizes and how a request is managed during its API calls. To implement\nour scheduling, LAMPS predicts the strategy that minimizes memory waste of a\nrequest during its API calls, aligning with but improving upon existing\napproaches. We also propose starvation prevention techniques and optimizations\nto mitigate the overhead of our scheduling. We implement LAMPS on top of vLLM\nand evaluate its performance against baseline LLM inference systems,\ndemonstrating improvements in end-to-end latency by 27%-85% and reductions in\nTTFT by 4%-96% compared to the existing augmented-LLM system, with even greater\ngains over vLLM."
                },
                "authors": [
                    {
                        "name": "Rana Shahout"
                    },
                    {
                        "name": "Cong Liang"
                    },
                    {
                        "name": "Shiji Xin"
                    },
                    {
                        "name": "Qianru Lao"
                    },
                    {
                        "name": "Yong Cui"
                    },
                    {
                        "name": "Minlan Yu"
                    },
                    {
                        "name": "Michael Mitzenmacher"
                    }
                ],
                "author_detail": {
                    "name": "Michael Mitzenmacher"
                },
                "author": "Michael Mitzenmacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18248v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18248v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18002v1",
                "updated": "2024-10-23T16:25:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T16:25:22Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "title": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges"
                },
                "summary": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks."
                },
                "authors": [
                    {
                        "name": "Yuchen Liu"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Hanzhi Yu"
                    },
                    {
                        "name": "Mingzhe Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mingzhe Chen"
                },
                "author": "Mingzhe Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08437v2",
                "updated": "2024-10-23T15:44:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    44,
                    9,
                    2,
                    297,
                    0
                ],
                "published": "2023-10-12T16:01:46Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    16,
                    1,
                    46,
                    3,
                    285,
                    0
                ],
                "title": "Cold Start Latency in Serverless Computing: A Systematic Review,\n  Taxonomy, and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold Start Latency in Serverless Computing: A Systematic Review,\n  Taxonomy, and Future Directions"
                },
                "summary": "Recently, academics and the corporate sector have paid attention to\nserverless computing, which enables dynamic scalability and an economic model.\nIn serverless computing, users only pay for the time they actually use\nresources, enabling zero scaling to optimise cost and resource utilisation.\nHowever, this approach also introduces the serverless cold start problem.\nResearchers have developed various solutions to address the cold start problem,\nyet it remains an unresolved research area. In this article, we propose a\nsystematic literature review on clod start latency in serverless computing.\nFurthermore, we create a detailed taxonomy of approaches to cold start latency,\nwhich we use to investigate existing techniques for reducing the cold start\ntime and frequency. We have classified the current studies on cold start\nlatency into several categories such as caching and application-level\noptimisation-based solutions, as well as Artificial Intelligence (AI)/Machine\nLearning (ML)-based solutions. Moreover, we have analyzed the impact of cold\nstart latency on quality of service, explored current cold start latency\nmitigation methods, datasets, and implementation platforms, and classified them\ninto categories based on their common characteristics and features. Finally, we\noutline the open challenges and highlight the possible future directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, academics and the corporate sector have paid attention to\nserverless computing, which enables dynamic scalability and an economic model.\nIn serverless computing, users only pay for the time they actually use\nresources, enabling zero scaling to optimise cost and resource utilisation.\nHowever, this approach also introduces the serverless cold start problem.\nResearchers have developed various solutions to address the cold start problem,\nyet it remains an unresolved research area. In this article, we propose a\nsystematic literature review on clod start latency in serverless computing.\nFurthermore, we create a detailed taxonomy of approaches to cold start latency,\nwhich we use to investigate existing techniques for reducing the cold start\ntime and frequency. We have classified the current studies on cold start\nlatency into several categories such as caching and application-level\noptimisation-based solutions, as well as Artificial Intelligence (AI)/Machine\nLearning (ML)-based solutions. Moreover, we have analyzed the impact of cold\nstart latency on quality of service, explored current cold start latency\nmitigation methods, datasets, and implementation platforms, and classified them\ninto categories based on their common characteristics and features. Finally, we\noutline the open challenges and highlight the possible future directions."
                },
                "authors": [
                    {
                        "name": "Muhammed Golec"
                    },
                    {
                        "name": "Guneet Kaur Walia"
                    },
                    {
                        "name": "Mohit Kumar"
                    },
                    {
                        "name": "Felix Cuadrado"
                    },
                    {
                        "name": "Sukhpal Singh Gill"
                    },
                    {
                        "name": "Steve Uhlig"
                    }
                ],
                "author_detail": {
                    "name": "Steve Uhlig"
                },
                "author": "Steve Uhlig",
                "arxiv_doi": "10.1145/3700875",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3700875",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.08437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Preprint Version Accepted for Publication in ACM Computing Survey,\n  2024",
                "arxiv_journal_ref": "ACM Computing Surveys 2024",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17954v1",
                "updated": "2024-10-23T15:24:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    54,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T15:24:54Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    54,
                    2,
                    297,
                    0
                ],
                "title": "ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference"
                },
                "summary": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios."
                },
                "authors": [
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Shunkang Zhang"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Haiyan Yin"
                    },
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Shaohuai Shi"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Ivor Tsang"
                    },
                    {
                        "name": "Ong Yew Soon"
                    }
                ],
                "author_detail": {
                    "name": "Ong Yew Soon"
                },
                "author": "Ong Yew Soon",
                "arxiv_comment": "Mixture-of-Experts, Inference, Offloading",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v1",
                "updated": "2024-10-23T14:15:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the KV cache by nearly 50%. Comprehensive empirical\nevidence demonstrates that ResFormer mitigates attention concentration problem\nin deeper layers and enhances representation across most layers, outperforming\nthe vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as\ndownstream tasks. SVFormer trains significantly faster than the vanilla\nTransformer and performs better than other methods like GQA and CLA, with\nperformance influenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the KV cache by nearly 50%. Comprehensive empirical\nevidence demonstrates that ResFormer mitigates attention concentration problem\nin deeper layers and enhances representation across most layers, outperforming\nthe vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as\ndownstream tasks. SVFormer trains significantly faster than the vanilla\nTransformer and performs better than other methods like GQA and CLA, with\nperformance influenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05118v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05118v3",
                "updated": "2024-10-23T10:39:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    10,
                    39,
                    15,
                    2,
                    297,
                    0
                ],
                "published": "2024-05-08T15:16:02Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    15,
                    16,
                    2,
                    2,
                    129,
                    0
                ],
                "title": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms"
                },
                "summary": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning."
                },
                "authors": [
                    {
                        "name": "Ari Rasch"
                    }
                ],
                "author_detail": {
                    "name": "Ari Rasch"
                },
                "author": "Ari Rasch",
                "arxiv_doi": "10.1145/3665643",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3665643",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.05118v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05118v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A short version of this paper is published at ACM TOPLAS and\n  presented at PLDI'24",
                "arxiv_journal_ref": "ACM Trans. Program. Lang. Syst. (May 2024)",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17635v1",
                "updated": "2024-10-23T07:53:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T07:53:29Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "title": "Markov Chain of Thought for Efficient Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov Chain of Thought for Efficient Mathematical Reasoning"
                },
                "summary": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, ``derive, then reduce'', we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the \\texttt{MCoTInstruct} dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, ``derive, then reduce'', we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the \\texttt{MCoTInstruct} dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Wen Yang"
                    },
                    {
                        "name": "Kai Fan"
                    },
                    {
                        "name": "Minpeng Liao"
                    }
                ],
                "author_detail": {
                    "name": "Minpeng Liao"
                },
                "author": "Minpeng Liao",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v5",
                "updated": "2024-10-23T05:55:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    5,
                    55,
                    31,
                    2,
                    297,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14740v2",
                "updated": "2024-10-23T01:08:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    1,
                    8,
                    59,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-17T08:33:39Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    8,
                    33,
                    39,
                    3,
                    291,
                    0
                ],
                "title": "Harnessing Your DRAM and SSD for Sustainable and Accessible LLM\n  Inference with Mixed-Precision and Multi-level Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Your DRAM and SSD for Sustainable and Accessible LLM\n  Inference with Mixed-Precision and Multi-level Caching"
                },
                "summary": "Although Large Language Models (LLMs) have demonstrated remarkable\ncapabilities, their massive parameter counts and associated extensive computing\nmake LLMs' deployment the main part of carbon emission from nowadays AI\napplications. Compared to modern GPUs like H$100$, it would be significantly\ncarbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as\nshown in Figure 1, M$40$ only has one third carbon emission of H$100$'s) for\nLLM servings. However, the limited High Bandwidth Memory (HBM) available on\nsuch GPU often cannot support the loading of LLMs due to the gigantic model\nsize and intermediate activation data, making their serving challenging. For\ninstance, a LLaMA2 model with $70$B parameters typically requires $128$GB for\ninference, which substantially surpasses $24$GB HBM in a $3090$ GPU and remains\ninfeasible even considering the additional $64$GB DRAM. To address this\nchallenge, this paper proposes a mixed-precision with a model modularization\nalgorithm to enable LLM inference on outdated hardware with resource\nconstraints. (The precision denotes the numerical precision like FP16, INT8,\nINT4) and multi-level caching (M2Cache).)\n  Specifically, our M2Cache first modulizes neurons in LLM and creates their\nimportance ranking. Then, it adopts a dynamic sparse mixed-precision\nquantization mechanism in weight space to reduce computational demands and\ncommunication overhead at each decoding step. It collectively lowers the\noperational carbon emissions associated with LLM inference. Moreover, M2Cache\nintroduces a three-level cache management system with HBM, DRAM, and SSDs that\ncomplements the dynamic sparse mixed-precision inference. To enhance\ncommunication efficiency, M2Cache maintains a neuron-level mixed-precision LRU\ncache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models (LLMs) have demonstrated remarkable\ncapabilities, their massive parameter counts and associated extensive computing\nmake LLMs' deployment the main part of carbon emission from nowadays AI\napplications. Compared to modern GPUs like H$100$, it would be significantly\ncarbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as\nshown in Figure 1, M$40$ only has one third carbon emission of H$100$'s) for\nLLM servings. However, the limited High Bandwidth Memory (HBM) available on\nsuch GPU often cannot support the loading of LLMs due to the gigantic model\nsize and intermediate activation data, making their serving challenging. For\ninstance, a LLaMA2 model with $70$B parameters typically requires $128$GB for\ninference, which substantially surpasses $24$GB HBM in a $3090$ GPU and remains\ninfeasible even considering the additional $64$GB DRAM. To address this\nchallenge, this paper proposes a mixed-precision with a model modularization\nalgorithm to enable LLM inference on outdated hardware with resource\nconstraints. (The precision denotes the numerical precision like FP16, INT8,\nINT4) and multi-level caching (M2Cache).)\n  Specifically, our M2Cache first modulizes neurons in LLM and creates their\nimportance ranking. Then, it adopts a dynamic sparse mixed-precision\nquantization mechanism in weight space to reduce computational demands and\ncommunication overhead at each decoding step. It collectively lowers the\noperational carbon emissions associated with LLM inference. Moreover, M2Cache\nintroduces a three-level cache management system with HBM, DRAM, and SSDs that\ncomplements the dynamic sparse mixed-precision inference. To enhance\ncommunication efficiency, M2Cache maintains a neuron-level mixed-precision LRU\ncache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD."
                },
                "authors": [
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Zhang Cao"
                    },
                    {
                        "name": "Huaizhi Qu"
                    },
                    {
                        "name": "Zhengyu Zhang"
                    },
                    {
                        "name": "Chang Guo"
                    },
                    {
                        "name": "Yanyong Zhang"
                    },
                    {
                        "name": "Zhichao Cao"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "24 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11724v2",
                "updated": "2024-10-22T19:07:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    22,
                    19,
                    7,
                    8,
                    1,
                    296,
                    0
                ],
                "published": "2024-05-20T01:57:34Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    1,
                    57,
                    34,
                    0,
                    141,
                    0
                ],
                "title": "Token-wise Influential Training Data Retrieval for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-wise Influential Training Data Retrieval for Large Language Models"
                },
                "summary": "Given a Large Language Model (LLM) generation, how can we identify which\ntraining data led to this generation? In this paper, we proposed RapidIn, a\nscalable framework adapting to LLMs for estimating the influence of each\ntraining data. The proposed framework consists of two stages: caching and\nretrieval. First, we compress the gradient vectors by over 200,000x, allowing\nthem to be cached on disk or in GPU/CPU memory. Then, given a generation,\nRapidIn efficiently traverses the cached gradients to estimate the influence\nwithin minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports\nmulti-GPU parallelization to substantially accelerate caching and retrieval.\nOur empirical result confirms the efficiency and effectiveness of RapidIn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given a Large Language Model (LLM) generation, how can we identify which\ntraining data led to this generation? In this paper, we proposed RapidIn, a\nscalable framework adapting to LLMs for estimating the influence of each\ntraining data. The proposed framework consists of two stages: caching and\nretrieval. First, we compress the gradient vectors by over 200,000x, allowing\nthem to be cached on disk or in GPU/CPU memory. Then, given a generation,\nRapidIn efficiently traverses the cached gradients to estimate the influence\nwithin minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports\nmulti-GPU parallelization to substantially accelerate caching and retrieval.\nOur empirical result confirms the efficiency and effectiveness of RapidIn."
                },
                "authors": [
                    {
                        "name": "Huawei Lin"
                    },
                    {
                        "name": "Jikai Long"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Weijie Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Weijie Zhao"
                },
                "author": "Weijie Zhao",
                "arxiv_comment": "Accepted to ACL 2024. Keywords: Influence Function, Influence\n  Estimation, Training Data Attribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.11724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18400v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18400v5",
                "updated": "2024-10-21T22:56:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    22,
                    56,
                    6,
                    0,
                    295,
                    0
                ],
                "published": "2024-05-28T17:40:48Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    17,
                    40,
                    48,
                    1,
                    149,
                    0
                ],
                "title": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass"
                },
                "summary": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding."
                },
                "authors": [
                    {
                        "name": "Ethan Shen"
                    },
                    {
                        "name": "Alan Fan"
                    },
                    {
                        "name": "Sarah M. Pratt"
                    },
                    {
                        "name": "Jae Sung Park"
                    },
                    {
                        "name": "Matthew Wallingford"
                    },
                    {
                        "name": "Sham M. Kakade"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Ali Farhadi"
                    },
                    {
                        "name": "Aditya Kusupati"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Kusupati"
                },
                "author": "Aditya Kusupati",
                "arxiv_comment": "23 pages, 16 figures, accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18400v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18400v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16218v1",
                "updated": "2024-10-21T17:23:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    23,
                    3,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T17:23:03Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    23,
                    3,
                    0,
                    295,
                    0
                ],
                "title": "3 kV Monolithic Bidirectional GaN HEMT on Sapphire",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3 kV Monolithic Bidirectional GaN HEMT on Sapphire"
                },
                "summary": "More than 3 kV breakdown voltage was demonstrated in monolithic bidirectional\nGaN HEMTs for the first time having potential applications in 1200V or\n1700V-class novel power converters. The on resistance of the fabricated\ntransistors was ~20 ohm.mm or ~11 mili ohm.cm^2. Breakdown voltage was\noptimized by utilizing two field plates in either side of the transistor and\noptimizing their geometry. Shorter first field plate lengths (less than 2\nmicron) resulted in higher breakdown voltage and the possible reason for this\nwas discussed. The transistors had a steep subthreshold swing of 92 mV / dec.\nThe on/off ratio was greater than 10^5 and it was limited by the tool capacity.\nThe fabricated 3 kV transistor was benchmarked against the state-of-the-art\nmonolithic bidirectional GaN HEMTs in the performance matrices of breakdown\nvoltage and on resistance, that showed crucial progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More than 3 kV breakdown voltage was demonstrated in monolithic bidirectional\nGaN HEMTs for the first time having potential applications in 1200V or\n1700V-class novel power converters. The on resistance of the fabricated\ntransistors was ~20 ohm.mm or ~11 mili ohm.cm^2. Breakdown voltage was\noptimized by utilizing two field plates in either side of the transistor and\noptimizing their geometry. Shorter first field plate lengths (less than 2\nmicron) resulted in higher breakdown voltage and the possible reason for this\nwas discussed. The transistors had a steep subthreshold swing of 92 mV / dec.\nThe on/off ratio was greater than 10^5 and it was limited by the tool capacity.\nThe fabricated 3 kV transistor was benchmarked against the state-of-the-art\nmonolithic bidirectional GaN HEMTs in the performance matrices of breakdown\nvoltage and on resistance, that showed crucial progress."
                },
                "authors": [
                    {
                        "name": "Md Tahmidul Alam"
                    },
                    {
                        "name": "Swarnav Mukhopadhyay"
                    },
                    {
                        "name": "Md Mobinul Haque"
                    },
                    {
                        "name": "Shubhra S. Pasayat"
                    },
                    {
                        "name": "Chirag Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Chirag Gupta"
                },
                "author": "Chirag Gupta",
                "arxiv_comment": "4 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v1",
                "updated": "2024-10-21T16:44:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13761v2",
                "updated": "2024-10-21T15:59:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    15,
                    59,
                    18,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-16T18:46:24Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    18,
                    46,
                    24,
                    0,
                    260,
                    0
                ],
                "title": "Do Large Language Models Need a Content Delivery Network?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Need a Content Delivery Network?"
                },
                "summary": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15908v1",
                "updated": "2024-10-21T11:29:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T11:29:49Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "title": "Formalising CXL Cache Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formalising CXL Cache Coherence"
                },
                "summary": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs."
                },
                "authors": [
                    {
                        "name": "Chengsong Tan"
                    },
                    {
                        "name": "Alastair F. Donaldson"
                    },
                    {
                        "name": "John Wickerson"
                    }
                ],
                "author_detail": {
                    "name": "John Wickerson"
                },
                "author": "John Wickerson",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14142v2",
                "updated": "2024-10-21T07:24:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    24,
                    53,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-18T03:30:25Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    3,
                    30,
                    25,
                    4,
                    292,
                    0
                ],
                "title": "Secure Collaborative Computation Offloading and Resource Allocation in\n  Cache-Assisted Ultra-Dense IoT Networks With Multi-Slope Channels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure Collaborative Computation Offloading and Resource Allocation in\n  Cache-Assisted Ultra-Dense IoT Networks With Multi-Slope Channels"
                },
                "summary": "Cache-assisted ultra-dense mobile edge computing (MEC) networks are a\npromising solution for meeting the increasing demands of numerous\nInternet-of-Things mobile devices (IMDs). To address the complex interferences\ncaused by small base stations (SBSs) deployed densely in such networks, this\npaper explores the combination of orthogonal frequency division multiple access\n(OFDMA), non-orthogonal multiple access (NOMA), and base station (BS)\nclustering. Additionally, security measures are introduced to protect IMDs'\ntasks offloaded to BSs from potential eavesdropping and malicious attacks. As\nfor such a network framework, a computation offloading scheme is proposed to\nminimize IMDs' energy consumption while considering constraints such as delay,\npower, computing resources, and security costs, optimizing channel selections,\ntask execution decisions, device associations, power controls, security service\nassignments, and computing resource allocations. To solve the formulated\nproblem efficiently, we develop a further improved hierarchical adaptive search\n(FIHAS) algorithm, giving some insights into its parallel implementation,\ncomputation complexity, and convergence. Simulation results demonstrate that\nthe proposed algorithms can achieve lower total energy consumption and delay\ncompared to other algorithms when strict latency and cost constraints are\nimposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-assisted ultra-dense mobile edge computing (MEC) networks are a\npromising solution for meeting the increasing demands of numerous\nInternet-of-Things mobile devices (IMDs). To address the complex interferences\ncaused by small base stations (SBSs) deployed densely in such networks, this\npaper explores the combination of orthogonal frequency division multiple access\n(OFDMA), non-orthogonal multiple access (NOMA), and base station (BS)\nclustering. Additionally, security measures are introduced to protect IMDs'\ntasks offloaded to BSs from potential eavesdropping and malicious attacks. As\nfor such a network framework, a computation offloading scheme is proposed to\nminimize IMDs' energy consumption while considering constraints such as delay,\npower, computing resources, and security costs, optimizing channel selections,\ntask execution decisions, device associations, power controls, security service\nassignments, and computing resource allocations. To solve the formulated\nproblem efficiently, we develop a further improved hierarchical adaptive search\n(FIHAS) algorithm, giving some insights into its parallel implementation,\ncomputation complexity, and convergence. Simulation results demonstrate that\nthe proposed algorithms can achieve lower total energy consumption and delay\ncompared to other algorithms when strict latency and cost constraints are\nimposed."
                },
                "authors": [
                    {
                        "name": "Tianqing Zhou"
                    },
                    {
                        "name": "Bobo Wang"
                    },
                    {
                        "name": "Dong Qin"
                    },
                    {
                        "name": "Xuefang Nie"
                    },
                    {
                        "name": "Nan Jiang"
                    },
                    {
                        "name": "Chunguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Chunguo Li"
                },
                "author": "Chunguo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15704v1",
                "updated": "2024-10-21T07:20:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    20,
                    41,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T07:20:41Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    20,
                    41,
                    0,
                    295,
                    0
                ],
                "title": "Residual vector quantization for KV cache compression in large language\n  model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Residual vector quantization for KV cache compression in large language\n  model"
                },
                "summary": "KV cache compression methods have mainly relied on scalar quantization\ntechniques to reduce the memory requirements during decoding. In this work, we\napply residual vector quantization, which has been widely used for high\nfidelity audio compression, to compress KV cache in large language models\n(LLM). We adapt the standard recipe with minimal changes to compress the output\nof any key or value projection matrix in a pretrained LLM: we scale the vector\nby its standard deviation, divide channels into groups and then quantize each\ngroup with the same residual vector quantizer. We learn the codebook using\nexponential moving average and there are no other learnable parameters\nincluding the input and output projections normally used in a vector\nquantization set up. We find that a residual depth of 8 recovers most of the\nperformance of the unquantized model. We also find that grouping non-contiguous\nchannels together works better than grouping contiguous channels for\ncompressing key matrix and the method further benefits from a light weight\nfinetuning of LLM together with the quantization. Overall, the proposed\ntechnique is competitive with existing quantization methods while being much\nsimpler and results in 5.5x compression compared to half precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache compression methods have mainly relied on scalar quantization\ntechniques to reduce the memory requirements during decoding. In this work, we\napply residual vector quantization, which has been widely used for high\nfidelity audio compression, to compress KV cache in large language models\n(LLM). We adapt the standard recipe with minimal changes to compress the output\nof any key or value projection matrix in a pretrained LLM: we scale the vector\nby its standard deviation, divide channels into groups and then quantize each\ngroup with the same residual vector quantizer. We learn the codebook using\nexponential moving average and there are no other learnable parameters\nincluding the input and output projections normally used in a vector\nquantization set up. We find that a residual depth of 8 recovers most of the\nperformance of the unquantized model. We also find that grouping non-contiguous\nchannels together works better than grouping contiguous channels for\ncompressing key matrix and the method further benefits from a light weight\nfinetuning of LLM together with the quantization. Overall, the proposed\ntechnique is competitive with existing quantization methods while being much\nsimpler and results in 5.5x compression compared to half precision."
                },
                "authors": [
                    {
                        "name": "Ankur Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Ankur Kumar"
                },
                "author": "Ankur Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16546v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16546v2",
                "updated": "2024-10-21T05:06:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    5,
                    6,
                    1,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-25T01:39:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization"
                },
                "summary": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision."
                },
                "authors": [
                    {
                        "name": "Yifan Tan"
                    },
                    {
                        "name": "Haoze Wang"
                    },
                    {
                        "name": "Chao Yan"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16546v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16546v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09202v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09202v2",
                "updated": "2024-10-21T02:35:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    2,
                    35,
                    8,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-13T21:31:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions"
                },
                "summary": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. In those tests, WarmSwap\naccelerates dependency loading for serverless functions with large dependency\nrequirements by a factor ranging from 2.2 to 3.2. Simulation experiments using\nAzure traces indicate that WarmSwap can save 88\\% of optimization space when\nsharing a dependency image among ten different functions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. In those tests, WarmSwap\naccelerates dependency loading for serverless functions with large dependency\nrequirements by a factor ranging from 2.2 to 3.2. Simulation experiments using\nAzure traces indicate that WarmSwap can save 88\\% of optimization space when\nsharing a dependency image among ten different functions."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Devesh Tiwari"
                    },
                    {
                        "name": "Gene Cooperman"
                    }
                ],
                "author_detail": {
                    "name": "Gene Cooperman"
                },
                "author": "Gene Cooperman",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09202v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09202v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04053v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04053v2",
                "updated": "2024-10-20T13:37:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    13,
                    37,
                    46,
                    6,
                    294,
                    0
                ],
                "published": "2024-07-04T16:51:17Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    16,
                    51,
                    17,
                    3,
                    186,
                    0
                ],
                "title": "Edge AI: A Taxonomy, Systematic Review and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge AI: A Taxonomy, Systematic Review and Future Directions"
                },
                "summary": "Edge Artificial Intelligence (AI) incorporates a network of interconnected\nsystems and devices that receive, cache, process, and analyze data in close\ncommunication with the location where the data is captured with AI technology.\nRecent advancements in AI efficiency, the widespread use of Internet of Things\n(IoT) devices, and the emergence of edge computing have unlocked the enormous\nscope of Edge AI. Edge AI aims to optimize data processing efficiency and\nvelocity while ensuring data confidentiality and integrity. Despite being a\nrelatively new field of research from 2014 to the present, it has shown\nsignificant and rapid development over the last five years. This article\npresents a systematic literature review for Edge AI to discuss the existing\nresearch, recent advancements, and future research directions. We created a\ncollaborative edge AI learning system for cloud and edge computing analysis,\nincluding an in-depth study of the architectures that facilitate this\nmechanism. The taxonomy for Edge AI facilitates the classification and\nconfiguration of Edge AI systems while examining its potential influence across\nmany fields through compassing infrastructure, cloud computing, fog computing,\nservices, use cases, ML and deep learning, and resource management. This study\nhighlights the significance of Edge AI in processing real-time data at the edge\nof the network. Additionally, it emphasizes the research challenges encountered\nby Edge AI systems, including constraints on resources, vulnerabilities to\nsecurity threats, and problems with scalability. Finally, this study highlights\nthe potential future research directions that aim to address the current\nlimitations of Edge AI by providing innovative solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Artificial Intelligence (AI) incorporates a network of interconnected\nsystems and devices that receive, cache, process, and analyze data in close\ncommunication with the location where the data is captured with AI technology.\nRecent advancements in AI efficiency, the widespread use of Internet of Things\n(IoT) devices, and the emergence of edge computing have unlocked the enormous\nscope of Edge AI. Edge AI aims to optimize data processing efficiency and\nvelocity while ensuring data confidentiality and integrity. Despite being a\nrelatively new field of research from 2014 to the present, it has shown\nsignificant and rapid development over the last five years. This article\npresents a systematic literature review for Edge AI to discuss the existing\nresearch, recent advancements, and future research directions. We created a\ncollaborative edge AI learning system for cloud and edge computing analysis,\nincluding an in-depth study of the architectures that facilitate this\nmechanism. The taxonomy for Edge AI facilitates the classification and\nconfiguration of Edge AI systems while examining its potential influence across\nmany fields through compassing infrastructure, cloud computing, fog computing,\nservices, use cases, ML and deep learning, and resource management. This study\nhighlights the significance of Edge AI in processing real-time data at the edge\nof the network. Additionally, it emphasizes the research challenges encountered\nby Edge AI systems, including constraints on resources, vulnerabilities to\nsecurity threats, and problems with scalability. Finally, this study highlights\nthe potential future research directions that aim to address the current\nlimitations of Edge AI by providing innovative solutions."
                },
                "authors": [
                    {
                        "name": "Sukhpal Singh Gill"
                    },
                    {
                        "name": "Muhammed Golec"
                    },
                    {
                        "name": "Jianmin Hu"
                    },
                    {
                        "name": "Minxian Xu"
                    },
                    {
                        "name": "Junhui Du"
                    },
                    {
                        "name": "Huaming Wu"
                    },
                    {
                        "name": "Guneet Kaur Walia"
                    },
                    {
                        "name": "Subramaniam Subramanian Murugesan"
                    },
                    {
                        "name": "Babar Ali"
                    },
                    {
                        "name": "Mohit Kumar"
                    },
                    {
                        "name": "Kejiang Ye"
                    },
                    {
                        "name": "Prabal Verma"
                    },
                    {
                        "name": "Surendra Kumar"
                    },
                    {
                        "name": "Felix Cuadrado"
                    },
                    {
                        "name": "Steve Uhlig"
                    }
                ],
                "author_detail": {
                    "name": "Steve Uhlig"
                },
                "author": "Steve Uhlig",
                "arxiv_doi": "10.1007/s10586-024-04686-y",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10586-024-04686-y",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.04053v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04053v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Preprint Version Accepted for Publication in Springer Cluster\n  Computing, 2024",
                "arxiv_journal_ref": "Springer Cluster Computing, Volume 28, article number 18, pages\n  11953 - 11981, (2025)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15344v1",
                "updated": "2024-10-20T09:37:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    9,
                    37,
                    7,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T09:37:07Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    9,
                    37,
                    7,
                    6,
                    294,
                    0
                ],
                "title": "LLC Intra-set Write Balancing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLC Intra-set Write Balancing"
                },
                "summary": "The increasing use of Non-Volatile Memory (NVM) in computer architecture has\nbrought about new challenges, one of which is the write endurance problem.\nFrequent writes to a particular cache cell in NVM can lead to degradation of\nthe memory cell and reduce its lifespan. To solve this problem, we propose a\nsample-based blocking technique for the Last Level Cache (LLC). Our approach\ninvolves defining a threshold value and sampling a subset of cache sets. If the\nnumber of writes to a way in a sampled set exceeds the threshold, the way is\nblocked, and writes are redirected to other ways. We also maintain a history\nstructure to record the number of writes in a set and a PC-Table to use for\nblocking in unsampled sets. Based on blocking on sampled sets, variance of\nvalues stored in history is used to determine whether blocking had a positive\nimpact or not, and on this basis, value corresponding to instruction pointer is\nincremented or decremented. This value is later used for blocking in unsampled\nsets. Our results show that our approach significantly balances write traffic\nto the cache and improves the overall lifespan of the memory cells while having\nbetter performance to the base-line system. Our approach can also be applied to\nother cache hierarchies and NVM technologies to mitigate the problem of write\nendurance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing use of Non-Volatile Memory (NVM) in computer architecture has\nbrought about new challenges, one of which is the write endurance problem.\nFrequent writes to a particular cache cell in NVM can lead to degradation of\nthe memory cell and reduce its lifespan. To solve this problem, we propose a\nsample-based blocking technique for the Last Level Cache (LLC). Our approach\ninvolves defining a threshold value and sampling a subset of cache sets. If the\nnumber of writes to a way in a sampled set exceeds the threshold, the way is\nblocked, and writes are redirected to other ways. We also maintain a history\nstructure to record the number of writes in a set and a PC-Table to use for\nblocking in unsampled sets. Based on blocking on sampled sets, variance of\nvalues stored in history is used to determine whether blocking had a positive\nimpact or not, and on this basis, value corresponding to instruction pointer is\nincremented or decremented. This value is later used for blocking in unsampled\nsets. Our results show that our approach significantly balances write traffic\nto the cache and improves the overall lifespan of the memory cells while having\nbetter performance to the base-line system. Our approach can also be applied to\nother cache hierarchies and NVM technologies to mitigate the problem of write\nendurance."
                },
                "authors": [
                    {
                        "name": "Keshav Krishna"
                    },
                    {
                        "name": "Ayush Verma"
                    }
                ],
                "author_detail": {
                    "name": "Ayush Verma"
                },
                "author": "Ayush Verma",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15332v1",
                "updated": "2024-10-20T08:42:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T08:42:29Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "title": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Haoyi Wang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Tao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xie"
                },
                "author": "Tao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15252v1",
                "updated": "2024-10-20T02:17:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    2,
                    17,
                    35,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T02:17:35Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    2,
                    17,
                    35,
                    6,
                    294,
                    0
                ],
                "title": "Lossless KV Cache Compression to 2%",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lossless KV Cache Compression to 2%"
                },
                "summary": "Large language models have revolutionized data processing in numerous\ndomains, with their ability to handle extended context reasoning receiving\nnotable recognition. To speed up inference, maintaining a key-value (KV) cache\nmemory is essential. Nonetheless, the growing demands for KV cache memory\ncreate significant hurdles for efficient implementation. This work introduces a\nnovel architecture, Cross-Layer Latent Attention (CLLA), aimed at compressing\nthe KV cache to less than 2% of its original size while maintaining comparable\nperformance levels. CLLA integrates multiple aspects of KV cache compression,\nincluding attention head/dimension reduction, layer sharing, and quantization\ntechniques, into a cohesive framework. Our extensive experiments demonstrate\nthat CLLA achieves lossless performance on most tasks while utilizing minimal\nKV cache, marking a significant advancement in practical KV cache compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have revolutionized data processing in numerous\ndomains, with their ability to handle extended context reasoning receiving\nnotable recognition. To speed up inference, maintaining a key-value (KV) cache\nmemory is essential. Nonetheless, the growing demands for KV cache memory\ncreate significant hurdles for efficient implementation. This work introduces a\nnovel architecture, Cross-Layer Latent Attention (CLLA), aimed at compressing\nthe KV cache to less than 2% of its original size while maintaining comparable\nperformance levels. CLLA integrates multiple aspects of KV cache compression,\nincluding attention head/dimension reduction, layer sharing, and quantization\ntechniques, into a cohesive framework. Our extensive experiments demonstrate\nthat CLLA achieves lossless performance on most tasks while utilizing minimal\nKV cache, marking a significant advancement in practical KV cache compression."
                },
                "authors": [
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "J. N. Han"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "An Wang"
                    },
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Zhanhui Kang"
                    }
                ],
                "author_detail": {
                    "name": "Zhanhui Kang"
                },
                "author": "Zhanhui Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2206.05579v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2206.05579v4",
                "updated": "2024-10-19T12:15:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    19,
                    12,
                    15,
                    50,
                    5,
                    293,
                    0
                ],
                "published": "2022-06-11T17:52:10Z",
                "published_parsed": [
                    2022,
                    6,
                    11,
                    17,
                    52,
                    10,
                    5,
                    162,
                    0
                ],
                "title": "Online Paging with Heterogeneous Cache Slots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Paging with Heterogeneous Cache Slots"
                },
                "summary": "It is natural to generalize the online $k$-Server problem by allowing each\nrequest to specify not only a point $p$, but also a subset $S$ of servers that\nmay serve it. For uniform metrics, the problem is equivalent to a\ngeneralization of Paging in which each request specifies not only a page $p$,\nbut also a subset $S$ of cache slots, and is satisfied by having a copy of $p$\nin some slot in $S$. We call this problem Slot-Heterogenous Paging.\n  We parameterize the problem by specifying a family $\\mathcal S \\subseteq\n2^{[k]}$ of requestable slot sets, and we establish bounds on the competitive\nratio as a function of the cache size $k$ and family $\\mathcal S$:\n  - If all request sets are allowed ($\\mathcal\nS=2^{[k]}\\setminus\\{\\emptyset\\}$), the optimal deterministic and randomized\ncompetitive ratios are exponentially worse than for standard \\Paging ($\\mathcal\nS=\\{[k]\\}$).\n  - As a function of $|\\mathcal S|$ and $k$, the optimal deterministic ratio is\npolynomial: at most $O(k^2|\\mathcal S|)$ and at least $\\Omega(\\sqrt{|\\mathcal\nS|})$.\n  - For any laminar family $\\mathcal S$ of height $h$, the optimal ratios are\n$O(hk)$ (deterministic) and $O(h^2\\log k)$ (randomized).\n  - The special case of laminar $\\mathcal S$ that we call All-or-One Paging\nextends standard Paging by allowing each request to specify a specific slot to\nput the requested page in. The optimal deterministic ratio for weighted\nAll-or-One Paging is $\\Theta(k)$. Offline All-or-One Paging is NP-hard.\n  Some results for the laminar case are shown via a reduction to the\ngeneralization of Paging in which each request specifies a set $\\mathcal P of\npages, and is satisfied by fetching any page from $\\mathcal P into the cache.\nThe optimal ratios for the latter problem (with laminar family of height $h$)\nare at most $hk$ (deterministic) and $h\\,H_k$ (randomized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is natural to generalize the online $k$-Server problem by allowing each\nrequest to specify not only a point $p$, but also a subset $S$ of servers that\nmay serve it. For uniform metrics, the problem is equivalent to a\ngeneralization of Paging in which each request specifies not only a page $p$,\nbut also a subset $S$ of cache slots, and is satisfied by having a copy of $p$\nin some slot in $S$. We call this problem Slot-Heterogenous Paging.\n  We parameterize the problem by specifying a family $\\mathcal S \\subseteq\n2^{[k]}$ of requestable slot sets, and we establish bounds on the competitive\nratio as a function of the cache size $k$ and family $\\mathcal S$:\n  - If all request sets are allowed ($\\mathcal\nS=2^{[k]}\\setminus\\{\\emptyset\\}$), the optimal deterministic and randomized\ncompetitive ratios are exponentially worse than for standard \\Paging ($\\mathcal\nS=\\{[k]\\}$).\n  - As a function of $|\\mathcal S|$ and $k$, the optimal deterministic ratio is\npolynomial: at most $O(k^2|\\mathcal S|)$ and at least $\\Omega(\\sqrt{|\\mathcal\nS|})$.\n  - For any laminar family $\\mathcal S$ of height $h$, the optimal ratios are\n$O(hk)$ (deterministic) and $O(h^2\\log k)$ (randomized).\n  - The special case of laminar $\\mathcal S$ that we call All-or-One Paging\nextends standard Paging by allowing each request to specify a specific slot to\nput the requested page in. The optimal deterministic ratio for weighted\nAll-or-One Paging is $\\Theta(k)$. Offline All-or-One Paging is NP-hard.\n  Some results for the laminar case are shown via a reduction to the\ngeneralization of Paging in which each request specifies a set $\\mathcal P of\npages, and is satisfied by fetching any page from $\\mathcal P into the cache.\nThe optimal ratios for the latter problem (with laminar family of height $h$)\nare at most $hk$ (deterministic) and $h\\,H_k$ (randomized)."
                },
                "authors": [
                    {
                        "name": "Marek Chrobak"
                    },
                    {
                        "name": "Samuel Haney"
                    },
                    {
                        "name": "Mehraneh Liaee"
                    },
                    {
                        "name": "Debmalya Panigrahi"
                    },
                    {
                        "name": "Rajmohan Rajaraman"
                    },
                    {
                        "name": "Ravi Sundaram"
                    },
                    {
                        "name": "Neal E. Young"
                    }
                ],
                "author_detail": {
                    "name": "Neal E. Young"
                },
                "author": "Neal E. Young",
                "arxiv_doi": "10.1007/s00453-024-01270-z",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s00453-024-01270-z",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2206.05579v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2206.05579v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "conference and journal versions appear in STACS 2023 and Algorithmica\n  (2004)",
                "arxiv_journal_ref": "Algorithmica (2004)",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.0; F.1.2; C.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12876v2",
                "updated": "2024-10-19T08:45:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    19,
                    8,
                    45,
                    11,
                    5,
                    293,
                    0
                ],
                "published": "2024-10-15T05:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    1,
                    19,
                    1,
                    289,
                    0
                ],
                "title": "In-context KV-Cache Eviction for LLMs via Attention-Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context KV-Cache Eviction for LLMs via Attention-Gate"
                },
                "summary": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). It caches states of self-attention to avoid\nrecomputation. Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system, especially when confronted with\nultra-large models and long-context queries. A natural remedy is to discard the\nKV-Cache for less important tokens, with StreamingLLM as an example, but the\nused static eviction strategies cannot flexibly adapt to varying contexts.\nRemedies like H2O leverage accumulative attention scores to perform dynamic\neviction but suffer from the attention bias issue in capturing contextual\ninformation. This paper bridges this gap by devising a parameterized KV-Cache\neviction mechanism, dubbed as Attention-Gate, which accepts the whole context\nas input and yields eviction flags for each token to realize in-context\neviction. The subsequent self-attention module proceeds according to the flags\nand only the KV states for the remaining tokens need to be cached. The\nAttention-Gates can vary among different heads and layers and be trivially\nplugged into pre-trained LLMs, tuned by cost-effective continual pre-training\nor supervised fine-tuning objectives to acquire what to discard. The\ncomputational and memory overhead introduced by Attention-Gates is minimal. Our\nmethod is validated across multiple tasks, demonstrating both efficiency and\nadaptability. After a highly efficient continual pre-training, it achieves\nhigher average accuracy and evicts more tokens compared to traditional\ntraining-free methods. In supervised fine-tuning, it not only evicts many\ntokens but also outperforms LoRA-finetuned LLMs on some datasets, such as RTE,\nwhere it improves accuracy by 13.9% while evicting 62.8% of tokens, showing\nthat effective eviction of redundant tokens can even enhance performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). It caches states of self-attention to avoid\nrecomputation. Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system, especially when confronted with\nultra-large models and long-context queries. A natural remedy is to discard the\nKV-Cache for less important tokens, with StreamingLLM as an example, but the\nused static eviction strategies cannot flexibly adapt to varying contexts.\nRemedies like H2O leverage accumulative attention scores to perform dynamic\neviction but suffer from the attention bias issue in capturing contextual\ninformation. This paper bridges this gap by devising a parameterized KV-Cache\neviction mechanism, dubbed as Attention-Gate, which accepts the whole context\nas input and yields eviction flags for each token to realize in-context\neviction. The subsequent self-attention module proceeds according to the flags\nand only the KV states for the remaining tokens need to be cached. The\nAttention-Gates can vary among different heads and layers and be trivially\nplugged into pre-trained LLMs, tuned by cost-effective continual pre-training\nor supervised fine-tuning objectives to acquire what to discard. The\ncomputational and memory overhead introduced by Attention-Gates is minimal. Our\nmethod is validated across multiple tasks, demonstrating both efficiency and\nadaptability. After a highly efficient continual pre-training, it achieves\nhigher average accuracy and evicts more tokens compared to traditional\ntraining-free methods. In supervised fine-tuning, it not only evicts many\ntokens but also outperforms LoRA-finetuned LLMs on some datasets, such as RTE,\nwhere it improves accuracy by 13.9% while evicting 62.8% of tokens, showing\nthat effective eviction of redundant tokens can even enhance performance."
                },
                "authors": [
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10593v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10593v3",
                "updated": "2024-10-18T19:30:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    19,
                    30,
                    35,
                    4,
                    292,
                    0
                ],
                "published": "2024-09-16T17:36:50Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    36,
                    50,
                    0,
                    260,
                    0
                ],
                "title": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios"
                },
                "summary": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%. Code is\navailable at https://github.com/wln20/CSKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%. Code is\navailable at https://github.com/wln20/CSKV."
                },
                "authors": [
                    {
                        "name": "Luning Wang"
                    },
                    {
                        "name": "Shiyao Li"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "4th NeurIPS Efficient Natural Language and Speech Processing Workshop\n  (ENLSP-IV 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10593v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10593v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14346v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14346v2",
                "updated": "2024-10-18T13:59:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    59,
                    54,
                    4,
                    292,
                    0
                ],
                "published": "2024-07-19T14:28:53Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    14,
                    28,
                    53,
                    4,
                    201,
                    0
                ],
                "title": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals"
                },
                "summary": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue."
                },
                "authors": [
                    {
                        "name": "Akash Kumar Mohankumar"
                    },
                    {
                        "name": "Gururaj K"
                    },
                    {
                        "name": "Gagan Madan"
                    },
                    {
                        "name": "Amit Singh"
                    }
                ],
                "author_detail": {
                    "name": "Amit Singh"
                },
                "author": "Amit Singh",
                "arxiv_comment": "Accepted to EMNLP 2024 Industry Track. 10 pages, 10 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14346v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14346v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14442v1",
                "updated": "2024-10-18T13:01:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T13:01:14Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "title": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference"
                },
                "summary": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2x, most configurations can achieve competitive performance to and\nhigher throughput than standard transformers, but when further reducing the\nsize of the KV cache, pairing queries of all layers with KVs of upper layers\ncan better maintain performance, although it also introduces additional\ntraining cost and prefilling latency. We hope that this work will help users\nchoose the appropriate approach according to their requirements and facilitate\nresearch on the acceleration of LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2x, most configurations can achieve competitive performance to and\nhigher throughput than standard transformers, but when further reducing the\nsize of the KV cache, pairing queries of all layers with KVs of upper layers\ncan better maintain performance, although it also introduces additional\ntraining cost and prefilling latency. We hope that this work will help users\nchoose the appropriate approach according to their requirements and facilitate\nresearch on the acceleration of LLM inference."
                },
                "authors": [
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Haoyi Wu"
                    },
                    {
                        "name": "Kewei Tu"
                    }
                ],
                "author_detail": {
                    "name": "Kewei Tu"
                },
                "author": "Kewei Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10859v2",
                "updated": "2024-10-18T10:02:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    10,
                    2,
                    3,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-07T13:46:06Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    46,
                    6,
                    0,
                    281,
                    0
                ],
                "title": "FAME: Towards Factual Multi-Task Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAME: Towards Factual Multi-Task Model Editing"
                },
                "summary": "Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality."
                },
                "authors": [
                    {
                        "name": "Li Zeng"
                    },
                    {
                        "name": "Yingyu Shan"
                    },
                    {
                        "name": "Zeming Liu"
                    },
                    {
                        "name": "Jiashu Yao"
                    },
                    {
                        "name": "Yuhang Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yuhang Guo"
                },
                "author": "Yuhang Guo",
                "arxiv_comment": "9 pages, 3 figures. This paper has been accepted by EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14003v1",
                "updated": "2024-10-17T20:11:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    20,
                    11,
                    34,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T20:11:34Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    20,
                    11,
                    34,
                    3,
                    291,
                    0
                ],
                "title": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems"
                },
                "summary": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches."
                },
                "authors": [
                    {
                        "name": "Connor Sullivan"
                    },
                    {
                        "name": "Alex Manley"
                    },
                    {
                        "name": "Mohammad Alian"
                    },
                    {
                        "name": "Heechul Yun"
                    }
                ],
                "author_detail": {
                    "name": "Heechul Yun"
                },
                "author": "Heechul Yun",
                "arxiv_comment": "RTSS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13846v1",
                "updated": "2024-10-17T17:58:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T17:58:14Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "title": "SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction"
                },
                "summary": "Recent advancements in large language models (LLMs) have extended their\ncapabilities to handle long contexts. However, increasing the number of model\nlayers and the length of input sequences significantly escalates the memory\nrequired to store key-value (KV) cache, posing challenges for efficient\ninference. To mitigate this issue, we present SimLayerKV, a simple yet\neffective method that reduces inter-layer KV cache redundancies by selectively\ndropping cache in identified lazy layers. Our approach is based on the\nobservation that certain layers in long-context LLMs exhibit \"lazy\" behavior,\ncontributing less to modeling long-range dependencies compared to non-lazy\nlayers. By analyzing attention weight patterns, we find that the behavior of\nthese lazy layers is consistent across tokens during generation for a given\ninput. This insight motivates our SimLayerKV, which identifies lazy layers and\nreduces their KV cache accordingly. SimLayerKV is training-free, generalizable,\nand can be implemented with only seven lines of code. We conduct extensive\nexperiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and\nMistral-7B across 16 tasks from the LongBench benchmark. The results\ndemonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\\times$\nwith only a 1.2% performance drop when combined with 4-bit quantization. Our\ncode is available at https://github.com/sail-sg/SimLayerKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have extended their\ncapabilities to handle long contexts. However, increasing the number of model\nlayers and the length of input sequences significantly escalates the memory\nrequired to store key-value (KV) cache, posing challenges for efficient\ninference. To mitigate this issue, we present SimLayerKV, a simple yet\neffective method that reduces inter-layer KV cache redundancies by selectively\ndropping cache in identified lazy layers. Our approach is based on the\nobservation that certain layers in long-context LLMs exhibit \"lazy\" behavior,\ncontributing less to modeling long-range dependencies compared to non-lazy\nlayers. By analyzing attention weight patterns, we find that the behavior of\nthese lazy layers is consistent across tokens during generation for a given\ninput. This insight motivates our SimLayerKV, which identifies lazy layers and\nreduces their KV cache accordingly. SimLayerKV is training-free, generalizable,\nand can be implemented with only seven lines of code. We conduct extensive\nexperiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and\nMistral-7B across 16 tasks from the LongBench benchmark. The results\ndemonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\\times$\nwith only a 1.2% performance drop when combined with 4-bit quantization. Our\ncode is available at https://github.com/sail-sg/SimLayerKV."
                },
                "authors": [
                    {
                        "name": "Xuan Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v4",
                "updated": "2024-10-17T15:27:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    15,
                    27,
                    30,
                    3,
                    291,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient RAG"
                },
                "summary": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "East Sun"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07979v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07979v2",
                "updated": "2024-10-17T08:54:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    8,
                    54,
                    37,
                    3,
                    291,
                    0
                ],
                "published": "2024-04-11T17:57:22Z",
                "published_parsed": [
                    2024,
                    4,
                    11,
                    17,
                    57,
                    22,
                    3,
                    102,
                    0
                ],
                "title": "LLoCO: Learning Long Contexts Offline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLoCO: Learning Long Contexts Offline"
                },
                "summary": "Processing long contexts remains a challenge for large language models (LLMs)\ndue to the quadratic computational and memory overhead of the self-attention\nmechanism and the substantial KV cache sizes during generation. We propose\nLLoCO, a novel approach to address this problem by learning contexts offline\nthrough context compression and in-domain parameter-efficient finetuning with\nLoRA. Our method enables an LLM to create a concise representation of the\noriginal context and efficiently retrieve relevant information to answer\nquestions accurately. Our approach extends the effective context window of a 4k\ntoken LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on\nseveral long-context question-answering datasets, demonstrating that LLoCO\nsignificantly outperforms in-context learning while using $30\\times$ fewer\ntokens during inference. LLoCO achieves up to $7.62\\times$ speed-up during\ninference and $11.52\\times$ higher throughput during finetuning, substantially\nreduces the cost of long document question answering. This makes it a promising\nsolution for efficient long context processing. Our code is publicly available\non https://github.com/jeffreysijuntan/lloco.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts remains a challenge for large language models (LLMs)\ndue to the quadratic computational and memory overhead of the self-attention\nmechanism and the substantial KV cache sizes during generation. We propose\nLLoCO, a novel approach to address this problem by learning contexts offline\nthrough context compression and in-domain parameter-efficient finetuning with\nLoRA. Our method enables an LLM to create a concise representation of the\noriginal context and efficiently retrieve relevant information to answer\nquestions accurately. Our approach extends the effective context window of a 4k\ntoken LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on\nseveral long-context question-answering datasets, demonstrating that LLoCO\nsignificantly outperforms in-context learning while using $30\\times$ fewer\ntokens during inference. LLoCO achieves up to $7.62\\times$ speed-up during\ninference and $11.52\\times$ higher throughput during finetuning, substantially\nreduces the cost of long document question answering. This makes it a promising\nsolution for efficient long context processing. Our code is publicly available\non https://github.com/jeffreysijuntan/lloco."
                },
                "authors": [
                    {
                        "name": "Sijun Tan"
                    },
                    {
                        "name": "Xiuyu Li"
                    },
                    {
                        "name": "Shishir Patil"
                    },
                    {
                        "name": "Ziyang Wu"
                    },
                    {
                        "name": "Tianjun Zhang"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Raluca Ada Popa"
                    }
                ],
                "author_detail": {
                    "name": "Raluca Ada Popa"
                },
                "author": "Raluca Ada Popa",
                "arxiv_comment": "EMNLP 2024. The first two authors contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07979v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07979v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18126v1",
                "updated": "2024-10-17T04:37:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    37,
                    43,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T04:37:43Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    37,
                    43,
                    3,
                    291,
                    0
                ],
                "title": "Leveraging Hardware Performance Counters for Predicting Workload\n  Interference in Vector Supercomputers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Hardware Performance Counters for Predicting Workload\n  Interference in Vector Supercomputers"
                },
                "summary": "In the rapidly evolving domain of high-performance computing (HPC),\nheterogeneous architectures such as the SX-Aurora TSUBASA (SX-AT) system\narchitecture, which integrate diverse processor types, present both\nopportunities and challenges for optimizing resource utilization. This paper\ninvestigates workload interference within an SX-AT system, with a specific\nfocus on resource contention between Vector Hosts (VHs) and Vector Engines\n(VEs). Through comprehensive empirical analysis, the study identifies key\nfactors contributing to performance degradation, such as cache and memory\nbandwidth contention, when jobs with varying computational demands share\nresources. To address these issues, we develop a predictive model that\nleverages hardware performance counters (HCs) and machine learning (ML)\nalgorithms to classify and predict workload interference. Our results\ndemonstrate that the model accurately forecasts performance degradation,\noffering valuable insights for future research on optimizing job scheduling and\nresource allocation. This approach highlights the importance of adaptive\nresource management strategies in maintaining system efficiency and provides a\nfoundation for future enhancements in heterogeneous supercomputing\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly evolving domain of high-performance computing (HPC),\nheterogeneous architectures such as the SX-Aurora TSUBASA (SX-AT) system\narchitecture, which integrate diverse processor types, present both\nopportunities and challenges for optimizing resource utilization. This paper\ninvestigates workload interference within an SX-AT system, with a specific\nfocus on resource contention between Vector Hosts (VHs) and Vector Engines\n(VEs). Through comprehensive empirical analysis, the study identifies key\nfactors contributing to performance degradation, such as cache and memory\nbandwidth contention, when jobs with varying computational demands share\nresources. To address these issues, we develop a predictive model that\nleverages hardware performance counters (HCs) and machine learning (ML)\nalgorithms to classify and predict workload interference. Our results\ndemonstrate that the model accurately forecasts performance degradation,\noffering valuable insights for future research on optimizing job scheduling and\nresource allocation. This approach highlights the importance of adaptive\nresource management strategies in maintaining system efficiency and provides a\nfoundation for future enhancements in heterogeneous supercomputing\nenvironments."
                },
                "authors": [
                    {
                        "name": "Shubham"
                    },
                    {
                        "name": "Keichi Takahashi"
                    },
                    {
                        "name": "Hiroyuki Takizawa"
                    }
                ],
                "author_detail": {
                    "name": "Hiroyuki Takizawa"
                },
                "author": "Hiroyuki Takizawa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13212v1",
                "updated": "2024-10-17T04:35:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    35,
                    57,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T04:35:57Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    35,
                    57,
                    3,
                    291,
                    0
                ],
                "title": "AsymKV: Enabling 1-Bit Quantization of KV Cache with Layer-Wise\n  Asymmetric Quantization Configurations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymKV: Enabling 1-Bit Quantization of KV Cache with Layer-Wise\n  Asymmetric Quantization Configurations"
                },
                "summary": "Large language models have shown exceptional capabilities in a wide range of\ntasks, such as text generation and video generation, among others. However, due\nto their massive parameter count, these models often require substantial\nstorage space, imposing significant constraints on the machines deploying LLMs.\nTo overcome this limitation, one research direction proposes to compress the\nmodels using integer replacements for floating-point numbers, in a process\nknown as Quantization. Some recent studies suggest quantizing the key and value\ncache (KV Cache) of LLMs, and designing quantization techniques that treat the\nkey and value matrices equivalently.\n  This work delves deeper into the asymmetric structural roles of KV Cache, a\nphenomenon where the transformer's output loss is more sensitive to the\nquantization of key matrices. We conduct a systematic examination of the\nattention output error resulting from key and value quantization. The\nphenomenon inspires us to propose an asymmetric quantization strategy. Our\napproach allows for 1-bit quantization of the KV cache by implementing distinct\nconfigurations for key and value matrices. We carry out experiments across a\nvariety of datasets, demonstrating that our proposed model allows for the\nquantization of up to 75% decoder layers with 1 bit, while simultaneously\nmaintaining performance levels comparable to those of the models with floating\nparameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have shown exceptional capabilities in a wide range of\ntasks, such as text generation and video generation, among others. However, due\nto their massive parameter count, these models often require substantial\nstorage space, imposing significant constraints on the machines deploying LLMs.\nTo overcome this limitation, one research direction proposes to compress the\nmodels using integer replacements for floating-point numbers, in a process\nknown as Quantization. Some recent studies suggest quantizing the key and value\ncache (KV Cache) of LLMs, and designing quantization techniques that treat the\nkey and value matrices equivalently.\n  This work delves deeper into the asymmetric structural roles of KV Cache, a\nphenomenon where the transformer's output loss is more sensitive to the\nquantization of key matrices. We conduct a systematic examination of the\nattention output error resulting from key and value quantization. The\nphenomenon inspires us to propose an asymmetric quantization strategy. Our\napproach allows for 1-bit quantization of the KV cache by implementing distinct\nconfigurations for key and value matrices. We carry out experiments across a\nvariety of datasets, demonstrating that our proposed model allows for the\nquantization of up to 75% decoder layers with 1 bit, while simultaneously\nmaintaining performance levels comparable to those of the models with floating\nparameters."
                },
                "authors": [
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08895v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08895v3",
                "updated": "2024-10-16T17:54:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    54,
                    15,
                    2,
                    290,
                    0
                ],
                "published": "2024-01-17T00:36:58Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    0,
                    36,
                    58,
                    2,
                    17,
                    0
                ],
                "title": "cedar: Optimized and Unified Machine Learning Input Data Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cedar: Optimized and Unified Machine Learning Input Data Pipelines"
                },
                "summary": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems."
                },
                "authors": [
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Emanuel Adamiak"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "Accepted to PVLDB Volume 18",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08895v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08895v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12749v1",
                "updated": "2024-10-16T17:10:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    10,
                    48,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T17:10:48Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    10,
                    48,
                    2,
                    290,
                    0
                ],
                "title": "Toleo: Scaling Freshness to Tera-scale Memory using CXL and PIM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toleo: Scaling Freshness to Tera-scale Memory using CXL and PIM"
                },
                "summary": "Trusted hardware's freshness guarantee ensures that an adversary cannot\nreplay an old value in response to a memory read request. They rely on\nmaintaining a version number for each cache block and ensuring their integrity\nusing a Merkle tree. However, these existing solutions protect only a small\namount of main memory (few MBs), as the extraneous memory accesses to the\nMerkle tree increase prohibitively with the protected memory size. We present\nToleo, which uses trusted smart memory connected through a secure CXL IDE\nnetwork to safely store version numbers. Toleo eliminates the need for an\nunscalable Merkle tree to protect the integrity of version numbers by instead\nusing smart memory as the root of trust. Additionally, Toleo ensures version\nconfidentiality which enables stealth versions that reduce the version storage\noverhead in half.\n  Furthermore, in the absence of Merkle tree imposed constraints, we\neffectively exploit version locality at page granularity to compress version\nnumber by a factor of 240. These space optimizations make it feasible for one\n168 GB Toleo smart memory device to provide freshness to a 28 TB CXL-expanded\nmain memory pool in a rack server for a negligible performance overhead. We\nanalyze the benefits of Toleo using several privacy-sensitive genomics, graph,\ngenerative AI, and database workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trusted hardware's freshness guarantee ensures that an adversary cannot\nreplay an old value in response to a memory read request. They rely on\nmaintaining a version number for each cache block and ensuring their integrity\nusing a Merkle tree. However, these existing solutions protect only a small\namount of main memory (few MBs), as the extraneous memory accesses to the\nMerkle tree increase prohibitively with the protected memory size. We present\nToleo, which uses trusted smart memory connected through a secure CXL IDE\nnetwork to safely store version numbers. Toleo eliminates the need for an\nunscalable Merkle tree to protect the integrity of version numbers by instead\nusing smart memory as the root of trust. Additionally, Toleo ensures version\nconfidentiality which enables stealth versions that reduce the version storage\noverhead in half.\n  Furthermore, in the absence of Merkle tree imposed constraints, we\neffectively exploit version locality at page granularity to compress version\nnumber by a factor of 240. These space optimizations make it feasible for one\n168 GB Toleo smart memory device to provide freshness to a 28 TB CXL-expanded\nmain memory pool in a rack server for a negligible performance overhead. We\nanalyze the benefits of Toleo using several privacy-sensitive genomics, graph,\ngenerative AI, and database workloads."
                },
                "authors": [
                    {
                        "name": "Juechu Dong"
                    },
                    {
                        "name": "Jonah Rosenblum"
                    },
                    {
                        "name": "Satish Narayanasamy"
                    }
                ],
                "author_detail": {
                    "name": "Satish Narayanasamy"
                },
                "author": "Satish Narayanasamy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12605v1",
                "updated": "2024-10-16T14:24:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    24,
                    16,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T14:24:16Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    24,
                    16,
                    2,
                    290,
                    0
                ],
                "title": "Advancing Web Browser Forensics: Critical Evaluation of Emerging Tools\n  and Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Web Browser Forensics: Critical Evaluation of Emerging Tools\n  and Techniques"
                },
                "summary": "As the use of web browsers continues to grow, the potential for cybercrime\nand web-related criminal activities also increases. Digital forensic\ninvestigators must understand how different browsers function and the critical\nareas to consider during web forensic analysis. Web forensics, a subfield of\ndigital forensics, involves collecting and analyzing browser artifacts, such as\nbrowser history, search keywords, and downloads, which serve as potential\nevidence. While existing research has provided valuable insights, many studies\nfocus on individual browsing modes or limited forensic scenarios, leaving gaps\nin understanding the full scope of data retention and recovery across different\nmodes and browsers. This paper addresses these gaps by defining four browsing\nscenarios and critically analyzing browser artifacts across normal, private,\nand portable modes using various forensic tools. We define four browsing\nscenarios to perform a comprehensive evaluation of popular browsers -- Google\nChrome, Mozilla Firefox, Brave, Tor, and Microsoft Edge -- by monitoring\nchanges in key data storage areas such as cache files, cookies, browsing\nhistory, and local storage across different browsing modes. Overall, this paper\ncontributes to a deeper understanding of browser forensic analysis and\nidentifies key areas for enhancing privacy protection and forensic\nmethodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the use of web browsers continues to grow, the potential for cybercrime\nand web-related criminal activities also increases. Digital forensic\ninvestigators must understand how different browsers function and the critical\nareas to consider during web forensic analysis. Web forensics, a subfield of\ndigital forensics, involves collecting and analyzing browser artifacts, such as\nbrowser history, search keywords, and downloads, which serve as potential\nevidence. While existing research has provided valuable insights, many studies\nfocus on individual browsing modes or limited forensic scenarios, leaving gaps\nin understanding the full scope of data retention and recovery across different\nmodes and browsers. This paper addresses these gaps by defining four browsing\nscenarios and critically analyzing browser artifacts across normal, private,\nand portable modes using various forensic tools. We define four browsing\nscenarios to perform a comprehensive evaluation of popular browsers -- Google\nChrome, Mozilla Firefox, Brave, Tor, and Microsoft Edge -- by monitoring\nchanges in key data storage areas such as cache files, cookies, browsing\nhistory, and local storage across different browsing modes. Overall, this paper\ncontributes to a deeper understanding of browser forensic analysis and\nidentifies key areas for enhancing privacy protection and forensic\nmethodologies."
                },
                "authors": [
                    {
                        "name": "Rishal Ravikesh Chand"
                    },
                    {
                        "name": "Neeraj Anand Sharma"
                    },
                    {
                        "name": "Muhammad Ashad Kabir"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Ashad Kabir"
                },
                "author": "Muhammad Ashad Kabir",
                "arxiv_comment": "34 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v1",
                "updated": "2024-10-16T12:45:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across domanins such as vision and language processing. However,\ndue to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit 2) Input-agnostic heuristics\nwhere tokens exit at pre-determined layers irrespective of input sequence. Both\nthe above strategies have limitations - the former cannot be applied to handle\nKV Caching necessary for speed-ups in modern framework and the latter does not\ncapture the variation in layer importance across tasks or more generally,\nacross input sequences. To address both limitations, we propose FIRST, an\nalgorithm that reduces inference latency by using layer-specific routers to\nselect a subset of transformer layers adaptively for each input sequence - the\nprompt (during prefill stage) decides which layers will be skipped during\ndecoding. FIRST preserves compatibility with KV caching enabling faster\ninference while being quality-aware. FIRST is model-agnostic and can be easily\nenabled on any pre-trained LLM. We further improve performance by incorporating\nLoRA adapters for fine-tuning on external datasets, enhancing task-specific\naccuracy while maintaining latency benefits. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on task. Extensive\nexperiments show that FIRST significantly reduces latency while retaining\ncompetitive performance (as compared to baselines), making our approach an\nefficient solution for LLM deployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across domanins such as vision and language processing. However,\ndue to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit 2) Input-agnostic heuristics\nwhere tokens exit at pre-determined layers irrespective of input sequence. Both\nthe above strategies have limitations - the former cannot be applied to handle\nKV Caching necessary for speed-ups in modern framework and the latter does not\ncapture the variation in layer importance across tasks or more generally,\nacross input sequences. To address both limitations, we propose FIRST, an\nalgorithm that reduces inference latency by using layer-specific routers to\nselect a subset of transformer layers adaptively for each input sequence - the\nprompt (during prefill stage) decides which layers will be skipped during\ndecoding. FIRST preserves compatibility with KV caching enabling faster\ninference while being quality-aware. FIRST is model-agnostic and can be easily\nenabled on any pre-trained LLM. We further improve performance by incorporating\nLoRA adapters for fine-tuning on external datasets, enhancing task-specific\naccuracy while maintaining latency benefits. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on task. Extensive\nexperiments show that FIRST significantly reduces latency while retaining\ncompetitive performance (as compared to baselines), making our approach an\nefficient solution for LLM deployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "arxiv_comment": "17 pages, 6 figures, Submitted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12423v1",
                "updated": "2024-10-16T10:06:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    6,
                    22,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T10:06:22Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    6,
                    22,
                    2,
                    290,
                    0
                ],
                "title": "An O(m+n)-Space Spatiotemporal Denoising Filter with Cache-Like Memories\n  for Dynamic Vision Sensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An O(m+n)-Space Spatiotemporal Denoising Filter with Cache-Like Memories\n  for Dynamic Vision Sensors"
                },
                "summary": "Dynamic vision sensor (DVS) is novel neuromorphic imaging device that\ngenerates asynchronous events. Despite the high temporal resolution and high\ndynamic range features, DVS is faced with background noise problem.\nSpatiotemporal filter is an effective and hardware-friendly solution for DVS\ndenoising but previous designs have large memory overhead or degraded\nperformance issues. In this paper, we present a lightweight and real-time\nspatiotemporal denoising filter with set-associative cache-like memories, which\nhas low space complexity of \\text{O(m+n)} for DVS of $m\\times n$ resolution. A\ntwo-stage pipeline for memory access with read cancellation feature is proposed\nto reduce power consumption. Further the bitwidth redundancy for event storage\nis exploited to minimize the memory footprint. We implemented our design on\nFPGA and experimental results show that it achieves state-of-the-art\nperformance compared with previous spatiotemporal filters while maintaining low\nresource utilization and low power consumption of about 125mW to 210mW at\n100MHz clock frequency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic vision sensor (DVS) is novel neuromorphic imaging device that\ngenerates asynchronous events. Despite the high temporal resolution and high\ndynamic range features, DVS is faced with background noise problem.\nSpatiotemporal filter is an effective and hardware-friendly solution for DVS\ndenoising but previous designs have large memory overhead or degraded\nperformance issues. In this paper, we present a lightweight and real-time\nspatiotemporal denoising filter with set-associative cache-like memories, which\nhas low space complexity of \\text{O(m+n)} for DVS of $m\\times n$ resolution. A\ntwo-stage pipeline for memory access with read cancellation feature is proposed\nto reduce power consumption. Further the bitwidth redundancy for event storage\nis exploited to minimize the memory footprint. We implemented our design on\nFPGA and experimental results show that it achieves state-of-the-art\nperformance compared with previous spatiotemporal filters while maintaining low\nresource utilization and low power consumption of about 125mW to 210mW at\n100MHz clock frequency."
                },
                "authors": [
                    {
                        "name": "Qinghang Zhao"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Yixi Ji"
                    },
                    {
                        "name": "Jinjian Wu"
                    },
                    {
                        "name": "Guangming Shi"
                    }
                ],
                "author_detail": {
                    "name": "Guangming Shi"
                },
                "author": "Guangming Shi",
                "arxiv_doi": "10.1145/3676536.3676710",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676536.3676710",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.12423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14731v1",
                "updated": "2024-10-16T08:34:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    34,
                    51,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T08:34:51Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    34,
                    51,
                    2,
                    290,
                    0
                ],
                "title": "MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal\n  Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal\n  Projection"
                },
                "summary": "KV cache has become a de facto technique for the inference of large language\nmodels (LLMs), where tensors of shape (layer number, head number, sequence\nlength, feature dimension) are introduced to cache historical information for\nself-attention. As the size of the model and data grows, the KV cache can\nquickly become a bottleneck within the system in both storage and memory\ntransfer. To address this, prior studies usually focus on the first three axes\nof the cache tensors for compression. This paper supplements them, focusing on\nthe feature dimension axis, by utilizing low-rank projection matrices to\ntransform the cache features into spaces with reduced dimensions. We begin by\ninvestigating the canonical orthogonal projection method for data compression\nthrough principal component analysis (PCA). We observe the issue with PCA\nprojection where significant performance degradation is observed at low\ncompression rates. To bridge the gap, we propose to directly tune the\northogonal projection matrices with a distillation objective using an elaborate\nMatryoshka training strategy. After training, we adaptively search for the\noptimal compression rates for various layers and heads given varying\ncompression budgets. Compared to previous works, our method can easily embrace\npre-trained LLMs and hold a smooth tradeoff between performance and compression\nrate. We empirically witness the high data efficiency of our training procedure\nand find that our method can sustain over 90% performance with an average KV\ncache compression rate of 60% (and up to 75% in certain extreme scenarios) for\npopular LLMs like LLaMA2-7B-base and Mistral-7B-v0.3-base.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache has become a de facto technique for the inference of large language\nmodels (LLMs), where tensors of shape (layer number, head number, sequence\nlength, feature dimension) are introduced to cache historical information for\nself-attention. As the size of the model and data grows, the KV cache can\nquickly become a bottleneck within the system in both storage and memory\ntransfer. To address this, prior studies usually focus on the first three axes\nof the cache tensors for compression. This paper supplements them, focusing on\nthe feature dimension axis, by utilizing low-rank projection matrices to\ntransform the cache features into spaces with reduced dimensions. We begin by\ninvestigating the canonical orthogonal projection method for data compression\nthrough principal component analysis (PCA). We observe the issue with PCA\nprojection where significant performance degradation is observed at low\ncompression rates. To bridge the gap, we propose to directly tune the\northogonal projection matrices with a distillation objective using an elaborate\nMatryoshka training strategy. After training, we adaptively search for the\noptimal compression rates for various layers and heads given varying\ncompression budgets. Compared to previous works, our method can easily embrace\npre-trained LLMs and hold a smooth tradeoff between performance and compression\nrate. We empirically witness the high data efficiency of our training procedure\nand find that our method can sustain over 90% performance with an average KV\ncache compression rate of 60% (and up to 75% in certain extreme scenarios) for\npopular LLMs like LLaMA2-7B-base and Mistral-7B-v0.3-base."
                },
                "authors": [
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Zipeng Xiao"
                    },
                    {
                        "name": "Siqi Kou"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Xiaofeng Gao"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12168v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12168v1",
                "updated": "2024-10-16T02:16:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    2,
                    16,
                    53,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T02:16:53Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    2,
                    16,
                    53,
                    2,
                    290,
                    0
                ],
                "title": "COMET: Towards Partical W4A4KV4 LLMs Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COMET: Towards Partical W4A4KV4 LLMs Serving"
                },
                "summary": "Quantization is a widely-used compression technology to reduce the overhead\nof serving large language models (LLMs) on terminal devices and in cloud data\ncenters. However, prevalent quantization methods, such as 8-bit\nweight-activation or 4-bit weight-only quantization, achieve limited\nperformance improvements due to poor support for low-precision (e.g., 4-bit)\nactivation. This work, for the first time, realizes practical W4A4KV4 serving\nfor LLMs, fully utilizing the INT4 tensor cores on modern GPUs and reducing the\nmemory bottleneck caused by the KV cache. Specifically, we propose a novel\nfine-grained mixed-precision quantization algorithm (FMPQ) that compresses most\nactivations into 4-bit with negligible accuracy loss. To support\nmixed-precision matrix multiplication for W4A4 and W4A8, we develop a highly\noptimized W4Ax kernel. Our approach introduces a novel mixed-precision data\nlayout to facilitate access and fast dequantization for activation and weight\ntensors, utilizing the GPU's software pipeline to hide the overhead of data\nloading and conversion. Additionally, we propose fine-grained streaming\nmultiprocessor (SM) scheduling to achieve load balance across different SMs. We\nintegrate the optimized W4Ax kernel into our inference framework, COMET, and\nprovide efficient management to support popular LLMs such as LLaMA-3-70B.\nExtensive evaluations demonstrate that, when running LLaMA family models on a\nsingle A100-80G-SMX4, COMET achieves a kernel-level speedup of\n\\textbf{$2.88\\times$} over cuBLAS and a \\textbf{$2.02 \\times$} throughput\nimprovement compared to TensorRT-LLM from an end-to-end framework perspective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is a widely-used compression technology to reduce the overhead\nof serving large language models (LLMs) on terminal devices and in cloud data\ncenters. However, prevalent quantization methods, such as 8-bit\nweight-activation or 4-bit weight-only quantization, achieve limited\nperformance improvements due to poor support for low-precision (e.g., 4-bit)\nactivation. This work, for the first time, realizes practical W4A4KV4 serving\nfor LLMs, fully utilizing the INT4 tensor cores on modern GPUs and reducing the\nmemory bottleneck caused by the KV cache. Specifically, we propose a novel\nfine-grained mixed-precision quantization algorithm (FMPQ) that compresses most\nactivations into 4-bit with negligible accuracy loss. To support\nmixed-precision matrix multiplication for W4A4 and W4A8, we develop a highly\noptimized W4Ax kernel. Our approach introduces a novel mixed-precision data\nlayout to facilitate access and fast dequantization for activation and weight\ntensors, utilizing the GPU's software pipeline to hide the overhead of data\nloading and conversion. Additionally, we propose fine-grained streaming\nmultiprocessor (SM) scheduling to achieve load balance across different SMs. We\nintegrate the optimized W4Ax kernel into our inference framework, COMET, and\nprovide efficient management to support popular LLMs such as LLaMA-3-70B.\nExtensive evaluations demonstrate that, when running LLaMA family models on a\nsingle A100-80G-SMX4, COMET achieves a kernel-level speedup of\n\\textbf{$2.88\\times$} over cuBLAS and a \\textbf{$2.02 \\times$} throughput\nimprovement compared to TensorRT-LLM from an end-to-end framework perspective."
                },
                "authors": [
                    {
                        "name": "Lian Liu"
                    },
                    {
                        "name": "Haimeng Ren"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Zhaohui Xu"
                    },
                    {
                        "name": "Yudong Pan"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Xiaowei Li"
                    },
                    {
                        "name": "Yinhe Han"
                    },
                    {
                        "name": "Ying Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ying Wang"
                },
                "author": "Ying Wang",
                "arxiv_comment": "14 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12168v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02536v2",
                "updated": "2024-10-15T15:58:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    58,
                    7,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-04T17:55:38Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    55,
                    38,
                    1,
                    156,
                    0
                ],
                "title": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension"
                },
                "summary": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden."
                },
                "authors": [
                    {
                        "name": "Yijiong Yu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Chin-Yew Lin"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yongfeng Huang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11417v1",
                "updated": "2024-10-15T09:07:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    9,
                    7,
                    25,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T09:07:25Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    9,
                    7,
                    25,
                    1,
                    289,
                    0
                ],
                "title": "VidCompress: Memory-Enhanced Temporal Compression for Video\n  Understanding in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VidCompress: Memory-Enhanced Temporal Compression for Video\n  Understanding in Large Language Models"
                },
                "summary": "Video-based multimodal large language models (Video-LLMs) possess significant\npotential for video understanding tasks. However, most Video-LLMs treat videos\nas a sequential set of individual frames, which results in insufficient\ntemporal-spatial interaction that hinders fine-grained comprehension and\ndifficulty in processing longer videos due to limited visual token capacity. To\naddress these challenges, we propose VidCompress, a novel Video-LLM featuring\nmemory-enhanced temporal compression. VidCompress employs a dual-compressor\napproach: a memory-enhanced compressor captures both short-term and long-term\ntemporal relationships in videos and compresses the visual tokens using a\nmultiscale transformer with a memory-cache mechanism, while a text-perceived\ncompressor generates condensed visual tokens by utilizing Q-Former and\nintegrating temporal contexts into query embeddings with cross attention.\nExperiments on several VideoQA datasets and comprehensive benchmarks\ndemonstrate that VidCompress efficiently models complex temporal-spatial\nrelations and significantly outperforms existing Video-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-based multimodal large language models (Video-LLMs) possess significant\npotential for video understanding tasks. However, most Video-LLMs treat videos\nas a sequential set of individual frames, which results in insufficient\ntemporal-spatial interaction that hinders fine-grained comprehension and\ndifficulty in processing longer videos due to limited visual token capacity. To\naddress these challenges, we propose VidCompress, a novel Video-LLM featuring\nmemory-enhanced temporal compression. VidCompress employs a dual-compressor\napproach: a memory-enhanced compressor captures both short-term and long-term\ntemporal relationships in videos and compresses the visual tokens using a\nmultiscale transformer with a memory-cache mechanism, while a text-perceived\ncompressor generates condensed visual tokens by utilizing Q-Former and\nintegrating temporal contexts into query embeddings with cross attention.\nExperiments on several VideoQA datasets and comprehensive benchmarks\ndemonstrate that VidCompress efficiently models complex temporal-spatial\nrelations and significantly outperforms existing Video-LLMs."
                },
                "authors": [
                    {
                        "name": "Xiaohan Lan"
                    },
                    {
                        "name": "Yitian Yuan"
                    },
                    {
                        "name": "Zequn Jie"
                    },
                    {
                        "name": "Lin Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lin Ma"
                },
                "author": "Lin Ma",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09297v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09297v3",
                "updated": "2024-10-15T08:45:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    8,
                    45,
                    18,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-13T16:33:44Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    16,
                    33,
                    44,
                    3,
                    165,
                    0
                ],
                "title": "MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer\n  Decoding"
                },
                "summary": "Auto-regressive inference of transformers benefit greatly from Key-Value (KV)\ncaching, but can lead to major memory bottlenecks as model size, batch size,\nand sequence length grow at scale. We introduce Multi-Layer Key-Value (MLKV)\nsharing, a novel approach extending KV sharing across transformer layers to\nreduce memory usage beyond what was possible with Multi-Query Attention (MQA)\nand Grouped-Query Attention (GQA). Evaluations on various NLP benchmarks and\ninference metrics using uptrained Pythia-160M variants demonstrate that MLKV\nsignificantly reduces memory usage with minimal performance loss, reducing KV\ncache size down to a factor of 6x compared to MQA. These results highlight\nMLKV's potential for efficient deployment of transformer models at scale. We\nprovide code at https://github.com/zaydzuhri/pythia-mlkv",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive inference of transformers benefit greatly from Key-Value (KV)\ncaching, but can lead to major memory bottlenecks as model size, batch size,\nand sequence length grow at scale. We introduce Multi-Layer Key-Value (MLKV)\nsharing, a novel approach extending KV sharing across transformer layers to\nreduce memory usage beyond what was possible with Multi-Query Attention (MQA)\nand Grouped-Query Attention (GQA). Evaluations on various NLP benchmarks and\ninference metrics using uptrained Pythia-160M variants demonstrate that MLKV\nsignificantly reduces memory usage with minimal performance loss, reducing KV\ncache size down to a factor of 6x compared to MQA. These results highlight\nMLKV's potential for efficient deployment of transformer models at scale. We\nprovide code at https://github.com/zaydzuhri/pythia-mlkv"
                },
                "authors": [
                    {
                        "name": "Zayd Muhammad Kawakibi Zuhri"
                    },
                    {
                        "name": "Muhammad Farid Adilazuarda"
                    },
                    {
                        "name": "Ayu Purwarianti"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    }
                ],
                "author_detail": {
                    "name": "Alham Fikri Aji"
                },
                "author": "Alham Fikri Aji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09297v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09297v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09827v2",
                "updated": "2024-10-15T06:09:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    6,
                    9,
                    35,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-14T08:32:45Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    8,
                    32,
                    45,
                    4,
                    166,
                    0
                ],
                "title": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention"
                },
                "summary": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible."
                },
                "authors": [
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Geon Park"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Jaduk Suh"
                    },
                    {
                        "name": "Jina Kim"
                    },
                    {
                        "name": "Wonyoung Jeong"
                    },
                    {
                        "name": "Bumsik Kim"
                    },
                    {
                        "name": "Hyemin Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "44 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11305v1",
                "updated": "2024-10-15T05:57:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T05:57:51Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "title": "QSpec: Speculative Decoding with Complementary Quantization Schemes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSpec: Speculative Decoding with Complementary Quantization Schemes"
                },
                "summary": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.80x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Unlike existing speculative\ndecoding techniques, our approach reuses weights and the KV cache, avoiding\nadditional memory overhead. Furthermore, QSPEC offers a plug-and-play advantage\nwithout requiring any training. We believe that QSPEC demonstrates unique\nstrengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.80x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Unlike existing speculative\ndecoding techniques, our approach reuses weights and the KV cache, avoiding\nadditional memory overhead. Furthermore, QSPEC offers a plug-and-play advantage\nwithout requiring any training. We believe that QSPEC demonstrates unique\nstrengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices)."
                },
                "authors": [
                    {
                        "name": "Juntao Zhao"
                    },
                    {
                        "name": "Wenhao Lu"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00080v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00080v3",
                "updated": "2024-10-15T05:34:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    34,
                    7,
                    1,
                    289,
                    0
                ],
                "published": "2024-04-30T16:35:08Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    16,
                    35,
                    8,
                    1,
                    121,
                    0
                ],
                "title": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits"
                },
                "summary": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms."
                },
                "authors": [
                    {
                        "name": "Pavamana K J"
                    },
                    {
                        "name": "Chandramani Kishore Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Kishore Singh"
                },
                "author": "Chandramani Kishore Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00080v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00080v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11260v1",
                "updated": "2024-10-15T04:35:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    4,
                    35,
                    49,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T04:35:49Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    4,
                    35,
                    49,
                    1,
                    289,
                    0
                ],
                "title": "A Zoned Storage Optimized Flash Cache on ZNS SSDs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Zoned Storage Optimized Flash Cache on ZNS SSDs"
                },
                "summary": "Zoned Namespace SSDs (ZNS) are introduced recently to mitigate the block\ninterface penalties of flash-based SSDs. It is a good opportunity for flash\ncache to address cache throughput and write amplification (WA) issues by fully\ncontrolling data allocation and garbage collection via zone-based interfaces.\nHowever, there are several critical challenges that need to be addressed\nincluding zone-interface compatibility, data management of large zone size, and\na better tradeoff between throughput, cache hit ratio, and WA.\n  In this paper, we present Z-CacheLib, a zoned storage optimized flash cache\non ZNS SSDs. In Z-CacheLib, we propose: 1) a new zStorage Engine for ZNS SSDs\nwith low mapping and operational overhead, and 2) a novel zCache Engine with\ncross-layer optimizations to resolve the throughput regression and WA issues of\ngarbage collection, which consists of delayed data eviction with virtual\nover-provisioning (vOP), a top-down eviction policy (zLRU) optimized from LRU,\nand a bottom-up drop mechanism (zDrop) for low WA. Our evaluation shows that\nZ-CacheLib can achieve up to 2X throughput, 5% improvement hit ratio, and\nalmost no WA compared to CacheLib with compatible regular SSDs, demonstrating\nbenefits of using ZNS SSDs for cache. Moreover, Z-CacheLib can achieve up to 6X\nthroughput and 92% WA reduction compared with F2FS-based scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zoned Namespace SSDs (ZNS) are introduced recently to mitigate the block\ninterface penalties of flash-based SSDs. It is a good opportunity for flash\ncache to address cache throughput and write amplification (WA) issues by fully\ncontrolling data allocation and garbage collection via zone-based interfaces.\nHowever, there are several critical challenges that need to be addressed\nincluding zone-interface compatibility, data management of large zone size, and\na better tradeoff between throughput, cache hit ratio, and WA.\n  In this paper, we present Z-CacheLib, a zoned storage optimized flash cache\non ZNS SSDs. In Z-CacheLib, we propose: 1) a new zStorage Engine for ZNS SSDs\nwith low mapping and operational overhead, and 2) a novel zCache Engine with\ncross-layer optimizations to resolve the throughput regression and WA issues of\ngarbage collection, which consists of delayed data eviction with virtual\nover-provisioning (vOP), a top-down eviction policy (zLRU) optimized from LRU,\nand a bottom-up drop mechanism (zDrop) for low WA. Our evaluation shows that\nZ-CacheLib can achieve up to 2X throughput, 5% improvement hit ratio, and\nalmost no WA compared to CacheLib with compatible regular SSDs, demonstrating\nbenefits of using ZNS SSDs for cache. Moreover, Z-CacheLib can achieve up to 6X\nthroughput and 92% WA reduction compared with F2FS-based scheme."
                },
                "authors": [
                    {
                        "name": "Chongzhuo Yang"
                    },
                    {
                        "name": "Chang Guo"
                    },
                    {
                        "name": "Ming Zhao"
                    },
                    {
                        "name": "Zhichao Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zhichao Cao"
                },
                "author": "Zhichao Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03058v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03058v4",
                "updated": "2024-10-14T19:12:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    19,
                    12,
                    48,
                    0,
                    288,
                    0
                ],
                "published": "2024-05-05T21:41:43Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    21,
                    41,
                    43,
                    6,
                    126,
                    0
                ],
                "title": "Enhancing High-Level Synthesis with Automated Pragma Insertion and Code\n  Transformation Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing High-Level Synthesis with Automated Pragma Insertion and Code\n  Transformation Framework"
                },
                "summary": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results."
                },
                "authors": [
                    {
                        "name": "Stéphane Pouget"
                    },
                    {
                        "name": "Louis-Noël Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03058v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03058v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10819v1",
                "updated": "2024-10-14T17:59:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    58,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:59:58Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    58,
                    0,
                    288,
                    0
                ],
                "title": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and\n  Streaming Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and\n  Streaming Heads"
                },
                "summary": "Deploying long-context large language models (LLMs) is essential but poses\nsignificant computational and memory challenges. Caching all Key and Value (KV)\nstates across all attention heads consumes substantial memory. Existing KV\ncache pruning methods either damage the long-context capabilities of LLMs or\noffer only limited efficiency improvements. In this paper, we identify that\nonly a fraction of attention heads, a.k.a, Retrieval Heads, are critical for\nprocessing long contexts and require full attention across all tokens. In\ncontrast, all other heads, which primarily focus on recent tokens and attention\nsinks--referred to as Streaming Heads--do not require full attention. Based on\nthis insight, we introduce DuoAttention, a framework that only applies a full\nKV cache to retrieval heads while using a light-weight, constant-length KV\ncache for streaming heads, which reduces both LLM's decoding and pre-filling\nmemory and latency without compromising its long-context abilities.\nDuoAttention uses a lightweight, optimization-based algorithm with synthetic\ndata to identify retrieval heads accurately. Our method significantly reduces\nlong-context inference memory by up to 2.55x for MHA and 1.67x for GQA models\nwhile speeding up decoding by up to 2.18x and 1.50x and accelerating\npre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with\nminimal accuracy loss compared to full attention. Notably, combined with\nquantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context\nlength on a single A100 GPU. Code is provided in\nhttps://github.com/mit-han-lab/duo-attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying long-context large language models (LLMs) is essential but poses\nsignificant computational and memory challenges. Caching all Key and Value (KV)\nstates across all attention heads consumes substantial memory. Existing KV\ncache pruning methods either damage the long-context capabilities of LLMs or\noffer only limited efficiency improvements. In this paper, we identify that\nonly a fraction of attention heads, a.k.a, Retrieval Heads, are critical for\nprocessing long contexts and require full attention across all tokens. In\ncontrast, all other heads, which primarily focus on recent tokens and attention\nsinks--referred to as Streaming Heads--do not require full attention. Based on\nthis insight, we introduce DuoAttention, a framework that only applies a full\nKV cache to retrieval heads while using a light-weight, constant-length KV\ncache for streaming heads, which reduces both LLM's decoding and pre-filling\nmemory and latency without compromising its long-context abilities.\nDuoAttention uses a lightweight, optimization-based algorithm with synthetic\ndata to identify retrieval heads accurately. Our method significantly reduces\nlong-context inference memory by up to 2.55x for MHA and 1.67x for GQA models\nwhile speeding up decoding by up to 2.18x and 1.50x and accelerating\npre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with\nminimal accuracy loss compared to full attention. Notably, combined with\nquantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context\nlength on a single A100 GPU. Code is provided in\nhttps://github.com/mit-han-lab/duo-attention."
                },
                "authors": [
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Jingwei Zuo"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10781v1",
                "updated": "2024-10-14T17:50:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    28,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:50:28Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    28,
                    0,
                    288,
                    0
                ],
                "title": "When Attention Sink Emerges in Language Models: An Empirical View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Attention Sink Emerges in Language Models: An Empirical View"
                },
                "summary": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink."
                },
                "authors": [
                    {
                        "name": "Xiangming Gu"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10511v1",
                "updated": "2024-10-14T13:49:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    49,
                    6,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T13:49:06Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    49,
                    6,
                    0,
                    288,
                    0
                ],
                "title": "Customize Your Visual Autoregressive Recipe with Set Autoregressive\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customize Your Visual Autoregressive Recipe with Set Autoregressive\n  Modeling"
                },
                "summary": "We introduce a new paradigm for AutoRegressive (AR) image generation, termed\nSet AutoRegressive Modeling (SAR). SAR generalizes the conventional AR to the\nnext-set setting, i.e., splitting the sequence into arbitrary sets containing\nmultiple tokens, rather than outputting each token in a fixed raster order. To\naccommodate SAR, we develop a straightforward architecture termed Fully Masked\nTransformer. We reveal that existing AR variants correspond to specific design\nchoices of sequence order and output intervals within the SAR framework, with\nAR and Masked AR (MAR) as two extreme instances. Notably, SAR facilitates a\nseamless transition from AR to MAR, where intermediate states allow for\ntraining a causal model that benefits from both few-step inference and KV cache\nacceleration, thus leveraging the advantages of both AR and MAR. On the\nImageNet benchmark, we carefully explore the properties of SAR by analyzing the\nimpact of sequence order and output intervals on performance, as well as the\ngeneralization ability regarding inference order and steps. We further validate\nthe potential of SAR by training a 900M text-to-image model capable of\nsynthesizing photo-realistic images with any resolution. We hope our work may\ninspire more exploration and application of AR-based modeling across diverse\nmodalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new paradigm for AutoRegressive (AR) image generation, termed\nSet AutoRegressive Modeling (SAR). SAR generalizes the conventional AR to the\nnext-set setting, i.e., splitting the sequence into arbitrary sets containing\nmultiple tokens, rather than outputting each token in a fixed raster order. To\naccommodate SAR, we develop a straightforward architecture termed Fully Masked\nTransformer. We reveal that existing AR variants correspond to specific design\nchoices of sequence order and output intervals within the SAR framework, with\nAR and Masked AR (MAR) as two extreme instances. Notably, SAR facilitates a\nseamless transition from AR to MAR, where intermediate states allow for\ntraining a causal model that benefits from both few-step inference and KV cache\nacceleration, thus leveraging the advantages of both AR and MAR. On the\nImageNet benchmark, we carefully explore the properties of SAR by analyzing the\nimpact of sequence order and output intervals on performance, as well as the\ngeneralization ability regarding inference order and steps. We further validate\nthe potential of SAR by training a 900M text-to-image model capable of\nsynthesizing photo-realistic images with any resolution. We hope our work may\ninspire more exploration and application of AR-based modeling across diverse\nmodalities."
                },
                "authors": [
                    {
                        "name": "Wenze Liu"
                    },
                    {
                        "name": "Le Zhuo"
                    },
                    {
                        "name": "Yi Xin"
                    },
                    {
                        "name": "Sheng Xia"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "arxiv_comment": "19 pages, 17 figures, 8 tables, github repo:\n  https://github.com/poppuppy/SAR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v2",
                "updated": "2024-10-14T09:35:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    9,
                    35,
                    35,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13378v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13378v2",
                "updated": "2024-10-14T07:58:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    7,
                    58,
                    39,
                    0,
                    288,
                    0
                ],
                "published": "2024-05-22T06:19:43Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    6,
                    19,
                    43,
                    2,
                    143,
                    0
                ],
                "title": "FedCache 2.0: Federated Edge Learning with Knowledge Caching and Dataset\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedCache 2.0: Federated Edge Learning with Knowledge Caching and Dataset\n  Distillation"
                },
                "summary": "Federated Edge Learning (FEL) has emerged as a promising approach for\nenabling edge devices to collaboratively train machine learning models while\npreserving data privacy. Despite its advantages, practical FEL deployment faces\nsignificant challenges related to device constraints and device-server\ninteractions, necessitating heterogeneous, user-adaptive model training with\nlimited and uncertain communication. In this paper, we introduce FedCache 2.0,\na novel personalized FEL architecture that simultaneously addresses these\nchallenges. FedCache 2.0 incorporates the benefits of both dataset distillation\nand knowledge cache-driven federated learning by storing and organizing\ndistilled data as knowledge in the server-side knowledge cache. Moreover, a\ndevice-centric cache sampling strategy is introduced to tailor transferred\nknowledge for individual devices within controlled communication bandwidth.\nExtensive experiments on five datasets covering image recognition, audio\nunderstanding, and mobile sensor data mining tasks demonstrate that (1)\nFedCache 2.0 significantly outperforms state-of-the-art methods regardless of\nmodel structures, data distributions, and modalities. (2) FedCache 2.0 can\ntrain splendid personalized on-device models with at least $\\times$28.6\nimprovement in communication efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Edge Learning (FEL) has emerged as a promising approach for\nenabling edge devices to collaboratively train machine learning models while\npreserving data privacy. Despite its advantages, practical FEL deployment faces\nsignificant challenges related to device constraints and device-server\ninteractions, necessitating heterogeneous, user-adaptive model training with\nlimited and uncertain communication. In this paper, we introduce FedCache 2.0,\na novel personalized FEL architecture that simultaneously addresses these\nchallenges. FedCache 2.0 incorporates the benefits of both dataset distillation\nand knowledge cache-driven federated learning by storing and organizing\ndistilled data as knowledge in the server-side knowledge cache. Moreover, a\ndevice-centric cache sampling strategy is introduced to tailor transferred\nknowledge for individual devices within controlled communication bandwidth.\nExtensive experiments on five datasets covering image recognition, audio\nunderstanding, and mobile sensor data mining tasks demonstrate that (1)\nFedCache 2.0 significantly outperforms state-of-the-art methods regardless of\nmodel structures, data distributions, and modalities. (2) FedCache 2.0 can\ntrain splendid personalized on-device models with at least $\\times$28.6\nimprovement in communication efficiency."
                },
                "authors": [
                    {
                        "name": "Quyang Pan"
                    },
                    {
                        "name": "Sheng Sun"
                    },
                    {
                        "name": "Zhiyuan Wu"
                    },
                    {
                        "name": "Yuwei Wang"
                    },
                    {
                        "name": "Min Liu"
                    },
                    {
                        "name": "Bo Gao"
                    },
                    {
                        "name": "Jingyuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jingyuan Wang"
                },
                "author": "Jingyuan Wang",
                "arxiv_comment": "17 pages, 7 figures, 14 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13378v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13378v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10157v1",
                "updated": "2024-10-14T04:49:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    49,
                    22,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T04:49:22Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    49,
                    22,
                    0,
                    288,
                    0
                ],
                "title": "Caching Content Placement and Beamforming Co-design for IRS-Aided MIMO\n  Systems with Imperfect CSI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Content Placement and Beamforming Co-design for IRS-Aided MIMO\n  Systems with Imperfect CSI"
                },
                "summary": "When offloading links encounter deep fading and obstruction, edge caching\ncannot fully enhance wireless network performance and improve the QoS of edge\nnodes, as it fails to effectively reduce backhaul burden. The emerging\ntechnology of intelligent reflecting surfaces (IRS) compensates for this\ndisadvantage by creating a smart and reconfigurable wireless environment.\nSubsequently, we jointly design content placement and active/passive\nbeamforming to minimize network costs under imperfect channel state information\n(CSI) in the IRS-oriented edge caching system. This minimization problem is\ndecomposed into two subproblems. The content placement subproblem is addressed\nby applying KKT optimality conditions. We then develop the alternating\noptimization method to resolve precoder and reflection beamforming.\nSpecifically, we reduce transmission power by first fixing the phase shift,\nreducing the problem to a convex one relative to the precoder, which is solved\nthrough convex optimization. Next, we fix the precoder and resolve the\nresulting reflection beamforming problem using the penalty convex-concave\nprocedure (CCP) method. Results demonstrate that our proposed method\noutperforms uniform caching and random phase approaches in reducing\ntransmission power and saving network costs. Eventually, the proposed approach\noffers potential improvements in the caching optimization and transmission\nrobustness of wireless communication with imperfect CSI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When offloading links encounter deep fading and obstruction, edge caching\ncannot fully enhance wireless network performance and improve the QoS of edge\nnodes, as it fails to effectively reduce backhaul burden. The emerging\ntechnology of intelligent reflecting surfaces (IRS) compensates for this\ndisadvantage by creating a smart and reconfigurable wireless environment.\nSubsequently, we jointly design content placement and active/passive\nbeamforming to minimize network costs under imperfect channel state information\n(CSI) in the IRS-oriented edge caching system. This minimization problem is\ndecomposed into two subproblems. The content placement subproblem is addressed\nby applying KKT optimality conditions. We then develop the alternating\noptimization method to resolve precoder and reflection beamforming.\nSpecifically, we reduce transmission power by first fixing the phase shift,\nreducing the problem to a convex one relative to the precoder, which is solved\nthrough convex optimization. Next, we fix the precoder and resolve the\nresulting reflection beamforming problem using the penalty convex-concave\nprocedure (CCP) method. Results demonstrate that our proposed method\noutperforms uniform caching and random phase approaches in reducing\ntransmission power and saving network costs. Eventually, the proposed approach\noffers potential improvements in the caching optimization and transmission\nrobustness of wireless communication with imperfect CSI."
                },
                "authors": [
                    {
                        "name": "Meng Gao"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Huafu Li"
                    },
                    {
                        "name": "Junqi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Junqi Guo"
                },
                "author": "Junqi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10149v1",
                "updated": "2024-10-14T04:30:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    30,
                    38,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T04:30:38Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    30,
                    38,
                    0,
                    288,
                    0
                ],
                "title": "Fast and Accurate Neural Rendering Using Semi-Gradients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Accurate Neural Rendering Using Semi-Gradients"
                },
                "summary": "We propose a simple yet effective neural network-based framework for global\nillumination rendering. Recently, rendering techniques that learn neural\nradiance caches by minimizing the difference (i.e., residual) between the left\nand right sides of the rendering equation have been suggested. Due to their\nease of implementation and the advantage of excluding path integral\ncalculations, these techniques have been applied to various fields, such as\nfree-viewpoint rendering, differentiable rendering, and real-time rendering.\nHowever, issues of slow training and occasionally darkened renders have been\nnoted. We identify the cause of these issues as the bias and high variance\npresent in the gradient estimates of the existing residual-based objective\nfunction. To address this, we introduce a new objective function that maintains\nthe same global optimum as before but allows for unbiased and low-variance\ngradient estimates, enabling faster and more accurate training of neural\nnetworks. In conclusion, this method is simply implemented by ignoring the\npartial derivatives of the right-hand side, and theoretical and experimental\nanalyses demonstrate the effectiveness of the proposed loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple yet effective neural network-based framework for global\nillumination rendering. Recently, rendering techniques that learn neural\nradiance caches by minimizing the difference (i.e., residual) between the left\nand right sides of the rendering equation have been suggested. Due to their\nease of implementation and the advantage of excluding path integral\ncalculations, these techniques have been applied to various fields, such as\nfree-viewpoint rendering, differentiable rendering, and real-time rendering.\nHowever, issues of slow training and occasionally darkened renders have been\nnoted. We identify the cause of these issues as the bias and high variance\npresent in the gradient estimates of the existing residual-based objective\nfunction. To address this, we introduce a new objective function that maintains\nthe same global optimum as before but allows for unbiased and low-variance\ngradient estimates, enabling faster and more accurate training of neural\nnetworks. In conclusion, this method is simply implemented by ignoring the\npartial derivatives of the right-hand side, and theoretical and experimental\nanalyses demonstrate the effectiveness of the proposed loss."
                },
                "authors": [
                    {
                        "name": "In-Young Cho"
                    },
                    {
                        "name": "Jaewoong Cho"
                    }
                ],
                "author_detail": {
                    "name": "Jaewoong Cho"
                },
                "author": "Jaewoong Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10071v1",
                "updated": "2024-10-14T01:25:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    1,
                    25,
                    56,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T01:25:56Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    1,
                    25,
                    56,
                    0,
                    288,
                    0
                ],
                "title": "Content Caching-Assisted Vehicular Edge Computing Using Multi-Agent\n  Graph Attention Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content Caching-Assisted Vehicular Edge Computing Using Multi-Agent\n  Graph Attention Reinforcement Learning"
                },
                "summary": "In order to avoid repeated task offloading and realize the reuse of popular\ntask computing results, we construct a novel content caching-assisted vehicular\nedge computing (VEC) framework. In the face of irregular network topology and\nunknown environmental dynamics, we further propose a multi-agent graph\nattention reinforcement learning (MGARL) based edge caching scheme, which\nutilizes the graph attention convolution kernel to integrate the neighboring\nnodes' features of each agent and further enhance the cooperation among agents.\nOur simulation results show that our proposed scheme is capable of improving\nthe utilization of caching resources while reducing the long-term task\ncomputing latency compared to the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In order to avoid repeated task offloading and realize the reuse of popular\ntask computing results, we construct a novel content caching-assisted vehicular\nedge computing (VEC) framework. In the face of irregular network topology and\nunknown environmental dynamics, we further propose a multi-agent graph\nattention reinforcement learning (MGARL) based edge caching scheme, which\nutilizes the graph attention convolution kernel to integrate the neighboring\nnodes' features of each agent and further enhance the cooperation among agents.\nOur simulation results show that our proposed scheme is capable of improving\nthe utilization of caching resources while reducing the long-term task\ncomputing latency compared to the baselines."
                },
                "authors": [
                    {
                        "name": "Jinjin Shen"
                    },
                    {
                        "name": "Yan Lin"
                    },
                    {
                        "name": "Yijin Zhang"
                    },
                    {
                        "name": "Weibin Zhang"
                    },
                    {
                        "name": "Feng Shu"
                    },
                    {
                        "name": "Jun Li"
                    }
                ],
                "author_detail": {
                    "name": "Jun Li"
                },
                "author": "Jun Li",
                "arxiv_comment": "6 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09533v1",
                "updated": "2024-10-12T13:45:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    13,
                    45,
                    26,
                    5,
                    286,
                    0
                ],
                "published": "2024-10-12T13:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    13,
                    45,
                    26,
                    5,
                    286,
                    0
                ],
                "title": "Leveraging Semantic Cues from Foundation Vision Models for Enhanced\n  Local Feature Correspondence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Semantic Cues from Foundation Vision Models for Enhanced\n  Local Feature Correspondence"
                },
                "summary": "Visual correspondence is a crucial step in key computer vision tasks,\nincluding camera localization, image registration, and structure from motion.\nThe most effective techniques for matching keypoints currently involve using\nlearned sparse or dense matchers, which need pairs of images. These neural\nnetworks have a good general understanding of features from both images, but\nthey often struggle to match points from different semantic areas. This paper\npresents a new method that uses semantic cues from foundation vision model\nfeatures (like DINOv2) to enhance local feature matching by incorporating\nsemantic reasoning into existing descriptors. Therefore, the learned\ndescriptors do not require image pairs at inference time, allowing feature\ncaching and fast matching using similarity search, unlike learned matchers. We\npresent adapted versions of six existing descriptors, with an average increase\nin performance of 29% in camera localization, with comparable accuracy to\nexisting matchers as LightGlue and LoFTR in two existing benchmarks. Both code\nand trained models are available at\nhttps://www.verlab.dcc.ufmg.br/descriptors/reasoning_accv24",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual correspondence is a crucial step in key computer vision tasks,\nincluding camera localization, image registration, and structure from motion.\nThe most effective techniques for matching keypoints currently involve using\nlearned sparse or dense matchers, which need pairs of images. These neural\nnetworks have a good general understanding of features from both images, but\nthey often struggle to match points from different semantic areas. This paper\npresents a new method that uses semantic cues from foundation vision model\nfeatures (like DINOv2) to enhance local feature matching by incorporating\nsemantic reasoning into existing descriptors. Therefore, the learned\ndescriptors do not require image pairs at inference time, allowing feature\ncaching and fast matching using similarity search, unlike learned matchers. We\npresent adapted versions of six existing descriptors, with an average increase\nin performance of 29% in camera localization, with comparable accuracy to\nexisting matchers as LightGlue and LoFTR in two existing benchmarks. Both code\nand trained models are available at\nhttps://www.verlab.dcc.ufmg.br/descriptors/reasoning_accv24"
                },
                "authors": [
                    {
                        "name": "Felipe Cadar"
                    },
                    {
                        "name": "Guilherme Potje"
                    },
                    {
                        "name": "Renato Martins"
                    },
                    {
                        "name": "Cédric Demonceaux"
                    },
                    {
                        "name": "Erickson R. Nascimento"
                    }
                ],
                "author_detail": {
                    "name": "Erickson R. Nascimento"
                },
                "author": "Erickson R. Nascimento",
                "arxiv_comment": "Accepted in ACCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09479v1",
                "updated": "2024-10-12T10:38:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    10,
                    38,
                    39,
                    5,
                    286,
                    0
                ],
                "published": "2024-10-12T10:38:39Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    10,
                    38,
                    39,
                    5,
                    286,
                    0
                ],
                "title": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle"
                },
                "summary": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system."
                },
                "authors": [
                    {
                        "name": "KVS Chaithanya"
                    },
                    {
                        "name": "Sumesh P. Thampi"
                    }
                ],
                "author_detail": {
                    "name": "Sumesh P. Thampi"
                },
                "author": "Sumesh P. Thampi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09397v1",
                "updated": "2024-10-12T07:01:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    7,
                    1,
                    30,
                    5,
                    286,
                    0
                ],
                "published": "2024-10-12T07:01:30Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    7,
                    1,
                    30,
                    5,
                    286,
                    0
                ],
                "title": "Fine-grained Attention I/O Complexity: Comprehensive Analysis for\n  Backward Passes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained Attention I/O Complexity: Comprehensive Analysis for\n  Backward Passes"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nprocessing long-context information. However, the quadratic complexity of\nattention computation with respect to sequence length poses significant\ncomputational challenges, and I/O aware algorithms have been proposed. This\npaper presents a comprehensive analysis of the I/O complexity for attention\nmechanisms, focusing on backward passes by categorizing into small and large\ncache scenarios. Using the red-blue pebble game framework, we establish tight\nbounds on I/O complexity across all cache sizes. We confirm that the de facto\nstandard I/O aware algorithm FlashAttention is optimal for both forward and\nbackward passes for the large cache size scenario. For small cache sizes, we\nprovide an algorithm that improves over existing methods and achieves the tight\nbounds. Additionally, we extend our analysis to sparse attention, a mainstream\nspeeding-up approach, deriving fine-grained lower bounds for both forward and\nbackward passes and both small and large caches. Our findings complete the\ntheoretical foundation for I/O complexity in attention mechanisms, offering\ninsights for designing efficient algorithms of LLM training and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nprocessing long-context information. However, the quadratic complexity of\nattention computation with respect to sequence length poses significant\ncomputational challenges, and I/O aware algorithms have been proposed. This\npaper presents a comprehensive analysis of the I/O complexity for attention\nmechanisms, focusing on backward passes by categorizing into small and large\ncache scenarios. Using the red-blue pebble game framework, we establish tight\nbounds on I/O complexity across all cache sizes. We confirm that the de facto\nstandard I/O aware algorithm FlashAttention is optimal for both forward and\nbackward passes for the large cache size scenario. For small cache sizes, we\nprovide an algorithm that improves over existing methods and achieves the tight\nbounds. Additionally, we extend our analysis to sparse attention, a mainstream\nspeeding-up approach, deriving fine-grained lower bounds for both forward and\nbackward passes and both small and large caches. Our findings complete the\ntheoretical foundation for I/O complexity in attention mechanisms, offering\ninsights for designing efficient algorithms of LLM training and inference."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yufa Zhou"
                },
                "author": "Yufa Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14360v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14360v3",
                "updated": "2024-10-12T02:11:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    2,
                    11,
                    14,
                    5,
                    286,
                    0
                ],
                "published": "2024-09-22T08:30:43Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    8,
                    30,
                    43,
                    6,
                    266,
                    0
                ],
                "title": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs"
                },
                "summary": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies."
                },
                "authors": [
                    {
                        "name": "Xufeng Yang"
                    },
                    {
                        "name": "Zhengjian Cong"
                    },
                    {
                        "name": "Congming Gao"
                    }
                ],
                "author_detail": {
                    "name": "Congming Gao"
                },
                "author": "Congming Gao",
                "arxiv_comment": "This paper has been submitted to NAS'24 (The 17th International\n  Conference on Networking, Architecture and Storage)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14360v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14360v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09237v1",
                "updated": "2024-10-11T20:23:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    20,
                    23,
                    0,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T20:23:00Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    20,
                    23,
                    0,
                    4,
                    285,
                    0
                ],
                "title": "Foundation Model-Powered 3D Few-Shot Class Incremental Learning via\n  Training-free Adaptor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation Model-Powered 3D Few-Shot Class Incremental Learning via\n  Training-free Adaptor"
                },
                "summary": "Recent advances in deep learning for processing point clouds hold increased\ninterest in Few-Shot Class Incremental Learning (FSCIL) for 3D computer vision.\nThis paper introduces a new method to tackle the Few-Shot Continual Incremental\nLearning (FSCIL) problem in 3D point cloud environments. We leverage a\nfoundational 3D model trained extensively on point cloud data. Drawing from\nrecent improvements in foundation models, known for their ability to work well\nacross different tasks, we propose a novel strategy that does not require\nadditional training to adapt to new tasks. Our approach uses a dual cache\nsystem: first, it uses previous test samples based on how confident the model\nwas in its predictions to prevent forgetting, and second, it includes a small\nnumber of new task samples to prevent overfitting. This dynamic adaptation\nensures strong performance across different learning tasks without needing lots\nof fine-tuning. We tested our approach on datasets like ModelNet, ShapeNet,\nScanObjectNN, and CO3D, showing that it outperforms other FSCIL methods and\ndemonstrating its effectiveness and versatility. The code is available at\n\\url{https://github.com/ahmadisahar/ACCV_FCIL3D}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in deep learning for processing point clouds hold increased\ninterest in Few-Shot Class Incremental Learning (FSCIL) for 3D computer vision.\nThis paper introduces a new method to tackle the Few-Shot Continual Incremental\nLearning (FSCIL) problem in 3D point cloud environments. We leverage a\nfoundational 3D model trained extensively on point cloud data. Drawing from\nrecent improvements in foundation models, known for their ability to work well\nacross different tasks, we propose a novel strategy that does not require\nadditional training to adapt to new tasks. Our approach uses a dual cache\nsystem: first, it uses previous test samples based on how confident the model\nwas in its predictions to prevent forgetting, and second, it includes a small\nnumber of new task samples to prevent overfitting. This dynamic adaptation\nensures strong performance across different learning tasks without needing lots\nof fine-tuning. We tested our approach on datasets like ModelNet, ShapeNet,\nScanObjectNN, and CO3D, showing that it outperforms other FSCIL methods and\ndemonstrating its effectiveness and versatility. The code is available at\n\\url{https://github.com/ahmadisahar/ACCV_FCIL3D}."
                },
                "authors": [
                    {
                        "name": "Sahar Ahmadi"
                    },
                    {
                        "name": "Ali Cheraghian"
                    },
                    {
                        "name": "Morteza Saberi"
                    },
                    {
                        "name": "Md. Towsif Abir"
                    },
                    {
                        "name": "Hamidreza Dastmalchi"
                    },
                    {
                        "name": "Farookh Hussain"
                    },
                    {
                        "name": "Shafin Rahman"
                    }
                ],
                "author_detail": {
                    "name": "Shafin Rahman"
                },
                "author": "Shafin Rahman",
                "arxiv_comment": "ACCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08895v1",
                "updated": "2024-10-11T15:12:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    12,
                    30,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T15:12:30Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    12,
                    30,
                    4,
                    285,
                    0
                ],
                "title": "Calibrated Cache Model for Few-Shot Vision-Language Model Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrated Cache Model for Few-Shot Vision-Language Model Adaptation"
                },
                "summary": "Cache-based approaches stand out as both effective and efficient for adapting\nvision-language models (VLMs). Nonetheless, the existing cache model overlooks\nthree crucial aspects. 1) Pre-trained VLMs are mainly optimized for image-text\nsimilarity, neglecting the importance of image-image similarity, leading to a\ngap between pre-training and adaptation. 2) The current cache model is based on\nthe Nadaraya-Watson (N-W) estimator, which disregards the intricate\nrelationships among training samples while constructing weight function. 3)\nUnder the condition of limited samples, the logits generated by cache model are\nof high uncertainty, directly using these logits without accounting for the\nconfidence could be problematic. This work presents three calibration modules\naimed at addressing the above challenges. Similarity Calibration refines the\nimage-image similarity by using unlabeled images. We add a learnable projection\nlayer with residual connection on top of the pre-trained image encoder of CLIP\nand optimize the parameters by minimizing self-supervised contrastive loss.\nWeight Calibration introduces a precision matrix into the weight function to\nadequately model the relation between training samples, transforming the\nexisting cache model to a Gaussian Process (GP) regressor, which could be more\naccurate than N-W estimator. Confidence Calibration leverages the predictive\nvariances computed by GP Regression to dynamically re-scale the logits of cache\nmodel, ensuring that the cache model's outputs are appropriately adjusted based\non their confidence levels. Besides, to reduce the high complexity of GPs, we\nfurther propose a group-based learning strategy. Integrating the above designs,\nwe propose both training-free and training-required variants. Extensive\nexperiments on 11 few-shot classification datasets validate that the proposed\nmethods can achieve state-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-based approaches stand out as both effective and efficient for adapting\nvision-language models (VLMs). Nonetheless, the existing cache model overlooks\nthree crucial aspects. 1) Pre-trained VLMs are mainly optimized for image-text\nsimilarity, neglecting the importance of image-image similarity, leading to a\ngap between pre-training and adaptation. 2) The current cache model is based on\nthe Nadaraya-Watson (N-W) estimator, which disregards the intricate\nrelationships among training samples while constructing weight function. 3)\nUnder the condition of limited samples, the logits generated by cache model are\nof high uncertainty, directly using these logits without accounting for the\nconfidence could be problematic. This work presents three calibration modules\naimed at addressing the above challenges. Similarity Calibration refines the\nimage-image similarity by using unlabeled images. We add a learnable projection\nlayer with residual connection on top of the pre-trained image encoder of CLIP\nand optimize the parameters by minimizing self-supervised contrastive loss.\nWeight Calibration introduces a precision matrix into the weight function to\nadequately model the relation between training samples, transforming the\nexisting cache model to a Gaussian Process (GP) regressor, which could be more\naccurate than N-W estimator. Confidence Calibration leverages the predictive\nvariances computed by GP Regression to dynamically re-scale the logits of cache\nmodel, ensuring that the cache model's outputs are appropriately adjusted based\non their confidence levels. Besides, to reduce the high complexity of GPs, we\nfurther propose a group-based learning strategy. Integrating the above designs,\nwe propose both training-free and training-required variants. Extensive\nexperiments on 11 few-shot classification datasets validate that the proposed\nmethods can achieve state-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Kun Ding"
                    },
                    {
                        "name": "Qiang Yu"
                    },
                    {
                        "name": "Haojian Zhang"
                    },
                    {
                        "name": "Gaofeng Meng"
                    },
                    {
                        "name": "Shiming Xiang"
                    }
                ],
                "author_detail": {
                    "name": "Shiming Xiang"
                },
                "author": "Shiming Xiang",
                "arxiv_comment": "submitted to IJCV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08760v1",
                "updated": "2024-10-11T12:19:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    19,
                    18,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T12:19:18Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    19,
                    18,
                    4,
                    285,
                    0
                ],
                "title": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation"
                },
                "summary": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL."
                },
                "authors": [
                    {
                        "name": "Konstantin Burlachenko"
                    },
                    {
                        "name": "Peter Richtárik"
                    }
                ],
                "author_detail": {
                    "name": "Peter Richtárik"
                },
                "author": "Peter Richtárik",
                "arxiv_comment": "55 pages, 12 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4; C.3; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08618v1",
                "updated": "2024-10-11T08:33:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    8,
                    33,
                    58,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T08:33:58Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    8,
                    33,
                    58,
                    4,
                    285,
                    0
                ],
                "title": "AsyncFS: Metadata Updates Made Asynchronous for Distributed Filesystems\n  with In-Network Coordination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsyncFS: Metadata Updates Made Asynchronous for Distributed Filesystems\n  with In-Network Coordination"
                },
                "summary": "Distributed filesystems typically employ synchronous metadata updates, facing\ninherent challenges for access efficiency, load balancing, and directory\ncontention, especially under dynamic and skewed workloads. This paper argues\nthat synchronous updates are overly conservative for distributed filesystems.\nWe propose AsyncFS with asynchronous metadata updates, allowing operations to\nreturn early and defer directory updates until respective read to enable\nlatency hiding and conflict resolution. The key challenge is efficiently\nmaintaining the synchronous semantics of metadata updates. To address this,\nAsyncFS is co-designed with a programmable switch, leveraging the constrained\non-switch resources to holistically track directory states in the network with\nnegligible cost. This allows AsyncFS to timely aggregate and efficiently apply\ndelayed updates using batching and consolidation before directory reads.\nEvaluation shows that AsyncFS achieves up to 13.34$\\times$ and 3.85$\\times$\nhigher throughput, and 61.6% and 57.3% lower latency than two state-of-the-art\ndistributed filesystems, InfiniFS and CFS-KV, respectively, on skewed\nworkloads. For real-world workloads, AsyncFS improves end-to-end throughput by\n21.1$\\times$, 1.1$\\times$ and 30.1% over Ceph, IndexFS and CFS-KV,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed filesystems typically employ synchronous metadata updates, facing\ninherent challenges for access efficiency, load balancing, and directory\ncontention, especially under dynamic and skewed workloads. This paper argues\nthat synchronous updates are overly conservative for distributed filesystems.\nWe propose AsyncFS with asynchronous metadata updates, allowing operations to\nreturn early and defer directory updates until respective read to enable\nlatency hiding and conflict resolution. The key challenge is efficiently\nmaintaining the synchronous semantics of metadata updates. To address this,\nAsyncFS is co-designed with a programmable switch, leveraging the constrained\non-switch resources to holistically track directory states in the network with\nnegligible cost. This allows AsyncFS to timely aggregate and efficiently apply\ndelayed updates using batching and consolidation before directory reads.\nEvaluation shows that AsyncFS achieves up to 13.34$\\times$ and 3.85$\\times$\nhigher throughput, and 61.6% and 57.3% lower latency than two state-of-the-art\ndistributed filesystems, InfiniFS and CFS-KV, respectively, on skewed\nworkloads. For real-world workloads, AsyncFS improves end-to-end throughput by\n21.1$\\times$, 1.1$\\times$ and 30.1% over Ceph, IndexFS and CFS-KV,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Qiulin Tian"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Tong Xin"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08584v1",
                "updated": "2024-10-11T07:24:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T07:24:21Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "title": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification and KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification and KV Cache Compression"
                },
                "summary": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs that resolves both computation\nand memory bottlenecks through a dynamic ratio allocation strategy of important\ntokens. This ratio is adaptively determined based on the layer-specific\ndistribution of attention scores, rather than fixed hyper-parameters, thereby\nimproving efficiency for less complex tasks while maintaining high performance\nfor more challenging ones. Then we select important tokens based on their\nnormalized attention scores and perform attention mechanism solely on those\nimportant tokens to accelerate the prefill phase. To mitigate the memory\nbottleneck in the decoding phase, we employ mixed-precision quantization to the\nKV cache, where high-bit quantization is used for caches of important tokens,\nwhile low-bit quantization is applied to those of less importance. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.6$\\times$ and reduce GPU memory usage by 50.0%, with a minimal accuracy\nreduction of only 0.2% on Video-MME benchmark over LongVA-7B model, effectively\nenhancing the generation efficiency of LVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs that resolves both computation\nand memory bottlenecks through a dynamic ratio allocation strategy of important\ntokens. This ratio is adaptively determined based on the layer-specific\ndistribution of attention scores, rather than fixed hyper-parameters, thereby\nimproving efficiency for less complex tasks while maintaining high performance\nfor more challenging ones. Then we select important tokens based on their\nnormalized attention scores and perform attention mechanism solely on those\nimportant tokens to accelerate the prefill phase. To mitigate the memory\nbottleneck in the decoding phase, we employ mixed-precision quantization to the\nKV cache, where high-bit quantization is used for caches of important tokens,\nwhile low-bit quantization is applied to those of less importance. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.6$\\times$ and reduce GPU memory usage by 50.0%, with a minimal accuracy\nreduction of only 0.2% on Video-MME benchmark over LongVA-7B model, effectively\nenhancing the generation efficiency of LVLMs."
                },
                "authors": [
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Hong Zhou"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.03462v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.03462v3",
                "updated": "2024-10-11T02:18:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    2,
                    18,
                    24,
                    4,
                    285,
                    0
                ],
                "published": "2024-01-07T11:57:40Z",
                "published_parsed": [
                    2024,
                    1,
                    7,
                    11,
                    57,
                    40,
                    6,
                    7,
                    0
                ],
                "title": "Long Context Compression with Activation Beacon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Context Compression with Activation Beacon"
                },
                "summary": "Long context compression is a critical research problem due to its\nsignificance in reducing the high computational and memory costs associated\nwith LLMs. In this paper, we propose Activation Beacon, a plug-in module for\ntransformer-based LLMs that targets effective, efficient, and flexible\ncompression of long contexts. To achieve this, our method introduces the\nfollowing technical designs. 1) We directly compress the activations (i.e. keys\nand values at every layer), rather than leveraging soft prompts to relay\ninformation (which constitute a major bottleneck to encapsulate the complex\ninformation within long contexts). 2) We tailor the compression workflow, where\neach fine-grained input unit is progressively compressed, enabling high-quality\ncompression and efficient computation during both training and inference. 3) We\ntrain the model through compression-based auto-regression, making full use of\nplain texts and instructional data to optimize the model's compression\nperformance. 4) During training, we randomly sample a compression ratio at each\nstep, teaching the model to support a wide range of compression configurations.\nExtensive evaluations are conducted on various long-context tasks whose lengths\n(e.g., 128K) may far exceed the maximum training length (20K), such as document\nunderstanding, few-shot learning, and Needle-in-a-Haystack. Whilst existing\nmethods struggle to handle these challenging tasks, Activation Beacon maintains\na comparable performance to the uncompressed baseline across various scenarios,\nachieving a 2x acceleration in inference time and an 8x reduction of memory\ncosts for KV cache. Our data, model, and code have been released at\n\\url{https://github.com/FlagOpen/FlagEmbedding/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context compression is a critical research problem due to its\nsignificance in reducing the high computational and memory costs associated\nwith LLMs. In this paper, we propose Activation Beacon, a plug-in module for\ntransformer-based LLMs that targets effective, efficient, and flexible\ncompression of long contexts. To achieve this, our method introduces the\nfollowing technical designs. 1) We directly compress the activations (i.e. keys\nand values at every layer), rather than leveraging soft prompts to relay\ninformation (which constitute a major bottleneck to encapsulate the complex\ninformation within long contexts). 2) We tailor the compression workflow, where\neach fine-grained input unit is progressively compressed, enabling high-quality\ncompression and efficient computation during both training and inference. 3) We\ntrain the model through compression-based auto-regression, making full use of\nplain texts and instructional data to optimize the model's compression\nperformance. 4) During training, we randomly sample a compression ratio at each\nstep, teaching the model to support a wide range of compression configurations.\nExtensive evaluations are conducted on various long-context tasks whose lengths\n(e.g., 128K) may far exceed the maximum training length (20K), such as document\nunderstanding, few-shot learning, and Needle-in-a-Haystack. Whilst existing\nmethods struggle to handle these challenging tasks, Activation Beacon maintains\na comparable performance to the uncompressed baseline across various scenarios,\nachieving a 2x acceleration in inference time and an 8x reduction of memory\ncosts for KV cache. Our data, model, and code have been released at\n\\url{https://github.com/FlagOpen/FlagEmbedding/}."
                },
                "authors": [
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Ninglu Shao"
                    },
                    {
                        "name": "Qiwei Ye"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "arxiv_comment": "Newer version of Activation Beacon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.03462v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.03462v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08391v1",
                "updated": "2024-10-10T21:55:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    21,
                    55,
                    11,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T21:55:11Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    21,
                    55,
                    11,
                    3,
                    284,
                    0
                ],
                "title": "KV Prediction for Improved Time to First Token",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Prediction for Improved Time to First Token"
                },
                "summary": "Inference with transformer-based language models begins with a prompt\nprocessing step. In this step, the model generates the first output token and\nstores the KV cache needed for future generation steps. This prompt processing\nstep can be computationally expensive, taking 10s of seconds or more for\nbillion-parameter models on edge devices when prompt lengths or batch sizes\nrise. This degrades user experience by introducing significant latency into the\nmodel's outputs. To reduce the time spent producing the first output (known as\nthe ``time to first token'', or TTFT) of a pretrained model, we introduce a\nnovel method called KV Prediction. In our method, a small auxiliary model is\nused to process the prompt and produce an approximation of the KV cache used by\na base model. This approximated KV cache is then used with the base model for\nautoregressive generation without the need to query the auxiliary model again.\nWe demonstrate that our method produces a pareto-optimal efficiency-accuracy\ntrade-off when compared to baselines. On TriviaQA, we demonstrate relative\naccuracy improvements in the range of $15\\%-50\\%$ across a range of TTFT FLOPs\nbudgets. We also demonstrate accuracy improvements of up to $30\\%$ on HumanEval\npython code completion at fixed TTFT FLOPs budgets. Additionally, we benchmark\nmodels on an Apple M2 Pro CPU and demonstrate that our improvement in FLOPs\ntranslates to a TTFT speedup on hardware. We release our code at\nhttps://github.com/apple/corenet/tree/main/projects/kv-prediction .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with transformer-based language models begins with a prompt\nprocessing step. In this step, the model generates the first output token and\nstores the KV cache needed for future generation steps. This prompt processing\nstep can be computationally expensive, taking 10s of seconds or more for\nbillion-parameter models on edge devices when prompt lengths or batch sizes\nrise. This degrades user experience by introducing significant latency into the\nmodel's outputs. To reduce the time spent producing the first output (known as\nthe ``time to first token'', or TTFT) of a pretrained model, we introduce a\nnovel method called KV Prediction. In our method, a small auxiliary model is\nused to process the prompt and produce an approximation of the KV cache used by\na base model. This approximated KV cache is then used with the base model for\nautoregressive generation without the need to query the auxiliary model again.\nWe demonstrate that our method produces a pareto-optimal efficiency-accuracy\ntrade-off when compared to baselines. On TriviaQA, we demonstrate relative\naccuracy improvements in the range of $15\\%-50\\%$ across a range of TTFT FLOPs\nbudgets. We also demonstrate accuracy improvements of up to $30\\%$ on HumanEval\npython code completion at fixed TTFT FLOPs budgets. Additionally, we benchmark\nmodels on an Apple M2 Pro CPU and demonstrate that our improvement in FLOPs\ntranslates to a TTFT speedup on hardware. We release our code at\nhttps://github.com/apple/corenet/tree/main/projects/kv-prediction ."
                },
                "authors": [
                    {
                        "name": "Maxwell Horton"
                    },
                    {
                        "name": "Qingqing Cao"
                    },
                    {
                        "name": "Chenfan Sun"
                    },
                    {
                        "name": "Yanzi Jin"
                    },
                    {
                        "name": "Sachin Mehta"
                    },
                    {
                        "name": "Mohammad Rastegari"
                    },
                    {
                        "name": "Moin Nabi"
                    }
                ],
                "author_detail": {
                    "name": "Moin Nabi"
                },
                "author": "Moin Nabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11284v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11284v3",
                "updated": "2024-10-10T16:57:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    57,
                    34,
                    3,
                    284,
                    0
                ],
                "published": "2024-04-17T11:48:14Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    11,
                    48,
                    14,
                    2,
                    108,
                    0
                ],
                "title": "Amplifying Main Memory-Based Timing Covert and Side Channels using\n  Processing-in-Memory Operations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amplifying Main Memory-Based Timing Covert and Side Channels using\n  Processing-in-Memory Operations"
                },
                "summary": "The adoption of processing-in-memory (PiM) architectures has been gaining\nmomentum because they provide high performance and low energy consumption by\nalleviating the data movement bottleneck. Yet, the security of such\narchitectures has not been thoroughly explored. The adoption of PiM solutions\nprovides a new way to directly access main memory, which malicious user\napplications can exploit. We show that this new way to access main memory opens\nopportunities for high-throughput timing attacks that are hard-to-mitigate\nwithout significant performance overhead.\n  We introduce IMPACT, a set of high-throughput main memory-based timing\nattacks that leverage characteristics of PiM architectures to establish covert\nand side channels. IMPACT enables high-throughput communication and private\ninformation leakage by exploiting the shared DRAM row buffer. To achieve high\nthroughput, IMPACT (i) eliminates cache bypassing steps required by\nprocessor-centric main memory and cache-based timing attacks and (ii) leverages\nthe intrinsic parallelism of PiM operations. We showcase two applications of\nIMPACT. First, we build two covert-channel attacks that run on the host CPU and\nleverage different PiM approaches to gain direct and fast access to main memory\nand establish high-throughput communication covert channels. Second, we\nshowcase a side-channel attack that leaks private information of concurrently\nrunning victim applications that are accelerated with PiM. Our results\ndemonstrate that (i) our covert channels achieve 12.87 Mb/s and 14.16 Mb/s\ncommunication throughput, respectively, which is up to 4.91x and 5.41x faster\nthan the state-of-the-art main memory-based covert channels, and (ii) our\nside-channel attack allows the attacker to leak secrets with a low error rate.\nTo avoid such covert and side channels in emerging PiM systems, we propose and\nevaluate three defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adoption of processing-in-memory (PiM) architectures has been gaining\nmomentum because they provide high performance and low energy consumption by\nalleviating the data movement bottleneck. Yet, the security of such\narchitectures has not been thoroughly explored. The adoption of PiM solutions\nprovides a new way to directly access main memory, which malicious user\napplications can exploit. We show that this new way to access main memory opens\nopportunities for high-throughput timing attacks that are hard-to-mitigate\nwithout significant performance overhead.\n  We introduce IMPACT, a set of high-throughput main memory-based timing\nattacks that leverage characteristics of PiM architectures to establish covert\nand side channels. IMPACT enables high-throughput communication and private\ninformation leakage by exploiting the shared DRAM row buffer. To achieve high\nthroughput, IMPACT (i) eliminates cache bypassing steps required by\nprocessor-centric main memory and cache-based timing attacks and (ii) leverages\nthe intrinsic parallelism of PiM operations. We showcase two applications of\nIMPACT. First, we build two covert-channel attacks that run on the host CPU and\nleverage different PiM approaches to gain direct and fast access to main memory\nand establish high-throughput communication covert channels. Second, we\nshowcase a side-channel attack that leaks private information of concurrently\nrunning victim applications that are accelerated with PiM. Our results\ndemonstrate that (i) our covert channels achieve 12.87 Mb/s and 14.16 Mb/s\ncommunication throughput, respectively, which is up to 4.91x and 5.41x faster\nthan the state-of-the-art main memory-based covert channels, and (ii) our\nside-channel attack allows the attacker to leak secrets with a low error rate.\nTo avoid such covert and side channels in emerging PiM systems, we propose and\nevaluate three defenses."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kanellopoulos"
                    },
                    {
                        "name": "F. Nisa Bostanci"
                    },
                    {
                        "name": "Ataberk Olgun"
                    },
                    {
                        "name": "A. Giray Yaglikci"
                    },
                    {
                        "name": "Ismail Emir Yuksel"
                    },
                    {
                        "name": "Nika Mansouri Ghiasi"
                    },
                    {
                        "name": "Zulal Bingol"
                    },
                    {
                        "name": "Mohammad Sadrosadati"
                    },
                    {
                        "name": "Onur Mutlu"
                    }
                ],
                "author_detail": {
                    "name": "Onur Mutlu"
                },
                "author": "Onur Mutlu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11284v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11284v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12850v1",
                "updated": "2024-10-10T15:24:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    24,
                    12,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T15:24:12Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    24,
                    12,
                    3,
                    284,
                    0
                ],
                "title": "RecurFormer: Not All Transformer Heads Need Self-Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RecurFormer: Not All Transformer Heads Need Self-Attention"
                },
                "summary": "Transformer-based large language models (LLMs) excel in modeling complex\nlanguage patterns but face significant computational costs during inference,\nespecially with long inputs due to the attention mechanism's memory overhead.\nWe observe that certain attention heads exhibit a distribution where the\nattention weights concentrate on tokens near the query token, termed as recency\naware, which focuses on local and short-range dependencies. Leveraging this\ninsight, we propose RecurFormer, a novel architecture that replaces these\nattention heads with linear recurrent neural networks (RNNs), specifically the\nMamba architecture. This replacement reduces the cache size without evicting\ntokens, thus maintaining generation quality. RecurFormer retains the ability to\nmodel long-range dependencies through the remaining attention heads and allows\nfor reusing pre-trained Transformer-based LLMs weights with continual training.\nExperiments demonstrate that RecurFormer matches the original model's\nperformance while significantly enhancing inference efficiency. Our approach\nprovides a practical solution to the computational challenges of\nTransformer-based LLMs inference, making it highly attractive for tasks\ninvolving long inputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) excel in modeling complex\nlanguage patterns but face significant computational costs during inference,\nespecially with long inputs due to the attention mechanism's memory overhead.\nWe observe that certain attention heads exhibit a distribution where the\nattention weights concentrate on tokens near the query token, termed as recency\naware, which focuses on local and short-range dependencies. Leveraging this\ninsight, we propose RecurFormer, a novel architecture that replaces these\nattention heads with linear recurrent neural networks (RNNs), specifically the\nMamba architecture. This replacement reduces the cache size without evicting\ntokens, thus maintaining generation quality. RecurFormer retains the ability to\nmodel long-range dependencies through the remaining attention heads and allows\nfor reusing pre-trained Transformer-based LLMs weights with continual training.\nExperiments demonstrate that RecurFormer matches the original model's\nperformance while significantly enhancing inference efficiency. Our approach\nprovides a practical solution to the computational challenges of\nTransformer-based LLMs inference, making it highly attractive for tasks\ninvolving long inputs."
                },
                "authors": [
                    {
                        "name": "Ruiqing Yan"
                    },
                    {
                        "name": "Linghan Zheng"
                    },
                    {
                        "name": "Xingbo Du"
                    },
                    {
                        "name": "Han Zou"
                    },
                    {
                        "name": "Yufeng Guo"
                    },
                    {
                        "name": "Jianfei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Yang"
                },
                "author": "Jianfei Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01195v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01195v2",
                "updated": "2024-10-10T11:01:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    1,
                    44,
                    3,
                    284,
                    0
                ],
                "published": "2024-06-03T10:58:32Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    10,
                    58,
                    32,
                    0,
                    155,
                    0
                ],
                "title": "C$^3$P-VoxelMap: Compact, Cumulative and Coalescible Probabilistic Voxel\n  Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C$^3$P-VoxelMap: Compact, Cumulative and Coalescible Probabilistic Voxel\n  Mapping"
                },
                "summary": "This work presents a compact, cumulative and coalescible probabilistic voxel\nmapping method to enhance performance, accuracy and memory efficiency in LiDAR\nodometry. Probabilistic voxel mapping requires storing past point clouds and\nre-iterating on them to update the uncertainty every iteration, which consumes\nlarge memory space and CPU cycles. To solve this problem, we propose a\ntwo-folded strategy. First, we introduce a compact point-free representation\nfor probabilistic voxels and derive a cumulative update of the planar\nuncertainty without caching original point clouds. Our voxel structure only\nkeeps track of a predetermined set of statistics for points that lie inside it.\nThis method reduces the runtime complexity from $O(MN)$ to $O(N)$ and the space\ncomplexity from $O(N)$ to $O(1)$ where $M$ is the number of iterations and $N$\nis the number of points. Second, to further minimize memory usage and enhance\nmapping accuracy, we provide a strategy to dynamically merge voxels associated\nwith the same physical planes by taking advantage of the geometric features in\nthe real world. Rather than scanning for these coalescible voxels constantly at\nevery iteration, our merging strategy accumulates voxels in a\nlocality-sensitive hash and triggers merging lazily. On-demand merging not only\nreduces memory footprint with minimal computational overhead but also improves\nlocalization accuracy thanks to cross-voxel denoising. Experiments exhibit 20%\nhigher accuracy, 20% faster performance and 70% lower memory consumption than\nthe state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a compact, cumulative and coalescible probabilistic voxel\nmapping method to enhance performance, accuracy and memory efficiency in LiDAR\nodometry. Probabilistic voxel mapping requires storing past point clouds and\nre-iterating on them to update the uncertainty every iteration, which consumes\nlarge memory space and CPU cycles. To solve this problem, we propose a\ntwo-folded strategy. First, we introduce a compact point-free representation\nfor probabilistic voxels and derive a cumulative update of the planar\nuncertainty without caching original point clouds. Our voxel structure only\nkeeps track of a predetermined set of statistics for points that lie inside it.\nThis method reduces the runtime complexity from $O(MN)$ to $O(N)$ and the space\ncomplexity from $O(N)$ to $O(1)$ where $M$ is the number of iterations and $N$\nis the number of points. Second, to further minimize memory usage and enhance\nmapping accuracy, we provide a strategy to dynamically merge voxels associated\nwith the same physical planes by taking advantage of the geometric features in\nthe real world. Rather than scanning for these coalescible voxels constantly at\nevery iteration, our merging strategy accumulates voxels in a\nlocality-sensitive hash and triggers merging lazily. On-demand merging not only\nreduces memory footprint with minimal computational overhead but also improves\nlocalization accuracy thanks to cross-voxel denoising. Experiments exhibit 20%\nhigher accuracy, 20% faster performance and 70% lower memory consumption than\nthe state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Xu Yang"
                    },
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Qijie Ge"
                    },
                    {
                        "name": "Lulu Suo"
                    },
                    {
                        "name": "Weijie Tang"
                    },
                    {
                        "name": "Zhengyu Wei"
                    },
                    {
                        "name": "Longxiang Huang"
                    },
                    {
                        "name": "Bo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Wang"
                },
                "author": "Bo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01195v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01195v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04793v2",
                "updated": "2024-10-10T05:11:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    5,
                    11,
                    52,
                    3,
                    284,
                    0
                ],
                "published": "2024-04-07T03:08:14Z",
                "published_parsed": [
                    2024,
                    4,
                    7,
                    3,
                    8,
                    14,
                    6,
                    98,
                    0
                ],
                "title": "SqueezeAttention: 2D Management of KV-Cache in LLM Inference via\n  Layer-wise Optimal Budget",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SqueezeAttention: 2D Management of KV-Cache in LLM Inference via\n  Layer-wise Optimal Budget"
                },
                "summary": "Optimizing the Key-Value (KV) cache of the Large Language Model (LLM) has\nbeen considered critical to saving the cost of inference. Most of the existing\nKV-cache compression algorithms attempted to sparsify the sequence of tokens by\ntaking advantage of the different importance of tokens. However, most of these\nmethods treat all layers equally, allocating the same KV budget to each layer.\nThis approach is suboptimal, as some layers may be less sensitive to input\ntokens yet still receive the same budget as others. In this work, we found that\nby identifying the importance of attention layers, we could optimize the\nKV-cache jointly from two dimensions, i.e., sequence-wise and layer-wise. Based\non our observations regarding layer-wise importance in inference, we propose\nSqueezeAttention to precisely optimize the allocation of KV-cache budget among\nlayers on-the-fly and then incorporate three representative sequence-wise\nalgorithms to compress the KV-cache for each layer with its very own budget.\nSpecifically, we first measure each layer's importance by calculating the\ncosine similarity of the input prompt differences before and after the\nself-attention layers. Based on this similarity, we then categorize the layers\ninto two groups and adjust their KV budgets accordingly. By optimizing the\nKV-cache from both sequence's and layer's dimensions, SqueezeAttention achieves\naround 30% to 70% of the memory reductions and up to 2.2 times of throughput\nimprovements in a wide range of LLMs and benchmarks. The code is available at\nhttps://github.com/hetailang/SqueezeAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing the Key-Value (KV) cache of the Large Language Model (LLM) has\nbeen considered critical to saving the cost of inference. Most of the existing\nKV-cache compression algorithms attempted to sparsify the sequence of tokens by\ntaking advantage of the different importance of tokens. However, most of these\nmethods treat all layers equally, allocating the same KV budget to each layer.\nThis approach is suboptimal, as some layers may be less sensitive to input\ntokens yet still receive the same budget as others. In this work, we found that\nby identifying the importance of attention layers, we could optimize the\nKV-cache jointly from two dimensions, i.e., sequence-wise and layer-wise. Based\non our observations regarding layer-wise importance in inference, we propose\nSqueezeAttention to precisely optimize the allocation of KV-cache budget among\nlayers on-the-fly and then incorporate three representative sequence-wise\nalgorithms to compress the KV-cache for each layer with its very own budget.\nSpecifically, we first measure each layer's importance by calculating the\ncosine similarity of the input prompt differences before and after the\nself-attention layers. Based on this similarity, we then categorize the layers\ninto two groups and adjust their KV budgets accordingly. By optimizing the\nKV-cache from both sequence's and layer's dimensions, SqueezeAttention achieves\naround 30% to 70% of the memory reductions and up to 2.2 times of throughput\nimprovements in a wide range of LLMs and benchmarks. The code is available at\nhttps://github.com/hetailang/SqueezeAttention."
                },
                "authors": [
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Shaoduo Gan"
                    }
                ],
                "author_detail": {
                    "name": "Shaoduo Gan"
                },
                "author": "Shaoduo Gan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07590v1",
                "updated": "2024-10-10T03:52:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    52,
                    54,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T03:52:54Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    52,
                    54,
                    3,
                    284,
                    0
                ],
                "title": "TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed\n  KV Caches for Chunked Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed\n  KV Caches for Chunked Text"
                },
                "summary": "Current Retrieval-Augmented Generation (RAG) systems concatenate and process\nnumerous retrieved document chunks for prefill which requires a large volume of\ncomputation, therefore leading to significant latency in time-to-first-token\n(TTFT). To reduce the computation overhead as well as TTFT, we introduce\nTurboRAG, a novel RAG system that redesigns the inference paradigm of the\ncurrent RAG system by first pre-computing and storing the key-value (KV) caches\nof documents offline, and then directly retrieving the saved KV cache for\nprefill. Hence, online computation of KV caches is eliminated during inference.\nIn addition, we provide a number of insights into the mask matrix and\npositional embedding mechanisms, plus fine-tune a pretrained language model to\nmaintain model accuracy of TurboRAG. Our approach is applicable to most\nexisting large language models and their applications without any requirement\nin modification of models and inference systems. Experimental results across a\nsuite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x\ncompared to the conventional RAG systems (on an average of 8.6x), but reserving\ncomparable performance to the standard RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Retrieval-Augmented Generation (RAG) systems concatenate and process\nnumerous retrieved document chunks for prefill which requires a large volume of\ncomputation, therefore leading to significant latency in time-to-first-token\n(TTFT). To reduce the computation overhead as well as TTFT, we introduce\nTurboRAG, a novel RAG system that redesigns the inference paradigm of the\ncurrent RAG system by first pre-computing and storing the key-value (KV) caches\nof documents offline, and then directly retrieving the saved KV cache for\nprefill. Hence, online computation of KV caches is eliminated during inference.\nIn addition, we provide a number of insights into the mask matrix and\npositional embedding mechanisms, plus fine-tune a pretrained language model to\nmaintain model accuracy of TurboRAG. Our approach is applicable to most\nexisting large language models and their applications without any requirement\nin modification of models and inference systems. Experimental results across a\nsuite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x\ncompared to the conventional RAG systems (on an average of 8.6x), but reserving\ncomparable performance to the standard RAG systems."
                },
                "authors": [
                    {
                        "name": "Songshuo Lu"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Yutian Rong"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Yaohua Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yaohua Tang"
                },
                "author": "Yaohua Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07579v1",
                "updated": "2024-10-10T03:28:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    28,
                    46,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T03:28:46Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    28,
                    46,
                    3,
                    284,
                    0
                ],
                "title": "Teddy: Efficient Large-Scale Dataset Distillation via\n  Taylor-Approximated Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teddy: Efficient Large-Scale Dataset Distillation via\n  Taylor-Approximated Matching"
                },
                "summary": "Dataset distillation or condensation refers to compressing a large-scale\ndataset into a much smaller one, enabling models trained on this synthetic\ndataset to generalize effectively on real data. Tackling this challenge, as\ndefined, relies on a bi-level optimization algorithm: a novel model is trained\nin each iteration within a nested loop, with gradients propagated through an\nunrolled computation graph. However, this approach incurs high memory and time\ncomplexity, posing difficulties in scaling up to large datasets such as\nImageNet. Addressing these concerns, this paper introduces Teddy, a\nTaylor-approximated dataset distillation framework designed to handle\nlarge-scale dataset and enhance efficiency. On the one hand, backed up by\ntheoretical analysis, we propose a memory-efficient approximation derived from\nTaylor expansion, which transforms the original form dependent on multi-step\ngradients to a first-order one. On the other hand, rather than repeatedly\ntraining a novel model in each iteration, we unveil that employing a pre-cached\npool of weak models, which can be generated from a single base model, enhances\nboth time efficiency and performance concurrently, particularly when dealing\nwith large-scale datasets. Extensive experiments demonstrate that the proposed\nTeddy attains state-of-the-art efficiency and performance on the Tiny-ImageNet\nand original-sized ImageNet-1K dataset, notably surpassing prior methods by up\nto 12.8%, while reducing 46.6% runtime. Our code will be available at\nhttps://github.com/Lexie-YU/Teddy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dataset distillation or condensation refers to compressing a large-scale\ndataset into a much smaller one, enabling models trained on this synthetic\ndataset to generalize effectively on real data. Tackling this challenge, as\ndefined, relies on a bi-level optimization algorithm: a novel model is trained\nin each iteration within a nested loop, with gradients propagated through an\nunrolled computation graph. However, this approach incurs high memory and time\ncomplexity, posing difficulties in scaling up to large datasets such as\nImageNet. Addressing these concerns, this paper introduces Teddy, a\nTaylor-approximated dataset distillation framework designed to handle\nlarge-scale dataset and enhance efficiency. On the one hand, backed up by\ntheoretical analysis, we propose a memory-efficient approximation derived from\nTaylor expansion, which transforms the original form dependent on multi-step\ngradients to a first-order one. On the other hand, rather than repeatedly\ntraining a novel model in each iteration, we unveil that employing a pre-cached\npool of weak models, which can be generated from a single base model, enhances\nboth time efficiency and performance concurrently, particularly when dealing\nwith large-scale datasets. Extensive experiments demonstrate that the proposed\nTeddy attains state-of-the-art efficiency and performance on the Tiny-ImageNet\nand original-sized ImageNet-1K dataset, notably surpassing prior methods by up\nto 12.8%, while reducing 46.6% runtime. Our code will be available at\nhttps://github.com/Lexie-YU/Teddy."
                },
                "authors": [
                    {
                        "name": "Ruonan Yu"
                    },
                    {
                        "name": "Songhua Liu"
                    },
                    {
                        "name": "Jingwen Ye"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "Accepted by ECCV2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19519v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19519v3",
                "updated": "2024-10-09T15:57:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    57,
                    3,
                    2,
                    283,
                    0
                ],
                "published": "2024-03-28T15:52:15Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    15,
                    52,
                    15,
                    3,
                    88,
                    0
                ],
                "title": "Laser Interactions with Gas Jets: EMP Emission and Nozzle Damage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Laser Interactions with Gas Jets: EMP Emission and Nozzle Damage"
                },
                "summary": "Understanding the physics of electromagnetic pulse emission and nozzle damage\nis critical for the long-term operation of laser experiments with gas targets,\nparticularly at facilities looking to produce stable sources of radiation at\nhigh repetition rate. We present a theoretical model of plasma formation and\nelectrostatic charging when high-power lasers are focused inside gases. The\nmodel can be used to estimate the amplitude of gigahertz electromagnetic pulses\n(EMPs) produced by the laser and the extent of damage to the gas jet nozzle.\nLooking at a range of laser and target properties relevant to existing\nhigh-power laser systems, we find that EMP fields of tens to hundreds of kV/m\ncan be generated several metres from the gas jet. Model predictions are\ncompared with measurements of EMP, plasma formation and nozzle damage from two\nexperiments on the VEGA-3 laser and one experiment on the Vulcan Petawatt\nlaser.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the physics of electromagnetic pulse emission and nozzle damage\nis critical for the long-term operation of laser experiments with gas targets,\nparticularly at facilities looking to produce stable sources of radiation at\nhigh repetition rate. We present a theoretical model of plasma formation and\nelectrostatic charging when high-power lasers are focused inside gases. The\nmodel can be used to estimate the amplitude of gigahertz electromagnetic pulses\n(EMPs) produced by the laser and the extent of damage to the gas jet nozzle.\nLooking at a range of laser and target properties relevant to existing\nhigh-power laser systems, we find that EMP fields of tens to hundreds of kV/m\ncan be generated several metres from the gas jet. Model predictions are\ncompared with measurements of EMP, plasma formation and nozzle damage from two\nexperiments on the VEGA-3 laser and one experiment on the Vulcan Petawatt\nlaser."
                },
                "authors": [
                    {
                        "name": "Philip Wykeham Bradford"
                    },
                    {
                        "name": "Valeria Ospina-Bohorquez"
                    },
                    {
                        "name": "Michael Ehret"
                    },
                    {
                        "name": "Jose-Luis Henares"
                    },
                    {
                        "name": "Pilar Puyuelo-Valdes"
                    },
                    {
                        "name": "Tomasz Chodukowski"
                    },
                    {
                        "name": "Tadeusz Pisarczyk"
                    },
                    {
                        "name": "Zofia Rusiniak"
                    },
                    {
                        "name": "Carlos Salgado-Lopez"
                    },
                    {
                        "name": "Christos Vlachos"
                    },
                    {
                        "name": "Massimiliano Sciscio"
                    },
                    {
                        "name": "Martina Salvadori"
                    },
                    {
                        "name": "Claudio Verona"
                    },
                    {
                        "name": "George Hicks"
                    },
                    {
                        "name": "Oliver Ettlinger"
                    },
                    {
                        "name": "Zulfikar Najmudin"
                    },
                    {
                        "name": "Jean-Raphael Marques"
                    },
                    {
                        "name": "Laurent Gremillet"
                    },
                    {
                        "name": "Joao Jorge Santos"
                    },
                    {
                        "name": "Fabrizio Consoli"
                    },
                    {
                        "name": "Vladimir Tikhonchuk"
                    }
                ],
                "author_detail": {
                    "name": "Vladimir Tikhonchuk"
                },
                "author": "Vladimir Tikhonchuk",
                "arxiv_comment": "18 pages (total), 12 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19519v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19519v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06934v1",
                "updated": "2024-10-09T14:28:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    28,
                    59,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T14:28:59Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    28,
                    59,
                    2,
                    283,
                    0
                ],
                "title": "VEC-Sim: A Simulation Platform for Evaluating Service Caching and\n  Computation Offloading Policies in Vehicular Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VEC-Sim: A Simulation Platform for Evaluating Service Caching and\n  Computation Offloading Policies in Vehicular Edge Networks"
                },
                "summary": "Computer simulation platforms offer an alternative solution by emulating\ncomplex systems in a controlled manner. However, existing Edge Computing (EC)\nsimulators, as well as general-purpose vehicular network simulators, are not\ntailored for VEC and lack dedicated support for modeling the distinct access\npattern, entity mobility trajectory and other unique characteristics of VEC\nnetworks. To fill this gap, this paper proposes VEC-Sim, a versatile simulation\nplatform for in-depth evaluation and analysis of various service caching and\ncomputation offloading policies in VEC networks. VEC-Sim incorporates realistic\nmechanisms to replicate real-world access patterns, including service feature\nvector, vehicle mobility modeling, evolving service popularity, new service\nupload and user preference shifts, etc. Moreover, its modular architecture and\nextensive Application Programming Interfaces (APIs) allow seamless integration\nof customized scheduling policies and user-defined metrics. A comprehensive\nevaluation of VEC-Sim's capabilities is undertaken in comparison to real-world\nground truths. Results prove it to be accurate in reproducing classical\nscheduling algorithms and extremely effective in conducting case studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer simulation platforms offer an alternative solution by emulating\ncomplex systems in a controlled manner. However, existing Edge Computing (EC)\nsimulators, as well as general-purpose vehicular network simulators, are not\ntailored for VEC and lack dedicated support for modeling the distinct access\npattern, entity mobility trajectory and other unique characteristics of VEC\nnetworks. To fill this gap, this paper proposes VEC-Sim, a versatile simulation\nplatform for in-depth evaluation and analysis of various service caching and\ncomputation offloading policies in VEC networks. VEC-Sim incorporates realistic\nmechanisms to replicate real-world access patterns, including service feature\nvector, vehicle mobility modeling, evolving service popularity, new service\nupload and user preference shifts, etc. Moreover, its modular architecture and\nextensive Application Programming Interfaces (APIs) allow seamless integration\nof customized scheduling policies and user-defined metrics. A comprehensive\nevaluation of VEC-Sim's capabilities is undertaken in comparison to real-world\nground truths. Results prove it to be accurate in reproducing classical\nscheduling algorithms and extremely effective in conducting case studies."
                },
                "authors": [
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Xiaolong Xu"
                    },
                    {
                        "name": "Muhammad Bilal"
                    },
                    {
                        "name": "Xiangwei Wang"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Siyu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Siyu Wu"
                },
                "author": "Siyu Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00428v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00428v3",
                "updated": "2024-10-09T11:40:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    11,
                    40,
                    31,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-01T06:23:17Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    6,
                    23,
                    17,
                    1,
                    275,
                    0
                ],
                "title": "LayerKV: Optimizing Large Language Model Serving with Layer-wise KV\n  Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LayerKV: Optimizing Large Language Model Serving with Layer-wise KV\n  Cache Management"
                },
                "summary": "The expanding context windows in large language models (LLMs) have greatly\nenhanced their capabilities in various applications, but they also introduce\nsignificant challenges in maintaining low latency, particularly in Time to\nFirst Token (TTFT). This paper identifies that the sharp rise in TTFT as\ncontext length increases is predominantly driven by queuing delays, which are\ncaused by the growing demands for GPU Key-Value (KV) cache allocation clashing\nwith the limited availability of KV cache blocks. To address this issue, we\npropose LayerKV, a simple yet effective plug-in method that effectively reduces\nTTFT without requiring additional hardware or compromising output performance,\nwhile seamlessly integrating with existing parallelism strategies and\nscheduling techniques. Specifically, LayerKV introduces layer-wise KV block\nallocation, management, and offloading for fine-grained control over system\nmemory, coupled with an SLO-aware scheduler to optimize overall Service Level\nObjectives (SLOs). Comprehensive evaluations on representative models, ranging\nfrom 7B to 70B parameters, across various GPU configurations, demonstrate that\nLayerKV improves TTFT latency up to 69x and reduces SLO violation rates by\n28.7%, significantly enhancing the user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expanding context windows in large language models (LLMs) have greatly\nenhanced their capabilities in various applications, but they also introduce\nsignificant challenges in maintaining low latency, particularly in Time to\nFirst Token (TTFT). This paper identifies that the sharp rise in TTFT as\ncontext length increases is predominantly driven by queuing delays, which are\ncaused by the growing demands for GPU Key-Value (KV) cache allocation clashing\nwith the limited availability of KV cache blocks. To address this issue, we\npropose LayerKV, a simple yet effective plug-in method that effectively reduces\nTTFT without requiring additional hardware or compromising output performance,\nwhile seamlessly integrating with existing parallelism strategies and\nscheduling techniques. Specifically, LayerKV introduces layer-wise KV block\nallocation, management, and offloading for fine-grained control over system\nmemory, coupled with an SLO-aware scheduler to optimize overall Service Level\nObjectives (SLOs). Comprehensive evaluations on representative models, ranging\nfrom 7B to 70B parameters, across various GPU configurations, demonstrate that\nLayerKV improves TTFT latency up to 69x and reduces SLO violation rates by\n28.7%, significantly enhancing the user experience."
                },
                "authors": [
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Ziqing Wang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Yuhong Guo"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Zhenxuan Pan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenxuan Pan"
                },
                "author": "Zhenxuan Pan",
                "arxiv_comment": "11 pages, 7 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00428v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00428v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06627v1",
                "updated": "2024-10-09T07:22:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    7,
                    22,
                    40,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T07:22:40Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    7,
                    22,
                    40,
                    2,
                    283,
                    0
                ],
                "title": "Variations in Multi-Agent Actor-Critic Frameworks for Joint\n  Optimizations in UAV Swarm Networks: Recent Evolution, Challenges, and\n  Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variations in Multi-Agent Actor-Critic Frameworks for Joint\n  Optimizations in UAV Swarm Networks: Recent Evolution, Challenges, and\n  Directions"
                },
                "summary": "Autonomous unmanned aerial vehicle (UAV) swarm networks (UAVSNs) can\neffectively execute surveillance, connectivity, and computing services to\nground users (GUs). These missions require trajectory planning, UAV-GUs\nassociation, task offloading, next-hop selection, and resources such as\ntransmit power, bandwidth, caching, and computing allocation to improve network\nperformances. Owing to the highly dynamic topology, limited resources, and\nnon-availability of global knowledge, optimizing network performance in UAVSNs\nis very intricate. Hence, it requires an adaptive joint optimization framework\nthat can tackle both discrete and continuous decision variables to ensure\noptimal network performance under dynamic constraints. Multi-agent deep\nreinforcement learning-based adaptive actor-critic framework can efficiently\naddress these problems. This paper investigates the recent evolutions of\nactor-critic frameworks to deal with joint optimization problems in UAVSNs. In\naddition, challenges and potential solutions are addressed as research\ndirections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous unmanned aerial vehicle (UAV) swarm networks (UAVSNs) can\neffectively execute surveillance, connectivity, and computing services to\nground users (GUs). These missions require trajectory planning, UAV-GUs\nassociation, task offloading, next-hop selection, and resources such as\ntransmit power, bandwidth, caching, and computing allocation to improve network\nperformances. Owing to the highly dynamic topology, limited resources, and\nnon-availability of global knowledge, optimizing network performance in UAVSNs\nis very intricate. Hence, it requires an adaptive joint optimization framework\nthat can tackle both discrete and continuous decision variables to ensure\noptimal network performance under dynamic constraints. Multi-agent deep\nreinforcement learning-based adaptive actor-critic framework can efficiently\naddress these problems. This paper investigates the recent evolutions of\nactor-critic frameworks to deal with joint optimization problems in UAVSNs. In\naddition, challenges and potential solutions are addressed as research\ndirections."
                },
                "authors": [
                    {
                        "name": "Muhammad Morshed Alam"
                    },
                    {
                        "name": "Muhammad Yeasir Aarafat"
                    },
                    {
                        "name": "Tamim Hossain"
                    }
                ],
                "author_detail": {
                    "name": "Tamim Hossain"
                },
                "author": "Tamim Hossain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13941v2",
                "updated": "2024-10-09T04:11:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    4,
                    11,
                    28,
                    2,
                    283,
                    0
                ],
                "published": "2024-06-20T02:20:21Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    2,
                    20,
                    21,
                    3,
                    172,
                    0
                ],
                "title": "UpDLRM: Accelerating Personalized Recommendation using Real-World PIM\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UpDLRM: Accelerating Personalized Recommendation using Real-World PIM\n  Architecture"
                },
                "summary": "Deep Learning Recommendation Models (DLRMs) have gained popularity in\nrecommendation systems due to their effectiveness in handling large-scale\nrecommendation tasks. The embedding layers of DLRMs have become the performance\nbottleneck due to their intensive needs on memory capacity and memory\nbandwidth. In this paper, we propose UpDLRM, which utilizes real-world\nprocessingin-memory (PIM) hardware, UPMEM DPU, to boost the memory bandwidth\nand reduce recommendation latency. The parallel nature of the DPU memory can\nprovide high aggregated bandwidth for the large number of irregular memory\naccesses in embedding lookups, thus offering great potential to reduce the\ninference latency. To fully utilize the DPU memory bandwidth, we further\nstudied the embedding table partitioning problem to achieve good\nworkload-balance and efficient data caching. Evaluations using real-world\ndatasets show that, UpDLRM achieves much lower inference time for DLRM compared\nto both CPU-only and CPU-GPU hybrid counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning Recommendation Models (DLRMs) have gained popularity in\nrecommendation systems due to their effectiveness in handling large-scale\nrecommendation tasks. The embedding layers of DLRMs have become the performance\nbottleneck due to their intensive needs on memory capacity and memory\nbandwidth. In this paper, we propose UpDLRM, which utilizes real-world\nprocessingin-memory (PIM) hardware, UPMEM DPU, to boost the memory bandwidth\nand reduce recommendation latency. The parallel nature of the DPU memory can\nprovide high aggregated bandwidth for the large number of irregular memory\naccesses in embedding lookups, thus offering great potential to reduce the\ninference latency. To fully utilize the DPU memory bandwidth, we further\nstudied the embedding table partitioning problem to achieve good\nworkload-balance and efficient data caching. Evaluations using real-world\ndatasets show that, UpDLRM achieves much lower inference time for DLRM compared\nto both CPU-only and CPU-GPU hybrid counterparts."
                },
                "authors": [
                    {
                        "name": "Sitian Chen"
                    },
                    {
                        "name": "Haobin Tan"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Yusen Li"
                    },
                    {
                        "name": "Pavan Balaji"
                    }
                ],
                "author_detail": {
                    "name": "Pavan Balaji"
                },
                "author": "Pavan Balaji",
                "arxiv_doi": "10.1145/3649329.3658266",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3649329.3658266",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.13941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by DAC 2024",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06497v1",
                "updated": "2024-10-09T02:51:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    2,
                    51,
                    27,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T02:51:27Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    2,
                    51,
                    27,
                    2,
                    283,
                    0
                ],
                "title": "ERCache: An Efficient and Reliable Caching Framework for Large-Scale\n  User Representations in Meta's Ads System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ERCache: An Efficient and Reliable Caching Framework for Large-Scale\n  User Representations in Meta's Ads System"
                },
                "summary": "The increasing complexity of deep learning models used for calculating user\nrepresentations presents significant challenges, particularly with limited\ncomputational resources and strict service-level agreements (SLAs). Previous\nresearch efforts have focused on optimizing model inference but have overlooked\na critical question: is it necessary to perform user model inference for every\nad request in large-scale social networks? To address this question and these\nchallenges, we first analyze user access patterns at Meta and find that most\nuser model inferences occur within a short timeframe. T his observation reveals\na triangular relationship among model complexity, embedding freshness, and\nservice SLAs. Building on this insight, we designed, implemented, and evaluated\nERCache, an efficient and robust caching framework for large-scale user\nrepresentations in ads recommendation systems on social networks. ERCache\ncategorizes cache into direct and failover types and applies customized\nsettings and eviction policies for each model, effectively balancing model\ncomplexity, embedding freshness, and service SLAs, even considering the\nstaleness introduced by caching. ERCache has been deployed at Meta for over six\nmonths, supporting more than 30 ranking models while efficiently conserving\ncomputational resources and complying with service SLA requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of deep learning models used for calculating user\nrepresentations presents significant challenges, particularly with limited\ncomputational resources and strict service-level agreements (SLAs). Previous\nresearch efforts have focused on optimizing model inference but have overlooked\na critical question: is it necessary to perform user model inference for every\nad request in large-scale social networks? To address this question and these\nchallenges, we first analyze user access patterns at Meta and find that most\nuser model inferences occur within a short timeframe. T his observation reveals\na triangular relationship among model complexity, embedding freshness, and\nservice SLAs. Building on this insight, we designed, implemented, and evaluated\nERCache, an efficient and robust caching framework for large-scale user\nrepresentations in ads recommendation systems on social networks. ERCache\ncategorizes cache into direct and failover types and applies customized\nsettings and eviction policies for each model, effectively balancing model\ncomplexity, embedding freshness, and service SLAs, even considering the\nstaleness introduced by caching. ERCache has been deployed at Meta for over six\nmonths, supporting more than 30 ranking models while efficiently conserving\ncomputational resources and complying with service SLA requirements."
                },
                "authors": [
                    {
                        "name": "Fang Zhou"
                    },
                    {
                        "name": "Yaning Huang"
                    },
                    {
                        "name": "Dong Liang"
                    },
                    {
                        "name": "Dai Li"
                    },
                    {
                        "name": "Zhongke Zhang"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Xiao Xin"
                    },
                    {
                        "name": "Abdallah Aboelela"
                    },
                    {
                        "name": "Zheliang Jiang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Jeff Song"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Chen Liang"
                    },
                    {
                        "name": "Huayu Li"
                    },
                    {
                        "name": "ChongLin Sun"
                    },
                    {
                        "name": "Hang Yang"
                    },
                    {
                        "name": "Lei Qu"
                    },
                    {
                        "name": "Zhan Shu"
                    },
                    {
                        "name": "Mindi Yuan"
                    },
                    {
                        "name": "Emanuele Maccherani"
                    },
                    {
                        "name": "Taha Hayat"
                    },
                    {
                        "name": "John Guo"
                    },
                    {
                        "name": "Varna Puvvada"
                    },
                    {
                        "name": "Uladzimir Pashkevich"
                    }
                ],
                "author_detail": {
                    "name": "Uladzimir Pashkevich"
                },
                "author": "Uladzimir Pashkevich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10443v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10443v4",
                "updated": "2024-10-09T01:12:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    1,
                    12,
                    19,
                    2,
                    283,
                    0
                ],
                "published": "2024-05-16T21:07:42Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    21,
                    7,
                    42,
                    3,
                    137,
                    0
                ],
                "title": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation"
                },
                "summary": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost."
                },
                "authors": [
                    {
                        "name": "Matthew Raffel"
                    },
                    {
                        "name": "Victor Agostinelli"
                    },
                    {
                        "name": "Lizhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lizhong Chen"
                },
                "author": "Lizhong Chen",
                "arxiv_comment": "Accepted at EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10443v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10443v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01527v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01527v2",
                "updated": "2024-10-08T19:34:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    19,
                    34,
                    3,
                    1,
                    282,
                    0
                ],
                "published": "2024-07-01T17:59:47Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    17,
                    59,
                    47,
                    0,
                    183,
                    0
                ],
                "title": "KV Cache Compression, But What Must We Give in Return? A Comprehensive\n  Benchmark of Long Context Capable Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Compression, But What Must We Give in Return? A Comprehensive\n  Benchmark of Long Context Capable Approaches"
                },
                "summary": "Long context capability is a crucial competency for large language models\n(LLMs) as it mitigates the human struggle to digest long-form texts. This\ncapability enables complex task-solving scenarios such as book summarization,\ncode assistance, and many more tasks that are traditionally manpower-intensive.\nHowever, transformer-based LLMs face significant challenges with long context\ninput due to the growing size of the KV cache and the intrinsic complexity of\nattending to extended inputs; where multiple schools of efficiency-driven\napproaches - such as KV cache quantization, token dropping, prompt compression,\nlinear-time sequence models, and hybrid architectures - have been proposed to\nproduce efficient yet long context-capable models. Despite these advancements,\nno existing work has comprehensively benchmarked these methods in a reasonably\naligned environment. In this work, we fill this gap by providing a taxonomy of\ncurrent methods and evaluating 10+ state-of-the-art approaches across seven\ncategories of long context tasks. Our work reveals numerous previously unknown\nphenomena and offers insights - as well as a friendly workbench - for the\nfuture development of long context-capable LLMs. The source code is available\nat https://github.com/henryzhongsc/longctx_bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context capability is a crucial competency for large language models\n(LLMs) as it mitigates the human struggle to digest long-form texts. This\ncapability enables complex task-solving scenarios such as book summarization,\ncode assistance, and many more tasks that are traditionally manpower-intensive.\nHowever, transformer-based LLMs face significant challenges with long context\ninput due to the growing size of the KV cache and the intrinsic complexity of\nattending to extended inputs; where multiple schools of efficiency-driven\napproaches - such as KV cache quantization, token dropping, prompt compression,\nlinear-time sequence models, and hybrid architectures - have been proposed to\nproduce efficient yet long context-capable models. Despite these advancements,\nno existing work has comprehensively benchmarked these methods in a reasonably\naligned environment. In this work, we fill this gap by providing a taxonomy of\ncurrent methods and evaluating 10+ state-of-the-art approaches across seven\ncategories of long context tasks. Our work reveals numerous previously unknown\nphenomena and offers insights - as well as a friendly workbench - for the\nfuture development of long context-capable LLMs. The source code is available\nat https://github.com/henryzhongsc/longctx_bench."
                },
                "authors": [
                    {
                        "name": "Jiayi Yuan"
                    },
                    {
                        "name": "Hongyi Liu"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Yu-Neng Chuang"
                    },
                    {
                        "name": "Songchen Li"
                    },
                    {
                        "name": "Guanchu Wang"
                    },
                    {
                        "name": "Duy Le"
                    },
                    {
                        "name": "Hongye Jin"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01527v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01527v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05927v1",
                "updated": "2024-10-08T11:28:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    11,
                    28,
                    30,
                    1,
                    282,
                    0
                ],
                "published": "2024-10-08T11:28:30Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    11,
                    28,
                    30,
                    1,
                    282,
                    0
                ],
                "title": "Numerical analysis of partial discharge ignition in H2 bubbles floating\n  in dielectric oils, for High-Voltage Solid State Transformer applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerical analysis of partial discharge ignition in H2 bubbles floating\n  in dielectric oils, for High-Voltage Solid State Transformer applications"
                },
                "summary": "We report on a self-consistent numerical analysis campaign of partial\ndischarge (PD) ignition in H2 bubbles floating in biobased dielectric oils. We\ninvestigate various configurations (bubble sizes, bubble position, existence of\nprotrusion) on a cylinder-to-cylinder setup that emulates a specific SST module\n(from SSTAR Horizon Europe project) under transient overvoltage as well as in\nits design operational conditions (VRMS = 66 kV, AC excitation of 50 Hz). Our\nresults on electrical characteristics and plasma dynamics leading to the PD\nignition, indicate that under transient overvoltage and for mm size bubbles\n(diameter 1 -4.5 mm), the smaller the bubble the less the inception voltage,\nwhile the peak inception voltage is higher than 70 kV. The existence of\nmetallic protrusion can affect the inception voltage of a remote floating\nbubble only slightly and when this is close to the sharp tip. The extreme\nscenario of a protrusion in contact (inside) a gas bubble severely affects the\ninsulation properties and drops the PD inception voltage remarkably. The larger\nthe bubble and the sharper the tip of the protrusion the lower the inception\npeak voltage, that can reach values well below 40 kV. On the contrary and under\ndesign operation, larger bubbles increase the severity and probability of PD\nevents, leading to lower instantaneous inception voltages. Current pulses\nproduced in bubbles can quickly transit to intense streamer discharges (which\ncan also transit to catastrophic arcing) if the operational frequency is\nreduced and/or under transient, HF overvoltage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on a self-consistent numerical analysis campaign of partial\ndischarge (PD) ignition in H2 bubbles floating in biobased dielectric oils. We\ninvestigate various configurations (bubble sizes, bubble position, existence of\nprotrusion) on a cylinder-to-cylinder setup that emulates a specific SST module\n(from SSTAR Horizon Europe project) under transient overvoltage as well as in\nits design operational conditions (VRMS = 66 kV, AC excitation of 50 Hz). Our\nresults on electrical characteristics and plasma dynamics leading to the PD\nignition, indicate that under transient overvoltage and for mm size bubbles\n(diameter 1 -4.5 mm), the smaller the bubble the less the inception voltage,\nwhile the peak inception voltage is higher than 70 kV. The existence of\nmetallic protrusion can affect the inception voltage of a remote floating\nbubble only slightly and when this is close to the sharp tip. The extreme\nscenario of a protrusion in contact (inside) a gas bubble severely affects the\ninsulation properties and drops the PD inception voltage remarkably. The larger\nthe bubble and the sharper the tip of the protrusion the lower the inception\npeak voltage, that can reach values well below 40 kV. On the contrary and under\ndesign operation, larger bubbles increase the severity and probability of PD\nevents, leading to lower instantaneous inception voltages. Current pulses\nproduced in bubbles can quickly transit to intense streamer discharges (which\ncan also transit to catastrophic arcing) if the operational frequency is\nreduced and/or under transient, HF overvoltage."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kourtzanidis"
                    },
                    {
                        "name": "Panagiotis Dimitrakellis"
                    },
                    {
                        "name": "Dimitrios Rakopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Rakopoulos"
                },
                "author": "Dimitrios Rakopoulos",
                "arxiv_comment": "Submitted to IEEE Transactions on Dielectrics and Electrical\n  Insulation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05863v1",
                "updated": "2024-10-08T09:53:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    53,
                    10,
                    1,
                    282,
                    0
                ],
                "published": "2024-10-08T09:53:10Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    53,
                    10,
                    1,
                    282,
                    0
                ],
                "title": "Enhancing Playback Performance in Video Recommender Systems with an\n  On-Device Gating and Ranking Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Playback Performance in Video Recommender Systems with an\n  On-Device Gating and Ranking Framework"
                },
                "summary": "Video recommender systems (RSs) have gained increasing attention in recent\nyears. Existing mainstream RSs focus on optimizing the matching function\nbetween users and items. However, we noticed that users frequently encounter\nplayback issues such as slow loading or stuttering while browsing the videos,\nespecially in weak network conditions, which will lead to a subpar browsing\nexperience, and may cause users to leave, even when the video content and\nrecommendations are superior. It is quite a serious issue, yet easily\noverlooked. To tackle this issue, we propose an on-device Gating and Ranking\nFramework (GRF) that cooperates with server-side RS. Specifically, we utilize a\ngate model to identify videos that may have playback issues in real-time, and\nthen we employ a ranking model to select the optimal result from a\nlocally-cached pool to replace the stuttering videos. Our solution has been\nfully deployed on Kwai, a large-scale short video platform with hundreds of\nmillions of users globally. Moreover, it significantly enhances video playback\nperformance and improves overall user experience and retention rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video recommender systems (RSs) have gained increasing attention in recent\nyears. Existing mainstream RSs focus on optimizing the matching function\nbetween users and items. However, we noticed that users frequently encounter\nplayback issues such as slow loading or stuttering while browsing the videos,\nespecially in weak network conditions, which will lead to a subpar browsing\nexperience, and may cause users to leave, even when the video content and\nrecommendations are superior. It is quite a serious issue, yet easily\noverlooked. To tackle this issue, we propose an on-device Gating and Ranking\nFramework (GRF) that cooperates with server-side RS. Specifically, we utilize a\ngate model to identify videos that may have playback issues in real-time, and\nthen we employ a ranking model to select the optimal result from a\nlocally-cached pool to replace the stuttering videos. Our solution has been\nfully deployed on Kwai, a large-scale short video platform with hundreds of\nmillions of users globally. Moreover, it significantly enhances video playback\nperformance and improves overall user experience and retention rates."
                },
                "authors": [
                    {
                        "name": "Yunfei Yang"
                    },
                    {
                        "name": "Zhenghao Qi"
                    },
                    {
                        "name": "Honghuan Wu"
                    },
                    {
                        "name": "Qi Song"
                    },
                    {
                        "name": "Tieyao Zhang"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Yimin Tu"
                    },
                    {
                        "name": "Kaiqiao Zhan"
                    },
                    {
                        "name": "Ben Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ben Wang"
                },
                "author": "Ben Wang",
                "arxiv_comment": "CIKM 2024 applied research track, 7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05854v1",
                "updated": "2024-10-08T09:46:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    46,
                    38,
                    1,
                    282,
                    0
                ],
                "published": "2024-10-08T09:46:38Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    46,
                    38,
                    1,
                    282,
                    0
                ],
                "title": "A Scalable State Sharing Protocol for Low-Resource Validator Nodes in\n  Blockchain Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable State Sharing Protocol for Low-Resource Validator Nodes in\n  Blockchain Networks"
                },
                "summary": "The perpetual growth of data stored on popular blockchains such as Ethereum\nleads to significant scalability challenges and substantial storage costs for\noperators of full nodes. Increasing costs may lead to fewer independently\noperated nodes in the network, which poses risks to decentralization (and hence\nnetwork security), but also pushes decentralized app developers towards\ncentrally hosted API services.\n  This paper introduces a new protocol that allows validator nodes to\nparticipate in a blockchain network without the need to store the full state of\nthe network on each node. The key idea is to use the blockchain network as both\na replicated state machine and as a distributed storage system. By distributing\nstates across nodes and enabling efficient data retrieval through a\nKademlia-inspired routing protocol, we reduce storage costs for validators.\nCryptographic proofs (such as Merkle proofs) are used to allow nodes to verify\ndata stored by other nodes without having to trust those nodes directly. While\nthe protocol trades off data storage for increased network bandwidth, we show\nhow gossiping and caching can minimize the increased bandwidth needs.\n  To validate our state sharing protocol, we conduct an extensive quantitative\nanalysis of Ethereum's data storage and data access patterns. Our findings\nindicate that while our protocol significantly lowers storage needs, it comes\nwith an increased bandwidth usage ranging from 1.5 MB to 5 MB per block,\ntranslating to an additional monthly bandwidth of 319 GB to 1,065 GB. Despite\nthis, the size remains small enough such that it can be passed to all nodes and\nvalidated within Ethereum's 12-second block validation window. Further analysis\nshows that Merkle proofs are the most significant contributor to the additional\nbandwidth. To address this concern, we also analyze the impact of switching to\nthe more space-efficient Verkle Proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The perpetual growth of data stored on popular blockchains such as Ethereum\nleads to significant scalability challenges and substantial storage costs for\noperators of full nodes. Increasing costs may lead to fewer independently\noperated nodes in the network, which poses risks to decentralization (and hence\nnetwork security), but also pushes decentralized app developers towards\ncentrally hosted API services.\n  This paper introduces a new protocol that allows validator nodes to\nparticipate in a blockchain network without the need to store the full state of\nthe network on each node. The key idea is to use the blockchain network as both\na replicated state machine and as a distributed storage system. By distributing\nstates across nodes and enabling efficient data retrieval through a\nKademlia-inspired routing protocol, we reduce storage costs for validators.\nCryptographic proofs (such as Merkle proofs) are used to allow nodes to verify\ndata stored by other nodes without having to trust those nodes directly. While\nthe protocol trades off data storage for increased network bandwidth, we show\nhow gossiping and caching can minimize the increased bandwidth needs.\n  To validate our state sharing protocol, we conduct an extensive quantitative\nanalysis of Ethereum's data storage and data access patterns. Our findings\nindicate that while our protocol significantly lowers storage needs, it comes\nwith an increased bandwidth usage ranging from 1.5 MB to 5 MB per block,\ntranslating to an additional monthly bandwidth of 319 GB to 1,065 GB. Despite\nthis, the size remains small enough such that it can be passed to all nodes and\nvalidated within Ethereum's 12-second block validation window. Further analysis\nshows that Merkle proofs are the most significant contributor to the additional\nbandwidth. To address this concern, we also analyze the impact of switching to\nthe more space-efficient Verkle Proofs."
                },
                "authors": [
                    {
                        "name": "Ruben Hias"
                    },
                    {
                        "name": "Weihong Wang"
                    },
                    {
                        "name": "Jan Vanhoof"
                    },
                    {
                        "name": "Tom Van Cutsem"
                    }
                ],
                "author_detail": {
                    "name": "Tom Van Cutsem"
                },
                "author": "Tom Van Cutsem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12018v2",
                "updated": "2024-10-08T04:25:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    4,
                    25,
                    41,
                    1,
                    282,
                    0
                ],
                "published": "2024-06-17T18:34:58Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    18,
                    34,
                    58,
                    0,
                    169,
                    0
                ],
                "title": "CItruS: Chunked Instruction-aware State Eviction for Long Sequence\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CItruS: Chunked Instruction-aware State Eviction for Long Sequence\n  Modeling"
                },
                "summary": "Long sequence modeling has gained broad interest as large language models\n(LLMs) continue to advance. Recent research has identified that a large portion\nof hidden states within the key-value caches of Transformer models can be\ndiscarded (also termed evicted) without affecting the perplexity performance in\ngenerating long sequences. However, we show that these methods, despite\npreserving perplexity performance, often drop information that is important for\nsolving downstream tasks, a problem which we call information neglect. To\naddress this issue, we introduce Chunked Instruction-aware State Eviction\n(CItruS), a novel modeling technique that integrates the attention preferences\nuseful for a downstream task into the eviction process of hidden states. In\naddition, we design a method for chunked sequence processing to further improve\nefficiency. Our training-free method exhibits superior performance on long\nsequence comprehension and retrieval tasks over several strong baselines under\nthe same memory budget, while preserving language modeling perplexity. The code\nand data have been released at https://github.com/ybai-nlp/CItruS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long sequence modeling has gained broad interest as large language models\n(LLMs) continue to advance. Recent research has identified that a large portion\nof hidden states within the key-value caches of Transformer models can be\ndiscarded (also termed evicted) without affecting the perplexity performance in\ngenerating long sequences. However, we show that these methods, despite\npreserving perplexity performance, often drop information that is important for\nsolving downstream tasks, a problem which we call information neglect. To\naddress this issue, we introduce Chunked Instruction-aware State Eviction\n(CItruS), a novel modeling technique that integrates the attention preferences\nuseful for a downstream task into the eviction process of hidden states. In\naddition, we design a method for chunked sequence processing to further improve\nefficiency. Our training-free method exhibits superior performance on long\nsequence comprehension and retrieval tasks over several strong baselines under\nthe same memory budget, while preserving language modeling perplexity. The code\nand data have been released at https://github.com/ybai-nlp/CItruS."
                },
                "authors": [
                    {
                        "name": "Yu Bai"
                    },
                    {
                        "name": "Xiyuan Zou"
                    },
                    {
                        "name": "Heyan Huang"
                    },
                    {
                        "name": "Sanxing Chen"
                    },
                    {
                        "name": "Marc-Antoine Rondeau"
                    },
                    {
                        "name": "Yang Gao"
                    },
                    {
                        "name": "Jackie Chi Kit Cheung"
                    }
                ],
                "author_detail": {
                    "name": "Jackie Chi Kit Cheung"
                },
                "author": "Jackie Chi Kit Cheung",
                "arxiv_comment": "EMNLP 2024 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05265v1",
                "updated": "2024-10-07T17:59:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:59:35Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "title": "PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers\n  in LLMs"
                },
                "summary": "Quantization is essential for deploying Large Language Models (LLMs) by\nenhancing memory efficiency and inference speed. Existing methods for\nactivation quantization mainly address channel-wise outliers, often neglecting\ntoken-wise outliers, leading to reliance on costly per-token dynamic\nquantization. To address this, we introduce PrefixQuant, a novel technique that\nisolates outlier tokens offline without re-training. Specifically, PrefixQuant\nidentifies high-frequency outlier tokens and prefixes them in the KV cache,\npreventing the generation of outlier tokens during inference and simplifying\nquantization. To our knowledge, PrefixQuant is the first to enable efficient\nper-tensor static quantization to outperform expensive per-token dynamic\nquantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and\n4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization\nachieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5\ncommon-sense reasoning tasks, outperforming previous per-token dynamic\nquantization methods like QuaRot with 0.98 perplexity improvement and +5.98\npoints accuracy. Additionally, the inference speed of W4A4 quantized models\nusing PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot\nmodels by 1.2x to 1.3x. Our code is available at\n\\url{https://github.com/ChenMnZ/PrefixQuant}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is essential for deploying Large Language Models (LLMs) by\nenhancing memory efficiency and inference speed. Existing methods for\nactivation quantization mainly address channel-wise outliers, often neglecting\ntoken-wise outliers, leading to reliance on costly per-token dynamic\nquantization. To address this, we introduce PrefixQuant, a novel technique that\nisolates outlier tokens offline without re-training. Specifically, PrefixQuant\nidentifies high-frequency outlier tokens and prefixes them in the KV cache,\npreventing the generation of outlier tokens during inference and simplifying\nquantization. To our knowledge, PrefixQuant is the first to enable efficient\nper-tensor static quantization to outperform expensive per-token dynamic\nquantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and\n4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization\nachieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5\ncommon-sense reasoning tasks, outperforming previous per-token dynamic\nquantization methods like QuaRot with 0.98 perplexity improvement and +5.98\npoints accuracy. Additionally, the inference speed of W4A4 quantized models\nusing PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot\nmodels by 1.2x to 1.3x. Our code is available at\n\\url{https://github.com/ChenMnZ/PrefixQuant}."
                },
                "authors": [
                    {
                        "name": "Mengzhao Chen"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Yi Bin"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "A PTQ method to significantly boost the performance of static\n  activation quantization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05516v3",
                "updated": "2024-10-07T17:21:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    21,
                    57,
                    0,
                    281,
                    0
                ],
                "published": "2023-12-09T09:55:07Z",
                "published_parsed": [
                    2023,
                    12,
                    9,
                    9,
                    55,
                    7,
                    5,
                    343,
                    0
                ],
                "title": "Stateful Large Language Model Serving with Pensieve",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stateful Large Language Model Serving with Pensieve"
                },
                "summary": "Large Language Models (LLMs) are wildly popular today and it is important to\nserve them efficiently. Existing LLM serving systems are stateless across\nrequests. Consequently, when LLMs are used in the common setting of multi-turn\nconversations, a growing log of the conversation history must be processed\nalongside any request by the serving system at each turn, resulting in repeated\nprocessing.\n  In this paper, we design $Pensieve$, a system optimized for multi-turn\nconversation LLM serving. $Pensieve$ maintains the conversation state across\nrequests by caching previously processed history to avoid duplicate processing.\n$Pensieve$'s multi-tier caching strategy can utilize both GPU and CPU memory to\nefficiently store and retrieve cached data. $Pensieve$ also generalizes the\nrecent PagedAttention kernel to support attention between multiple input tokens\nwith a GPU cache spread over non-contiguous memory. Our evaluation shows that\n$Pensieve$ can achieve $1.14$-$3.0\\times$ the throughput of vLLM and\nTensorRT-LLM and significantly reduce latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are wildly popular today and it is important to\nserve them efficiently. Existing LLM serving systems are stateless across\nrequests. Consequently, when LLMs are used in the common setting of multi-turn\nconversations, a growing log of the conversation history must be processed\nalongside any request by the serving system at each turn, resulting in repeated\nprocessing.\n  In this paper, we design $Pensieve$, a system optimized for multi-turn\nconversation LLM serving. $Pensieve$ maintains the conversation state across\nrequests by caching previously processed history to avoid duplicate processing.\n$Pensieve$'s multi-tier caching strategy can utilize both GPU and CPU memory to\nefficiently store and retrieve cached data. $Pensieve$ also generalizes the\nrecent PagedAttention kernel to support attention between multiple input tokens\nwith a GPU cache spread over non-contiguous memory. Our evaluation shows that\n$Pensieve$ can achieve $1.14$-$3.0\\times$ the throughput of vLLM and\nTensorRT-LLM and significantly reduce latency."
                },
                "authors": [
                    {
                        "name": "Lingfan Yu"
                    },
                    {
                        "name": "Jinkun Lin"
                    },
                    {
                        "name": "Jinyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinyang Li"
                },
                "author": "Jinyang Li",
                "arxiv_doi": "10.1145/3689031.3696086",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689031.3696086",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.05516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00161v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00161v2",
                "updated": "2024-10-07T15:07:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    7,
                    9,
                    0,
                    281,
                    0
                ],
                "published": "2024-09-30T19:09:13Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    19,
                    9,
                    13,
                    0,
                    274,
                    0
                ],
                "title": "KV-Compress: Paged KV-Cache Compression with Variable Compression Rates\n  per Attention Head",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Compress: Paged KV-Cache Compression with Variable Compression Rates\n  per Attention Head"
                },
                "summary": "Context lengths of Large Language Models (LLMs) have exploded in recent\nyears, with 128k-token context becoming a standard and million-token context\nbecoming a reality. Efficiently supporting long-context inference remains\nchallenging as the memory that must be allocated in key-value (KV) cache for a\ngeneration scales with its context length, limiting the number of long-context\nrequests that can be served concurrently under a given memory budget. KV cache\ncompression can mitigate this issue by removing under-utilized KVs from each\nattention head's cache and reducing its memory footprint. Higher theoretical\ncompression rates can be achieved when the number of removed KVs varies across\nattention heads, but application of such a strategy within existing inference\nframeworks adds fragmentation and cannot realize the theoretical compression\nrates in physical memory. We introduce KV-Compress, a novel compression method\nthat evicts contiguous KV blocks within a PagedAttention framework, reducing\nthe memory footprint of the KV cache proportionally to this theoretical\ncompression rate. Our method achieves state-of-the-art performance on LongBench\nfor both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the\ntotal number of compressed KVs by 4x compared with prior methods. Evaluations\non Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression\nrates up to 8x with negligible impact on performance, and up to 64x while\nretaining over 90% of full-cache performance for all but three of the suite's\nsubsets. We benchmark an integration of our method with vLLM that increases\ntotal throughput by up to 5.18x by enabling larger decoding batches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context lengths of Large Language Models (LLMs) have exploded in recent\nyears, with 128k-token context becoming a standard and million-token context\nbecoming a reality. Efficiently supporting long-context inference remains\nchallenging as the memory that must be allocated in key-value (KV) cache for a\ngeneration scales with its context length, limiting the number of long-context\nrequests that can be served concurrently under a given memory budget. KV cache\ncompression can mitigate this issue by removing under-utilized KVs from each\nattention head's cache and reducing its memory footprint. Higher theoretical\ncompression rates can be achieved when the number of removed KVs varies across\nattention heads, but application of such a strategy within existing inference\nframeworks adds fragmentation and cannot realize the theoretical compression\nrates in physical memory. We introduce KV-Compress, a novel compression method\nthat evicts contiguous KV blocks within a PagedAttention framework, reducing\nthe memory footprint of the KV cache proportionally to this theoretical\ncompression rate. Our method achieves state-of-the-art performance on LongBench\nfor both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the\ntotal number of compressed KVs by 4x compared with prior methods. Evaluations\non Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression\nrates up to 8x with negligible impact on performance, and up to 64x while\nretaining over 90% of full-cache performance for all but three of the suite's\nsubsets. We benchmark an integration of our method with vLLM that increases\ntotal throughput by up to 5.18x by enabling larger decoding batches."
                },
                "authors": [
                    {
                        "name": "Isaac Rehg"
                    }
                ],
                "author_detail": {
                    "name": "Isaac Rehg"
                },
                "author": "Isaac Rehg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00161v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00161v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05076v1",
                "updated": "2024-10-07T14:30:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    30,
                    27,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T14:30:27Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    30,
                    27,
                    0,
                    281,
                    0
                ],
                "title": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent\n  Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent\n  Sparse Attention"
                },
                "summary": "Large language models (LLMs) have driven significant advancements across\ndiverse NLP tasks, with long-context models gaining prominence for handling\nextended inputs. However, the expanding key-value (KV) cache size required by\nTransformer architectures intensifies the memory constraints, particularly\nduring the decoding phase, creating a significant bottleneck. Existing sparse\nattention mechanisms designed to address this bottleneck have two limitations:\n(1) they often fail to reliably identify the most relevant tokens for\nattention, and (2) they overlook the spatial coherence of token selection\nacross consecutive Transformer layers, which can lead to performance\ndegradation and substantial overhead in token selection. This paper introduces\nTidalDecode, a simple yet effective algorithm and system for fast and accurate\nLLM decoding through position persistent sparse attention. TidalDecode\nleverages the spatial coherence of tokens selected by existing sparse attention\nmethods and introduces a few token selection layers that perform full attention\nto identify the tokens with the highest attention scores, while all other\nlayers perform sparse attention with the pre-selected tokens. This design\nenables TidalDecode to substantially reduce the overhead of token selection for\nsparse attention without sacrificing the quality of the generated results.\nEvaluation on a diverse set of LLMs and tasks shows that TidalDecode closely\nmatches the generative performance of full attention methods while reducing the\nLLM decoding latency by up to 2.1x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have driven significant advancements across\ndiverse NLP tasks, with long-context models gaining prominence for handling\nextended inputs. However, the expanding key-value (KV) cache size required by\nTransformer architectures intensifies the memory constraints, particularly\nduring the decoding phase, creating a significant bottleneck. Existing sparse\nattention mechanisms designed to address this bottleneck have two limitations:\n(1) they often fail to reliably identify the most relevant tokens for\nattention, and (2) they overlook the spatial coherence of token selection\nacross consecutive Transformer layers, which can lead to performance\ndegradation and substantial overhead in token selection. This paper introduces\nTidalDecode, a simple yet effective algorithm and system for fast and accurate\nLLM decoding through position persistent sparse attention. TidalDecode\nleverages the spatial coherence of tokens selected by existing sparse attention\nmethods and introduces a few token selection layers that perform full attention\nto identify the tokens with the highest attention scores, while all other\nlayers perform sparse attention with the pre-selected tokens. This design\nenables TidalDecode to substantially reduce the overhead of token selection for\nsparse attention without sacrificing the quality of the generated results.\nEvaluation on a diverse set of LLMs and tasks shows that TidalDecode closely\nmatches the generative performance of full attention methods while reducing the\nLLM decoding latency by up to 2.1x."
                },
                "authors": [
                    {
                        "name": "Lijie Yang"
                    },
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Zhuofu Chen"
                    },
                    {
                        "name": "Zikun Li"
                    },
                    {
                        "name": "Zhihao Jia"
                    }
                ],
                "author_detail": {
                    "name": "Zhihao Jia"
                },
                "author": "Zhihao Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05033v1",
                "updated": "2024-10-07T13:33:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    33,
                    23,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T13:33:23Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    33,
                    23,
                    0,
                    281,
                    0
                ],
                "title": "Extended Functional Representation Lemma: A Tool For Privacy, Semantic\n  Representation, Caching, and Compression Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extended Functional Representation Lemma: A Tool For Privacy, Semantic\n  Representation, Caching, and Compression Design"
                },
                "summary": "This paper provides an overview of a problem in information-theoretic privacy\nmechanism design, addressing two scenarios in which private data is either\nobservable or hidden. In each scenario, different privacy measures are used,\nincluding bounded mutual information and two types of per-letter privacy\nconstraints. Considering the first scenario, an agent observes useful data that\nis correlated with private data, and wants to disclose the useful information\nto a user. Due to the privacy concerns, direct disclosure is prohibited. Hence,\na privacy mechanism is designed to generate disclosed data which maximizes the\nrevealed information about the useful data while satisfying a privacy\nconstraint. In the second scenario, the agent has additionally access to the\nprivate data. We discuss how the Functional Representation Lemma, the Strong\nFunctional Representation Lemma, and their extended versions are useful for\ndesigning low-complexity privacy mechanisms that achieve optimal\nprivacy-utility trade-offs under certain constraints. Furthermore, another\nprivacy design problem is presented where part of the private attribute is more\nprivate than the remaining part. Finally, we provide applications including\nsemantic communications, caching and delivery, and compression designs, where\nthe approach can be applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides an overview of a problem in information-theoretic privacy\nmechanism design, addressing two scenarios in which private data is either\nobservable or hidden. In each scenario, different privacy measures are used,\nincluding bounded mutual information and two types of per-letter privacy\nconstraints. Considering the first scenario, an agent observes useful data that\nis correlated with private data, and wants to disclose the useful information\nto a user. Due to the privacy concerns, direct disclosure is prohibited. Hence,\na privacy mechanism is designed to generate disclosed data which maximizes the\nrevealed information about the useful data while satisfying a privacy\nconstraint. In the second scenario, the agent has additionally access to the\nprivate data. We discuss how the Functional Representation Lemma, the Strong\nFunctional Representation Lemma, and their extended versions are useful for\ndesigning low-complexity privacy mechanisms that achieve optimal\nprivacy-utility trade-offs under certain constraints. Furthermore, another\nprivacy design problem is presented where part of the private attribute is more\nprivate than the remaining part. Finally, we provide applications including\nsemantic communications, caching and delivery, and compression designs, where\nthe approach can be applied."
                },
                "authors": [
                    {
                        "name": "Amirreza Zamani"
                    },
                    {
                        "name": "Mikael Skoglund"
                    }
                ],
                "author_detail": {
                    "name": "Mikael Skoglund"
                },
                "author": "Mikael Skoglund",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2212.12475",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.10254v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10254v2",
                "updated": "2024-10-25T17:59:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    59,
                    4,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-14T08:10:34Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    8,
                    10,
                    34,
                    0,
                    288,
                    0
                ],
                "title": "LoLCATs: On Low-Rank Linearizing of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoLCATs: On Low-Rank Linearizing of Large Language Models"
                },
                "summary": "Recent works show we can linearize large language models (LLMs) -- swapping\nthe quadratic attentions of popular Transformer-based LLMs with subquadratic\nanalogs, such as linear attention -- avoiding the expensive pretraining costs.\nHowever, linearizing LLMs often significantly degrades model quality, still\nrequires training over billions of tokens, and remains limited to smaller 1.3B\nto 7B LLMs. We thus propose Low-rank Linear Conversion via Attention Transfer\n(LoLCATs), a simple two-step method that improves LLM linearizing quality with\norders of magnitudes less memory and compute. We base these steps on two\nfindings. First, we can replace an LLM's softmax attentions with\nclosely-approximating linear attentions, simply by training the linear\nattentions to match their softmax counterparts with an output MSE loss\n(\"attention transfer\"). Then, this enables adjusting for approximation errors\nand recovering LLM quality simply with low-rank adaptation (LoRA). LoLCATs\nsignificantly improves linearizing quality, training efficiency, and\nscalability. We significantly reduce the linearizing quality gap and produce\nstate-of-the-art subquadratic LLMs from Llama 3 8B and Mistral 7B v0.1, leading\nto 20+ points of improvement on 5-shot MMLU. Furthermore, LoLCATs does so with\nonly 0.2% of past methods' model parameters and 0.4% of their training tokens.\nFinally, we apply LoLCATs to create the first linearized 70B and 405B LLMs (50x\nlarger than prior work). When compared with prior approaches under the same\ncompute budgets, LoLCATs significantly improves linearizing quality, closing\nthe gap between linearized and original Llama 3.1 70B and 405B LLMs by 77.8%\nand 78.1% on 5-shot MMLU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works show we can linearize large language models (LLMs) -- swapping\nthe quadratic attentions of popular Transformer-based LLMs with subquadratic\nanalogs, such as linear attention -- avoiding the expensive pretraining costs.\nHowever, linearizing LLMs often significantly degrades model quality, still\nrequires training over billions of tokens, and remains limited to smaller 1.3B\nto 7B LLMs. We thus propose Low-rank Linear Conversion via Attention Transfer\n(LoLCATs), a simple two-step method that improves LLM linearizing quality with\norders of magnitudes less memory and compute. We base these steps on two\nfindings. First, we can replace an LLM's softmax attentions with\nclosely-approximating linear attentions, simply by training the linear\nattentions to match their softmax counterparts with an output MSE loss\n(\"attention transfer\"). Then, this enables adjusting for approximation errors\nand recovering LLM quality simply with low-rank adaptation (LoRA). LoLCATs\nsignificantly improves linearizing quality, training efficiency, and\nscalability. We significantly reduce the linearizing quality gap and produce\nstate-of-the-art subquadratic LLMs from Llama 3 8B and Mistral 7B v0.1, leading\nto 20+ points of improvement on 5-shot MMLU. Furthermore, LoLCATs does so with\nonly 0.2% of past methods' model parameters and 0.4% of their training tokens.\nFinally, we apply LoLCATs to create the first linearized 70B and 405B LLMs (50x\nlarger than prior work). When compared with prior approaches under the same\ncompute budgets, LoLCATs significantly improves linearizing quality, closing\nthe gap between linearized and original Llama 3.1 70B and 405B LLMs by 77.8%\nand 78.1% on 5-shot MMLU."
                },
                "authors": [
                    {
                        "name": "Michael Zhang"
                    },
                    {
                        "name": "Simran Arora"
                    },
                    {
                        "name": "Rahul Chalamala"
                    },
                    {
                        "name": "Alan Wu"
                    },
                    {
                        "name": "Benjamin Spector"
                    },
                    {
                        "name": "Aaryan Singhal"
                    },
                    {
                        "name": "Krithik Ramesh"
                    },
                    {
                        "name": "Christopher Ré"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Ré"
                },
                "author": "Christopher Ré",
                "arxiv_comment": "47 pages, 20 figures, 18 tables, preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10254v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10254v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19730v1",
                "updated": "2024-10-25T17:56:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    56,
                    24,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T17:56:24Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    56,
                    24,
                    4,
                    299,
                    0
                ],
                "title": "Counting Ability of Large Language Models and Impact of Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counting Ability of Large Language Models and Impact of Tokenization"
                },
                "summary": "Transformers, the backbone of modern large language models (LLMs), face\ninherent architectural limitations that impede their reasoning capabilities.\nUnlike recurrent networks, Transformers lack recurrent connections, confining\nthem to constant-depth computation. This restriction places them in the\ncomplexity class TC$^0$, making them theoretically incapable of solving tasks\nthat demand increasingly deep reasoning as input length grows. Counting, a\nfundamental component of many reasoning tasks, also requires reasoning depth to\ngrow linearly to be performed inductively. While previous studies have\nestablished the upper limits of counting ability in Transformer-based expert\nmodels (i.e., models specifically trained for counting tasks), these findings\ndo not directly extend to general-purpose LLMs due to differences in reasoning\nmechanisms. Recent work has highlighted how Chain of Thought (CoT) reasoning\ncan help alleviate some of the architectural limitations of Transformers in\ncounting tasks. However, little attention has been paid to the role of\ntokenization in these models. Unlike expert models that often use\ncharacter-level tokenization, LLMs typically rely on byte-level (BPE)\ntokenizers, which fundamentally alters the way reasoning is processed. Our work\ninvestigates the impact of tokenization on the counting abilities of LLMs,\nuncovering substantial performance variations based on input tokenization\ndifferences. We provide both theoretical and experimental analyses, offering\ninsights into how tokenization choices can undermine models' theoretical\ncomputability, thereby inspiring the design of new tokenization methods to\nenhance reasoning in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, the backbone of modern large language models (LLMs), face\ninherent architectural limitations that impede their reasoning capabilities.\nUnlike recurrent networks, Transformers lack recurrent connections, confining\nthem to constant-depth computation. This restriction places them in the\ncomplexity class TC$^0$, making them theoretically incapable of solving tasks\nthat demand increasingly deep reasoning as input length grows. Counting, a\nfundamental component of many reasoning tasks, also requires reasoning depth to\ngrow linearly to be performed inductively. While previous studies have\nestablished the upper limits of counting ability in Transformer-based expert\nmodels (i.e., models specifically trained for counting tasks), these findings\ndo not directly extend to general-purpose LLMs due to differences in reasoning\nmechanisms. Recent work has highlighted how Chain of Thought (CoT) reasoning\ncan help alleviate some of the architectural limitations of Transformers in\ncounting tasks. However, little attention has been paid to the role of\ntokenization in these models. Unlike expert models that often use\ncharacter-level tokenization, LLMs typically rely on byte-level (BPE)\ntokenizers, which fundamentally alters the way reasoning is processed. Our work\ninvestigates the impact of tokenization on the counting abilities of LLMs,\nuncovering substantial performance variations based on input tokenization\ndifferences. We provide both theoretical and experimental analyses, offering\ninsights into how tokenization choices can undermine models' theoretical\ncomputability, thereby inspiring the design of new tokenization methods to\nenhance reasoning in LLMs."
                },
                "authors": [
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Juntai Cao"
                    },
                    {
                        "name": "Chenyu You"
                    }
                ],
                "author_detail": {
                    "name": "Chenyu You"
                },
                "author": "Chenyu You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19727v1",
                "updated": "2024-10-25T17:53:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    53,
                    47,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T17:53:47Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    53,
                    47,
                    4,
                    299,
                    0
                ],
                "title": "FISHNET: Financial Intelligence from Sub-querying, Harmonizing,\n  Neural-Conditioning, Expert Swarms, and Task Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FISHNET: Financial Intelligence from Sub-querying, Harmonizing,\n  Neural-Conditioning, Expert Swarms, and Task Planning"
                },
                "summary": "Financial intelligence generation from vast data sources has typically relied\non traditional methods of knowledge-graph construction or database engineering.\nRecently, fine-tuned financial domain-specific Large Language Models (LLMs),\nhave emerged. While these advancements are promising, limitations such as high\ninference costs, hallucinations, and the complexity of concurrently analyzing\nhigh-dimensional financial data, emerge. This motivates our invention FISHNET\n(Financial Intelligence from Sub-querying, Harmonizing, Neural-Conditioning,\nExpert swarming, and Task planning), an agentic architecture that accomplishes\nhighly complex analytical tasks for more than 98,000 regulatory filings that\nvary immensely in terms of semantics, data hierarchy, or format. FISHNET shows\nremarkable performance for financial insight generation (61.8% success rate\nover 5.0% Routing, 45.6% RAG R-Precision). We conduct rigorous ablations to\nempirically prove the success of FISHNET, each agent's importance, and the\noptimized performance of assembling all agents. Our modular architecture can be\nleveraged for a myriad of use-cases, enabling scalability, flexibility, and\ndata integrity that are critical for financial tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Financial intelligence generation from vast data sources has typically relied\non traditional methods of knowledge-graph construction or database engineering.\nRecently, fine-tuned financial domain-specific Large Language Models (LLMs),\nhave emerged. While these advancements are promising, limitations such as high\ninference costs, hallucinations, and the complexity of concurrently analyzing\nhigh-dimensional financial data, emerge. This motivates our invention FISHNET\n(Financial Intelligence from Sub-querying, Harmonizing, Neural-Conditioning,\nExpert swarming, and Task planning), an agentic architecture that accomplishes\nhighly complex analytical tasks for more than 98,000 regulatory filings that\nvary immensely in terms of semantics, data hierarchy, or format. FISHNET shows\nremarkable performance for financial insight generation (61.8% success rate\nover 5.0% Routing, 45.6% RAG R-Precision). We conduct rigorous ablations to\nempirically prove the success of FISHNET, each agent's importance, and the\noptimized performance of assembling all agents. Our modular architecture can be\nleveraged for a myriad of use-cases, enabling scalability, flexibility, and\ndata integrity that are critical for financial tasks."
                },
                "authors": [
                    {
                        "name": "Nicole Cho"
                    },
                    {
                        "name": "Nishan Srishankar"
                    },
                    {
                        "name": "Lucas Cecchi"
                    },
                    {
                        "name": "William Watson"
                    }
                ],
                "author_detail": {
                    "name": "William Watson"
                },
                "author": "William Watson",
                "arxiv_doi": "10.1145/3677052.3698597",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3677052.3698597",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.19727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at the 5th ACM International Conference on AI in Finance\n  (ICAIF '24)",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19723v1",
                "updated": "2024-10-25T17:52:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    52,
                    16,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T17:52:16Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    52,
                    16,
                    4,
                    299,
                    0
                ],
                "title": "Sparse Decomposition of Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Decomposition of Graph Neural Networks"
                },
                "summary": "Graph Neural Networks (GNN) exhibit superior performance in graph\nrepresentation learning, but their inference cost can be high, due to an\naggregation operation that can require a memory fetch for a very large number\nof nodes. This inference cost is the major obstacle to deploying GNN models\nwith \\emph{online prediction} to reflect the potentially dynamic node features.\nTo address this, we propose an approach to reduce the number of nodes that are\nincluded during aggregation. We achieve this through a sparse decomposition,\nlearning to approximate node representations using a weighted sum of linearly\ntransformed features of a carefully selected subset of nodes within the\nextended neighbourhood. The approach achieves linear complexity with respect to\nthe average node degree and the number of layers in the graph neural network.\nWe introduce an algorithm to compute the optimal parameters for the sparse\ndecomposition, ensuring an accurate approximation of the original GNN model,\nand present effective strategies to reduce the training time and improve the\nlearning process. We demonstrate via extensive experiments that our method\noutperforms other baselines designed for inference speedup, achieving\nsignificant accuracy gains with comparable inference times for both node\nclassification and spatio-temporal forecasting tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNN) exhibit superior performance in graph\nrepresentation learning, but their inference cost can be high, due to an\naggregation operation that can require a memory fetch for a very large number\nof nodes. This inference cost is the major obstacle to deploying GNN models\nwith \\emph{online prediction} to reflect the potentially dynamic node features.\nTo address this, we propose an approach to reduce the number of nodes that are\nincluded during aggregation. We achieve this through a sparse decomposition,\nlearning to approximate node representations using a weighted sum of linearly\ntransformed features of a carefully selected subset of nodes within the\nextended neighbourhood. The approach achieves linear complexity with respect to\nthe average node degree and the number of layers in the graph neural network.\nWe introduce an algorithm to compute the optimal parameters for the sparse\ndecomposition, ensuring an accurate approximation of the original GNN model,\nand present effective strategies to reduce the training time and improve the\nlearning process. We demonstrate via extensive experiments that our method\noutperforms other baselines designed for inference speedup, achieving\nsignificant accuracy gains with comparable inference times for both node\nclassification and spatio-temporal forecasting tasks."
                },
                "authors": [
                    {
                        "name": "Yaochen Hu"
                    },
                    {
                        "name": "Mai Zeng"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Pavel Rumiantsev"
                    },
                    {
                        "name": "Liheng Ma"
                    },
                    {
                        "name": "Yingxue Zhang"
                    },
                    {
                        "name": "Mark Coates"
                    }
                ],
                "author_detail": {
                    "name": "Mark Coates"
                },
                "author": "Mark Coates",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19720v1",
                "updated": "2024-10-25T17:47:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    47,
                    35,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T17:47:35Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    47,
                    35,
                    4,
                    299,
                    0
                ],
                "title": "2D-DPO: Scaling Direct Preference Optimization with 2-Dimensional\n  Supervision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2D-DPO: Scaling Direct Preference Optimization with 2-Dimensional\n  Supervision"
                },
                "summary": "Recent advancements in Direct Preference Optimization (DPO) have\nsignificantly enhanced the alignment of Large Language Models (LLMs) with human\npreferences, owing to its simplicity and effectiveness. However, existing\nmethods typically optimize a scalar score or ranking reward, thereby\noverlooking the multi-dimensional nature of human preferences. In this work, we\npropose to extend the preference of DPO to two dimensions: segments and\naspects. We first introduce a 2D supervision dataset called HelpSteer-2D. For\nthe segment dimension, we divide the response into sentences and assign scores\nto each segment. For the aspect dimension, we meticulously design several\ncriteria covering the response quality rubrics. With the 2-dimensional signals\nas feedback, we develop a 2D-DPO framework, decomposing the overall objective\ninto multi-segment and multi-aspect objectives. Extensive experiments on\npopular benchmarks demonstrate that 2D-DPO performs better than methods that\noptimize for scalar or 1-dimensional preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Direct Preference Optimization (DPO) have\nsignificantly enhanced the alignment of Large Language Models (LLMs) with human\npreferences, owing to its simplicity and effectiveness. However, existing\nmethods typically optimize a scalar score or ranking reward, thereby\noverlooking the multi-dimensional nature of human preferences. In this work, we\npropose to extend the preference of DPO to two dimensions: segments and\naspects. We first introduce a 2D supervision dataset called HelpSteer-2D. For\nthe segment dimension, we divide the response into sentences and assign scores\nto each segment. For the aspect dimension, we meticulously design several\ncriteria covering the response quality rubrics. With the 2-dimensional signals\nas feedback, we develop a 2D-DPO framework, decomposing the overall objective\ninto multi-segment and multi-aspect objectives. Extensive experiments on\npopular benchmarks demonstrate that 2D-DPO performs better than methods that\noptimize for scalar or 1-dimensional preferences."
                },
                "authors": [
                    {
                        "name": "Shilong Li"
                    },
                    {
                        "name": "Yancheng He"
                    },
                    {
                        "name": "Hui Huang"
                    },
                    {
                        "name": "Xingyuan Bu"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Hangyu Guo"
                    },
                    {
                        "name": "Weixun Wang"
                    },
                    {
                        "name": "Jihao Gu"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "The first four authors contributed equally, 25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09187v2",
                "updated": "2024-10-25T17:37:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    37,
                    59,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-11T18:41:15Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    18,
                    41,
                    15,
                    4,
                    285,
                    0
                ],
                "title": "Automated Rewards via LLM-Generated Progress Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Rewards via LLM-Generated Progress Functions"
                },
                "summary": "Large Language Models (LLMs) have the potential to automate reward\nengineering by leveraging their broad domain knowledge across various tasks.\nHowever, they often need many iterations of trial-and-error to generate\neffective reward functions. This process is costly because evaluating every\nsampled reward function requires completing the full policy optimization\nprocess for each function. In this paper, we introduce an LLM-driven reward\ngeneration framework that is able to produce state-of-the-art policies on the\nchallenging Bi-DexHands benchmark with 20x fewer reward function samples than\nthe prior state-of-the-art work. Our key insight is that we reduce the problem\nof generating task-specific rewards to the problem of coarsely estimating task\nprogress. Our two-step solution leverages the task domain knowledge and the\ncode synthesis abilities of LLMs to author progress functions that estimate\ntask progress from a given state. Then, we use this notion of progress to\ndiscretize states, and generate count-based intrinsic rewards using the\nlow-dimensional state space. We show that the combination of LLM-generated\nprogress functions and count-based intrinsic rewards is essential for our\nperformance gains, while alternatives such as generic hash-based counts or\nusing progress directly as a reward function fall short.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have the potential to automate reward\nengineering by leveraging their broad domain knowledge across various tasks.\nHowever, they often need many iterations of trial-and-error to generate\neffective reward functions. This process is costly because evaluating every\nsampled reward function requires completing the full policy optimization\nprocess for each function. In this paper, we introduce an LLM-driven reward\ngeneration framework that is able to produce state-of-the-art policies on the\nchallenging Bi-DexHands benchmark with 20x fewer reward function samples than\nthe prior state-of-the-art work. Our key insight is that we reduce the problem\nof generating task-specific rewards to the problem of coarsely estimating task\nprogress. Our two-step solution leverages the task domain knowledge and the\ncode synthesis abilities of LLMs to author progress functions that estimate\ntask progress from a given state. Then, we use this notion of progress to\ndiscretize states, and generate count-based intrinsic rewards using the\nlow-dimensional state space. We show that the combination of LLM-generated\nprogress functions and count-based intrinsic rewards is essential for our\nperformance gains, while alternatives such as generic hash-based counts or\nusing progress directly as a reward function fall short."
                },
                "authors": [
                    {
                        "name": "Vishnu Sarukkai"
                    },
                    {
                        "name": "Brennan Shacklett"
                    },
                    {
                        "name": "Zander Majercik"
                    },
                    {
                        "name": "Kush Bhatia"
                    },
                    {
                        "name": "Christopher Ré"
                    },
                    {
                        "name": "Kayvon Fatahalian"
                    }
                ],
                "author_detail": {
                    "name": "Kayvon Fatahalian"
                },
                "author": "Kayvon Fatahalian",
                "arxiv_comment": "26 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18074v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18074v2",
                "updated": "2024-10-25T17:37:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    37,
                    29,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-23T17:56:33Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    56,
                    33,
                    2,
                    297,
                    0
                ],
                "title": "UnCLe: Unsupervised Continual Learning of Depth Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UnCLe: Unsupervised Continual Learning of Depth Completion"
                },
                "summary": "We propose UnCLe, a standardized benchmark for Unsupervised Continual\nLearning of a multimodal depth estimation task: Depth completion aims to infer\na dense depth map from a pair of synchronized RGB image and sparse depth map.\nWe benchmark depth completion models under the practical scenario of\nunsupervised learning over continuous streams of data. Existing methods are\ntypically trained on a static, or stationary, dataset. However, when adapting\nto novel non-stationary distributions, they \"catastrophically forget\"\npreviously learned information. UnCLe simulates these non-stationary\ndistributions by adapting depth completion models to sequences of datasets\ncontaining diverse scenes captured from distinct domains using different visual\nand range sensors. We adopt representative methods from continual learning\nparadigms and translate them to enable unsupervised continual learning of depth\ncompletion. We benchmark these models for indoor and outdoor and investigate\nthe degree of catastrophic forgetting through standard quantitative metrics.\nFurthermore, we introduce model inversion quality as an additional measure of\nforgetting. We find that unsupervised continual learning of depth completion is\nan open problem, and we invite researchers to leverage UnCLe as a development\nplatform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose UnCLe, a standardized benchmark for Unsupervised Continual\nLearning of a multimodal depth estimation task: Depth completion aims to infer\na dense depth map from a pair of synchronized RGB image and sparse depth map.\nWe benchmark depth completion models under the practical scenario of\nunsupervised learning over continuous streams of data. Existing methods are\ntypically trained on a static, or stationary, dataset. However, when adapting\nto novel non-stationary distributions, they \"catastrophically forget\"\npreviously learned information. UnCLe simulates these non-stationary\ndistributions by adapting depth completion models to sequences of datasets\ncontaining diverse scenes captured from distinct domains using different visual\nand range sensors. We adopt representative methods from continual learning\nparadigms and translate them to enable unsupervised continual learning of depth\ncompletion. We benchmark these models for indoor and outdoor and investigate\nthe degree of catastrophic forgetting through standard quantitative metrics.\nFurthermore, we introduce model inversion quality as an additional measure of\nforgetting. We find that unsupervised continual learning of depth completion is\nan open problem, and we invite researchers to leverage UnCLe as a development\nplatform."
                },
                "authors": [
                    {
                        "name": "Suchisrit Gangopadhyay"
                    },
                    {
                        "name": "Xien Chen"
                    },
                    {
                        "name": "Michael Chu"
                    },
                    {
                        "name": "Patrick Rim"
                    },
                    {
                        "name": "Hyoungseob Park"
                    },
                    {
                        "name": "Alex Wong"
                    }
                ],
                "author_detail": {
                    "name": "Alex Wong"
                },
                "author": "Alex Wong",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18074v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18074v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15115v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15115v2",
                "updated": "2024-10-25T17:34:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    34,
                    22,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-19T13:53:50Z",
                "published_parsed": [
                    2024,
                    10,
                    19,
                    13,
                    53,
                    50,
                    5,
                    293,
                    0
                ],
                "title": "On Designing Effective RL Reward at Training Time for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Designing Effective RL Reward at Training Time for LLM Reasoning"
                },
                "summary": "Reward models have been increasingly critical for improving the reasoning\ncapability of LLMs. Existing research has shown that a well-trained reward\nmodel can substantially improve model performances at inference time via\nsearch. However, the potential of reward models during RL training time still\nremains largely under-explored. It is currently unclear whether these reward\nmodels can provide additional training signals to enhance the reasoning\ncapabilities of LLMs in RL training that uses sparse success rewards, which\nverify the correctness of solutions. In this work, we evaluate popular reward\nmodels for RL training, including the Outcome-supervised Reward Model (ORM) and\nthe Process-supervised Reward Model (PRM), and train a collection of LLMs for\nmath problems using RL by combining these learned rewards with success rewards.\nSurprisingly, even though these learned reward models have strong\ninference-time performances, they may NOT help or even hurt RL training,\nproducing worse performances than LLMs trained with the success reward only.\nOur analysis reveals that an LLM can receive high rewards from some of these\nreward models by repeating correct but unnecessary reasoning steps, leading to\na severe reward hacking issue. Therefore, we introduce two novel reward\nrefinement techniques, including Clipping and Delta. The key idea is to ensure\nthe accumulative reward of any reasoning trajectory is upper-bounded to keep a\nlearned reward model effective without being exploited. We evaluate our\ntechniques with multiple reward models over a set of 1.5B and 7B LLMs on MATH\nand GSM8K benchmarks and demonstrate that with a carefully designed reward\nfunction, RL training without any additional supervised tuning can improve all\nthe evaluated LLMs, including the state-of-the-art 7B LLM\nQwen2.5-Math-7B-Instruct on MATH and GSM8K benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models have been increasingly critical for improving the reasoning\ncapability of LLMs. Existing research has shown that a well-trained reward\nmodel can substantially improve model performances at inference time via\nsearch. However, the potential of reward models during RL training time still\nremains largely under-explored. It is currently unclear whether these reward\nmodels can provide additional training signals to enhance the reasoning\ncapabilities of LLMs in RL training that uses sparse success rewards, which\nverify the correctness of solutions. In this work, we evaluate popular reward\nmodels for RL training, including the Outcome-supervised Reward Model (ORM) and\nthe Process-supervised Reward Model (PRM), and train a collection of LLMs for\nmath problems using RL by combining these learned rewards with success rewards.\nSurprisingly, even though these learned reward models have strong\ninference-time performances, they may NOT help or even hurt RL training,\nproducing worse performances than LLMs trained with the success reward only.\nOur analysis reveals that an LLM can receive high rewards from some of these\nreward models by repeating correct but unnecessary reasoning steps, leading to\na severe reward hacking issue. Therefore, we introduce two novel reward\nrefinement techniques, including Clipping and Delta. The key idea is to ensure\nthe accumulative reward of any reasoning trajectory is upper-bounded to keep a\nlearned reward model effective without being exploited. We evaluate our\ntechniques with multiple reward models over a set of 1.5B and 7B LLMs on MATH\nand GSM8K benchmarks and demonstrate that with a carefully designed reward\nfunction, RL training without any additional supervised tuning can improve all\nthe evaluated LLMs, including the state-of-the-art 7B LLM\nQwen2.5-Math-7B-Instruct on MATH and GSM8K benchmarks."
                },
                "authors": [
                    {
                        "name": "Jiaxuan Gao"
                    },
                    {
                        "name": "Shusheng Xu"
                    },
                    {
                        "name": "Wenjie Ye"
                    },
                    {
                        "name": "Weilin Liu"
                    },
                    {
                        "name": "Chuyi He"
                    },
                    {
                        "name": "Wei Fu"
                    },
                    {
                        "name": "Zhiyu Mei"
                    },
                    {
                        "name": "Guangju Wang"
                    },
                    {
                        "name": "Yi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Wu"
                },
                "author": "Yi Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15115v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15115v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14262v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14262v3",
                "updated": "2024-10-25T17:24:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    24,
                    16,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-18T08:18:18Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    18,
                    18,
                    4,
                    292,
                    0
                ],
                "title": "Good Parenting is all you need -- Multi-agentic LLM Hallucination\n  Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Good Parenting is all you need -- Multi-agentic LLM Hallucination\n  Mitigation"
                },
                "summary": "This study explores the ability of Large Language Model (LLM) agents to\ndetect and correct hallucinations in AI-generated content. A primary agent was\ntasked with creating a blog about a fictional Danish artist named Flipfloppidy,\nwhich was then reviewed by another agent for factual inaccuracies. Most LLMs\nhallucinated the existence of this artist. Across 4,900 test runs involving\nvarious combinations of primary and reviewing agents, advanced AI models such\nas Llama3-70b and GPT-4 variants demonstrated near-perfect accuracy in\nidentifying hallucinations and successfully revised outputs in 85% to 100% of\ncases following feedback. These findings underscore the potential of advanced\nAI models to significantly enhance the accuracy and reliability of generated\ncontent, providing a promising approach to improving AI workflow orchestration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the ability of Large Language Model (LLM) agents to\ndetect and correct hallucinations in AI-generated content. A primary agent was\ntasked with creating a blog about a fictional Danish artist named Flipfloppidy,\nwhich was then reviewed by another agent for factual inaccuracies. Most LLMs\nhallucinated the existence of this artist. Across 4,900 test runs involving\nvarious combinations of primary and reviewing agents, advanced AI models such\nas Llama3-70b and GPT-4 variants demonstrated near-perfect accuracy in\nidentifying hallucinations and successfully revised outputs in 85% to 100% of\ncases following feedback. These findings underscore the potential of advanced\nAI models to significantly enhance the accuracy and reliability of generated\ncontent, providing a promising approach to improving AI workflow orchestration."
                },
                "authors": [
                    {
                        "name": "Ted Kwartler"
                    },
                    {
                        "name": "Matthew Berman"
                    },
                    {
                        "name": "Alan Aqrawi"
                    }
                ],
                "author_detail": {
                    "name": "Alan Aqrawi"
                },
                "author": "Alan Aqrawi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14262v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14262v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19702v1",
                "updated": "2024-10-25T17:19:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    19,
                    55,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T17:19:55Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    19,
                    55,
                    4,
                    299,
                    0
                ],
                "title": "TimeSuite: Improving MLLMs for Long Video Understanding via Grounded\n  Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TimeSuite: Improving MLLMs for Long Video Understanding via Grounded\n  Tuning"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\nperformance in short video understanding. However, understanding long-form\nvideos still remains challenging for MLLMs. This paper proposes TimeSuite, a\ncollection of new designs to adapt the existing short-form video MLLMs for long\nvideo understanding, including a simple yet efficient framework to process long\nvideo sequence, a high-quality video dataset for grounded tuning of MLLMs, and\na carefully-designed instruction tuning task to explicitly incorporate the\ngrounding supervision in the traditional QA format. Specifically, based on\nVideoChat, we propose our long-video MLLM, coined as VideoChat-T, by\nimplementing a token shuffling to compress long video tokens and introducing\nTemporal Adaptive Position Encoding (TAPE) to enhance the temporal awareness of\nvisual representation. Meanwhile, we introduce the TimePro, a comprehensive\ngrounding-centric instruction tuning dataset composed of 9 tasks and 349k\nhigh-quality grounded annotations. Notably, we design a new instruction tuning\ntask type, called Temporal Grounded Caption, to peform detailed video\ndescriptions with the corresponding time stamps prediction. This explicit\ntemporal location prediction will guide MLLM to correctly attend on the visual\ncontent when generating description, and thus reduce the hallucination risk\ncaused by the LLMs. Experimental results demonstrate that our TimeSuite\nprovides a successful solution to enhance the long video understanding\ncapability of short-form MLLM, achieving improvement of 5.6% and 6.8% on the\nbenchmarks of Egoschema and VideoMME, respectively. In addition, VideoChat-T\nexhibits robust zero-shot temporal grounding capabilities, significantly\noutperforming the existing state-of-the-art MLLMs. After fine-tuning, it\nperforms on par with the traditional supervised expert models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\nperformance in short video understanding. However, understanding long-form\nvideos still remains challenging for MLLMs. This paper proposes TimeSuite, a\ncollection of new designs to adapt the existing short-form video MLLMs for long\nvideo understanding, including a simple yet efficient framework to process long\nvideo sequence, a high-quality video dataset for grounded tuning of MLLMs, and\na carefully-designed instruction tuning task to explicitly incorporate the\ngrounding supervision in the traditional QA format. Specifically, based on\nVideoChat, we propose our long-video MLLM, coined as VideoChat-T, by\nimplementing a token shuffling to compress long video tokens and introducing\nTemporal Adaptive Position Encoding (TAPE) to enhance the temporal awareness of\nvisual representation. Meanwhile, we introduce the TimePro, a comprehensive\ngrounding-centric instruction tuning dataset composed of 9 tasks and 349k\nhigh-quality grounded annotations. Notably, we design a new instruction tuning\ntask type, called Temporal Grounded Caption, to peform detailed video\ndescriptions with the corresponding time stamps prediction. This explicit\ntemporal location prediction will guide MLLM to correctly attend on the visual\ncontent when generating description, and thus reduce the hallucination risk\ncaused by the LLMs. Experimental results demonstrate that our TimeSuite\nprovides a successful solution to enhance the long video understanding\ncapability of short-form MLLM, achieving improvement of 5.6% and 6.8% on the\nbenchmarks of Egoschema and VideoMME, respectively. In addition, VideoChat-T\nexhibits robust zero-shot temporal grounding capabilities, significantly\noutperforming the existing state-of-the-art MLLMs. After fine-tuning, it\nperforms on par with the traditional supervised expert models."
                },
                "authors": [
                    {
                        "name": "Xiangyu Zeng"
                    },
                    {
                        "name": "Kunchang Li"
                    },
                    {
                        "name": "Chenting Wang"
                    },
                    {
                        "name": "Xinhao Li"
                    },
                    {
                        "name": "Tianxiang Jiang"
                    },
                    {
                        "name": "Ziang Yan"
                    },
                    {
                        "name": "Songze Li"
                    },
                    {
                        "name": "Yansong Shi"
                    },
                    {
                        "name": "Zhengrong Yue"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Yali Wang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19694v1",
                "updated": "2024-10-25T17:07:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    7,
                    13,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T17:07:13Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    7,
                    13,
                    4,
                    299,
                    0
                ],
                "title": "Less is More: Extreme Gradient Boost Rank-1 Adaption for Efficient\n  Finetuning of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less is More: Extreme Gradient Boost Rank-1 Adaption for Efficient\n  Finetuning of LLMs"
                },
                "summary": "Fine-tuning Large Language Models (LLMs) has become a crucial technique for\nadapting pre-trained models to downstream tasks. However, the enormous size of\nLLMs poses significant challenges in terms of computational complexity and\nresource requirements. Low-Rank Adaptation (LoRA) has emerged as a promising\nsolution. However, there exists a gap between the practical performance of\nlow-rank adaptations and its theoretical optimum. In this work, we propose\neXtreme Gradient Boosting LoRA (XGBLoRA), a novel framework that bridges this\ngap by leveraging the power of ensemble learning. Inspired by gradient\nboosting, XGBLoRA iteratively learns and merges a sequence of LoRA adaptations\nto refine model predictions. It achieves better performance than the standard\nLoRA, while enjoying the computational efficiency of rank-1 adaptations. We\nprovide theoretical analysis to show the convergence and optimality of our\napproach, and conduct extensive experiments on a range of natural language\nprocessing tasks. The results demonstrate that XGBLoRA consistently outperforms\nstandard LoRA and achieves performance comparable to full fine-tuning with\nsignificantly fewer trainable parameters. This work advances\nparameter-efficient fine-tuning for LLMs, and offers a promising solution for\nadapting LLMs to downstream tasks while optimizing performance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning Large Language Models (LLMs) has become a crucial technique for\nadapting pre-trained models to downstream tasks. However, the enormous size of\nLLMs poses significant challenges in terms of computational complexity and\nresource requirements. Low-Rank Adaptation (LoRA) has emerged as a promising\nsolution. However, there exists a gap between the practical performance of\nlow-rank adaptations and its theoretical optimum. In this work, we propose\neXtreme Gradient Boosting LoRA (XGBLoRA), a novel framework that bridges this\ngap by leveraging the power of ensemble learning. Inspired by gradient\nboosting, XGBLoRA iteratively learns and merges a sequence of LoRA adaptations\nto refine model predictions. It achieves better performance than the standard\nLoRA, while enjoying the computational efficiency of rank-1 adaptations. We\nprovide theoretical analysis to show the convergence and optimality of our\napproach, and conduct extensive experiments on a range of natural language\nprocessing tasks. The results demonstrate that XGBLoRA consistently outperforms\nstandard LoRA and achieves performance comparable to full fine-tuning with\nsignificantly fewer trainable parameters. This work advances\nparameter-efficient fine-tuning for LLMs, and offers a promising solution for\nadapting LLMs to downstream tasks while optimizing performance and efficiency."
                },
                "authors": [
                    {
                        "name": "Yifei Zhang"
                    },
                    {
                        "name": "Hao Zhu"
                    },
                    {
                        "name": "Aiwei Liu"
                    },
                    {
                        "name": "Han Yu"
                    },
                    {
                        "name": "Piotr Koniusz"
                    },
                    {
                        "name": "Irwin King"
                    }
                ],
                "author_detail": {
                    "name": "Irwin King"
                },
                "author": "Irwin King",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19692v1",
                "updated": "2024-10-25T17:06:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    6,
                    27,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T17:06:27Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    6,
                    27,
                    4,
                    299,
                    0
                ],
                "title": "AGENT-CQ: Automatic Generation and Evaluation of Clarifying Questions\n  for Conversational Search with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGENT-CQ: Automatic Generation and Evaluation of Clarifying Questions\n  for Conversational Search with LLMs"
                },
                "summary": "Generating diverse and effective clarifying questions is crucial for\nimproving query understanding and retrieval performance in open-domain\nconversational search (CS) systems. We propose AGENT-CQ (Automatic GENeration,\nand evaluaTion of Clarifying Questions), an end-to-end LLM-based framework\naddressing the challenges of scalability and adaptability faced by existing\nmethods that rely on manual curation or template-based approaches. AGENT-CQ\nconsists of two stages: a generation stage employing LLM prompting strategies\nto generate clarifying questions, and an evaluation stage (CrowdLLM) that\nsimulates human crowdsourcing judgments using multiple LLM instances to assess\ngenerated questions and answers based on comprehensive quality metrics.\nExtensive experiments on the ClariQ dataset demonstrate CrowdLLM's\neffectiveness in evaluating question and answer quality. Human evaluation and\nCrowdLLM show that the AGENT-CQ - generation stage, consistently outperforms\nbaselines in various aspects of question and answer quality. In retrieval-based\nevaluation, LLM-generated questions significantly enhance retrieval\neffectiveness for both BM25 and cross-encoder models compared to\nhuman-generated questions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating diverse and effective clarifying questions is crucial for\nimproving query understanding and retrieval performance in open-domain\nconversational search (CS) systems. We propose AGENT-CQ (Automatic GENeration,\nand evaluaTion of Clarifying Questions), an end-to-end LLM-based framework\naddressing the challenges of scalability and adaptability faced by existing\nmethods that rely on manual curation or template-based approaches. AGENT-CQ\nconsists of two stages: a generation stage employing LLM prompting strategies\nto generate clarifying questions, and an evaluation stage (CrowdLLM) that\nsimulates human crowdsourcing judgments using multiple LLM instances to assess\ngenerated questions and answers based on comprehensive quality metrics.\nExtensive experiments on the ClariQ dataset demonstrate CrowdLLM's\neffectiveness in evaluating question and answer quality. Human evaluation and\nCrowdLLM show that the AGENT-CQ - generation stage, consistently outperforms\nbaselines in various aspects of question and answer quality. In retrieval-based\nevaluation, LLM-generated questions significantly enhance retrieval\neffectiveness for both BM25 and cross-encoder models compared to\nhuman-generated questions."
                },
                "authors": [
                    {
                        "name": "Clemencia Siro"
                    },
                    {
                        "name": "Yifei Yuan"
                    },
                    {
                        "name": "Mohammad Aliannejadi"
                    },
                    {
                        "name": "Maarten de Rijke"
                    }
                ],
                "author_detail": {
                    "name": "Maarten de Rijke"
                },
                "author": "Maarten de Rijke",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17141v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17141v2",
                "updated": "2024-10-25T16:58:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    16,
                    58,
                    37,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-22T16:18:41Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    16,
                    18,
                    41,
                    1,
                    296,
                    0
                ],
                "title": "Towards Automated Penetration Testing: Introducing LLM Benchmark,\n  Analysis, and Improvements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Automated Penetration Testing: Introducing LLM Benchmark,\n  Analysis, and Improvements"
                },
                "summary": "Hacking poses a significant threat to cybersecurity, inflicting billions of\ndollars in damages annually. To mitigate these risks, ethical hacking, or\npenetration testing, is employed to identify vulnerabilities in systems and\nnetworks. Recent advancements in large language models (LLMs) have shown\npotential across various domains, including cybersecurity. However, there is\ncurrently no comprehensive, open, end-to-end automated penetration testing\nbenchmark to drive progress and evaluate the capabilities of these models in\nsecurity contexts. This paper introduces a novel open benchmark for LLM-based\nautomated penetration testing, addressing this critical gap. We first evaluate\nthe performance of LLMs, including GPT-4o and Llama 3.1-405B, using the\nstate-of-the-art PentestGPT tool. Our findings reveal that while Llama 3.1\ndemonstrates an edge over GPT-4o, both models currently fall short of\nperforming fully automated, end-to-end penetration testing. Next, we advance\nthe state-of-the-art and present ablation studies that provide insights into\nimproving the PentestGPT tool. Our research illuminates the challenges LLMs\nface in each aspect of Pentesting, e.g. enumeration, exploitation, and\nprivilege escalation. This work contributes to the growing body of knowledge on\nAI-assisted cybersecurity and lays the foundation for future research in\nautomated penetration testing using large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hacking poses a significant threat to cybersecurity, inflicting billions of\ndollars in damages annually. To mitigate these risks, ethical hacking, or\npenetration testing, is employed to identify vulnerabilities in systems and\nnetworks. Recent advancements in large language models (LLMs) have shown\npotential across various domains, including cybersecurity. However, there is\ncurrently no comprehensive, open, end-to-end automated penetration testing\nbenchmark to drive progress and evaluate the capabilities of these models in\nsecurity contexts. This paper introduces a novel open benchmark for LLM-based\nautomated penetration testing, addressing this critical gap. We first evaluate\nthe performance of LLMs, including GPT-4o and Llama 3.1-405B, using the\nstate-of-the-art PentestGPT tool. Our findings reveal that while Llama 3.1\ndemonstrates an edge over GPT-4o, both models currently fall short of\nperforming fully automated, end-to-end penetration testing. Next, we advance\nthe state-of-the-art and present ablation studies that provide insights into\nimproving the PentestGPT tool. Our research illuminates the challenges LLMs\nface in each aspect of Pentesting, e.g. enumeration, exploitation, and\nprivilege escalation. This work contributes to the growing body of knowledge on\nAI-assisted cybersecurity and lays the foundation for future research in\nautomated penetration testing using large language models."
                },
                "authors": [
                    {
                        "name": "Isamu Isozaki"
                    },
                    {
                        "name": "Manil Shrestha"
                    },
                    {
                        "name": "Rick Console"
                    },
                    {
                        "name": "Edward Kim"
                    }
                ],
                "author_detail": {
                    "name": "Edward Kim"
                },
                "author": "Edward Kim",
                "arxiv_comment": "Main Paper 1-9 pages, Supplementary Materials: 10-17, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17141v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17141v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18682v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18682v2",
                "updated": "2024-10-25T16:57:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    16,
                    57,
                    43,
                    4,
                    299,
                    0
                ],
                "published": "2024-05-29T01:12:53Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    1,
                    12,
                    53,
                    2,
                    150,
                    0
                ],
                "title": "Can GPT Redefine Medical Understanding? Evaluating GPT on Biomedical\n  Machine Reading Comprehension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can GPT Redefine Medical Understanding? Evaluating GPT on Biomedical\n  Machine Reading Comprehension"
                },
                "summary": "Large language models (LLMs) have shown remarkable performance on many tasks\nin different domains. However, their performance in closed-book biomedical\nmachine reading comprehension (MRC) has not been evaluated in depth. In this\nwork, we evaluate GPT on four closed-book biomedical MRC benchmarks. We\nexperiment with different conventional prompting techniques as well as\nintroduce our own novel prompting method. To solve some of the retrieval\nproblems inherent to LLMs, we propose a prompting strategy named Implicit\nRetrieval Augmented Generation (RAG) that alleviates the need for using vector\ndatabases to retrieve important chunks in traditional RAG setups. Moreover, we\nreport qualitative assessments on the natural language generation outputs from\nour approach. The results show that our new prompting technique is able to get\nthe best performance in two out of four datasets and ranks second in rest of\nthem. Experiments show that modern-day LLMs like GPT even in a zero-shot\nsetting can outperform supervised models, leading to new state-of-the-art\n(SoTA) results on two of the benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable performance on many tasks\nin different domains. However, their performance in closed-book biomedical\nmachine reading comprehension (MRC) has not been evaluated in depth. In this\nwork, we evaluate GPT on four closed-book biomedical MRC benchmarks. We\nexperiment with different conventional prompting techniques as well as\nintroduce our own novel prompting method. To solve some of the retrieval\nproblems inherent to LLMs, we propose a prompting strategy named Implicit\nRetrieval Augmented Generation (RAG) that alleviates the need for using vector\ndatabases to retrieve important chunks in traditional RAG setups. Moreover, we\nreport qualitative assessments on the natural language generation outputs from\nour approach. The results show that our new prompting technique is able to get\nthe best performance in two out of four datasets and ranks second in rest of\nthem. Experiments show that modern-day LLMs like GPT even in a zero-shot\nsetting can outperform supervised models, leading to new state-of-the-art\n(SoTA) results on two of the benchmarks."
                },
                "authors": [
                    {
                        "name": "Shubham Vatsal"
                    },
                    {
                        "name": "Ayush Singh"
                    }
                ],
                "author_detail": {
                    "name": "Ayush Singh"
                },
                "author": "Ayush Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18682v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18682v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00066v2",
                "updated": "2024-10-25T16:57:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    16,
                    57,
                    10,
                    4,
                    299,
                    0
                ],
                "published": "2024-06-17T15:21:35Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    15,
                    21,
                    35,
                    0,
                    169,
                    0
                ],
                "title": "Compress then Serve: Serving Thousands of LoRA Adapters with Little\n  Overhead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compress then Serve: Serving Thousands of LoRA Adapters with Little\n  Overhead"
                },
                "summary": "Fine-tuning large language models (LLMs) with low-rank adaptations (LoRAs)\nhas become common practice, often yielding numerous copies of the same LLM\ndiffering only in their LoRA updates. This paradigm presents challenges for\nsystems that serve real-time responses to queries that each involve a different\nLoRA. Prior works optimize the design of such systems but still require\ncontinuous loading and offloading of LoRAs, as it is infeasible to store\nthousands of LoRAs in GPU memory. To mitigate this issue, we investigate the\nefficacy of model compression when serving LoRAs. We propose a method for joint\ncompression of LoRAs into a shared basis paired with LoRA-specific scaling\nmatrices. We extend our algorithm to learn clusters of LoRAs that are more\namenable to joint compression, allowing it to scale gracefully to large LoRA\ncollections. Our experiments with up to 500 LoRAs demonstrate that compressed\nLoRAs preserve performance while offering major throughput gains in realistic\nserving scenarios with over a thousand LoRAs, maintaining 80% of the throughput\nof serving a single LoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) with low-rank adaptations (LoRAs)\nhas become common practice, often yielding numerous copies of the same LLM\ndiffering only in their LoRA updates. This paradigm presents challenges for\nsystems that serve real-time responses to queries that each involve a different\nLoRA. Prior works optimize the design of such systems but still require\ncontinuous loading and offloading of LoRAs, as it is infeasible to store\nthousands of LoRAs in GPU memory. To mitigate this issue, we investigate the\nefficacy of model compression when serving LoRAs. We propose a method for joint\ncompression of LoRAs into a shared basis paired with LoRA-specific scaling\nmatrices. We extend our algorithm to learn clusters of LoRAs that are more\namenable to joint compression, allowing it to scale gracefully to large LoRA\ncollections. Our experiments with up to 500 LoRAs demonstrate that compressed\nLoRAs preserve performance while offering major throughput gains in realistic\nserving scenarios with over a thousand LoRAs, maintaining 80% of the throughput\nof serving a single LoRA."
                },
                "authors": [
                    {
                        "name": "Rickard Brüel-Gabrielsson"
                    },
                    {
                        "name": "Jiacheng Zhu"
                    },
                    {
                        "name": "Onkar Bhardwaj"
                    },
                    {
                        "name": "Leshem Choshen"
                    },
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Mikhail Yurochkin"
                    },
                    {
                        "name": "Justin Solomon"
                    }
                ],
                "author_detail": {
                    "name": "Justin Solomon"
                },
                "author": "Justin Solomon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19680v1",
                "updated": "2024-10-25T16:48:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    16,
                    48,
                    44,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T16:48:44Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    16,
                    48,
                    44,
                    4,
                    299,
                    0
                ],
                "title": "Inferring Neural Signed Distance Functions by Overfitting on Single\n  Noisy Point Clouds through Finetuning Data-Driven based Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Neural Signed Distance Functions by Overfitting on Single\n  Noisy Point Clouds through Finetuning Data-Driven based Priors"
                },
                "summary": "It is important to estimate an accurate signed distance function (SDF) from a\npoint cloud in many computer vision applications. The latest methods learn\nneural SDFs using either a data-driven based or an overfitting-based strategy.\nHowever, these two kinds of methods are with either poor generalization or slow\nconvergence, which limits their capability under challenging scenarios like\nhighly noisy point clouds. To resolve this issue, we propose a method to\npromote pros of both data-driven based and overfitting-based methods for better\ngeneralization, faster inference, and higher accuracy in learning neural SDFs.\nWe introduce a novel statistical reasoning algorithm in local regions which is\nable to finetune data-driven based priors without signed distance supervision,\nclean point cloud, or point normals. This helps our method start with a good\ninitialization, and converge to a minimum in a much faster way. Our numerical\nand visual comparisons with the state-of-the-art methods show our superiority\nover these methods in surface reconstruction and point cloud denoising on\nwidely used shape and scene benchmarks. The code is available at\nhttps://github.com/chenchao15/LocalN2NM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is important to estimate an accurate signed distance function (SDF) from a\npoint cloud in many computer vision applications. The latest methods learn\nneural SDFs using either a data-driven based or an overfitting-based strategy.\nHowever, these two kinds of methods are with either poor generalization or slow\nconvergence, which limits their capability under challenging scenarios like\nhighly noisy point clouds. To resolve this issue, we propose a method to\npromote pros of both data-driven based and overfitting-based methods for better\ngeneralization, faster inference, and higher accuracy in learning neural SDFs.\nWe introduce a novel statistical reasoning algorithm in local regions which is\nable to finetune data-driven based priors without signed distance supervision,\nclean point cloud, or point normals. This helps our method start with a good\ninitialization, and converge to a minimum in a much faster way. Our numerical\nand visual comparisons with the state-of-the-art methods show our superiority\nover these methods in surface reconstruction and point cloud denoising on\nwidely used shape and scene benchmarks. The code is available at\nhttps://github.com/chenchao15/LocalN2NM."
                },
                "authors": [
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Yu-Shen Liu"
                    },
                    {
                        "name": "Zhizhong Han"
                    }
                ],
                "author_detail": {
                    "name": "Zhizhong Han"
                },
                "author": "Zhizhong Han",
                "arxiv_comment": "Accepted by NeurlPS 2024. Project page:\n  https://chenchao15.github.io/LocalN2NM/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19040v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19040v2",
                "updated": "2024-10-25T16:48:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    16,
                    48,
                    36,
                    4,
                    299,
                    0
                ],
                "published": "2024-05-29T12:27:19Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    12,
                    27,
                    19,
                    2,
                    150,
                    0
                ],
                "title": "Finite-Choice Logic Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finite-Choice Logic Programming"
                },
                "summary": "Logic programming, as exemplified by datalog, defines the meaning of a\nprogram as its unique smallest model: the deductive closure of its inference\nrules. However, many problems call for an enumeration of models that vary along\nsome set of choices while maintaining structural and logical constraints --\nthere is no single canonical model. The notion of stable models for logic\nprograms with negation has successfully captured programmer intuition about the\nset of valid solutions for such problems, giving rise to a family of\nprogramming languages and associated solvers known as answer set programming.\nUnfortunately, the definition of a stable model is frustratingly indirect,\nespecially in the presence of rules containing free variables.\n  We propose a new formalism, finite-choice logic programming, that uses\nchoice, not negation, to admit multiple solutions. Finite-choice logic\nprogramming contains all the expressive power of the stable model semantics,\ngives meaning to a new and useful class of programs, and enjoys a\nleast-fixed-point interpretation over a novel domain. We present an algorithm\nfor exploring the solution space and prove it correct with respect to our\nsemantics. Our implementation, the Dusa logic programming language, has\nperformance that compares favorably with state-of-the-art answer set solvers\nand exhibits more predictable scaling with problem size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logic programming, as exemplified by datalog, defines the meaning of a\nprogram as its unique smallest model: the deductive closure of its inference\nrules. However, many problems call for an enumeration of models that vary along\nsome set of choices while maintaining structural and logical constraints --\nthere is no single canonical model. The notion of stable models for logic\nprograms with negation has successfully captured programmer intuition about the\nset of valid solutions for such problems, giving rise to a family of\nprogramming languages and associated solvers known as answer set programming.\nUnfortunately, the definition of a stable model is frustratingly indirect,\nespecially in the presence of rules containing free variables.\n  We propose a new formalism, finite-choice logic programming, that uses\nchoice, not negation, to admit multiple solutions. Finite-choice logic\nprogramming contains all the expressive power of the stable model semantics,\ngives meaning to a new and useful class of programs, and enjoys a\nleast-fixed-point interpretation over a novel domain. We present an algorithm\nfor exploring the solution space and prove it correct with respect to our\nsemantics. Our implementation, the Dusa logic programming language, has\nperformance that compares favorably with state-of-the-art answer set solvers\nand exhibits more predictable scaling with problem size."
                },
                "authors": [
                    {
                        "name": "Chris Martens"
                    },
                    {
                        "name": "Robert J. Simmons"
                    },
                    {
                        "name": "Michael Arntzenius"
                    }
                ],
                "author_detail": {
                    "name": "Michael Arntzenius"
                },
                "author": "Michael Arntzenius",
                "arxiv_comment": "Conditionally accepted for publication at POPL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19040v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19040v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19673v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19673v1",
                "updated": "2024-10-25T16:25:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    16,
                    25,
                    49,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T16:25:49Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    16,
                    25,
                    49,
                    4,
                    299,
                    0
                ],
                "title": "Spatial Shortcuts in Graph Neural Controlled Differential Equations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial Shortcuts in Graph Neural Controlled Differential Equations"
                },
                "summary": "We incorporate prior graph topology information into a Neural Controlled\nDifferential Equation (NCDE) to predict the future states of a dynamical system\ndefined on a graph. The informed NCDE infers the future dynamics at the\nvertices of simulated advection data on graph edges with a known causal graph,\nobserved only at vertices during training. We investigate different positions\nin the model architecture to inform the NCDE with graph information and\nidentify an outer position between hidden state and control as theoretically\nand empirically favorable. Our such informed NCDE requires fewer parameters to\nreach a lower Mean Absolute Error (MAE) compared to previous methods that do\nnot incorporate additional graph topology information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We incorporate prior graph topology information into a Neural Controlled\nDifferential Equation (NCDE) to predict the future states of a dynamical system\ndefined on a graph. The informed NCDE infers the future dynamics at the\nvertices of simulated advection data on graph edges with a known causal graph,\nobserved only at vertices during training. We investigate different positions\nin the model architecture to inform the NCDE with graph information and\nidentify an outer position between hidden state and control as theoretically\nand empirically favorable. Our such informed NCDE requires fewer parameters to\nreach a lower Mean Absolute Error (MAE) compared to previous methods that do\nnot incorporate additional graph topology information."
                },
                "authors": [
                    {
                        "name": "Michael Detzel"
                    },
                    {
                        "name": "Gabriel Nobis"
                    },
                    {
                        "name": "Jackie Ma"
                    },
                    {
                        "name": "Wojciech Samek"
                    }
                ],
                "author_detail": {
                    "name": "Wojciech Samek"
                },
                "author": "Wojciech Samek",
                "arxiv_comment": "Accepted as a workshop paper at the NeurIPS 2024 workshop on\n  Data-driven and Differentiable Simulations, Surrogates, and Solvers (D3S3)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19673v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19673v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.4; G.1.7; G.2.2; G.3; I.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19656v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19656v1",
                "updated": "2024-10-25T16:08:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    16,
                    8,
                    5,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T16:08:05Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    16,
                    8,
                    5,
                    4,
                    299,
                    0
                ],
                "title": "APRICOT: Active Preference Learning and Constraint-Aware Task Planning\n  with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APRICOT: Active Preference Learning and Constraint-Aware Task Planning\n  with LLMs"
                },
                "summary": "Home robots performing personalized tasks must adeptly balance user\npreferences with environmental affordances. We focus on organization tasks\nwithin constrained spaces, such as arranging items into a refrigerator, where\npreferences for placement collide with physical limitations. The robot must\ninfer user preferences based on a small set of demonstrations, which is easier\nfor users to provide than extensively defining all their requirements. While\nrecent works use Large Language Models (LLMs) to learn preferences from user\ndemonstrations, they encounter two fundamental challenges. First, there is\ninherent ambiguity in interpreting user actions, as multiple preferences can\noften explain a single observed behavior. Second, not all user preferences are\npractically feasible due to geometric constraints in the environment. To\naddress these challenges, we introduce APRICOT, a novel approach that merges\nLLM-based Bayesian active preference learning with constraint-aware task\nplanning. APRICOT refines its generated preferences by actively querying the\nuser and dynamically adapts its plan to respect environmental constraints. We\nevaluate APRICOT on a dataset of diverse organization tasks and demonstrate its\neffectiveness in real-world scenarios, showing significant improvements in both\npreference satisfaction and plan feasibility. The project website is at\nhttps://portal-cornell.github.io/apricot/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Home robots performing personalized tasks must adeptly balance user\npreferences with environmental affordances. We focus on organization tasks\nwithin constrained spaces, such as arranging items into a refrigerator, where\npreferences for placement collide with physical limitations. The robot must\ninfer user preferences based on a small set of demonstrations, which is easier\nfor users to provide than extensively defining all their requirements. While\nrecent works use Large Language Models (LLMs) to learn preferences from user\ndemonstrations, they encounter two fundamental challenges. First, there is\ninherent ambiguity in interpreting user actions, as multiple preferences can\noften explain a single observed behavior. Second, not all user preferences are\npractically feasible due to geometric constraints in the environment. To\naddress these challenges, we introduce APRICOT, a novel approach that merges\nLLM-based Bayesian active preference learning with constraint-aware task\nplanning. APRICOT refines its generated preferences by actively querying the\nuser and dynamically adapts its plan to respect environmental constraints. We\nevaluate APRICOT on a dataset of diverse organization tasks and demonstrate its\neffectiveness in real-world scenarios, showing significant improvements in both\npreference satisfaction and plan feasibility. The project website is at\nhttps://portal-cornell.github.io/apricot/"
                },
                "authors": [
                    {
                        "name": "Huaxiaoyue Wang"
                    },
                    {
                        "name": "Nathaniel Chin"
                    },
                    {
                        "name": "Gonzalo Gonzalez-Pumariega"
                    },
                    {
                        "name": "Xiangwan Sun"
                    },
                    {
                        "name": "Neha Sunkara"
                    },
                    {
                        "name": "Maximus Adrian Pace"
                    },
                    {
                        "name": "Jeannette Bohg"
                    },
                    {
                        "name": "Sanjiban Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Sanjiban Choudhury"
                },
                "author": "Sanjiban Choudhury",
                "arxiv_comment": "Conference on Robot Learning (CoRL) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19656v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19656v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19631v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19631v1",
                "updated": "2024-10-25T15:34:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    15,
                    34,
                    3,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T15:34:03Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    15,
                    34,
                    3,
                    4,
                    299,
                    0
                ],
                "title": "Efficient Biological Data Acquisition through Inference Set Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Biological Data Acquisition through Inference Set Design"
                },
                "summary": "In drug discovery, highly automated high-throughput laboratories are used to\nscreen a large number of compounds in search of effective drugs. These\nexperiments are expensive, so we might hope to reduce their cost by\nexperimenting on a subset of the compounds, and predicting the outcomes of the\nremaining experiments. In this work, we model this scenario as a sequential\nsubset selection problem: we aim to select the smallest set of candidates in\norder to achieve some desired level of accuracy for the system as a whole. Our\nkey observation is that, if there is heterogeneity in the difficulty of the\nprediction problem across the input space, selectively obtaining the labels for\nthe hardest examples in the acquisition pool will leave only the relatively\neasy examples to remain in the inference set, leading to better overall system\nperformance. We call this mechanism inference set design, and propose the use\nof an uncertainty-based active learning solution to prune out these challenging\nexamples. Our algorithm includes an explicit stopping criterion that stops\nrunning the experiments when it is sufficiently confident that the system has\nreached the target performance. Our empirical studies on image and molecular\ndatasets, as well as a real-world large-scale biological assay, show that\ndeploying active learning for inference set design leads to significant\nreduction in experimental cost while obtaining high system performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In drug discovery, highly automated high-throughput laboratories are used to\nscreen a large number of compounds in search of effective drugs. These\nexperiments are expensive, so we might hope to reduce their cost by\nexperimenting on a subset of the compounds, and predicting the outcomes of the\nremaining experiments. In this work, we model this scenario as a sequential\nsubset selection problem: we aim to select the smallest set of candidates in\norder to achieve some desired level of accuracy for the system as a whole. Our\nkey observation is that, if there is heterogeneity in the difficulty of the\nprediction problem across the input space, selectively obtaining the labels for\nthe hardest examples in the acquisition pool will leave only the relatively\neasy examples to remain in the inference set, leading to better overall system\nperformance. We call this mechanism inference set design, and propose the use\nof an uncertainty-based active learning solution to prune out these challenging\nexamples. Our algorithm includes an explicit stopping criterion that stops\nrunning the experiments when it is sufficiently confident that the system has\nreached the target performance. Our empirical studies on image and molecular\ndatasets, as well as a real-world large-scale biological assay, show that\ndeploying active learning for inference set design leads to significant\nreduction in experimental cost while obtaining high system performance."
                },
                "authors": [
                    {
                        "name": "Ihor Neporozhnii"
                    },
                    {
                        "name": "Julien Roy"
                    },
                    {
                        "name": "Emmanuel Bengio"
                    },
                    {
                        "name": "Jason Hartford"
                    }
                ],
                "author_detail": {
                    "name": "Jason Hartford"
                },
                "author": "Jason Hartford",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19631v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18652v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18652v2",
                "updated": "2024-10-25T15:23:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    15,
                    23,
                    54,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-24T11:32:00Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    11,
                    32,
                    0,
                    3,
                    298,
                    0
                ],
                "title": "$C^2$: Scalable Auto-Feedback for LLM-based Chart Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$C^2$: Scalable Auto-Feedback for LLM-based Chart Generation"
                },
                "summary": "Generating high-quality charts with Large Language Models presents\nsignificant challenges due to limited data and the high cost of scaling through\nhuman curation. Instruction, data, and code triplets are scarce and expensive\nto manually curate as their creation demands technical expertise. To address\nthis scalability issue, we introduce a reference-free automatic feedback\ngenerator, which eliminates the need for costly human intervention. Our novel\nframework, $C^2$, consists of (1) an automatic feedback provider (ChartAF) and\n(2) a diverse, reference-free dataset (ChartUIE-8K). Quantitative results are\ncompelling: in our first experiment, 74% of respondents strongly preferred, and\n10% preferred, the results after feedback. The second post-feedback experiment\ndemonstrates that ChartAF outperforms nine baselines. Moreover, ChartUIE-8K\nsignificantly improves data diversity by increasing queries, datasets, and\nchart types by 5982%, 1936%, and 91%, respectively, over benchmarks. Finally,\nan LLM user study revealed that 94% of participants preferred ChartUIE-8K's\nqueries, with 93% deeming them aligned with real-world use cases. Core\ncontributions are available as open-source at an anonymized project site, with\nample qualitative examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating high-quality charts with Large Language Models presents\nsignificant challenges due to limited data and the high cost of scaling through\nhuman curation. Instruction, data, and code triplets are scarce and expensive\nto manually curate as their creation demands technical expertise. To address\nthis scalability issue, we introduce a reference-free automatic feedback\ngenerator, which eliminates the need for costly human intervention. Our novel\nframework, $C^2$, consists of (1) an automatic feedback provider (ChartAF) and\n(2) a diverse, reference-free dataset (ChartUIE-8K). Quantitative results are\ncompelling: in our first experiment, 74% of respondents strongly preferred, and\n10% preferred, the results after feedback. The second post-feedback experiment\ndemonstrates that ChartAF outperforms nine baselines. Moreover, ChartUIE-8K\nsignificantly improves data diversity by increasing queries, datasets, and\nchart types by 5982%, 1936%, and 91%, respectively, over benchmarks. Finally,\nan LLM user study revealed that 94% of participants preferred ChartUIE-8K's\nqueries, with 93% deeming them aligned with real-world use cases. Core\ncontributions are available as open-source at an anonymized project site, with\nample qualitative examples."
                },
                "authors": [
                    {
                        "name": "Woosung Koh"
                    },
                    {
                        "name": "Jang Han Yoon"
                    },
                    {
                        "name": "MinHyung Lee"
                    },
                    {
                        "name": "Youngjin Song"
                    },
                    {
                        "name": "Jaegwan Cho"
                    },
                    {
                        "name": "Jaehyun Kang"
                    },
                    {
                        "name": "Taehyeon Kim"
                    },
                    {
                        "name": "Se-young Yun"
                    },
                    {
                        "name": "Youngjae Yu"
                    },
                    {
                        "name": "Bongshin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Bongshin Lee"
                },
                "author": "Bongshin Lee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18652v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18652v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19616v1",
                "updated": "2024-10-25T15:14:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    15,
                    14,
                    51,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T15:14:51Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    15,
                    14,
                    51,
                    4,
                    299,
                    0
                ],
                "title": "Uniqueness and Nondegeneracy of positive ground states of $ -Δu +\n  (-Δ)^s u+u = u^{p+1} \\quad \\hbox{in $\\mathbb{R}^n$}$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uniqueness and Nondegeneracy of positive ground states of $ -Δu +\n  (-Δ)^s u+u = u^{p+1} \\quad \\hbox{in $\\mathbb{R}^n$}$"
                },
                "summary": "We are concerned with the mixed local/nonlocal Schr\\\"{o}dinger equation\n  \\begin{equation}\n  - \\Delta u + (-\\Delta)^s u+u = u^{p+1} \\quad \\hbox{in $\\mathbb{R}^n$,}\n  \\end{equation}\n  for arbitrary space dimension $n\\geqslant1$, any $s\\in(0,1)$ and\n$p\\in(0,2^*-2)$ with $2^*$ the critical Sobolev exponent.\n  We provide the existence and several fundamental properties of nonnegative\nsolutions for the above equation inferred from \\cite{DSVZ24}. And then, we\nprove that such equation possesses a unique (up to translations) ground state,\nwhich is nondegenerate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We are concerned with the mixed local/nonlocal Schr\\\"{o}dinger equation\n  \\begin{equation}\n  - \\Delta u + (-\\Delta)^s u+u = u^{p+1} \\quad \\hbox{in $\\mathbb{R}^n$,}\n  \\end{equation}\n  for arbitrary space dimension $n\\geqslant1$, any $s\\in(0,1)$ and\n$p\\in(0,2^*-2)$ with $2^*$ the critical Sobolev exponent.\n  We provide the existence and several fundamental properties of nonnegative\nsolutions for the above equation inferred from \\cite{DSVZ24}. And then, we\nprove that such equation possesses a unique (up to translations) ground state,\nwhich is nondegenerate."
                },
                "authors": [
                    {
                        "name": "Xifeng Su"
                    },
                    {
                        "name": "Chengxiang Zhang"
                    },
                    {
                        "name": "Jiwen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Zhang"
                },
                "author": "Jiwen Zhang",
                "arxiv_comment": "34 pages. All comments are welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "35A02, 35B65, 35J10, 35R11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.06689v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.06689v4",
                "updated": "2024-10-25T15:08:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    15,
                    8,
                    3,
                    4,
                    299,
                    0
                ],
                "published": "2023-03-12T15:36:03Z",
                "published_parsed": [
                    2023,
                    3,
                    12,
                    15,
                    36,
                    3,
                    6,
                    71,
                    0
                ],
                "title": "Self-planning Code Generation with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-planning Code Generation with Large Language Models"
                },
                "summary": "Although large language models (LLMs) have demonstrated impressive ability in\ncode generation, they are still struggling to address the complicated intent\nprovided by humans. It is widely acknowledged that humans typically employ\nplanning to decompose complex problems and schedule solution steps prior to\nimplementation. To this end, we introduce planning into code generation to help\nthe model understand complex intent and reduce the difficulty of\nproblem-solving. This paper proposes a self-planning code generation approach\nwith large language models, which consists of two phases, namely planning phase\nand implementation phase. Specifically, in the planning phase, LLM plans out\nconcise solution steps from the intent combined with few-shot prompting.\nSubsequently, in the implementation phase, the model generates code step by\nstep, guided by the preceding solution steps. We conduct extensive experiments\non various code-generation benchmarks across multiple programming languages.\nExperimental results show that self-planning code generation achieves a\nrelative improvement of up to 25.4% in Pass@1 compared to direct code\ngeneration, and up to 11.9% compared to Chain-of-Thought of code generation.\nMoreover, our self-planning approach also enhances the quality of the generated\ncode with respect to correctness, readability, and robustness, as assessed by\nhumans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) have demonstrated impressive ability in\ncode generation, they are still struggling to address the complicated intent\nprovided by humans. It is widely acknowledged that humans typically employ\nplanning to decompose complex problems and schedule solution steps prior to\nimplementation. To this end, we introduce planning into code generation to help\nthe model understand complex intent and reduce the difficulty of\nproblem-solving. This paper proposes a self-planning code generation approach\nwith large language models, which consists of two phases, namely planning phase\nand implementation phase. Specifically, in the planning phase, LLM plans out\nconcise solution steps from the intent combined with few-shot prompting.\nSubsequently, in the implementation phase, the model generates code step by\nstep, guided by the preceding solution steps. We conduct extensive experiments\non various code-generation benchmarks across multiple programming languages.\nExperimental results show that self-planning code generation achieves a\nrelative improvement of up to 25.4% in Pass@1 compared to direct code\ngeneration, and up to 11.9% compared to Chain-of-Thought of code generation.\nMoreover, our self-planning approach also enhances the quality of the generated\ncode with respect to correctness, readability, and robustness, as assessed by\nhumans."
                },
                "authors": [
                    {
                        "name": "Xue Jiang"
                    },
                    {
                        "name": "Yihong Dong"
                    },
                    {
                        "name": "Lecheng Wang"
                    },
                    {
                        "name": "Zheng Fang"
                    },
                    {
                        "name": "Qiwei Shang"
                    },
                    {
                        "name": "Ge Li"
                    },
                    {
                        "name": "Zhi Jin"
                    },
                    {
                        "name": "Wenpin Jiao"
                    }
                ],
                "author_detail": {
                    "name": "Wenpin Jiao"
                },
                "author": "Wenpin Jiao",
                "arxiv_comment": "Accepted by TOSEM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.06689v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.06689v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12000v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12000v2",
                "updated": "2024-10-25T15:05:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    15,
                    5,
                    14,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-15T19:05:28Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    19,
                    5,
                    28,
                    1,
                    289,
                    0
                ],
                "title": "Parametric model reduction of mean-field and stochastic systems via\n  higher-order action matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parametric model reduction of mean-field and stochastic systems via\n  higher-order action matching"
                },
                "summary": "The aim of this work is to learn models of population dynamics of physical\nsystems that feature stochastic and mean-field effects and that depend on\nphysics parameters. The learned models can act as surrogates of classical\nnumerical models to efficiently predict the system behavior over the physics\nparameters. Building on the Benamou-Brenier formula from optimal transport and\naction matching, we use a variational problem to infer parameter- and\ntime-dependent gradient fields that represent approximations of the population\ndynamics. The inferred gradient fields can then be used to rapidly generate\nsample trajectories that mimic the dynamics of the physical system on a\npopulation level over varying physics parameters. We show that combining Monte\nCarlo sampling with higher-order quadrature rules is critical for accurately\nestimating the training objective from sample data and for stabilizing the\ntraining process. We demonstrate on Vlasov-Poisson instabilities as well as on\nhigh-dimensional particle and chaotic systems that our approach accurately\npredicts population dynamics over a wide range of parameters and outperforms\nstate-of-the-art diffusion-based and flow-based modeling that simply condition\non time and physics parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The aim of this work is to learn models of population dynamics of physical\nsystems that feature stochastic and mean-field effects and that depend on\nphysics parameters. The learned models can act as surrogates of classical\nnumerical models to efficiently predict the system behavior over the physics\nparameters. Building on the Benamou-Brenier formula from optimal transport and\naction matching, we use a variational problem to infer parameter- and\ntime-dependent gradient fields that represent approximations of the population\ndynamics. The inferred gradient fields can then be used to rapidly generate\nsample trajectories that mimic the dynamics of the physical system on a\npopulation level over varying physics parameters. We show that combining Monte\nCarlo sampling with higher-order quadrature rules is critical for accurately\nestimating the training objective from sample data and for stabilizing the\ntraining process. We demonstrate on Vlasov-Poisson instabilities as well as on\nhigh-dimensional particle and chaotic systems that our approach accurately\npredicts population dynamics over a wide range of parameters and outperforms\nstate-of-the-art diffusion-based and flow-based modeling that simply condition\non time and physics parameters."
                },
                "authors": [
                    {
                        "name": "Jules Berman"
                    },
                    {
                        "name": "Tobias Blickhan"
                    },
                    {
                        "name": "Benjamin Peherstorfer"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Peherstorfer"
                },
                "author": "Benjamin Peherstorfer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12000v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12000v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19605v1",
                "updated": "2024-10-25T14:57:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    57,
                    29,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T14:57:29Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    57,
                    29,
                    4,
                    299,
                    0
                ],
                "title": "CoqPilot, a plugin for LLM-based generation of proofs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoqPilot, a plugin for LLM-based generation of proofs"
                },
                "summary": "We present CoqPilot, a VS Code extension designed to help automate writing of\nCoq proofs. The plugin collects the parts of proofs marked with the admit\ntactic in a Coq file, i.e., proof holes, and combines LLMs along with\nnon-machine-learning methods to generate proof candidates for the holes. Then,\nCoqPilot checks if each proof candidate solves the given subgoal and, if\nsuccessful, replaces the hole with it. The focus of CoqPilot is twofold.\nFirstly, we want to allow users to seamlessly combine multiple Coq generation\napproaches and provide a zero-setup experience for our tool. Secondly, we want\nto deliver a platform for LLM-based experiments on Coq proof generation. We\ndeveloped a benchmarking system for Coq generation methods, available in the\nplugin, and conducted an experiment using it, showcasing the framework's\npossibilities. Demo of CoqPilot is available at: https://youtu.be/oB1Lx-So9Lo.\nCode at: https://github.com/JetBrains-Research/coqpilot",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present CoqPilot, a VS Code extension designed to help automate writing of\nCoq proofs. The plugin collects the parts of proofs marked with the admit\ntactic in a Coq file, i.e., proof holes, and combines LLMs along with\nnon-machine-learning methods to generate proof candidates for the holes. Then,\nCoqPilot checks if each proof candidate solves the given subgoal and, if\nsuccessful, replaces the hole with it. The focus of CoqPilot is twofold.\nFirstly, we want to allow users to seamlessly combine multiple Coq generation\napproaches and provide a zero-setup experience for our tool. Secondly, we want\nto deliver a platform for LLM-based experiments on Coq proof generation. We\ndeveloped a benchmarking system for Coq generation methods, available in the\nplugin, and conducted an experiment using it, showcasing the framework's\npossibilities. Demo of CoqPilot is available at: https://youtu.be/oB1Lx-So9Lo.\nCode at: https://github.com/JetBrains-Research/coqpilot"
                },
                "authors": [
                    {
                        "name": "Andrei Kozyrev"
                    },
                    {
                        "name": "Gleb Solovev"
                    },
                    {
                        "name": "Nikita Khramov"
                    },
                    {
                        "name": "Anton Podkopaev"
                    }
                ],
                "author_detail": {
                    "name": "Anton Podkopaev"
                },
                "author": "Anton Podkopaev",
                "arxiv_doi": "10.1145/3691620.3695357",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3691620.3695357",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.19605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in the proceedings of the ASE'24 Tool Demonstrations Track",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02856v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02856v2",
                "updated": "2024-10-25T14:52:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    52,
                    48,
                    4,
                    299,
                    0
                ],
                "published": "2024-02-05T10:16:29Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    10,
                    16,
                    29,
                    0,
                    36,
                    0
                ],
                "title": "Generalized dynamical phase reduction for stochastic oscillators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized dynamical phase reduction for stochastic oscillators"
                },
                "summary": "Phase reduction is an important tool for studying coupled and driven\noscillators. The question of how to generalize phase reduction to stochastic\noscillators remains actively debated. In this work, we propose a method to\nderive a self-contained stochastic phase equation of the form\n$\\mathop{}\\!\\mathrm{d} \\phi = a(\\phi)\\mathop{}\\!\\mathrm{d} t +\n\\sqrt{2D(\\phi)}\\,\\mathop{}\\!\\mathrm{d} W(t)$ that is valid not only for\nnoise-perturbed limit cycles, but also for noise-induced oscillations. We show\nthat our reduction captures the asymptotic statistics of qualitatively\ndifferent stochastic oscillators, and use it to infer their phase-response\nproperties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phase reduction is an important tool for studying coupled and driven\noscillators. The question of how to generalize phase reduction to stochastic\noscillators remains actively debated. In this work, we propose a method to\nderive a self-contained stochastic phase equation of the form\n$\\mathop{}\\!\\mathrm{d} \\phi = a(\\phi)\\mathop{}\\!\\mathrm{d} t +\n\\sqrt{2D(\\phi)}\\,\\mathop{}\\!\\mathrm{d} W(t)$ that is valid not only for\nnoise-perturbed limit cycles, but also for noise-induced oscillations. We show\nthat our reduction captures the asymptotic statistics of qualitatively\ndifferent stochastic oscillators, and use it to infer their phase-response\nproperties."
                },
                "authors": [
                    {
                        "name": "Pierre Houzelstein"
                    },
                    {
                        "name": "Peter J. Thomas"
                    },
                    {
                        "name": "Benjamin Lindner"
                    },
                    {
                        "name": "Boris S. Gutkin"
                    },
                    {
                        "name": "Alberto Pérez-Cervera"
                    }
                ],
                "author_detail": {
                    "name": "Alberto Pérez-Cervera"
                },
                "author": "Alberto Pérez-Cervera",
                "arxiv_comment": "16 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.02856v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02856v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19599v1",
                "updated": "2024-10-25T14:46:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    46,
                    7,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T14:46:07Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    46,
                    7,
                    4,
                    299,
                    0
                ],
                "title": "Take Caution in Using LLMs as Human Surrogates: Scylla Ex Machina",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Take Caution in Using LLMs as Human Surrogates: Scylla Ex Machina"
                },
                "summary": "Recent studies suggest large language models (LLMs) can exhibit human-like\nreasoning, aligning with human behavior in economic experiments, surveys, and\npolitical discourse. This has led many to propose that LLMs can be used as\nsurrogates for humans in social science research. However, LLMs differ\nfundamentally from humans, relying on probabilistic patterns, absent the\nembodied experiences or survival objectives that shape human cognition. We\nassess the reasoning depth of LLMs using the 11-20 money request game. Almost\nall advanced approaches fail to replicate human behavior distributions across\nmany models, except in one case involving fine-tuning using a substantial\namount of human behavior data. Causes of failure are diverse, relating to input\nlanguage, roles, and safeguarding. These results caution against using LLMs to\nstudy human behaviors or as human surrogates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies suggest large language models (LLMs) can exhibit human-like\nreasoning, aligning with human behavior in economic experiments, surveys, and\npolitical discourse. This has led many to propose that LLMs can be used as\nsurrogates for humans in social science research. However, LLMs differ\nfundamentally from humans, relying on probabilistic patterns, absent the\nembodied experiences or survival objectives that shape human cognition. We\nassess the reasoning depth of LLMs using the 11-20 money request game. Almost\nall advanced approaches fail to replicate human behavior distributions across\nmany models, except in one case involving fine-tuning using a substantial\namount of human behavior data. Causes of failure are diverse, relating to input\nlanguage, roles, and safeguarding. These results caution against using LLMs to\nstudy human behaviors or as human surrogates."
                },
                "authors": [
                    {
                        "name": "Yuan Gao"
                    },
                    {
                        "name": "Dokyun Lee"
                    },
                    {
                        "name": "Gordon Burtch"
                    },
                    {
                        "name": "Sina Fazelpour"
                    }
                ],
                "author_detail": {
                    "name": "Sina Fazelpour"
                },
                "author": "Sina Fazelpour",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19586v1",
                "updated": "2024-10-25T14:28:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    28,
                    20,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T14:28:20Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    28,
                    20,
                    4,
                    299,
                    0
                ],
                "title": "Diverse Sign Language Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diverse Sign Language Translation"
                },
                "summary": "Like spoken languages, a single sign language expression could correspond to\nmultiple valid textual interpretations. Hence, learning a rigid one-to-one\nmapping for sign language translation (SLT) models might be inadequate,\nparticularly in the case of limited data. In this work, we introduce a Diverse\nSign Language Translation (DivSLT) task, aiming to generate diverse yet\naccurate translations for sign language videos. Firstly, we employ large\nlanguage models (LLM) to generate multiple references for the widely-used\nCSL-Daily and PHOENIX14T SLT datasets. Here, native speakers are only invited\nto touch up inaccurate references, thus significantly improving the annotation\nefficiency. Secondly, we provide a benchmark model to spur research in this\ntask. Specifically, we investigate multi-reference training strategies to\nenable our DivSLT model to achieve diverse translations. Then, to enhance\ntranslation accuracy, we employ the max-reward-driven reinforcement learning\nobjective that maximizes the reward of the translated result. Additionally, we\nutilize multiple metrics to assess the accuracy, diversity, and semantic\nprecision of the DivSLT task. Experimental results on the enriched datasets\ndemonstrate that our DivSLT method achieves not only better translation\nperformance but also diverse translation results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Like spoken languages, a single sign language expression could correspond to\nmultiple valid textual interpretations. Hence, learning a rigid one-to-one\nmapping for sign language translation (SLT) models might be inadequate,\nparticularly in the case of limited data. In this work, we introduce a Diverse\nSign Language Translation (DivSLT) task, aiming to generate diverse yet\naccurate translations for sign language videos. Firstly, we employ large\nlanguage models (LLM) to generate multiple references for the widely-used\nCSL-Daily and PHOENIX14T SLT datasets. Here, native speakers are only invited\nto touch up inaccurate references, thus significantly improving the annotation\nefficiency. Secondly, we provide a benchmark model to spur research in this\ntask. Specifically, we investigate multi-reference training strategies to\nenable our DivSLT model to achieve diverse translations. Then, to enhance\ntranslation accuracy, we employ the max-reward-driven reinforcement learning\nobjective that maximizes the reward of the translated result. Additionally, we\nutilize multiple metrics to assess the accuracy, diversity, and semantic\nprecision of the DivSLT task. Experimental results on the enriched datasets\ndemonstrate that our DivSLT method achieves not only better translation\nperformance but also diverse translation results."
                },
                "authors": [
                    {
                        "name": "Xin Shen"
                    },
                    {
                        "name": "Lei Shen"
                    },
                    {
                        "name": "Shaozu Yuan"
                    },
                    {
                        "name": "Heming Du"
                    },
                    {
                        "name": "Haiyang Sun"
                    },
                    {
                        "name": "Xin Yu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Yu"
                },
                "author": "Xin Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14013v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14013v2",
                "updated": "2024-10-25T14:27:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    27,
                    24,
                    4,
                    299,
                    0
                ],
                "published": "2023-12-21T16:41:15Z",
                "published_parsed": [
                    2023,
                    12,
                    21,
                    16,
                    41,
                    15,
                    3,
                    355,
                    0
                ],
                "title": "Two-Stage Pseudo Maximum Likelihood Estimation of Semiparametric\n  Copula-based Regression Models for Semi-Competing Risks Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-Stage Pseudo Maximum Likelihood Estimation of Semiparametric\n  Copula-based Regression Models for Semi-Competing Risks Data"
                },
                "summary": "We propose a two-stage estimation procedure for a copula-based model with\nsemi-competing risks data, where the non-terminal event is subject to dependent\ncensoring by the terminal event, and both events are subject to independent\ncensoring. With a copula-based model, the marginal survival functions of\nindividual event times are specified by semiparametric transformation models,\nand the dependence between the bivariate event times is specified by a\nparametric copula function. For the estimation procedure, in the first stage,\nthe parameters associated with the marginal of the terminal event are estimated\nusing only the corresponding observed outcomes, and in the second stage, the\nmarginal parameters for the non-terminal event time and the copula parameter\nare estimated together via maximizing a pseudo-likelihood function based on the\njoint distribution of the bivariate event times. We derived the asymptotic\nproperties of the proposed estimator and provided an analytic variance\nestimator for inference. Through simulation studies, we showed that our\napproach leads to consistent estimates with less computational cost and more\nrobustness than the one-stage procedure developed in Chen (2012), where all\nparameters were estimated simultaneously. In addition, our approach\ndemonstrates more desirable finite-sample performances over another existing\ntwo-stage estimation method proposed in Zhu et al. (2021). An R package\nPMLE4SCR is developed to implement our proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a two-stage estimation procedure for a copula-based model with\nsemi-competing risks data, where the non-terminal event is subject to dependent\ncensoring by the terminal event, and both events are subject to independent\ncensoring. With a copula-based model, the marginal survival functions of\nindividual event times are specified by semiparametric transformation models,\nand the dependence between the bivariate event times is specified by a\nparametric copula function. For the estimation procedure, in the first stage,\nthe parameters associated with the marginal of the terminal event are estimated\nusing only the corresponding observed outcomes, and in the second stage, the\nmarginal parameters for the non-terminal event time and the copula parameter\nare estimated together via maximizing a pseudo-likelihood function based on the\njoint distribution of the bivariate event times. We derived the asymptotic\nproperties of the proposed estimator and provided an analytic variance\nestimator for inference. Through simulation studies, we showed that our\napproach leads to consistent estimates with less computational cost and more\nrobustness than the one-stage procedure developed in Chen (2012), where all\nparameters were estimated simultaneously. In addition, our approach\ndemonstrates more desirable finite-sample performances over another existing\ntwo-stage estimation method proposed in Zhu et al. (2021). An R package\nPMLE4SCR is developed to implement our proposed method."
                },
                "authors": [
                    {
                        "name": "Sakie J. Arachchige"
                    },
                    {
                        "name": "Xinyuan Chen"
                    },
                    {
                        "name": "Qian M. Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Qian M. Zhou"
                },
                "author": "Qian M. Zhou",
                "arxiv_doi": "10.1007/s10985-024-09640-z",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10985-024-09640-z",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.14013v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14013v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "23 pages, 1 figure",
                "arxiv_journal_ref": "Lifetime Data Analysis (2024)",
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09450v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09450v2",
                "updated": "2024-10-25T14:27:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    27,
                    23,
                    4,
                    299,
                    0
                ],
                "published": "2024-07-12T17:34:03Z",
                "published_parsed": [
                    2024,
                    7,
                    12,
                    17,
                    34,
                    3,
                    4,
                    194,
                    0
                ],
                "title": "Human-like Episodic Memory for Infinite Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-like Episodic Memory for Infinite Context LLMs"
                },
                "summary": "Large language models (LLMs) have shown remarkable capabilities, but still\nstruggle with processing extensive contexts, limiting their ability to maintain\ncoherence and accuracy over long sequences. In contrast, the human brain excels\nat organising and retrieving episodic experiences across vast temporal scales,\nspanning a lifetime. In this work, we introduce EM-LLM, a novel approach that\nintegrates key aspects of human episodic memory and event cognition into LLMs\nwith no fine-tuning, enabling them to handle practically infinite context\nlengths while maintaining computational efficiency. EM-LLM organises sequences\nof tokens into coherent episodic events using a combination of Bayesian\nsurprise and graph-theoretic boundary refinement in an online fashion. When\nneeded, these events are retrieved through a two-stage memory process,\ncombining similarity-based and temporally contiguous retrieval for efficient\nand human-like access to relevant information. Experiments on the LongBench and\nInfiniteBench benchmarks demonstrate EM-LLM's superior performance,\nconsistently outperforming the state-of-the-art retrieval model InfLLM across\nvarious baseline LLMs. In addition, EM-LLM outperforms its popular counterpart,\nRAG, in a wide range of tasks, while requiring similar resources. Notably,\nEM-LLM's performance even surpasses full-context models in most tasks, while\nsuccessfully performing retrieval across 10 million tokens - a scale\ncomputationally infeasible for such models. Finally, our analysis reveals\nstrong correlations between EM-LLM's event segmentation and human-perceived\nevents, suggesting a bridge between this artificial system and its biological\ncounterpart, thereby offering a novel computational framework for exploring\nhuman memory mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable capabilities, but still\nstruggle with processing extensive contexts, limiting their ability to maintain\ncoherence and accuracy over long sequences. In contrast, the human brain excels\nat organising and retrieving episodic experiences across vast temporal scales,\nspanning a lifetime. In this work, we introduce EM-LLM, a novel approach that\nintegrates key aspects of human episodic memory and event cognition into LLMs\nwith no fine-tuning, enabling them to handle practically infinite context\nlengths while maintaining computational efficiency. EM-LLM organises sequences\nof tokens into coherent episodic events using a combination of Bayesian\nsurprise and graph-theoretic boundary refinement in an online fashion. When\nneeded, these events are retrieved through a two-stage memory process,\ncombining similarity-based and temporally contiguous retrieval for efficient\nand human-like access to relevant information. Experiments on the LongBench and\nInfiniteBench benchmarks demonstrate EM-LLM's superior performance,\nconsistently outperforming the state-of-the-art retrieval model InfLLM across\nvarious baseline LLMs. In addition, EM-LLM outperforms its popular counterpart,\nRAG, in a wide range of tasks, while requiring similar resources. Notably,\nEM-LLM's performance even surpasses full-context models in most tasks, while\nsuccessfully performing retrieval across 10 million tokens - a scale\ncomputationally infeasible for such models. Finally, our analysis reveals\nstrong correlations between EM-LLM's event segmentation and human-perceived\nevents, suggesting a bridge between this artificial system and its biological\ncounterpart, thereby offering a novel computational framework for exploring\nhuman memory mechanisms."
                },
                "authors": [
                    {
                        "name": "Zafeirios Fountas"
                    },
                    {
                        "name": "Martin A Benfeghoul"
                    },
                    {
                        "name": "Adnan Oomerjee"
                    },
                    {
                        "name": "Fenia Christopoulou"
                    },
                    {
                        "name": "Gerasimos Lampouras"
                    },
                    {
                        "name": "Haitham Bou-Ammar"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09450v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09450v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.00953v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.00953v4",
                "updated": "2024-10-25T14:20:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    20,
                    32,
                    4,
                    299,
                    0
                ],
                "published": "2024-03-01T20:06:39Z",
                "published_parsed": [
                    2024,
                    3,
                    1,
                    20,
                    6,
                    39,
                    4,
                    61,
                    0
                ],
                "title": "AutoRD: An Automatic and End-to-End System for Rare Disease Knowledge\n  Graph Construction Based on Ontologies-enhanced Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoRD: An Automatic and End-to-End System for Rare Disease Knowledge\n  Graph Construction Based on Ontologies-enhanced Large Language Models"
                },
                "summary": "Rare diseases affect millions worldwide but often face limited research focus\ndue to their low prevalence. This results in prolonged diagnoses and a lack of\napproved therapies. Recent advancements in Large Language Models (LLMs) have\nshown promise in automating the extraction of medical information, offering\npotential to improve medical diagnosis and management. However, most LLMs lack\nprofessional medical knowledge, especially concerning rare diseases, and\nstruggle to handle the latest rare disease information. They also cannot\neffectively manage rare disease data and are not directly suitable for\ndiagnosis and management tasks. Our objective is to create an end-to-end system\ncalled AutoRD, which automates the extraction of information from medical texts\nabout rare diseases, focusing on entities and their relations. AutoRD\nintegrates up-to-date structured knowledge and demonstrates superior\nperformance in rare disease extraction tasks. We conduct various experiments to\nevaluate AutoRD's performance, aiming to surpass common LLMs and traditional\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rare diseases affect millions worldwide but often face limited research focus\ndue to their low prevalence. This results in prolonged diagnoses and a lack of\napproved therapies. Recent advancements in Large Language Models (LLMs) have\nshown promise in automating the extraction of medical information, offering\npotential to improve medical diagnosis and management. However, most LLMs lack\nprofessional medical knowledge, especially concerning rare diseases, and\nstruggle to handle the latest rare disease information. They also cannot\neffectively manage rare disease data and are not directly suitable for\ndiagnosis and management tasks. Our objective is to create an end-to-end system\ncalled AutoRD, which automates the extraction of information from medical texts\nabout rare diseases, focusing on entities and their relations. AutoRD\nintegrates up-to-date structured knowledge and demonstrates superior\nperformance in rare disease extraction tasks. We conduct various experiments to\nevaluate AutoRD's performance, aiming to surpass common LLMs and traditional\nmethods."
                },
                "authors": [
                    {
                        "name": "Lang Cao"
                    },
                    {
                        "name": "Jimeng Sun"
                    },
                    {
                        "name": "Adam Cross"
                    }
                ],
                "author_detail": {
                    "name": "Adam Cross"
                },
                "author": "Adam Cross",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.00953v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.00953v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18012v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18012v2",
                "updated": "2024-10-25T14:19:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    19,
                    22,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-23T16:40:38Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    40,
                    38,
                    2,
                    297,
                    0
                ],
                "title": "MiniFed : Integrating LLM-based Agentic-Workflow for Simulating FOMC\n  Meeting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniFed : Integrating LLM-based Agentic-Workflow for Simulating FOMC\n  Meeting"
                },
                "summary": "The Federal Funds rate in the United States plays a significant role in both\ndomestic and international financial markets. However, research has\npredominantly focused on the effects of adjustments to the Federal Funds rate\nrather than on the decision-making process itself. Recent advancements in large\nlanguage models(LLMs) offer a potential method for reconstructing the original\nFOMC meetings, which are responsible for setting the Federal Funds rate. In\nthis paper, we propose a five-stage FOMC meeting simulation framework, MiniFed,\nwhich employs LLM agents to simulate real-world FOMC meeting members and\noptimize the FOMC structure. This framework effectively revitalizes the FOMC\nmeeting process and facilitates projections of the Federal Funds rate.\nExperimental results demonstrate that our proposed MiniFed framework achieves\nboth high accuracy in Federal Funds rate projections and behavioral alignment\nwith the agents' real-world counterparts. Given that few studies have focused\non employing LLM agents to simulate large-scale real-world conferences, our\nwork can serve as a benchmark for future developments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Federal Funds rate in the United States plays a significant role in both\ndomestic and international financial markets. However, research has\npredominantly focused on the effects of adjustments to the Federal Funds rate\nrather than on the decision-making process itself. Recent advancements in large\nlanguage models(LLMs) offer a potential method for reconstructing the original\nFOMC meetings, which are responsible for setting the Federal Funds rate. In\nthis paper, we propose a five-stage FOMC meeting simulation framework, MiniFed,\nwhich employs LLM agents to simulate real-world FOMC meeting members and\noptimize the FOMC structure. This framework effectively revitalizes the FOMC\nmeeting process and facilitates projections of the Federal Funds rate.\nExperimental results demonstrate that our proposed MiniFed framework achieves\nboth high accuracy in Federal Funds rate projections and behavioral alignment\nwith the agents' real-world counterparts. Given that few studies have focused\non employing LLM agents to simulate large-scale real-world conferences, our\nwork can serve as a benchmark for future developments."
                },
                "authors": [
                    {
                        "name": "Sungil Seok"
                    },
                    {
                        "name": "Shuide Wen"
                    },
                    {
                        "name": "Qiyuan Yang"
                    },
                    {
                        "name": "Juan Feng"
                    },
                    {
                        "name": "Wenming Yang"
                    }
                ],
                "author_detail": {
                    "name": "Wenming Yang"
                },
                "author": "Wenming Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18012v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18012v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15999v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15999v2",
                "updated": "2024-10-25T14:17:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    17,
                    28,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-21T13:30:47Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    13,
                    30,
                    47,
                    0,
                    295,
                    0
                ],
                "title": "Steering Knowledge Selection Behaviours in LLMs via SAE-Based\n  Representation Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering Knowledge Selection Behaviours in LLMs via SAE-Based\n  Representation Engineering"
                },
                "summary": "Large language models (LLMs) can store a significant amount of factual\nknowledge in their parameters. However, their parametric knowledge may conflict\nwith the information provided in the context -- this phenomenon, known as\n\\emph{context-memory knowledge conflicts}, can lead to undesirable model\nbehaviour, such as reliance on outdated or incorrect information. Analysing the\ninternal activations of LLMs, we find that they can internally register the\nsignals of knowledge conflict at mid-layers. Such signals allow us to detect\nwhether a knowledge conflict occurs and use \\emph{inference-time} intervention\nstrategies to resolve it. In this work, we propose \\textsc{SpARE}, a\n\\emph{training-free} representation engineering method that uses pre-trained\nsparse auto-encoders (SAEs) to control the knowledge selection behaviour of\nLLMs. \\textsc{SpARE} identifies the functional features that control the\nknowledge selection behaviours and applies them to edit the internal\nactivations of LLMs at inference time. Our experimental results show that\n\\textsc{SpARE} can effectively control the usage of either knowledge source to\nresolve knowledge conflict in open-domain question-answering tasks, surpassing\nexisting representation engineering methods ($+10\\%$) as well as contrastive\ndecoding methods ($+15\\%$).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can store a significant amount of factual\nknowledge in their parameters. However, their parametric knowledge may conflict\nwith the information provided in the context -- this phenomenon, known as\n\\emph{context-memory knowledge conflicts}, can lead to undesirable model\nbehaviour, such as reliance on outdated or incorrect information. Analysing the\ninternal activations of LLMs, we find that they can internally register the\nsignals of knowledge conflict at mid-layers. Such signals allow us to detect\nwhether a knowledge conflict occurs and use \\emph{inference-time} intervention\nstrategies to resolve it. In this work, we propose \\textsc{SpARE}, a\n\\emph{training-free} representation engineering method that uses pre-trained\nsparse auto-encoders (SAEs) to control the knowledge selection behaviour of\nLLMs. \\textsc{SpARE} identifies the functional features that control the\nknowledge selection behaviours and applies them to edit the internal\nactivations of LLMs at inference time. Our experimental results show that\n\\textsc{SpARE} can effectively control the usage of either knowledge source to\nresolve knowledge conflict in open-domain question-answering tasks, surpassing\nexisting representation engineering methods ($+10\\%$) as well as contrastive\ndecoding methods ($+15\\%$)."
                },
                "authors": [
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Giwon Hong"
                    },
                    {
                        "name": "Xiaotang Du"
                    },
                    {
                        "name": "Aryo Pradipta Gema"
                    },
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Xuanli He"
                    },
                    {
                        "name": "Kam-Fai Wong"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15999v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15999v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19573v1",
                "updated": "2024-10-25T14:10:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    10,
                    17,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T14:10:17Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    10,
                    17,
                    4,
                    299,
                    0
                ],
                "title": "FastPCI: Motion-Structure Guided Fast Point Cloud Frame Interpolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastPCI: Motion-Structure Guided Fast Point Cloud Frame Interpolation"
                },
                "summary": "Point cloud frame interpolation is a challenging task that involves accurate\nscene flow estimation across frames and maintaining the geometry structure.\nPrevailing techniques often rely on pre-trained motion estimators or intensive\ntesting-time optimization, resulting in compromised interpolation accuracy or\nprolonged inference. This work presents FastPCI that introduces Pyramid\nConvolution-Transformer architecture for point cloud frame interpolation. Our\nhybrid Convolution-Transformer improves the local and long-range feature\nlearning, while the pyramid network offers multilevel features and reduces the\ncomputation. In addition, FastPCI proposes a unique Dual-Direction\nMotion-Structure block for more accurate scene flow estimation. Our design is\nmotivated by two facts: (1) accurate scene flow preserves 3D structure, and (2)\npoint cloud at the previous timestep should be reconstructable using reverse\nmotion from future timestep. Extensive experiments show that FastPCI\nsignificantly outperforms the state-of-the-art PointINet and NeuralPCI with\nnotable gains (e.g. 26.6% and 18.3% reduction in Chamfer Distance in KITTI),\nwhile being more than 10x and 600x faster, respectively. Code is available at\nhttps://github.com/genuszty/FastPCI",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point cloud frame interpolation is a challenging task that involves accurate\nscene flow estimation across frames and maintaining the geometry structure.\nPrevailing techniques often rely on pre-trained motion estimators or intensive\ntesting-time optimization, resulting in compromised interpolation accuracy or\nprolonged inference. This work presents FastPCI that introduces Pyramid\nConvolution-Transformer architecture for point cloud frame interpolation. Our\nhybrid Convolution-Transformer improves the local and long-range feature\nlearning, while the pyramid network offers multilevel features and reduces the\ncomputation. In addition, FastPCI proposes a unique Dual-Direction\nMotion-Structure block for more accurate scene flow estimation. Our design is\nmotivated by two facts: (1) accurate scene flow preserves 3D structure, and (2)\npoint cloud at the previous timestep should be reconstructable using reverse\nmotion from future timestep. Extensive experiments show that FastPCI\nsignificantly outperforms the state-of-the-art PointINet and NeuralPCI with\nnotable gains (e.g. 26.6% and 18.3% reduction in Chamfer Distance in KITTI),\nwhile being more than 10x and 600x faster, respectively. Code is available at\nhttps://github.com/genuszty/FastPCI"
                },
                "authors": [
                    {
                        "name": "Tianyu Zhang"
                    },
                    {
                        "name": "Guocheng Qian"
                    },
                    {
                        "name": "Jin Xie"
                    },
                    {
                        "name": "Jian Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Yang"
                },
                "author": "Jian Yang",
                "arxiv_comment": "To appear in ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19572v1",
                "updated": "2024-10-25T14:07:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    7,
                    53,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T14:07:53Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    7,
                    53,
                    4,
                    299,
                    0
                ],
                "title": "ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems using large language models\n(LLMs) often generate inaccurate responses due to the retrieval of irrelevant\nor loosely related information. Existing methods, which operate at the document\nlevel, fail to effectively filter out such content. We propose LLM-driven chunk\nfiltering, ChunkRAG, a framework that enhances RAG systems by evaluating and\nfiltering retrieved information at the chunk level. Our approach employs\nsemantic chunking to divide documents into coherent sections and utilizes\nLLM-based relevance scoring to assess each chunk's alignment with the user's\nquery. By filtering out less pertinent chunks before the generation phase, we\nsignificantly reduce hallucinations and improve factual accuracy. Experiments\nshow that our method outperforms existing RAG models, achieving higher accuracy\non tasks requiring precise information retrieval. This advancement enhances the\nreliability of RAG systems, making them particularly beneficial for\napplications like fact-checking and multi-hop reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems using large language models\n(LLMs) often generate inaccurate responses due to the retrieval of irrelevant\nor loosely related information. Existing methods, which operate at the document\nlevel, fail to effectively filter out such content. We propose LLM-driven chunk\nfiltering, ChunkRAG, a framework that enhances RAG systems by evaluating and\nfiltering retrieved information at the chunk level. Our approach employs\nsemantic chunking to divide documents into coherent sections and utilizes\nLLM-based relevance scoring to assess each chunk's alignment with the user's\nquery. By filtering out less pertinent chunks before the generation phase, we\nsignificantly reduce hallucinations and improve factual accuracy. Experiments\nshow that our method outperforms existing RAG models, achieving higher accuracy\non tasks requiring precise information retrieval. This advancement enhances the\nreliability of RAG systems, making them particularly beneficial for\napplications like fact-checking and multi-hop reasoning."
                },
                "authors": [
                    {
                        "name": "Ritvik Aggarwal Ishneet Sukhvinder Singh Ibrahim Allahverdiyev"
                    },
                    {
                        "name": "Muhammad Taha"
                    },
                    {
                        "name": "Aslihan Akalin"
                    },
                    {
                        "name": "Kevin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Zhu"
                },
                "author": "Kevin Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15302v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15302v2",
                "updated": "2024-10-25T14:05:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    5,
                    58,
                    4,
                    299,
                    0
                ],
                "published": "2024-03-22T15:52:00Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    15,
                    52,
                    0,
                    4,
                    82,
                    0
                ],
                "title": "Optimal Survival Analyses With Prevalent and Incident Patients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Survival Analyses With Prevalent and Incident Patients"
                },
                "summary": "Period-prevalent cohorts are often used for their cost-saving potential in\nepidemiological studies of survival outcomes. Under this design, prevalent\npatients allow for evaluations of long-term survival outcomes without the need\nfor long follow-up, whereas incident patients allow for evaluations of\nshort-term survival outcomes without the issue of left-truncation. In most\nperiod-prevalent survival analyses from the existing literature, patients have\nbeen recruited to achieve an overall sample size, with little attention given\nto the relative frequencies of prevalent and incident patients and their\nstatistical implications. Furthermore, there are no existing methods available\nto rigorously quantify the impact of these relative frequencies on estimation\nand inference and incorporate this information into study design strategies. To\naddress these gaps, we develop an approach to identify the optimal mix of\nprevalent and incident patients that maximizes precision over the entire\nestimated survival curve, subject to a flexible weighting scheme. In addition,\nwe prove that inference based on the weighted log-rank test or Cox proportional\nhazards model is most powerful with an entirely prevalent or incident cohort,\nand we derive theoretical formulas to determine the optimal choice. Simulations\nconfirm the validity of the proposed optimization criteria and show that\nsubstantial efficiency gains can be achieved by recruiting the optimal mix of\nprevalent and incident patients. The proposed methods are applied to assess\nwaitlist outcomes among kidney transplant candidates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Period-prevalent cohorts are often used for their cost-saving potential in\nepidemiological studies of survival outcomes. Under this design, prevalent\npatients allow for evaluations of long-term survival outcomes without the need\nfor long follow-up, whereas incident patients allow for evaluations of\nshort-term survival outcomes without the issue of left-truncation. In most\nperiod-prevalent survival analyses from the existing literature, patients have\nbeen recruited to achieve an overall sample size, with little attention given\nto the relative frequencies of prevalent and incident patients and their\nstatistical implications. Furthermore, there are no existing methods available\nto rigorously quantify the impact of these relative frequencies on estimation\nand inference and incorporate this information into study design strategies. To\naddress these gaps, we develop an approach to identify the optimal mix of\nprevalent and incident patients that maximizes precision over the entire\nestimated survival curve, subject to a flexible weighting scheme. In addition,\nwe prove that inference based on the weighted log-rank test or Cox proportional\nhazards model is most powerful with an entirely prevalent or incident cohort,\nand we derive theoretical formulas to determine the optimal choice. Simulations\nconfirm the validity of the proposed optimization criteria and show that\nsubstantial efficiency gains can be achieved by recruiting the optimal mix of\nprevalent and incident patients. The proposed methods are applied to assess\nwaitlist outcomes among kidney transplant candidates."
                },
                "authors": [
                    {
                        "name": "Nicholas Hartman"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Hartman"
                },
                "author": "Nicholas Hartman",
                "arxiv_comment": "This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this article is\n  published in Lifetime Data Analysis, and is available online at\n  https://doi.org/10.1007/s10985-024-09639-6",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15302v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15302v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12819v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12819v2",
                "updated": "2024-10-25T14:01:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    1,
                    37,
                    4,
                    299,
                    0
                ],
                "published": "2024-03-19T15:18:03Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    15,
                    18,
                    3,
                    1,
                    79,
                    0
                ],
                "title": "Two-dimensional inference of divertor plasma characteristics:\n  advancements to a multi-instrument Bayesian analysis system",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-dimensional inference of divertor plasma characteristics:\n  advancements to a multi-instrument Bayesian analysis system"
                },
                "summary": "An integrated data analysis system based on Bayesian inference has been\ndeveloped for application to data from multiple diagnostics over the\ntwo-dimensional cross-section of tokamak divertors. Tests of the divertor\nmulti-instrument Bayesian analysis system (D-MIBAS) on a synthetic data set\n(including realistic experimental uncertainties) generated from SOLPS-ITER\npredictions of the MAST-U divertor have been performed. The resulting inference\nwas within 6\\%, 5\\% and 30\\% median absolute percentage error of the\nSOLPS-predicted electron temperature, electron density and neutral atomic\nhydrogen density, respectively, across a two-dimensional poloidal cross-section\nof the MAST-U Super-X outer divertor.\n  To accommodate molecular contributions to Balmer emission, an advanced\nemission model has been developed which is shown to be crucial for inference\naccuracy. Our D-MIBAS system utilises a mesh aligned to poloidal magnetic\nflux-surfaces, throughout the divertor, with plasma parameters assigned to each\nmesh vertex and collectively considered in the inference. This allowed\ncomprehensive forward models to multiple diagnostics and the inclusion of\nexpected physics. This is shown to be important for inference precision when\nincluding molecular contributions to Balmer emission. These developments pave\nthe way for accurate two-dimensional electron temperature, electron density and\nneutral atomic hydrogen density inferences for MAST-U divertor experimental\ndata for the first time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An integrated data analysis system based on Bayesian inference has been\ndeveloped for application to data from multiple diagnostics over the\ntwo-dimensional cross-section of tokamak divertors. Tests of the divertor\nmulti-instrument Bayesian analysis system (D-MIBAS) on a synthetic data set\n(including realistic experimental uncertainties) generated from SOLPS-ITER\npredictions of the MAST-U divertor have been performed. The resulting inference\nwas within 6\\%, 5\\% and 30\\% median absolute percentage error of the\nSOLPS-predicted electron temperature, electron density and neutral atomic\nhydrogen density, respectively, across a two-dimensional poloidal cross-section\nof the MAST-U Super-X outer divertor.\n  To accommodate molecular contributions to Balmer emission, an advanced\nemission model has been developed which is shown to be crucial for inference\naccuracy. Our D-MIBAS system utilises a mesh aligned to poloidal magnetic\nflux-surfaces, throughout the divertor, with plasma parameters assigned to each\nmesh vertex and collectively considered in the inference. This allowed\ncomprehensive forward models to multiple diagnostics and the inclusion of\nexpected physics. This is shown to be important for inference precision when\nincluding molecular contributions to Balmer emission. These developments pave\nthe way for accurate two-dimensional electron temperature, electron density and\nneutral atomic hydrogen density inferences for MAST-U divertor experimental\ndata for the first time."
                },
                "authors": [
                    {
                        "name": "Daniel Greenhouse"
                    },
                    {
                        "name": "Chris Bowman"
                    },
                    {
                        "name": "Bruce Lipschultz"
                    },
                    {
                        "name": "Kevin Verhaegh"
                    },
                    {
                        "name": "James Harrison"
                    },
                    {
                        "name": "Alexandre Fil"
                    }
                ],
                "author_detail": {
                    "name": "Alexandre Fil"
                },
                "author": "Alexandre Fil",
                "arxiv_comment": "22 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12819v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12819v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14807v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14807v4",
                "updated": "2024-10-25T13:34:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    13,
                    34,
                    14,
                    4,
                    299,
                    0
                ],
                "published": "2024-02-22T18:58:27Z",
                "published_parsed": [
                    2024,
                    2,
                    22,
                    18,
                    58,
                    27,
                    3,
                    53,
                    0
                ],
                "title": "A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit\n  Tasks in Public Health",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit\n  Tasks in Public Health"
                },
                "summary": "Restless multi-armed bandits (RMAB) have demonstrated success in optimizing\nresource allocation for large beneficiary populations in public health\nsettings. Unfortunately, RMAB models lack flexibility to adapt to evolving\npublic health policy priorities. Concurrently, Large Language Models (LLMs)\nhave emerged as adept automated planners across domains of robotic control and\nnavigation. In this paper, we propose a Decision Language Model (DLM) for\nRMABs, enabling dynamic fine-tuning of RMAB policies in public health settings\nusing human-language commands. We propose using LLMs as automated planners to\n(1) interpret human policy preference prompts, (2) propose reward functions as\ncode for a multi-agent RMAB environment, and (3) iterate on the generated\nreward functions using feedback from grounded RMAB simulations. We illustrate\nthe application of DLM in collaboration with ARMMAN, an India-based non-profit\npromoting preventative care for pregnant mothers, that currently relies on RMAB\npolicies to optimally allocate health worker calls to low-resource populations.\nWe conduct a technology demonstration in simulation using the Gemini Pro model,\nshowing DLM can dynamically shape policy outcomes using only human prompts as\ninput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Restless multi-armed bandits (RMAB) have demonstrated success in optimizing\nresource allocation for large beneficiary populations in public health\nsettings. Unfortunately, RMAB models lack flexibility to adapt to evolving\npublic health policy priorities. Concurrently, Large Language Models (LLMs)\nhave emerged as adept automated planners across domains of robotic control and\nnavigation. In this paper, we propose a Decision Language Model (DLM) for\nRMABs, enabling dynamic fine-tuning of RMAB policies in public health settings\nusing human-language commands. We propose using LLMs as automated planners to\n(1) interpret human policy preference prompts, (2) propose reward functions as\ncode for a multi-agent RMAB environment, and (3) iterate on the generated\nreward functions using feedback from grounded RMAB simulations. We illustrate\nthe application of DLM in collaboration with ARMMAN, an India-based non-profit\npromoting preventative care for pregnant mothers, that currently relies on RMAB\npolicies to optimally allocate health worker calls to low-resource populations.\nWe conduct a technology demonstration in simulation using the Gemini Pro model,\nshowing DLM can dynamically shape policy outcomes using only human prompts as\ninput."
                },
                "authors": [
                    {
                        "name": "Nikhil Behari"
                    },
                    {
                        "name": "Edwin Zhang"
                    },
                    {
                        "name": "Yunfan Zhao"
                    },
                    {
                        "name": "Aparna Taneja"
                    },
                    {
                        "name": "Dheeraj Nagaraj"
                    },
                    {
                        "name": "Milind Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Milind Tambe"
                },
                "author": "Milind Tambe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14807v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14807v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05827v2",
                "updated": "2024-10-25T13:24:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    13,
                    24,
                    58,
                    4,
                    299,
                    0
                ],
                "published": "2024-02-08T17:06:45Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    17,
                    6,
                    45,
                    3,
                    39,
                    0
                ],
                "title": "On the Robustness of Editing Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Robustness of Editing Large Language Models"
                },
                "summary": "Large language models (LLMs) have played a pivotal role in building\ncommunicative AI, yet they encounter the challenge of efficient updates. Model\nediting enables the manipulation of specific knowledge memories and the\nbehavior of language generation without retraining. However, the robustness of\nmodel editing remains an open question. This work seeks to understand the\nstrengths and limitations of editing methods, facilitating practical\napplications of communicative AI. We focus on three key research questions.\nRQ1: Can edited LLMs behave consistently resembling communicative AI in\nrealistic situations? RQ2: To what extent does the rephrasing of prompts lead\nLLMs to deviate from the edited knowledge memory? RQ3: Which knowledge features\nare correlated with the performance and robustness of editing? Our empirical\nstudies uncover a substantial disparity between existing editing methods and\nthe practical application of LLMs. On rephrased prompts that are flexible but\ncommon in realistic applications, the performance of editing experiences a\nsignificant decline. Further analysis shows that more popular knowledge is\nmemorized better, easier to recall, and more challenging to edit effectively.\nCode is publicly available at https://github.com/xbmxb/edit_analysis .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have played a pivotal role in building\ncommunicative AI, yet they encounter the challenge of efficient updates. Model\nediting enables the manipulation of specific knowledge memories and the\nbehavior of language generation without retraining. However, the robustness of\nmodel editing remains an open question. This work seeks to understand the\nstrengths and limitations of editing methods, facilitating practical\napplications of communicative AI. We focus on three key research questions.\nRQ1: Can edited LLMs behave consistently resembling communicative AI in\nrealistic situations? RQ2: To what extent does the rephrasing of prompts lead\nLLMs to deviate from the edited knowledge memory? RQ3: Which knowledge features\nare correlated with the performance and robustness of editing? Our empirical\nstudies uncover a substantial disparity between existing editing methods and\nthe practical application of LLMs. On rephrased prompts that are flexible but\ncommon in realistic applications, the performance of editing experiences a\nsignificant decline. Further analysis shows that more popular knowledge is\nmemorized better, easier to recall, and more challenging to edit effectively.\nCode is publicly available at https://github.com/xbmxb/edit_analysis ."
                },
                "authors": [
                    {
                        "name": "Xinbei Ma"
                    },
                    {
                        "name": "Tianjie Ju"
                    },
                    {
                        "name": "Jiyang Qiu"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Lifeng Liu"
                    },
                    {
                        "name": "Yulong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yulong Wang"
                },
                "author": "Yulong Wang",
                "arxiv_comment": "EMNLP2024. Code is publicly available at\n  https://github.com/xbmxb/edit_analysis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19548v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19548v1",
                "updated": "2024-10-25T13:20:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    13,
                    20,
                    40,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T13:20:40Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    13,
                    20,
                    40,
                    4,
                    299,
                    0
                ],
                "title": "FLiP: Privacy-Preserving Federated Learning based on the Principle of\n  Least Privileg",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLiP: Privacy-Preserving Federated Learning based on the Principle of\n  Least Privileg"
                },
                "summary": "Federated Learning (FL) allows users to share knowledge instead of raw data\nto train a model with high accuracy. Unfortunately, during the training, users\nlose control over the knowledge shared, which causes serious data privacy\nissues. We hold that users are only willing and need to share the essential\nknowledge to the training task to obtain the FL model with high accuracy.\nHowever, existing efforts cannot help users minimize the shared knowledge\naccording to the user intention in the FL training procedure. This work\nproposes FLiP, which aims to bring the principle of least privilege (PoLP) to\nFL training. The key design of FLiP is applying elaborate information reduction\non the training data through a local-global dataset distillation design. We\nmeasure the privacy performance through attribute inference and membership\ninference attacks. Extensive experiments show that FLiP strikes a good balance\nbetween model accuracy and privacy protection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) allows users to share knowledge instead of raw data\nto train a model with high accuracy. Unfortunately, during the training, users\nlose control over the knowledge shared, which causes serious data privacy\nissues. We hold that users are only willing and need to share the essential\nknowledge to the training task to obtain the FL model with high accuracy.\nHowever, existing efforts cannot help users minimize the shared knowledge\naccording to the user intention in the FL training procedure. This work\nproposes FLiP, which aims to bring the principle of least privilege (PoLP) to\nFL training. The key design of FLiP is applying elaborate information reduction\non the training data through a local-global dataset distillation design. We\nmeasure the privacy performance through attribute inference and membership\ninference attacks. Extensive experiments show that FLiP strikes a good balance\nbetween model accuracy and privacy protection."
                },
                "authors": [
                    {
                        "name": "ShiMao Xu"
                    },
                    {
                        "name": "Xiaopeng Ke"
                    },
                    {
                        "name": "Xing Su"
                    },
                    {
                        "name": "Shucheng Li"
                    },
                    {
                        "name": "Hao wu"
                    },
                    {
                        "name": "Fengyuan Xu"
                    },
                    {
                        "name": "Sheng Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Zhong"
                },
                "author": "Sheng Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19548v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05858v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05858v2",
                "updated": "2024-10-25T13:19:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    13,
                    19,
                    5,
                    4,
                    299,
                    0
                ],
                "published": "2023-12-10T11:45:31Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    11,
                    45,
                    31,
                    6,
                    344,
                    0
                ],
                "title": "Causal inference and policy evaluation without a control group",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal inference and policy evaluation without a control group"
                },
                "summary": "Without a control group, the most widespread methodologies for estimating\ncausal effects cannot be applied. To fill this gap, we propose the Machine\nLearning Control Method, a new approach for causal panel analysis that\nestimates causal parameters without relying on untreated units. We formalize\nidentification within the potential outcomes framework and then provide\nestimation based on machine learning algorithms. To illustrate the practical\nrelevance of our method, we present simulation evidence, a replication study,\nand an empirical application on the impact of the COVID-19 crisis on\neducational inequality. We implement the proposed approach in the companion R\npackage MachineControl",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Without a control group, the most widespread methodologies for estimating\ncausal effects cannot be applied. To fill this gap, we propose the Machine\nLearning Control Method, a new approach for causal panel analysis that\nestimates causal parameters without relying on untreated units. We formalize\nidentification within the potential outcomes framework and then provide\nestimation based on machine learning algorithms. To illustrate the practical\nrelevance of our method, we present simulation evidence, a replication study,\nand an empirical application on the impact of the COVID-19 crisis on\neducational inequality. We implement the proposed approach in the companion R\npackage MachineControl"
                },
                "authors": [
                    {
                        "name": "Augusto Cerqua"
                    },
                    {
                        "name": "Marco Letta"
                    },
                    {
                        "name": "Fiammetta Menchetti"
                    }
                ],
                "author_detail": {
                    "name": "Fiammetta Menchetti"
                },
                "author": "Fiammetta Menchetti",
                "arxiv_comment": "40 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05858v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05858v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19544v1",
                "updated": "2024-10-25T13:16:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    13,
                    16,
                    27,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T13:16:27Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    13,
                    16,
                    27,
                    4,
                    299,
                    0
                ],
                "title": "PMM-Net: Single-stage Multi-agent Trajectory Prediction with\n  Patching-based Embedding and Explicit Modal Modulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PMM-Net: Single-stage Multi-agent Trajectory Prediction with\n  Patching-based Embedding and Explicit Modal Modulation"
                },
                "summary": "Analyzing and forecasting trajectories of agents like pedestrians plays a\npivotal role for embodied intelligent applications. The inherent indeterminacy\nof human behavior and complex social interaction among a rich variety of agents\nmake this task more challenging than common time-series forecasting. In this\nletter, we aim to explore a distinct formulation for multi-agent trajectory\nprediction framework. Specifically, we proposed a patching-based temporal\nfeature extraction module and a graph-based social feature extraction module,\nenabling effective feature extraction and cross-scenario generalization.\nMoreover, we reassess the role of social interaction and present a novel method\nbased on explicit modality modulation to integrate temporal and social\nfeatures, thereby constructing an efficient single-stage inference pipeline.\nResults on public benchmark datasets demonstrate the superior performance of\nour model compared with the state-of-the-art methods. The code is available at:\ngithub.com/TIB-K330/pmm-net.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing and forecasting trajectories of agents like pedestrians plays a\npivotal role for embodied intelligent applications. The inherent indeterminacy\nof human behavior and complex social interaction among a rich variety of agents\nmake this task more challenging than common time-series forecasting. In this\nletter, we aim to explore a distinct formulation for multi-agent trajectory\nprediction framework. Specifically, we proposed a patching-based temporal\nfeature extraction module and a graph-based social feature extraction module,\nenabling effective feature extraction and cross-scenario generalization.\nMoreover, we reassess the role of social interaction and present a novel method\nbased on explicit modality modulation to integrate temporal and social\nfeatures, thereby constructing an efficient single-stage inference pipeline.\nResults on public benchmark datasets demonstrate the superior performance of\nour model compared with the state-of-the-art methods. The code is available at:\ngithub.com/TIB-K330/pmm-net."
                },
                "authors": [
                    {
                        "name": "Huajian Liu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Kunpeng Fan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Yongzhuo Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yongzhuo Gao"
                },
                "author": "Yongzhuo Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19542v1",
                "updated": "2024-10-25T13:15:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    13,
                    15,
                    17,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T13:15:17Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    13,
                    15,
                    17,
                    4,
                    299,
                    0
                ],
                "title": "Brain-like Functional Organization within Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brain-like Functional Organization within Large Language Models"
                },
                "summary": "The human brain has long inspired the pursuit of artificial intelligence\n(AI). Recently, neuroimaging studies provide compelling evidence of alignment\nbetween the computational representation of artificial neural networks (ANNs)\nand the neural responses of the human brain to stimuli, suggesting that ANNs\nmay employ brain-like information processing strategies. While such alignment\nhas been observed across sensory modalities--visual, auditory, and\nlinguistic--much of the focus has been on the behaviors of artificial neurons\n(ANs) at the population level, leaving the functional organization of\nindividual ANs that facilitates such brain-like processes largely unexplored.\nIn this study, we bridge this gap by directly coupling sub-groups of artificial\nneurons with functional brain networks (FBNs), the foundational organizational\nstructure of the human brain. Specifically, we extract representative patterns\nfrom temporal responses of ANs in large language models (LLMs), and use them as\nfixed regressors to construct voxel-wise encoding models to predict brain\nactivity recorded by functional magnetic resonance imaging (fMRI). This\nframework links the AN sub-groups to FBNs, enabling the delineation of\nbrain-like functional organization within LLMs. Our findings reveal that LLMs\n(BERT and Llama 1-3) exhibit brain-like functional architecture, with\nsub-groups of artificial neurons mirroring the organizational patterns of\nwell-established FBNs. Notably, the brain-like functional organization of LLMs\nevolves with the increased sophistication and capability, achieving an improved\nbalance between the diversity of computational behaviors and the consistency of\nfunctional specializations. This research represents the first exploration of\nbrain-like functional organization within LLMs, offering novel insights to\ninform the development of artificial general intelligence (AGI) with human\nbrain principles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The human brain has long inspired the pursuit of artificial intelligence\n(AI). Recently, neuroimaging studies provide compelling evidence of alignment\nbetween the computational representation of artificial neural networks (ANNs)\nand the neural responses of the human brain to stimuli, suggesting that ANNs\nmay employ brain-like information processing strategies. While such alignment\nhas been observed across sensory modalities--visual, auditory, and\nlinguistic--much of the focus has been on the behaviors of artificial neurons\n(ANs) at the population level, leaving the functional organization of\nindividual ANs that facilitates such brain-like processes largely unexplored.\nIn this study, we bridge this gap by directly coupling sub-groups of artificial\nneurons with functional brain networks (FBNs), the foundational organizational\nstructure of the human brain. Specifically, we extract representative patterns\nfrom temporal responses of ANs in large language models (LLMs), and use them as\nfixed regressors to construct voxel-wise encoding models to predict brain\nactivity recorded by functional magnetic resonance imaging (fMRI). This\nframework links the AN sub-groups to FBNs, enabling the delineation of\nbrain-like functional organization within LLMs. Our findings reveal that LLMs\n(BERT and Llama 1-3) exhibit brain-like functional architecture, with\nsub-groups of artificial neurons mirroring the organizational patterns of\nwell-established FBNs. Notably, the brain-like functional organization of LLMs\nevolves with the increased sophistication and capability, achieving an improved\nbalance between the diversity of computational behaviors and the consistency of\nfunctional specializations. This research represents the first exploration of\nbrain-like functional organization within LLMs, offering novel insights to\ninform the development of artificial general intelligence (AGI) with human\nbrain principles."
                },
                "authors": [
                    {
                        "name": "H. Sun"
                    },
                    {
                        "name": "L. Zhao"
                    },
                    {
                        "name": "Z. Wu"
                    },
                    {
                        "name": "X. Gao"
                    },
                    {
                        "name": "Y. Hu"
                    },
                    {
                        "name": "M. Zuo"
                    },
                    {
                        "name": "W. Zhang"
                    },
                    {
                        "name": "J. Han"
                    },
                    {
                        "name": "T. Liu"
                    },
                    {
                        "name": "X. Hu"
                    }
                ],
                "author_detail": {
                    "name": "X. Hu"
                },
                "author": "X. Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17519v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17519v2",
                "updated": "2024-10-25T13:14:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    13,
                    14,
                    25,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-23T02:51:33Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    2,
                    51,
                    33,
                    2,
                    297,
                    0
                ],
                "title": "Large Language Models Still Exhibit Bias in Long Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Still Exhibit Bias in Long Text"
                },
                "summary": "Existing fairness benchmarks for large language models (LLMs) primarily focus\non simple tasks, such as multiple-choice questions, overlooking biases that may\narise in more complex scenarios like long-text generation. To address this gap,\nwe introduce the Long Text Fairness Test (LTF-TEST), a framework that evaluates\nbiases in LLMs through essay-style prompts. LTF-TEST covers 14 topics and 10\ndemographic axes, including gender and race, resulting in 11,948 samples. By\nassessing both model responses and the reasoning behind them, LTF-TEST uncovers\nsubtle biases that are difficult to detect in simple responses. In our\nevaluation of five recent LLMs, including GPT-4o and LLaMa3, we identify two\nkey patterns of bias. First, these models frequently favor certain demographic\ngroups in their responses. Second, they show excessive sensitivity toward\ntraditionally disadvantaged groups, often providing overly protective responses\nwhile neglecting others. To mitigate these biases, we propose FT-REGARD, a\nfinetuning approach that pairs biased prompts with neutral responses. FT-REGARD\nreduces gender bias by 34.6% and improves performance by 1.4 percentage points\non the BBQ benchmark, offering a promising approach to addressing biases in\nlong-text generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing fairness benchmarks for large language models (LLMs) primarily focus\non simple tasks, such as multiple-choice questions, overlooking biases that may\narise in more complex scenarios like long-text generation. To address this gap,\nwe introduce the Long Text Fairness Test (LTF-TEST), a framework that evaluates\nbiases in LLMs through essay-style prompts. LTF-TEST covers 14 topics and 10\ndemographic axes, including gender and race, resulting in 11,948 samples. By\nassessing both model responses and the reasoning behind them, LTF-TEST uncovers\nsubtle biases that are difficult to detect in simple responses. In our\nevaluation of five recent LLMs, including GPT-4o and LLaMa3, we identify two\nkey patterns of bias. First, these models frequently favor certain demographic\ngroups in their responses. Second, they show excessive sensitivity toward\ntraditionally disadvantaged groups, often providing overly protective responses\nwhile neglecting others. To mitigate these biases, we propose FT-REGARD, a\nfinetuning approach that pairs biased prompts with neutral responses. FT-REGARD\nreduces gender bias by 34.6% and improves performance by 1.4 percentage points\non the BBQ benchmark, offering a promising approach to addressing biases in\nlong-text generation tasks."
                },
                "authors": [
                    {
                        "name": "Wonje Jeung"
                    },
                    {
                        "name": "Dongjae Jeon"
                    },
                    {
                        "name": "Ashkan Yousefpour"
                    },
                    {
                        "name": "Jonghyun Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jonghyun Choi"
                },
                "author": "Jonghyun Choi",
                "arxiv_comment": "22 page, 38 figures, Neurips (SoLaR Workshop)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17519v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17519v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.16210v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.16210v4",
                "updated": "2024-10-25T12:51:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    12,
                    51,
                    37,
                    4,
                    299,
                    0
                ],
                "published": "2023-10-24T21:57:59Z",
                "published_parsed": [
                    2023,
                    10,
                    24,
                    21,
                    57,
                    59,
                    1,
                    297,
                    0
                ],
                "title": "Semantic Segmentation in Satellite Hyperspectral Imagery by Deep\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Segmentation in Satellite Hyperspectral Imagery by Deep\n  Learning"
                },
                "summary": "Satellites are increasingly adopting on-board AI to optimize operations and\nincrease autonomy through in-orbit inference. The use of Deep Learning (DL)\nmodels for segmentation in hyperspectral imagery offers advantages for remote\nsensing applications. In this work, we train and test 20 models for multi-class\nsegmentation in hyperspectral imagery, selected for their potential in future\nspace deployment. These models include 1D and 2D Convolutional Neural Networks\n(CNNs) and the latest vision transformers (ViTs). We propose a lightweight\n1D-CNN model, 1D-Justo-LiuNet, which outperforms state-of-the-art models in the\nhypespectral domain. 1D-Justo-LiuNet exceeds the performance of 2D-CNN UNets\nand outperforms Apple's lightweight vision transformers designed for mobile\ninference. 1D-Justo-LiuNet achieves the highest accuracy (0.93) with the\nsmallest model size (4,563 parameters) among all tested models, while\nmaintaining fast inference. Unlike 2D-CNNs and ViTs, which encode both spectral\nand spatial information, 1D-Justo-LiuNet focuses solely on the rich spectral\nfeatures in hyperspectral data, benefitting from the high-dimensional feature\nspace. Our findings are validated across various satellite datasets, with the\nHYPSO-1 mission serving as the primary case study for sea, land, and cloud\nsegmentation. We further confirm our conclusions through generalization tests\non other hyperspectral missions, such as NASA's EO-1. Based on its superior\nperformance and compact size, we conclude that 1D-Justo-LiuNet is highly\nsuitable for in-orbit deployment, providing an effective solution for\noptimizing and automating satellite operations at edge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satellites are increasingly adopting on-board AI to optimize operations and\nincrease autonomy through in-orbit inference. The use of Deep Learning (DL)\nmodels for segmentation in hyperspectral imagery offers advantages for remote\nsensing applications. In this work, we train and test 20 models for multi-class\nsegmentation in hyperspectral imagery, selected for their potential in future\nspace deployment. These models include 1D and 2D Convolutional Neural Networks\n(CNNs) and the latest vision transformers (ViTs). We propose a lightweight\n1D-CNN model, 1D-Justo-LiuNet, which outperforms state-of-the-art models in the\nhypespectral domain. 1D-Justo-LiuNet exceeds the performance of 2D-CNN UNets\nand outperforms Apple's lightweight vision transformers designed for mobile\ninference. 1D-Justo-LiuNet achieves the highest accuracy (0.93) with the\nsmallest model size (4,563 parameters) among all tested models, while\nmaintaining fast inference. Unlike 2D-CNNs and ViTs, which encode both spectral\nand spatial information, 1D-Justo-LiuNet focuses solely on the rich spectral\nfeatures in hyperspectral data, benefitting from the high-dimensional feature\nspace. Our findings are validated across various satellite datasets, with the\nHYPSO-1 mission serving as the primary case study for sea, land, and cloud\nsegmentation. We further confirm our conclusions through generalization tests\non other hyperspectral missions, such as NASA's EO-1. Based on its superior\nperformance and compact size, we conclude that 1D-Justo-LiuNet is highly\nsuitable for in-orbit deployment, providing an effective solution for\noptimizing and automating satellite operations at edge."
                },
                "authors": [
                    {
                        "name": "Jon Alvarez Justo"
                    },
                    {
                        "name": "Alexandru Ghita"
                    },
                    {
                        "name": "Daniel Kovac"
                    },
                    {
                        "name": "Joseph L. Garrett"
                    },
                    {
                        "name": "Mariana-Iuliana Georgescu"
                    },
                    {
                        "name": "Jesus Gonzalez-Llorente"
                    },
                    {
                        "name": "Radu Tudor Ionescu"
                    },
                    {
                        "name": "Tor Arne Johansen"
                    }
                ],
                "author_detail": {
                    "name": "Tor Arne Johansen"
                },
                "author": "Tor Arne Johansen",
                "arxiv_comment": "Remote Sensing, Satellite Hyperspectral Imagery, Segmentation, Deep\n  Learning, 1D-CNNs, 2D-CNNs, ViTs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.16210v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.16210v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19517v1",
                "updated": "2024-10-25T12:42:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    12,
                    42,
                    7,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T12:42:07Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    12,
                    42,
                    7,
                    4,
                    299,
                    0
                ],
                "title": "Detection of Human and Machine-Authored Fake News in Urdu",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detection of Human and Machine-Authored Fake News in Urdu"
                },
                "summary": "The rise of social media has amplified the spread of fake news, now further\ncomplicated by large language models (LLMs) like ChatGPT, which ease the\ngeneration of highly convincing, error-free misinformation, making it\nincreasingly challenging for the public to discern truth from falsehood.\nTraditional fake news detection methods relying on linguistic cues also becomes\nless effective. Moreover, current detectors primarily focus on binary\nclassification and English texts, often overlooking the distinction between\nmachine-generated true vs. fake news and the detection in low-resource\nlanguages. To this end, we updated detection schema to include\nmachine-generated news with focus on the Urdu language. We further propose a\nhierarchical detection strategy to improve the accuracy and robustness.\nExperiments show its effectiveness across four datasets in various settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of social media has amplified the spread of fake news, now further\ncomplicated by large language models (LLMs) like ChatGPT, which ease the\ngeneration of highly convincing, error-free misinformation, making it\nincreasingly challenging for the public to discern truth from falsehood.\nTraditional fake news detection methods relying on linguistic cues also becomes\nless effective. Moreover, current detectors primarily focus on binary\nclassification and English texts, often overlooking the distinction between\nmachine-generated true vs. fake news and the detection in low-resource\nlanguages. To this end, we updated detection schema to include\nmachine-generated news with focus on the Urdu language. We further propose a\nhierarchical detection strategy to improve the accuracy and robustness.\nExperiments show its effectiveness across four datasets in various settings."
                },
                "authors": [
                    {
                        "name": "Muhammad Zain Ali"
                    },
                    {
                        "name": "Yuxia Wang"
                    },
                    {
                        "name": "Bernhard Pfahringer"
                    },
                    {
                        "name": "Tony Smith"
                    }
                ],
                "author_detail": {
                    "name": "Tony Smith"
                },
                "author": "Tony Smith",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15548v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15548v3",
                "updated": "2024-10-25T12:36:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    12,
                    36,
                    5,
                    4,
                    299,
                    0
                ],
                "published": "2024-09-23T21:02:33Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    21,
                    2,
                    33,
                    0,
                    267,
                    0
                ],
                "title": "Beyond Conformal Predictors: Adaptive Conformal Inference with\n  Confidence Predictors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Conformal Predictors: Adaptive Conformal Inference with\n  Confidence Predictors"
                },
                "summary": "Conformal prediction (CP) is a robust framework for distribution-free\nuncertainty quantification, but it requires exchangeable data to ensure valid\nprediction sets at a user-specified significance level. When this assumption is\nviolated, as in time-series or other structured data, the validity guarantees\nof CP no longer hold. Adaptive conformal inference (ACI) was introduced to\naddress this limitation by adjusting the significance level dynamically,\nensuring finite-sample coverage guarantees even for non-exchangeable data. In\nthis paper, we show that ACI does not require the use of conformal predictors;\ninstead, it can be implemented with the more general confidence predictors,\nwhich are computationally simpler and still maintain the crucial property of\nnested prediction sets. Through experiments on synthetic and real-world data,\nwe demonstrate that confidence predictors can perform comparably to, or even\nbetter than, conformal predictors, particularly in terms of computational\nefficiency. These findings suggest that confidence predictors represent a\nviable and efficient alternative to conformal predictors in non-exchangeable\ndata settings, although further studies are needed to identify when one method\nis superior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal prediction (CP) is a robust framework for distribution-free\nuncertainty quantification, but it requires exchangeable data to ensure valid\nprediction sets at a user-specified significance level. When this assumption is\nviolated, as in time-series or other structured data, the validity guarantees\nof CP no longer hold. Adaptive conformal inference (ACI) was introduced to\naddress this limitation by adjusting the significance level dynamically,\nensuring finite-sample coverage guarantees even for non-exchangeable data. In\nthis paper, we show that ACI does not require the use of conformal predictors;\ninstead, it can be implemented with the more general confidence predictors,\nwhich are computationally simpler and still maintain the crucial property of\nnested prediction sets. Through experiments on synthetic and real-world data,\nwe demonstrate that confidence predictors can perform comparably to, or even\nbetter than, conformal predictors, particularly in terms of computational\nefficiency. These findings suggest that confidence predictors represent a\nviable and efficient alternative to conformal predictors in non-exchangeable\ndata settings, although further studies are needed to identify when one method\nis superior."
                },
                "authors": [
                    {
                        "name": "Johan Hallberg Szabadváry"
                    }
                ],
                "author_detail": {
                    "name": "Johan Hallberg Szabadváry"
                },
                "author": "Johan Hallberg Szabadváry",
                "arxiv_comment": "34 pages, 5 figures. Updated version after feedback",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15548v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15548v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08815v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08815v2",
                "updated": "2024-10-25T12:18:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    12,
                    18,
                    37,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-11T13:52:44Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    13,
                    52,
                    44,
                    4,
                    285,
                    0
                ],
                "title": "StructRAG: Boosting Knowledge Intensive Reasoning of LLMs via\n  Inference-time Hybrid Information Structurization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StructRAG: Boosting Knowledge Intensive Reasoning of LLMs via\n  Inference-time Hybrid Information Structurization"
                },
                "summary": "Retrieval-augmented generation (RAG) is a key means to effectively enhance\nlarge language models (LLMs) in many knowledge-based tasks. However, existing\nRAG methods struggle with knowledge-intensive reasoning tasks, because useful\ninformation required to these tasks are badly scattered. This characteristic\nmakes it difficult for existing RAG methods to accurately identify key\ninformation and perform global reasoning with such noisy augmentation. In this\npaper, motivated by the cognitive theories that humans convert raw information\ninto various structured knowledge when tackling knowledge-intensive reasoning,\nwe proposes a new framework, StructRAG, which can identify the optimal\nstructure type for the task at hand, reconstruct original documents into this\nstructured format, and infer answers based on the resulting structure.\nExtensive experiments across various knowledge-intensive tasks show that\nStructRAG achieves state-of-the-art performance, particularly excelling in\nchallenging scenarios, demonstrating its potential as an effective solution for\nenhancing LLMs in complex real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) is a key means to effectively enhance\nlarge language models (LLMs) in many knowledge-based tasks. However, existing\nRAG methods struggle with knowledge-intensive reasoning tasks, because useful\ninformation required to these tasks are badly scattered. This characteristic\nmakes it difficult for existing RAG methods to accurately identify key\ninformation and perform global reasoning with such noisy augmentation. In this\npaper, motivated by the cognitive theories that humans convert raw information\ninto various structured knowledge when tackling knowledge-intensive reasoning,\nwe proposes a new framework, StructRAG, which can identify the optimal\nstructure type for the task at hand, reconstruct original documents into this\nstructured format, and infer answers based on the resulting structure.\nExtensive experiments across various knowledge-intensive tasks show that\nStructRAG achieves state-of-the-art performance, particularly excelling in\nchallenging scenarios, demonstrating its potential as an effective solution for\nenhancing LLMs in complex real-world applications."
                },
                "authors": [
                    {
                        "name": "Zhuoqun Li"
                    },
                    {
                        "name": "Xuanang Chen"
                    },
                    {
                        "name": "Haiyang Yu"
                    },
                    {
                        "name": "Hongyu Lin"
                    },
                    {
                        "name": "Yaojie Lu"
                    },
                    {
                        "name": "Qiaoyu Tang"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Xianpei Han"
                    },
                    {
                        "name": "Le Sun"
                    },
                    {
                        "name": "Yongbin Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongbin Li"
                },
                "author": "Yongbin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08815v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08815v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10649v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10649v2",
                "updated": "2024-10-25T12:14:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    12,
                    14,
                    26,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-14T16:00:20Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    16,
                    0,
                    20,
                    0,
                    288,
                    0
                ],
                "title": "Vecchia Gaussian Processes: Probabilistic Properties, Minimax Rates and\n  Methodological Developments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vecchia Gaussian Processes: Probabilistic Properties, Minimax Rates and\n  Methodological Developments"
                },
                "summary": "Gaussian Processes (GPs) are widely used to model dependency in spatial\nstatistics and machine learning, yet the exact computation suffers an\nintractable time complexity of $O(n^3)$. Vecchia approximation allows scalable\nBayesian inference of GPs in $O(n)$ time by introducing sparsity in the spatial\ndependency structure that is characterized by a directed acyclic graph (DAG).\nDespite the popularity in practice, it is still unclear how to choose the DAG\nstructure and there are still no theoretical guarantees in nonparametric\nsettings. In this paper, we systematically study the Vecchia GPs as standalone\nstochastic processes and uncover important probabilistic properties and\nstatistical results in methodology and theory. For probabilistic properties, we\nprove that the conditional distributions of the Mat\\'{e}rn GPs, as well as the\nVecchia approximations of the Mat\\'{e}rn GPs, can be characterized by\npolynomials. This allows us to prove a series of results regarding the small\nball probabilities and RKHSs of Vecchia GPs. For statistical methodology, we\nprovide a principled guideline to choose parent sets as norming sets with fixed\ncardinality and provide detailed algorithms following such guidelines. For\nstatistical theory, we prove posterior contraction rates for applying Vecchia\nGPs to regression problems, where minimax optimality is achieved by optimally\ntuned GPs via either oracle rescaling or hierarchical Bayesian methods. Our\ntheory and methodology are demonstrated with numerical studies, where we also\nprovide efficient implementation of our methods in C++ with R interfaces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Processes (GPs) are widely used to model dependency in spatial\nstatistics and machine learning, yet the exact computation suffers an\nintractable time complexity of $O(n^3)$. Vecchia approximation allows scalable\nBayesian inference of GPs in $O(n)$ time by introducing sparsity in the spatial\ndependency structure that is characterized by a directed acyclic graph (DAG).\nDespite the popularity in practice, it is still unclear how to choose the DAG\nstructure and there are still no theoretical guarantees in nonparametric\nsettings. In this paper, we systematically study the Vecchia GPs as standalone\nstochastic processes and uncover important probabilistic properties and\nstatistical results in methodology and theory. For probabilistic properties, we\nprove that the conditional distributions of the Mat\\'{e}rn GPs, as well as the\nVecchia approximations of the Mat\\'{e}rn GPs, can be characterized by\npolynomials. This allows us to prove a series of results regarding the small\nball probabilities and RKHSs of Vecchia GPs. For statistical methodology, we\nprovide a principled guideline to choose parent sets as norming sets with fixed\ncardinality and provide detailed algorithms following such guidelines. For\nstatistical theory, we prove posterior contraction rates for applying Vecchia\nGPs to regression problems, where minimax optimality is achieved by optimally\ntuned GPs via either oracle rescaling or hierarchical Bayesian methods. Our\ntheory and methodology are demonstrated with numerical studies, where we also\nprovide efficient implementation of our methods in C++ with R interfaces."
                },
                "authors": [
                    {
                        "name": "Botond Szabo"
                    },
                    {
                        "name": "Yichen Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yichen Zhu"
                },
                "author": "Yichen Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10649v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10649v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62G08, 60G15, 62H11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19503v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19503v1",
                "updated": "2024-10-25T12:10:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    12,
                    10,
                    49,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T12:10:49Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    12,
                    10,
                    49,
                    4,
                    299,
                    0
                ],
                "title": "SWITCH: Studying with Teacher for Knowledge Distillation of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWITCH: Studying with Teacher for Knowledge Distillation of Large\n  Language Models"
                },
                "summary": "Despite the success of Large Language Models (LLMs), they still face\nchallenges related to high inference costs and memory requirements. To address\nthese issues, Knowledge Distillation (KD) has emerged as a popular method for\nmodel compression, with student-generated outputs (SGOs) being particularly\nnotable for reducing the mismatch between training and inference. However, SGOs\noften produce noisy and biased sequences, which can lead to misguidance from\nthe teacher model, especially in long sequences. To mitigate these challenges,\nwe propose SWITCH (Studying WIth TeaCHer for Knowledge Distillation), a novel\napproach that strategically incorporates the teacher model during the student's\nsequence generation. SWITCH identifies discrepancies between the token\nprobabilities of the teacher and student models, allowing the teacher to\nintervene selectively, particularly in long sequences that are more prone to\nteacher misguidance. Extensive experimental results across three model families\nand five instruction-following datasets show that SWITCH surpasses traditional\nKD methods, particularly excelling in the generation of long sequential data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the success of Large Language Models (LLMs), they still face\nchallenges related to high inference costs and memory requirements. To address\nthese issues, Knowledge Distillation (KD) has emerged as a popular method for\nmodel compression, with student-generated outputs (SGOs) being particularly\nnotable for reducing the mismatch between training and inference. However, SGOs\noften produce noisy and biased sequences, which can lead to misguidance from\nthe teacher model, especially in long sequences. To mitigate these challenges,\nwe propose SWITCH (Studying WIth TeaCHer for Knowledge Distillation), a novel\napproach that strategically incorporates the teacher model during the student's\nsequence generation. SWITCH identifies discrepancies between the token\nprobabilities of the teacher and student models, allowing the teacher to\nintervene selectively, particularly in long sequences that are more prone to\nteacher misguidance. Extensive experimental results across three model families\nand five instruction-following datasets show that SWITCH surpasses traditional\nKD methods, particularly excelling in the generation of long sequential data."
                },
                "authors": [
                    {
                        "name": "Jahyun Koo"
                    },
                    {
                        "name": "Yerin Hwang"
                    },
                    {
                        "name": "Yongil Kim"
                    },
                    {
                        "name": "Taegwan Kang"
                    },
                    {
                        "name": "Hyunkyung Bae"
                    },
                    {
                        "name": "Kyomin Jung"
                    }
                ],
                "author_detail": {
                    "name": "Kyomin Jung"
                },
                "author": "Kyomin Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19503v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19503v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19499v1",
                "updated": "2024-10-25T11:58:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    11,
                    58,
                    12,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T11:58:12Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    11,
                    58,
                    12,
                    4,
                    299,
                    0
                ],
                "title": "Introducing MAPO: Momentum-Aided Gradient Descent Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introducing MAPO: Momentum-Aided Gradient Descent Prompt Optimization"
                },
                "summary": "Momentum-Aided Prompt Optimization (MAPO) enhances the efficiency and\nefficacy of prompt optimization for Large Language Models (LLMs). Building on\nProTeGi, MAPO uses positive natural language \"gradients\" and a momentum-based\nextension to refine prompts effectively. By tracking gradient history, MAPO\navoids local minima and oscillations. It also utilizes beam search and an Upper\nConfidence Bound (UCB) algorithm for balanced candidate expansion and\nselection. Benchmark testing shows that MAPO achieves faster convergence time\nwith fewer API calls and higher F1 scores than ProTeGi, proving it as a robust\nand scalable solution for automated prompt engineering in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Momentum-Aided Prompt Optimization (MAPO) enhances the efficiency and\nefficacy of prompt optimization for Large Language Models (LLMs). Building on\nProTeGi, MAPO uses positive natural language \"gradients\" and a momentum-based\nextension to refine prompts effectively. By tracking gradient history, MAPO\navoids local minima and oscillations. It also utilizes beam search and an Upper\nConfidence Bound (UCB) algorithm for balanced candidate expansion and\nselection. Benchmark testing shows that MAPO achieves faster convergence time\nwith fewer API calls and higher F1 scores than ProTeGi, proving it as a robust\nand scalable solution for automated prompt engineering in LLMs."
                },
                "authors": [
                    {
                        "name": "Anthony Cui"
                    },
                    {
                        "name": "Pranav Nandyalam"
                    },
                    {
                        "name": "Kevin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Zhu"
                },
                "author": "Kevin Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19494v1",
                "updated": "2024-10-25T11:51:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    11,
                    51,
                    37,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T11:51:37Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    11,
                    51,
                    37,
                    4,
                    299,
                    0
                ],
                "title": "Graph Linearization Methods for Reasoning on Graphs with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Linearization Methods for Reasoning on Graphs with Large Language\n  Models"
                },
                "summary": "Large language models have evolved to process multiple modalities beyond\ntext, such as images and audio, which motivates us to explore how to\neffectively leverage them for graph machine learning tasks. The key question,\ntherefore, is how to transform graphs into linear sequences of tokens, a\nprocess we term graph linearization, so that LLMs can handle graphs naturally.\nWe consider that graphs should be linearized meaningfully to reflect certain\nproperties of natural language text, such as local dependency and global\nalignment, in order to ease contemporary LLMs, trained on trillions of textual\ntokens, better understand graphs. To achieve this, we developed several graph\nlinearization methods based on graph centrality, degeneracy, and node\nrelabeling schemes. We then investigated their effect on LLM performance in\ngraph reasoning tasks. Experimental results on synthetic graphs demonstrate the\neffectiveness of our methods compared to random linearization baselines. Our\nwork introduces novel graph representations suitable for LLMs, contributing to\nthe potential integration of graph machine learning with the trend of\nmulti-modal processing using a unified transformer model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have evolved to process multiple modalities beyond\ntext, such as images and audio, which motivates us to explore how to\neffectively leverage them for graph machine learning tasks. The key question,\ntherefore, is how to transform graphs into linear sequences of tokens, a\nprocess we term graph linearization, so that LLMs can handle graphs naturally.\nWe consider that graphs should be linearized meaningfully to reflect certain\nproperties of natural language text, such as local dependency and global\nalignment, in order to ease contemporary LLMs, trained on trillions of textual\ntokens, better understand graphs. To achieve this, we developed several graph\nlinearization methods based on graph centrality, degeneracy, and node\nrelabeling schemes. We then investigated their effect on LLM performance in\ngraph reasoning tasks. Experimental results on synthetic graphs demonstrate the\neffectiveness of our methods compared to random linearization baselines. Our\nwork introduces novel graph representations suitable for LLMs, contributing to\nthe potential integration of graph machine learning with the trend of\nmulti-modal processing using a unified transformer model."
                },
                "authors": [
                    {
                        "name": "Christos Xypolopoulos"
                    },
                    {
                        "name": "Guokan Shang"
                    },
                    {
                        "name": "Xiao Fei"
                    },
                    {
                        "name": "Giannis Nikolentzos"
                    },
                    {
                        "name": "Hadi Abdine"
                    },
                    {
                        "name": "Iakovos Evdaimon"
                    },
                    {
                        "name": "Michail Chatzianastasis"
                    },
                    {
                        "name": "Giorgos Stamou"
                    },
                    {
                        "name": "Michalis Vazirgiannis"
                    }
                ],
                "author_detail": {
                    "name": "Michalis Vazirgiannis"
                },
                "author": "Michalis Vazirgiannis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19492v1",
                "updated": "2024-10-25T11:49:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    11,
                    49,
                    40,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T11:49:40Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    11,
                    49,
                    40,
                    4,
                    299,
                    0
                ],
                "title": "TRADE: Transfer of Distributions between External Conditions with\n  Normalizing Flows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRADE: Transfer of Distributions between External Conditions with\n  Normalizing Flows"
                },
                "summary": "Modeling distributions that depend on external control parameters is a common\nscenario in diverse applications like molecular simulations, where system\nproperties like temperature affect molecular configurations. Despite the\nrelevance of these applications, existing solutions are unsatisfactory as they\nrequire severely restricted model architectures or rely on backward training,\nwhich is prone to unstable training. We introduce TRADE, which overcomes these\nlimitations by formulating the learning process as a boundary value problem. By\ninitially training the model for a specific condition using either i.i.d.\nsamples or backward KL training, we establish a boundary distribution. We then\npropagate this information across other conditions using the gradient of the\nunnormalized density with respect to the external parameter. This formulation,\nakin to the principles of physics-informed neural networks, allows us to\nefficiently learn parameter-dependent distributions without restrictive\nassumptions. Experimentally, we demonstrate that TRADE achieves excellent\nresults in a wide range of applications, ranging from Bayesian inference and\nmolecular simulations to physical lattice models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling distributions that depend on external control parameters is a common\nscenario in diverse applications like molecular simulations, where system\nproperties like temperature affect molecular configurations. Despite the\nrelevance of these applications, existing solutions are unsatisfactory as they\nrequire severely restricted model architectures or rely on backward training,\nwhich is prone to unstable training. We introduce TRADE, which overcomes these\nlimitations by formulating the learning process as a boundary value problem. By\ninitially training the model for a specific condition using either i.i.d.\nsamples or backward KL training, we establish a boundary distribution. We then\npropagate this information across other conditions using the gradient of the\nunnormalized density with respect to the external parameter. This formulation,\nakin to the principles of physics-informed neural networks, allows us to\nefficiently learn parameter-dependent distributions without restrictive\nassumptions. Experimentally, we demonstrate that TRADE achieves excellent\nresults in a wide range of applications, ranging from Bayesian inference and\nmolecular simulations to physical lattice models."
                },
                "authors": [
                    {
                        "name": "Stefan Wahl"
                    },
                    {
                        "name": "Armand Rousselot"
                    },
                    {
                        "name": "Felix Draxler"
                    },
                    {
                        "name": "Ullrich Köthe"
                    }
                ],
                "author_detail": {
                    "name": "Ullrich Köthe"
                },
                "author": "Ullrich Köthe",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19485v1",
                "updated": "2024-10-25T11:41:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    11,
                    41,
                    27,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T11:41:27Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    11,
                    41,
                    27,
                    4,
                    299,
                    0
                ],
                "title": "A Debate-Driven Experiment on LLM Hallucinations and Accuracy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Debate-Driven Experiment on LLM Hallucinations and Accuracy"
                },
                "summary": "Large language models (LLMs) have achieved a degree of success in generating\ncoherent and contextually relevant text, yet they remain prone to a significant\nchallenge known as hallucination: producing information that is not\nsubstantiated by the input or external knowledge. Previous efforts to mitigate\nhallucinations have focused on techniques such as fine-tuning models on\nhigh-quality datasets, incorporating fact-checking mechanisms, and developing\nadversarial training methods. While these approaches have shown some promise,\nthey often address the issue at the level of individual model outputs, leaving\nunexplored the effects of inter-model interactions on hallucination. This study\ninvestigates the phenomenon of hallucination in LLMs through a novel\nexperimental framework where multiple instances of GPT-4o-Mini models engage in\na debate-like interaction prompted with questions from the TruthfulQA dataset.\nOne model is deliberately instructed to generate plausible but false answers\nwhile the other models are asked to respond truthfully. The experiment is\ndesigned to assess whether the introduction of misinformation by one model can\nchallenge the truthful majority to better justify their reasoning, improving\nperformance on the TruthfulQA benchmark. The findings suggest that inter-model\ninteractions can offer valuable insights into improving the accuracy and\nrobustness of LLM outputs, complementing existing mitigation strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved a degree of success in generating\ncoherent and contextually relevant text, yet they remain prone to a significant\nchallenge known as hallucination: producing information that is not\nsubstantiated by the input or external knowledge. Previous efforts to mitigate\nhallucinations have focused on techniques such as fine-tuning models on\nhigh-quality datasets, incorporating fact-checking mechanisms, and developing\nadversarial training methods. While these approaches have shown some promise,\nthey often address the issue at the level of individual model outputs, leaving\nunexplored the effects of inter-model interactions on hallucination. This study\ninvestigates the phenomenon of hallucination in LLMs through a novel\nexperimental framework where multiple instances of GPT-4o-Mini models engage in\na debate-like interaction prompted with questions from the TruthfulQA dataset.\nOne model is deliberately instructed to generate plausible but false answers\nwhile the other models are asked to respond truthfully. The experiment is\ndesigned to assess whether the introduction of misinformation by one model can\nchallenge the truthful majority to better justify their reasoning, improving\nperformance on the TruthfulQA benchmark. The findings suggest that inter-model\ninteractions can offer valuable insights into improving the accuracy and\nrobustness of LLM outputs, complementing existing mitigation strategies."
                },
                "authors": [
                    {
                        "name": "Ray Li"
                    },
                    {
                        "name": "Tanishka Bagade"
                    },
                    {
                        "name": "Kevin Martinez"
                    },
                    {
                        "name": "Flora Yasmin"
                    },
                    {
                        "name": "Grant Ayala"
                    },
                    {
                        "name": "Michael Lam"
                    },
                    {
                        "name": "Kevin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Zhu"
                },
                "author": "Kevin Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16418v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16418v2",
                "updated": "2024-10-25T11:39:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    11,
                    39,
                    43,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-21T18:36:45Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    18,
                    36,
                    45,
                    0,
                    295,
                    0
                ],
                "title": "AttentionPainter: An Efficient and Adaptive Stroke Predictor for Scene\n  Painting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttentionPainter: An Efficient and Adaptive Stroke Predictor for Scene\n  Painting"
                },
                "summary": "Stroke-based Rendering (SBR) aims to decompose an input image into a sequence\nof parameterized strokes, which can be rendered into a painting that resembles\nthe input image. Recently, Neural Painting methods that utilize deep learning\nand reinforcement learning models to predict the stroke sequences have been\ndeveloped, but suffer from longer inference time or unstable training. To\naddress these issues, we propose AttentionPainter, an efficient and adaptive\nmodel for single-step neural painting. First, we propose a novel scalable\nstroke predictor, which predicts a large number of stroke parameters within a\nsingle forward process, instead of the iterative prediction of previous\nReinforcement Learning or auto-regressive methods, which makes AttentionPainter\nfaster than previous neural painting methods. To further increase the training\nefficiency, we propose a Fast Stroke Stacking algorithm, which brings 13 times\nacceleration for training. Moreover, we propose Stroke-density Loss, which\nencourages the model to use small strokes for detailed information, to help\nimprove the reconstruction quality. Finally, we propose a new stroke diffusion\nmodel for both conditional and unconditional stroke-based generation, which\ndenoises in the stroke parameter space and facilitates stroke-based inpainting\nand editing applications helpful for human artists design. Extensive\nexperiments show that AttentionPainter outperforms the state-of-the-art neural\npainting methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stroke-based Rendering (SBR) aims to decompose an input image into a sequence\nof parameterized strokes, which can be rendered into a painting that resembles\nthe input image. Recently, Neural Painting methods that utilize deep learning\nand reinforcement learning models to predict the stroke sequences have been\ndeveloped, but suffer from longer inference time or unstable training. To\naddress these issues, we propose AttentionPainter, an efficient and adaptive\nmodel for single-step neural painting. First, we propose a novel scalable\nstroke predictor, which predicts a large number of stroke parameters within a\nsingle forward process, instead of the iterative prediction of previous\nReinforcement Learning or auto-regressive methods, which makes AttentionPainter\nfaster than previous neural painting methods. To further increase the training\nefficiency, we propose a Fast Stroke Stacking algorithm, which brings 13 times\nacceleration for training. Moreover, we propose Stroke-density Loss, which\nencourages the model to use small strokes for detailed information, to help\nimprove the reconstruction quality. Finally, we propose a new stroke diffusion\nmodel for both conditional and unconditional stroke-based generation, which\ndenoises in the stroke parameter space and facilitates stroke-based inpainting\nand editing applications helpful for human artists design. Extensive\nexperiments show that AttentionPainter outperforms the state-of-the-art neural\npainting methods."
                },
                "authors": [
                    {
                        "name": "Yizhe Tang"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Teng Hu"
                    },
                    {
                        "name": "Ran Yi"
                    },
                    {
                        "name": "Xin Tan"
                    },
                    {
                        "name": "Lizhuang Ma"
                    },
                    {
                        "name": "Yu-Kun Lai"
                    },
                    {
                        "name": "Paul L. Rosin"
                    }
                ],
                "author_detail": {
                    "name": "Paul L. Rosin"
                },
                "author": "Paul L. Rosin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16418v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16418v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19482v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19482v1",
                "updated": "2024-10-25T11:37:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    11,
                    37,
                    4,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T11:37:04Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    11,
                    37,
                    4,
                    4,
                    299,
                    0
                ],
                "title": "Measuring memorization through probabilistic discoverable extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring memorization through probabilistic discoverable extraction"
                },
                "summary": "Large language models (LLMs) are susceptible to memorizing training data,\nraising concerns due to the potential extraction of sensitive information.\nCurrent methods to measure memorization rates of LLMs, primarily discoverable\nextraction (Carlini et al., 2022), rely on single-sequence greedy sampling,\npotentially underestimating the true extent of memorization. This paper\nintroduces a probabilistic relaxation of discoverable extraction that\nquantifies the probability of extracting a target sequence within a set of\ngenerated samples, considering various sampling schemes and multiple attempts.\nThis approach addresses the limitations of reporting memorization rates through\ndiscoverable extraction by accounting for the probabilistic nature of LLMs and\nuser interaction patterns. Our experiments demonstrate that this probabilistic\nmeasure can reveal cases of higher memorization rates compared to rates found\nthrough discoverable extraction. We further investigate the impact of different\nsampling schemes on extractability, providing a more comprehensive and\nrealistic assessment of LLM memorization and its associated risks. Our\ncontributions include a new probabilistic memorization definition, empirical\nevidence of its effectiveness, and a thorough evaluation across different\nmodels, sizes, sampling schemes, and training data repetitions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are susceptible to memorizing training data,\nraising concerns due to the potential extraction of sensitive information.\nCurrent methods to measure memorization rates of LLMs, primarily discoverable\nextraction (Carlini et al., 2022), rely on single-sequence greedy sampling,\npotentially underestimating the true extent of memorization. This paper\nintroduces a probabilistic relaxation of discoverable extraction that\nquantifies the probability of extracting a target sequence within a set of\ngenerated samples, considering various sampling schemes and multiple attempts.\nThis approach addresses the limitations of reporting memorization rates through\ndiscoverable extraction by accounting for the probabilistic nature of LLMs and\nuser interaction patterns. Our experiments demonstrate that this probabilistic\nmeasure can reveal cases of higher memorization rates compared to rates found\nthrough discoverable extraction. We further investigate the impact of different\nsampling schemes on extractability, providing a more comprehensive and\nrealistic assessment of LLM memorization and its associated risks. Our\ncontributions include a new probabilistic memorization definition, empirical\nevidence of its effectiveness, and a thorough evaluation across different\nmodels, sizes, sampling schemes, and training data repetitions."
                },
                "authors": [
                    {
                        "name": "Jamie Hayes"
                    },
                    {
                        "name": "Marika Swanberg"
                    },
                    {
                        "name": "Harsh Chaudhari"
                    },
                    {
                        "name": "Itay Yona"
                    },
                    {
                        "name": "Ilia Shumailov"
                    }
                ],
                "author_detail": {
                    "name": "Ilia Shumailov"
                },
                "author": "Ilia Shumailov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19482v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19482v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19479v1",
                "updated": "2024-10-25T11:16:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    11,
                    16,
                    28,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T11:16:28Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    11,
                    16,
                    28,
                    4,
                    299,
                    0
                ],
                "title": "Peter Parker or Spiderman? Disambiguating Multiple Class Labels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Peter Parker or Spiderman? Disambiguating Multiple Class Labels"
                },
                "summary": "In the supervised classification setting, during inference, deep networks\ntypically make multiple predictions. For a pair of such predictions (that are\nin the top-k predictions), two distinct possibilities might occur. On the one\nhand, each of the two predictions might be primarily driven by two distinct\nsets of entities in the input. On the other hand, it is possible that there is\na single entity or set of entities that is driving the prediction for both the\nclasses in question. This latter case, in effect, corresponds to the network\nmaking two separate guesses about the identity of a single entity type.\nClearly, both the guesses cannot be true, i.e. both the labels cannot be\npresent in the input. Current techniques in interpretability research do not\nreadily disambiguate these two cases, since they typically consider input\nattributions for one class label at a time. Here, we present a framework and\nmethod to do so, leveraging modern segmentation and input attribution\ntechniques. Notably, our framework also provides a simple counterfactual\n\"proof\" of each case, which can be verified for the input on the model (i.e.\nwithout running the method again). We demonstrate that the method performs well\nfor a number of samples from the ImageNet validation set and on multiple\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the supervised classification setting, during inference, deep networks\ntypically make multiple predictions. For a pair of such predictions (that are\nin the top-k predictions), two distinct possibilities might occur. On the one\nhand, each of the two predictions might be primarily driven by two distinct\nsets of entities in the input. On the other hand, it is possible that there is\na single entity or set of entities that is driving the prediction for both the\nclasses in question. This latter case, in effect, corresponds to the network\nmaking two separate guesses about the identity of a single entity type.\nClearly, both the guesses cannot be true, i.e. both the labels cannot be\npresent in the input. Current techniques in interpretability research do not\nreadily disambiguate these two cases, since they typically consider input\nattributions for one class label at a time. Here, we present a framework and\nmethod to do so, leveraging modern segmentation and input attribution\ntechniques. Notably, our framework also provides a simple counterfactual\n\"proof\" of each case, which can be verified for the input on the model (i.e.\nwithout running the method again). We demonstrate that the method performs well\nfor a number of samples from the ImageNet validation set and on multiple\nmodels."
                },
                "authors": [
                    {
                        "name": "Nuthan Mummani"
                    },
                    {
                        "name": "Simran Ketha"
                    },
                    {
                        "name": "Venkatakrishnan Ramaswamy"
                    }
                ],
                "author_detail": {
                    "name": "Venkatakrishnan Ramaswamy"
                },
                "author": "Venkatakrishnan Ramaswamy",
                "arxiv_comment": "Accepted to Neural Information Processing Systems (NeurIPS 2024).\n  ATTRIB Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19478v1",
                "updated": "2024-10-25T11:14:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    11,
                    14,
                    30,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T11:14:30Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    11,
                    14,
                    30,
                    4,
                    299,
                    0
                ],
                "title": "Physics-based inverse modeling of battery degradation with Bayesian\n  methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-based inverse modeling of battery degradation with Bayesian\n  methods"
                },
                "summary": "To further improve Lithium-ion batteries (LiBs), a profound understanding of\ncomplex battery processes is crucial. Physical models offer understanding but\nare difficult to validate and parameterize. Therefore, automated\nmachine-learning methods (ML) are necessary to evaluate models with\nexperimental data. Bayesian methods, e.g., Bayesian optimization for\nlikelihood-free inference (EP-BOLFI), stand out as they capture uncertainties\nin models and data while granting meaningful parameterization. An important\ntopic is prolonging battery lifetime, which is limited by degradation, such as\nthe solid-electrolyte interphase (SEI) growth. As a case study, we apply\nEP-BOLFI to parametrize SEI growth models with synthetic and real degradation\ndata. EP-BOLFI allows for incorporating human expertise in the form of suitable\nfeature selection, which improves the parametrization. We show that even under\nimpeded conditions, we achieve correct parameterization with reasonable\nuncertainty quantification, needing less computational effort than standard\nMarkov chain Monte Carlo methods. Additionally, the physically reliable summary\nstatistics show if parameters are strongly correlated and not unambiguously\nidentifiable. Further, we investigate Bayesian alternately subsampled\nquadrature (BASQ), which calculates model probabilities, to confirm electron\ndiffusion as the best theoretical model to describe SEI growth during battery\nstorage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To further improve Lithium-ion batteries (LiBs), a profound understanding of\ncomplex battery processes is crucial. Physical models offer understanding but\nare difficult to validate and parameterize. Therefore, automated\nmachine-learning methods (ML) are necessary to evaluate models with\nexperimental data. Bayesian methods, e.g., Bayesian optimization for\nlikelihood-free inference (EP-BOLFI), stand out as they capture uncertainties\nin models and data while granting meaningful parameterization. An important\ntopic is prolonging battery lifetime, which is limited by degradation, such as\nthe solid-electrolyte interphase (SEI) growth. As a case study, we apply\nEP-BOLFI to parametrize SEI growth models with synthetic and real degradation\ndata. EP-BOLFI allows for incorporating human expertise in the form of suitable\nfeature selection, which improves the parametrization. We show that even under\nimpeded conditions, we achieve correct parameterization with reasonable\nuncertainty quantification, needing less computational effort than standard\nMarkov chain Monte Carlo methods. Additionally, the physically reliable summary\nstatistics show if parameters are strongly correlated and not unambiguously\nidentifiable. Further, we investigate Bayesian alternately subsampled\nquadrature (BASQ), which calculates model probabilities, to confirm electron\ndiffusion as the best theoretical model to describe SEI growth during battery\nstorage."
                },
                "authors": [
                    {
                        "name": "Micha C. J. Philipp"
                    },
                    {
                        "name": "Yannick Kuhn"
                    },
                    {
                        "name": "Arnulf Latz"
                    },
                    {
                        "name": "Birger Horstmann"
                    }
                ],
                "author_detail": {
                    "name": "Birger Horstmann"
                },
                "author": "Birger Horstmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17020v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17020v2",
                "updated": "2024-10-25T11:02:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    11,
                    2,
                    49,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-22T13:44:10Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    13,
                    44,
                    10,
                    1,
                    296,
                    0
                ],
                "title": "LFME: A Simple Framework for Learning from Multiple Experts in Domain\n  Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LFME: A Simple Framework for Learning from Multiple Experts in Domain\n  Generalization"
                },
                "summary": "Domain generalization (DG) methods aim to maintain good performance in an\nunseen target domain by using training data from multiple source domains. While\nsuccess on certain occasions are observed, enhancing the baseline across most\nscenarios remains challenging. This work introduces a simple yet effective\nframework, dubbed learning from multiple experts (LFME), that aims to make the\ntarget model an expert in all source domains to improve DG. Specifically,\nbesides learning the target model used in inference, LFME will also train\nmultiple experts specialized in different domains, whose output probabilities\nprovide professional guidance by simply regularizing the logit of the target\nmodel. Delving deep into the framework, we reveal that the introduced logit\nregularization term implicitly provides effects of enabling the target model to\nharness more information, and mining hard samples from the experts during\ntraining. Extensive experiments on benchmarks from different DG tasks\ndemonstrate that LFME is consistently beneficial to the baseline and can\nachieve comparable performance to existing arts. Code is available\nat~\\url{https://github.com/liangchen527/LFME}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain generalization (DG) methods aim to maintain good performance in an\nunseen target domain by using training data from multiple source domains. While\nsuccess on certain occasions are observed, enhancing the baseline across most\nscenarios remains challenging. This work introduces a simple yet effective\nframework, dubbed learning from multiple experts (LFME), that aims to make the\ntarget model an expert in all source domains to improve DG. Specifically,\nbesides learning the target model used in inference, LFME will also train\nmultiple experts specialized in different domains, whose output probabilities\nprovide professional guidance by simply regularizing the logit of the target\nmodel. Delving deep into the framework, we reveal that the introduced logit\nregularization term implicitly provides effects of enabling the target model to\nharness more information, and mining hard samples from the experts during\ntraining. Extensive experiments on benchmarks from different DG tasks\ndemonstrate that LFME is consistently beneficial to the baseline and can\nachieve comparable performance to existing arts. Code is available\nat~\\url{https://github.com/liangchen527/LFME}."
                },
                "authors": [
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Yibing Song"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    },
                    {
                        "name": "Lingqiao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Lingqiao Liu"
                },
                "author": "Lingqiao Liu",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17020v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17020v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19464v1",
                "updated": "2024-10-25T10:48:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    10,
                    48,
                    41,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T10:48:41Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    10,
                    48,
                    41,
                    4,
                    299,
                    0
                ],
                "title": "LOCAL: Learning with Orientation Matrix to Infer Causal Structure from\n  Time Series Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LOCAL: Learning with Orientation Matrix to Infer Causal Structure from\n  Time Series Data"
                },
                "summary": "Discovering the underlying Directed Acyclic Graph (DAG) from time series\nobservational data is highly challenging due to the dynamic nature and complex\nnonlinear interactions between variables. Existing methods often struggle with\ninefficiency and the handling of high-dimensional data. To address these\nresearch gap, we propose LOCAL, a highly efficient, easy-to-implement, and\nconstraint-free method for recovering dynamic causal structures. LOCAL is the\nfirst attempt to formulate a quasi-maximum likelihood-based score function for\nlearning the dynamic DAG equivalent to the ground truth. On this basis, we\npropose two adaptive modules for enhancing the algebraic characterization of\nacyclicity with new capabilities: Asymptotic Causal Mask Learning (ACML) and\nDynamic Graph Parameter Learning (DGPL). ACML generates causal masks using\nlearnable priority vectors and the Gumbel-Sigmoid function, ensuring the\ncreation of DAGs while optimizing computational efficiency. DGPL transforms\ncausal learning into decomposed matrix products, capturing the dynamic causal\nstructure of high-dimensional data and enhancing interpretability. Extensive\nexperiments on synthetic and real-world datasets demonstrate that LOCAL\nsignificantly outperforms existing methods, and highlight LOCAL's potential as\na robust and efficient method for dynamic causal discovery. Our code will be\navailable soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering the underlying Directed Acyclic Graph (DAG) from time series\nobservational data is highly challenging due to the dynamic nature and complex\nnonlinear interactions between variables. Existing methods often struggle with\ninefficiency and the handling of high-dimensional data. To address these\nresearch gap, we propose LOCAL, a highly efficient, easy-to-implement, and\nconstraint-free method for recovering dynamic causal structures. LOCAL is the\nfirst attempt to formulate a quasi-maximum likelihood-based score function for\nlearning the dynamic DAG equivalent to the ground truth. On this basis, we\npropose two adaptive modules for enhancing the algebraic characterization of\nacyclicity with new capabilities: Asymptotic Causal Mask Learning (ACML) and\nDynamic Graph Parameter Learning (DGPL). ACML generates causal masks using\nlearnable priority vectors and the Gumbel-Sigmoid function, ensuring the\ncreation of DAGs while optimizing computational efficiency. DGPL transforms\ncausal learning into decomposed matrix products, capturing the dynamic causal\nstructure of high-dimensional data and enhancing interpretability. Extensive\nexperiments on synthetic and real-world datasets demonstrate that LOCAL\nsignificantly outperforms existing methods, and highlight LOCAL's potential as\na robust and efficient method for dynamic causal discovery. Our code will be\navailable soon."
                },
                "authors": [
                    {
                        "name": "Yue Cheng"
                    },
                    {
                        "name": "Jiajun Zhang"
                    },
                    {
                        "name": "Weiwei Xing"
                    },
                    {
                        "name": "Xiaoyu Guo"
                    },
                    {
                        "name": "Xiaohui Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohui Gao"
                },
                "author": "Xiaohui Gao",
                "arxiv_comment": "10 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19461v1",
                "updated": "2024-10-25T10:46:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    10,
                    46,
                    17,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T10:46:17Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    10,
                    46,
                    17,
                    4,
                    299,
                    0
                ],
                "title": "EDGE: Enhanced Grounded GUI Understanding with Enriched\n  Multi-Granularity Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EDGE: Enhanced Grounded GUI Understanding with Enriched\n  Multi-Granularity Synthetic Data"
                },
                "summary": "Autonomous agents operating on the graphical user interfaces (GUIs) of\nvarious applications hold immense practical value. Unlike the large language\nmodel (LLM)-based methods which rely on structured texts and customized\nbackends, the approaches using large vision-language models (LVLMs) are more\nintuitive and adaptable as they can visually perceive and directly interact\nwith screens, making them indispensable in general scenarios without text\nmetadata and tailored backends. Given the lack of high-quality training data\nfor GUI-related tasks in existing work, this paper aims to enhance the GUI\nunderstanding and interacting capabilities of LVLMs through a data-driven\napproach. We propose EDGE, a general data synthesis framework that\nautomatically generates large-scale, multi-granularity training data from\nwebpages across the Web. Evaluation results on various GUI and agent benchmarks\ndemonstrate that the model trained with the dataset generated through EDGE\nexhibits superior webpage understanding capabilities, which can then be easily\ntransferred to previously unseen desktop and mobile environments. Our approach\nsignificantly reduces the dependence on manual annotations, empowering\nresearchers to harness the vast public resources available on the Web to\nadvance their work. Our source code, the dataset and the model are available at\nhttps://anonymous.4open.science/r/EDGE-1CDB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous agents operating on the graphical user interfaces (GUIs) of\nvarious applications hold immense practical value. Unlike the large language\nmodel (LLM)-based methods which rely on structured texts and customized\nbackends, the approaches using large vision-language models (LVLMs) are more\nintuitive and adaptable as they can visually perceive and directly interact\nwith screens, making them indispensable in general scenarios without text\nmetadata and tailored backends. Given the lack of high-quality training data\nfor GUI-related tasks in existing work, this paper aims to enhance the GUI\nunderstanding and interacting capabilities of LVLMs through a data-driven\napproach. We propose EDGE, a general data synthesis framework that\nautomatically generates large-scale, multi-granularity training data from\nwebpages across the Web. Evaluation results on various GUI and agent benchmarks\ndemonstrate that the model trained with the dataset generated through EDGE\nexhibits superior webpage understanding capabilities, which can then be easily\ntransferred to previously unseen desktop and mobile environments. Our approach\nsignificantly reduces the dependence on manual annotations, empowering\nresearchers to harness the vast public resources available on the Web to\nadvance their work. Our source code, the dataset and the model are available at\nhttps://anonymous.4open.science/r/EDGE-1CDB."
                },
                "authors": [
                    {
                        "name": "Xuetian Chen"
                    },
                    {
                        "name": "Hangcheng Li"
                    },
                    {
                        "name": "Jiaqing Liang"
                    },
                    {
                        "name": "Sihang Jiang"
                    },
                    {
                        "name": "Deqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Deqing Yang"
                },
                "author": "Deqing Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19460v1",
                "updated": "2024-10-25T10:45:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    10,
                    45,
                    17,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T10:45:17Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    10,
                    45,
                    17,
                    4,
                    299,
                    0
                ],
                "title": "Accelerating AI Performance using Anderson Extrapolation on GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating AI Performance using Anderson Extrapolation on GPUs"
                },
                "summary": "We present a novel approach for accelerating AI performance by leveraging\nAnderson extrapolation, a vector-to-vector mapping technique based on a window\nof historical iterations. By identifying the crossover point where a mixing\npenalty is incurred, the method focuses on reducing iterations to convergence,\nwith fewer more compute-intensive but generally cacheable iterations, balancing\nspeed and memory usage with accuracy and algorithmic stability, respectively.\nWe demonstrate significant improvements, in both training and inference,\nmotivated by scalability and efficiency extensions to the realm of\nhigh-performance computing (HPC).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel approach for accelerating AI performance by leveraging\nAnderson extrapolation, a vector-to-vector mapping technique based on a window\nof historical iterations. By identifying the crossover point where a mixing\npenalty is incurred, the method focuses on reducing iterations to convergence,\nwith fewer more compute-intensive but generally cacheable iterations, balancing\nspeed and memory usage with accuracy and algorithmic stability, respectively.\nWe demonstrate significant improvements, in both training and inference,\nmotivated by scalability and efficiency extensions to the realm of\nhigh-performance computing (HPC)."
                },
                "authors": [
                    {
                        "name": "Saleem Abdul Fattah Ahmed Al Dajani"
                    },
                    {
                        "name": "David E. Keyes"
                    }
                ],
                "author_detail": {
                    "name": "David E. Keyes"
                },
                "author": "David E. Keyes",
                "arxiv_comment": "6 pages, 6 figures, 1 table, Accepted by NeurIPS 2024 Workshop MLNCP\n  https://openreview.net/forum?id=wkP2ZFRn9e",
                "arxiv_journal_ref": "Neural Information Processing Systems (NeurIPS). Machine Learning\n  with New Compute Paradigms (MLNCP) Workshop, October 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04795v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04795v4",
                "updated": "2024-10-25T10:35:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    10,
                    35,
                    33,
                    4,
                    299,
                    0
                ],
                "published": "2024-05-08T04:01:40Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    4,
                    1,
                    40,
                    2,
                    129,
                    0
                ],
                "title": "Variational Schrödinger Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Schrödinger Diffusion Models"
                },
                "summary": "Schr\\\"odinger bridge (SB) has emerged as the go-to method for optimizing\ntransportation plans in diffusion models. However, SB requires estimating the\nintractable forward score functions, inevitably resulting in the costly\nimplicit training loss based on simulated trajectories. To improve the\nscalability while preserving efficient transportation plans, we leverage\nvariational inference to linearize the forward score functions (variational\nscores) of SB and restore simulation-free properties in training backward\nscores. We propose the variational Schr\\\"odinger diffusion model (VSDM), where\nthe forward process is a multivariate diffusion and the variational scores are\nadaptively optimized for efficient transport. Theoretically, we use stochastic\napproximation to prove the convergence of the variational scores and show the\nconvergence of the adaptively generated samples based on the optimal\nvariational scores. Empirically, we test the algorithm in simulated examples\nand observe that VSDM is efficient in generations of anisotropic shapes and\nyields straighter sample trajectories compared to the single-variate diffusion.\nWe also verify the scalability of the algorithm in real-world data and achieve\ncompetitive unconditional generation performance in CIFAR10 and conditional\ngeneration in time series modeling. Notably, VSDM no longer depends on warm-up\ninitializations and has become tuning-friendly in training large-scale\nexperiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Schr\\\"odinger bridge (SB) has emerged as the go-to method for optimizing\ntransportation plans in diffusion models. However, SB requires estimating the\nintractable forward score functions, inevitably resulting in the costly\nimplicit training loss based on simulated trajectories. To improve the\nscalability while preserving efficient transportation plans, we leverage\nvariational inference to linearize the forward score functions (variational\nscores) of SB and restore simulation-free properties in training backward\nscores. We propose the variational Schr\\\"odinger diffusion model (VSDM), where\nthe forward process is a multivariate diffusion and the variational scores are\nadaptively optimized for efficient transport. Theoretically, we use stochastic\napproximation to prove the convergence of the variational scores and show the\nconvergence of the adaptively generated samples based on the optimal\nvariational scores. Empirically, we test the algorithm in simulated examples\nand observe that VSDM is efficient in generations of anisotropic shapes and\nyields straighter sample trajectories compared to the single-variate diffusion.\nWe also verify the scalability of the algorithm in real-world data and achieve\ncompetitive unconditional generation performance in CIFAR10 and conditional\ngeneration in time series modeling. Notably, VSDM no longer depends on warm-up\ninitializations and has become tuning-friendly in training large-scale\nexperiments."
                },
                "authors": [
                    {
                        "name": "Wei Deng"
                    },
                    {
                        "name": "Weijian Luo"
                    },
                    {
                        "name": "Yixin Tan"
                    },
                    {
                        "name": "Marin Biloš"
                    },
                    {
                        "name": "Yu Chen"
                    },
                    {
                        "name": "Yuriy Nevmyvaka"
                    },
                    {
                        "name": "Ricky T. Q. Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ricky T. Q. Chen"
                },
                "author": "Ricky T. Q. Chen",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04795v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04795v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19456v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19456v1",
                "updated": "2024-10-25T10:30:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    10,
                    30,
                    21,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T10:30:21Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    10,
                    30,
                    21,
                    4,
                    299,
                    0
                ],
                "title": "Computational Bottlenecks of Training Small-scale Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Bottlenecks of Training Small-scale Large Language Models"
                },
                "summary": "While large language models (LLMs) dominate the AI landscape, Small-scale\nlarge Language Models (SLMs) are gaining attention due to cost and efficiency\ndemands from consumers. However, there is limited research on the training\nbehavior and computational requirements of SLMs. In this study, we explore the\ncomputational bottlenecks of training SLMs (up to 2B parameters) by examining\nthe effects of various hyperparameters and configurations, including GPU type,\nbatch size, model size, communication protocol, attention type, and the number\nof GPUs. We assess these factors on popular cloud services using metrics such\nas loss per dollar and tokens per second. Our findings aim to support the\nbroader adoption and optimization of language model training for low-resource\nAI research institutes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) dominate the AI landscape, Small-scale\nlarge Language Models (SLMs) are gaining attention due to cost and efficiency\ndemands from consumers. However, there is limited research on the training\nbehavior and computational requirements of SLMs. In this study, we explore the\ncomputational bottlenecks of training SLMs (up to 2B parameters) by examining\nthe effects of various hyperparameters and configurations, including GPU type,\nbatch size, model size, communication protocol, attention type, and the number\nof GPUs. We assess these factors on popular cloud services using metrics such\nas loss per dollar and tokens per second. Our findings aim to support the\nbroader adoption and optimization of language model training for low-resource\nAI research institutes."
                },
                "authors": [
                    {
                        "name": "Saleh Ashkboos"
                    },
                    {
                        "name": "Iman Mirzadeh"
                    },
                    {
                        "name": "Keivan Alizadeh"
                    },
                    {
                        "name": "Mohammad Hossein Sekhavat"
                    },
                    {
                        "name": "Moin Nabi"
                    },
                    {
                        "name": "Mehrdad Farajtabar"
                    },
                    {
                        "name": "Fartash Faghri"
                    }
                ],
                "author_detail": {
                    "name": "Fartash Faghri"
                },
                "author": "Fartash Faghri",
                "arxiv_comment": "8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19456v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19453v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19453v1",
                "updated": "2024-10-25T10:28:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    10,
                    28,
                    59,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T10:28:59Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    10,
                    28,
                    59,
                    4,
                    299,
                    0
                ],
                "title": "ShifCon: Enhancing Non-Dominant Language Capabilities with a Shift-based\n  Contrastive Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShifCon: Enhancing Non-Dominant Language Capabilities with a Shift-based\n  Contrastive Framework"
                },
                "summary": "Although fine-tuning Large Language Models (LLMs) with multilingual data can\nrapidly enhance the multilingual capabilities of LLMs, they still exhibit a\nperformance gap between the dominant language (e.g., English) and non-dominant\nones due to the imbalance of training data across languages. To further enhance\nthe performance of non-dominant languages, we propose ShifCon, a Shift-based\nContrastive framework that aligns the internal forward process of other\nlanguages toward that of the dominant one. Specifically, it shifts the\nrepresentations of non-dominant languages into the dominant language subspace,\nallowing them to access relatively rich information encoded in the model\nparameters. The enriched representations are then shifted back into their\noriginal language subspace before generation. Moreover, we introduce a subspace\ndistance metric to pinpoint the optimal layer area for shifting representations\nand employ multilingual contrastive learning to further enhance the alignment\nof representations within this area. Experiments demonstrate that our ShifCon\nframework significantly enhances the performance of non-dominant languages,\nparticularly for low-resource ones. Further analysis offers extra insights to\nverify the effectiveness of ShifCon and propel future research",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although fine-tuning Large Language Models (LLMs) with multilingual data can\nrapidly enhance the multilingual capabilities of LLMs, they still exhibit a\nperformance gap between the dominant language (e.g., English) and non-dominant\nones due to the imbalance of training data across languages. To further enhance\nthe performance of non-dominant languages, we propose ShifCon, a Shift-based\nContrastive framework that aligns the internal forward process of other\nlanguages toward that of the dominant one. Specifically, it shifts the\nrepresentations of non-dominant languages into the dominant language subspace,\nallowing them to access relatively rich information encoded in the model\nparameters. The enriched representations are then shifted back into their\noriginal language subspace before generation. Moreover, we introduce a subspace\ndistance metric to pinpoint the optimal layer area for shifting representations\nand employ multilingual contrastive learning to further enhance the alignment\nof representations within this area. Experiments demonstrate that our ShifCon\nframework significantly enhances the performance of non-dominant languages,\nparticularly for low-resource ones. Further analysis offers extra insights to\nverify the effectiveness of ShifCon and propel future research"
                },
                "authors": [
                    {
                        "name": "Hengyuan Zhang"
                    },
                    {
                        "name": "Chenming Shang"
                    },
                    {
                        "name": "Sizhe Wang"
                    },
                    {
                        "name": "Dongdong Zhang"
                    },
                    {
                        "name": "Feng Yao"
                    },
                    {
                        "name": "Renliang Sun"
                    },
                    {
                        "name": "Yiyao Yu"
                    },
                    {
                        "name": "Yujiu Yang"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "23 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19453v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19453v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19452v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19452v1",
                "updated": "2024-10-25T10:28:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    10,
                    28,
                    26,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T10:28:26Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    10,
                    28,
                    26,
                    4,
                    299,
                    0
                ],
                "title": "NeuroClips: Towards High-fidelity and Smooth fMRI-to-Video\n  Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuroClips: Towards High-fidelity and Smooth fMRI-to-Video\n  Reconstruction"
                },
                "summary": "Reconstruction of static visual stimuli from non-invasion brain activity fMRI\nachieves great success, owning to advanced deep learning models such as CLIP\nand Stable Diffusion. However, the research on fMRI-to-video reconstruction\nremains limited since decoding the spatiotemporal perception of continuous\nvisual experiences is formidably challenging. We contend that the key to\naddressing these challenges lies in accurately decoding both high-level\nsemantics and low-level perception flows, as perceived by the brain in response\nto video stimuli. To the end, we propose NeuroClips, an innovative framework to\ndecode high-fidelity and smooth video from fMRI. NeuroClips utilizes a\nsemantics reconstructor to reconstruct video keyframes, guiding semantic\naccuracy and consistency, and employs a perception reconstructor to capture\nlow-level perceptual details, ensuring video smoothness. During inference, it\nadopts a pre-trained T2V diffusion model injected with both keyframes and\nlow-level perception flows for video reconstruction. Evaluated on a publicly\navailable fMRI-video dataset, NeuroClips achieves smooth high-fidelity video\nreconstruction of up to 6s at 8FPS, gaining significant improvements over\nstate-of-the-art models in various metrics, e.g., a 128\\% improvement in SSIM\nand an 81\\% improvement in spatiotemporal metrics. Our project is available at\nhttps://github.com/gongzix/NeuroClips}{https://github.com/gongzix/NeuroClips.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstruction of static visual stimuli from non-invasion brain activity fMRI\nachieves great success, owning to advanced deep learning models such as CLIP\nand Stable Diffusion. However, the research on fMRI-to-video reconstruction\nremains limited since decoding the spatiotemporal perception of continuous\nvisual experiences is formidably challenging. We contend that the key to\naddressing these challenges lies in accurately decoding both high-level\nsemantics and low-level perception flows, as perceived by the brain in response\nto video stimuli. To the end, we propose NeuroClips, an innovative framework to\ndecode high-fidelity and smooth video from fMRI. NeuroClips utilizes a\nsemantics reconstructor to reconstruct video keyframes, guiding semantic\naccuracy and consistency, and employs a perception reconstructor to capture\nlow-level perceptual details, ensuring video smoothness. During inference, it\nadopts a pre-trained T2V diffusion model injected with both keyframes and\nlow-level perception flows for video reconstruction. Evaluated on a publicly\navailable fMRI-video dataset, NeuroClips achieves smooth high-fidelity video\nreconstruction of up to 6s at 8FPS, gaining significant improvements over\nstate-of-the-art models in various metrics, e.g., a 128\\% improvement in SSIM\nand an 81\\% improvement in spatiotemporal metrics. Our project is available at\nhttps://github.com/gongzix/NeuroClips}{https://github.com/gongzix/NeuroClips."
                },
                "authors": [
                    {
                        "name": "Zixuan Gong"
                    },
                    {
                        "name": "Guangyin Bao"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Duoqian Miao"
                    },
                    {
                        "name": "Shoujin Wang"
                    },
                    {
                        "name": "Lei Zhu"
                    },
                    {
                        "name": "Changwei Wang"
                    },
                    {
                        "name": "Rongtao Xu"
                    },
                    {
                        "name": "Liang Hu"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Yu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Zhang"
                },
                "author": "Yu Zhang",
                "arxiv_comment": "NeurIPS 2024 Oral",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19452v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19452v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19451v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19451v1",
                "updated": "2024-10-25T10:24:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    10,
                    24,
                    30,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T10:24:30Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    10,
                    24,
                    30,
                    4,
                    299,
                    0
                ],
                "title": "Intelligent Understanding of Large Language Models in Traditional\n  Chinese Medicine Based on Prompt Engineering Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent Understanding of Large Language Models in Traditional\n  Chinese Medicine Based on Prompt Engineering Framework"
                },
                "summary": "This paper explores the application of prompt engineering to enhance the\nperformance of large language models (LLMs) in the domain of Traditional\nChinese Medicine (TCM). We propose TCM-Prompt, a framework that integrates\nvarious pre-trained language models (PLMs), templates, tokenization, and\nverbalization methods, allowing researchers to easily construct and fine-tune\nmodels for specific TCM-related tasks. We conducted experiments on disease\nclassification, syndrome identification, herbal medicine recommendation, and\ngeneral NLP tasks, demonstrating the effectiveness and superiority of our\napproach compared to baseline methods. Our findings suggest that prompt\nengineering is a promising technique for improving the performance of LLMs in\nspecialized domains like TCM, with potential applications in digitalization,\nmodernization, and personalized medicine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the application of prompt engineering to enhance the\nperformance of large language models (LLMs) in the domain of Traditional\nChinese Medicine (TCM). We propose TCM-Prompt, a framework that integrates\nvarious pre-trained language models (PLMs), templates, tokenization, and\nverbalization methods, allowing researchers to easily construct and fine-tune\nmodels for specific TCM-related tasks. We conducted experiments on disease\nclassification, syndrome identification, herbal medicine recommendation, and\ngeneral NLP tasks, demonstrating the effectiveness and superiority of our\napproach compared to baseline methods. Our findings suggest that prompt\nengineering is a promising technique for improving the performance of LLMs in\nspecialized domains like TCM, with potential applications in digitalization,\nmodernization, and personalized medicine."
                },
                "authors": [
                    {
                        "name": "Yirui Chen"
                    },
                    {
                        "name": "Qinyu Xiao"
                    },
                    {
                        "name": "Jia Yi"
                    },
                    {
                        "name": "Jing Chen"
                    },
                    {
                        "name": "Mengyang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengyang Wang"
                },
                "author": "Mengyang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19451v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10645v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10645v3",
                "updated": "2024-10-25T10:23:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    10,
                    23,
                    2,
                    4,
                    299,
                    0
                ],
                "published": "2024-08-20T08:36:59Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    8,
                    36,
                    59,
                    1,
                    233,
                    0
                ],
                "title": "CoRA: Collaborative Information Perception by Large Language Model's\n  Weights for Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoRA: Collaborative Information Perception by Large Language Model's\n  Weights for Recommendation"
                },
                "summary": "Involving collaborative information in Large Language Models (LLMs) is a\npromising technique for adapting LLMs for recommendation. Existing methods\nachieve this by concatenating collaborative features with text tokens into a\nunified sequence input and then fine-tuning to align these features with LLM's\ninput space. Although effective, in this work, we identify two limitations when\nadapting LLMs to recommendation tasks, which hinder the integration of general\nknowledge and collaborative information, resulting in sub-optimal\nrecommendation performance. (1) Fine-tuning LLM with recommendation data can\nundermine its inherent world knowledge and fundamental competencies, which are\ncrucial for interpreting and inferring recommendation text. (2) Incorporating\ncollaborative features into textual prompts disrupts the semantics of the\noriginal prompts, preventing LLM from generating appropriate outputs. In this\npaper, we propose a new paradigm, \\textbf{Co}llaborative \\textbf{Lo}RA (CoRA),\nwith a collaborative query generator. Rather than input space alignment, this\nmethod aligns collaborative information with LLM's parameter space,\nrepresenting them as incremental weights to update LLM's output. This way, LLM\nperceives collaborative information without altering its general knowledge and\ntext inference capabilities. Specifically, we employ a collaborative filtering\nmodel to extract user and item embeddings and inject them into a set number of\nlearnable queries. We then convert collaborative queries into collaborative\nweights with low-rank properties and merge the collaborative weights into LLM's\nweights, enabling LLM to perceive the collaborative signals and generate\npersonalized recommendations without fine-tuning or extra collaborative tokens\nin prompts. Extensive experiments confirm that CoRA effectively integrates\ncollaborative information into LLM, enhancing recommendation performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Involving collaborative information in Large Language Models (LLMs) is a\npromising technique for adapting LLMs for recommendation. Existing methods\nachieve this by concatenating collaborative features with text tokens into a\nunified sequence input and then fine-tuning to align these features with LLM's\ninput space. Although effective, in this work, we identify two limitations when\nadapting LLMs to recommendation tasks, which hinder the integration of general\nknowledge and collaborative information, resulting in sub-optimal\nrecommendation performance. (1) Fine-tuning LLM with recommendation data can\nundermine its inherent world knowledge and fundamental competencies, which are\ncrucial for interpreting and inferring recommendation text. (2) Incorporating\ncollaborative features into textual prompts disrupts the semantics of the\noriginal prompts, preventing LLM from generating appropriate outputs. In this\npaper, we propose a new paradigm, \\textbf{Co}llaborative \\textbf{Lo}RA (CoRA),\nwith a collaborative query generator. Rather than input space alignment, this\nmethod aligns collaborative information with LLM's parameter space,\nrepresenting them as incremental weights to update LLM's output. This way, LLM\nperceives collaborative information without altering its general knowledge and\ntext inference capabilities. Specifically, we employ a collaborative filtering\nmodel to extract user and item embeddings and inject them into a set number of\nlearnable queries. We then convert collaborative queries into collaborative\nweights with low-rank properties and merge the collaborative weights into LLM's\nweights, enabling LLM to perceive the collaborative signals and generate\npersonalized recommendations without fine-tuning or extra collaborative tokens\nin prompts. Extensive experiments confirm that CoRA effectively integrates\ncollaborative information into LLM, enhancing recommendation performance."
                },
                "authors": [
                    {
                        "name": "Yuting Liu"
                    },
                    {
                        "name": "Jinghao Zhang"
                    },
                    {
                        "name": "Yizhou Dang"
                    },
                    {
                        "name": "Yuliang Liang"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Guibing Guo"
                    },
                    {
                        "name": "Jianzhe Zhao"
                    },
                    {
                        "name": "Xingwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xingwei Wang"
                },
                "author": "Xingwei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10645v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10645v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14466v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14466v2",
                "updated": "2024-10-25T09:48:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    9,
                    48,
                    5,
                    4,
                    299,
                    0
                ],
                "published": "2024-06-20T16:30:01Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    16,
                    30,
                    1,
                    3,
                    172,
                    0
                ],
                "title": "The Radius of the High-mass Pulsar PSR J0740+6620 with 3.6 yr of NICER\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Radius of the High-mass Pulsar PSR J0740+6620 with 3.6 yr of NICER\n  Data"
                },
                "summary": "We report an updated analysis of the radius, mass, and heated surface regions\nof the massive pulsar PSR J0740+6620 using Neutron Star Interior Composition\nExplorer (NICER) data from 2018 September 21 to 2022 April 21, a substantial\nincrease in data set size compared to previous analyses. Using a tight mass\nprior from radio timing measurements and jointly modeling the new NICER data\nwith XMM-Newton data, the inferred equatorial radius and gravitational mass are\n$12.49_{-0.88}^{+1.28}$ km and $2.073_{-0.069}^{+0.069}$ $M_\\odot$\nrespectively, each reported as the posterior credible interval bounded by the\n$16\\,\\%$ and $84\\,\\%$ quantiles, with an estimated systematic error $\\lesssim\n0.1$ km. This result was obtained using the best computationally feasible\nsampler settings providing a strong radius lower limit but a slightly more\nuncertain radius upper limit. The inferred radius interval is also close to the\n$R=12.76_{-1.02}^{+1.49}$ km obtained by Dittmann et al., when they require the\nradius to be less than $16$ km as we do. The results continue to disfavor very\nsoft equations of state for dense matter, with $R<11.15$ km for this high-mass\npulsar excluded at the $95\\,\\%$ probability. The results do not depend\nsignificantly on the assumed cross-calibration uncertainty between NICER and\nXMM-Newton. Using simulated data that resemble the actual observations, we also\nshow that our pipeline is capable of recovering parameters for the inferred\nmodels reported in this paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report an updated analysis of the radius, mass, and heated surface regions\nof the massive pulsar PSR J0740+6620 using Neutron Star Interior Composition\nExplorer (NICER) data from 2018 September 21 to 2022 April 21, a substantial\nincrease in data set size compared to previous analyses. Using a tight mass\nprior from radio timing measurements and jointly modeling the new NICER data\nwith XMM-Newton data, the inferred equatorial radius and gravitational mass are\n$12.49_{-0.88}^{+1.28}$ km and $2.073_{-0.069}^{+0.069}$ $M_\\odot$\nrespectively, each reported as the posterior credible interval bounded by the\n$16\\,\\%$ and $84\\,\\%$ quantiles, with an estimated systematic error $\\lesssim\n0.1$ km. This result was obtained using the best computationally feasible\nsampler settings providing a strong radius lower limit but a slightly more\nuncertain radius upper limit. The inferred radius interval is also close to the\n$R=12.76_{-1.02}^{+1.49}$ km obtained by Dittmann et al., when they require the\nradius to be less than $16$ km as we do. The results continue to disfavor very\nsoft equations of state for dense matter, with $R<11.15$ km for this high-mass\npulsar excluded at the $95\\,\\%$ probability. The results do not depend\nsignificantly on the assumed cross-calibration uncertainty between NICER and\nXMM-Newton. Using simulated data that resemble the actual observations, we also\nshow that our pipeline is capable of recovering parameters for the inferred\nmodels reported in this paper."
                },
                "authors": [
                    {
                        "name": "Tuomo Salmi"
                    },
                    {
                        "name": "Devarshi Choudhury"
                    },
                    {
                        "name": "Yves Kini"
                    },
                    {
                        "name": "Thomas E. Riley"
                    },
                    {
                        "name": "Serena Vinciguerra"
                    },
                    {
                        "name": "Anna L. Watts"
                    },
                    {
                        "name": "Michael T. Wolff"
                    },
                    {
                        "name": "Zaven Arzoumanian"
                    },
                    {
                        "name": "Slavko Bogdanov"
                    },
                    {
                        "name": "Deepto Chakrabarty"
                    },
                    {
                        "name": "Keith Gendreau"
                    },
                    {
                        "name": "Sebastien Guillot"
                    },
                    {
                        "name": "Wynn C. G. Ho"
                    },
                    {
                        "name": "Daniela Huppenkothen"
                    },
                    {
                        "name": "Renee M. Ludlam"
                    },
                    {
                        "name": "Sharon M. Morsink"
                    },
                    {
                        "name": "Paul S. Ray"
                    }
                ],
                "author_detail": {
                    "name": "Paul S. Ray"
                },
                "author": "Paul S. Ray",
                "arxiv_doi": "10.3847/1538-4357/ad5f1f",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/ad5f1f",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.14466v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14466v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "17 pages, 9 figures (2 of which are figure sets), 2 tables, published\n  in ApJ",
                "arxiv_journal_ref": "ApJ 974 294 (2024)",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01469v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01469v2",
                "updated": "2024-10-25T09:30:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    9,
                    30,
                    32,
                    4,
                    299,
                    0
                ],
                "published": "2024-02-02T14:56:48Z",
                "published_parsed": [
                    2024,
                    2,
                    2,
                    14,
                    56,
                    48,
                    4,
                    33,
                    0
                ],
                "title": "AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through\n  Process Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through\n  Process Feedback"
                },
                "summary": "The notable success of large language models (LLMs) has sparked an upsurge in\nbuilding language agents to complete various complex tasks. We present AMOR, an\nagent framework based on open-source LLMs, which reasons with external\nknowledge bases and adapts to specific domains through human supervision to the\nreasoning process. AMOR builds reasoning logic over a finite state machine\n(FSM) that solves problems through autonomous executions and transitions over\ndisentangled modules. This allows humans to provide direct feedback to the\nindividual modules, and thus naturally forms process supervision. Based on this\nreasoning and feedback framework, we develop AMOR through two-stage\nfine-tuning: warm-up and adaptation. The former fine-tunes the LLM with\nexamples automatically constructed from various public datasets, enabling AMOR\nto generalize across different knowledge environments, while the latter tailors\nAMOR to specific domains using process feedback. Extensive experiments across\nmultiple domains demonstrate the advantage of AMOR to strong baselines, thanks\nto its FSM-based reasoning and process feedback mechanism. The code and data\nare publicly available at \\url{https://github.com/JianGuanTHU/AMOR}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The notable success of large language models (LLMs) has sparked an upsurge in\nbuilding language agents to complete various complex tasks. We present AMOR, an\nagent framework based on open-source LLMs, which reasons with external\nknowledge bases and adapts to specific domains through human supervision to the\nreasoning process. AMOR builds reasoning logic over a finite state machine\n(FSM) that solves problems through autonomous executions and transitions over\ndisentangled modules. This allows humans to provide direct feedback to the\nindividual modules, and thus naturally forms process supervision. Based on this\nreasoning and feedback framework, we develop AMOR through two-stage\nfine-tuning: warm-up and adaptation. The former fine-tunes the LLM with\nexamples automatically constructed from various public datasets, enabling AMOR\nto generalize across different knowledge environments, while the latter tailors\nAMOR to specific domains using process feedback. Extensive experiments across\nmultiple domains demonstrate the advantage of AMOR to strong baselines, thanks\nto its FSM-based reasoning and process feedback mechanism. The code and data\nare publicly available at \\url{https://github.com/JianGuanTHU/AMOR}."
                },
                "authors": [
                    {
                        "name": "Jian Guan"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zujie Wen"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Hongning Wang"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01469v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01469v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.05736v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.05736v2",
                "updated": "2024-10-25T09:28:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    9,
                    28,
                    5,
                    4,
                    299,
                    0
                ],
                "published": "2023-08-10T17:56:53Z",
                "published_parsed": [
                    2023,
                    8,
                    10,
                    17,
                    56,
                    53,
                    3,
                    222,
                    0
                ],
                "title": "MapTRv2: An End-to-End Framework for Online Vectorized HD Map\n  Construction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MapTRv2: An End-to-End Framework for Online Vectorized HD Map\n  Construction"
                },
                "summary": "High-definition (HD) map provides abundant and precise static environmental\ninformation of the driving scene, serving as a fundamental and indispensable\ncomponent for planning in autonomous driving system. In this paper, we present\n\\textbf{Map} \\textbf{TR}ansformer, an end-to-end framework for online\nvectorized HD map construction. We propose a unified permutation-equivalent\nmodeling approach, \\ie, modeling map element as a point set with a group of\nequivalent permutations, which accurately describes the shape of map element\nand stabilizes the learning process. We design a hierarchical query embedding\nscheme to flexibly encode structured map information and perform hierarchical\nbipartite matching for map element learning. To speed up convergence, we\nfurther introduce auxiliary one-to-many matching and dense supervision. The\nproposed method well copes with various map elements with arbitrary shapes. It\nruns at real-time inference speed and achieves state-of-the-art performance on\nboth nuScenes and Argoverse2 datasets. Abundant qualitative results show stable\nand robust map construction quality in complex and various driving scenes. Code\nand more demos are available at \\url{https://github.com/hustvl/MapTR} for\nfacilitating further studies and applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-definition (HD) map provides abundant and precise static environmental\ninformation of the driving scene, serving as a fundamental and indispensable\ncomponent for planning in autonomous driving system. In this paper, we present\n\\textbf{Map} \\textbf{TR}ansformer, an end-to-end framework for online\nvectorized HD map construction. We propose a unified permutation-equivalent\nmodeling approach, \\ie, modeling map element as a point set with a group of\nequivalent permutations, which accurately describes the shape of map element\nand stabilizes the learning process. We design a hierarchical query embedding\nscheme to flexibly encode structured map information and perform hierarchical\nbipartite matching for map element learning. To speed up convergence, we\nfurther introduce auxiliary one-to-many matching and dense supervision. The\nproposed method well copes with various map elements with arbitrary shapes. It\nruns at real-time inference speed and achieves state-of-the-art performance on\nboth nuScenes and Argoverse2 datasets. Abundant qualitative results show stable\nand robust map construction quality in complex and various driving scenes. Code\nand more demos are available at \\url{https://github.com/hustvl/MapTR} for\nfacilitating further studies and applications."
                },
                "authors": [
                    {
                        "name": "Bencheng Liao"
                    },
                    {
                        "name": "Shaoyu Chen"
                    },
                    {
                        "name": "Yunchi Zhang"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Chang Huang"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang",
                "arxiv_comment": "Accepted to IJCV 2024. Code available at\n  https://github.com/hustvl/MapTR . arXiv admin note: substantial text overlap\n  with arXiv:2208.14437",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.05736v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.05736v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04992v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04992v3",
                "updated": "2024-10-25T09:25:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    9,
                    25,
                    0,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-07T12:48:03Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    12,
                    48,
                    3,
                    0,
                    281,
                    0
                ],
                "title": "MC-QDSNN: Quantized Deep evolutionary SNN with Multi-Dendritic\n  Compartment Neurons for Stress Detection using Physiological Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MC-QDSNN: Quantized Deep evolutionary SNN with Multi-Dendritic\n  Compartment Neurons for Stress Detection using Physiological Signals"
                },
                "summary": "Long short-term memory (LSTM) has emerged as a definitive network for\nanalyzing and inferring time series data. LSTM has the capability to extract\nspectral features and a mixture of temporal features. Due to this benefit, a\nsimilar feature extraction method is explored for the spiking counterparts\ntargeting time-series data. Though LSTMs perform well in their spiking form,\nthey tend to be compute and power intensive. Addressing this issue, this work\nproposes Multi-Compartment Leaky (MCLeaky) neuron as a viable alternative for\nefficient processing of time series data. The MCLeaky neuron, derived from the\nLeaky Integrate and Fire (LIF) neuron model, contains multiple memristive\nsynapses interlinked to form a memory component, which emulates the human\nbrain's Hippocampus region. The proposed MCLeaky neuron based Spiking Neural\nNetwork model and its quantized variant were benchmarked against\nstate-of-the-art (SOTA) Spiking LSTMs to perform human stress detection, by\ncomparing compute requirements, latency and real-world performances on unseen\ndata with models derived through Neural Architecture Search (NAS). Results show\nthat networks with MCLeaky activation neuron managed a superior accuracy of\n98.8% to detect stress based on Electrodermal Activity (EDA) signals, better\nthan any other investigated models, while using 20% less parameters on average.\nMCLeaky neuron was also tested for various signals including EDA Wrist and\nChest, Temperature, ECG, and combinations of them. Quantized MCLeaky model was\nalso derived and validated to forecast their performance on hardware\narchitectures, which resulted in 91.84% accuracy. The neurons were evaluated\nfor multiple modalities of data towards stress detection, which resulted in\nenergy savings of 25.12x to 39.20x and EDP gains of 52.37x to 81.9x over ANNs,\nwhile offering a best accuracy of 98.8% when compared with the rest of the SOTA\nimplementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long short-term memory (LSTM) has emerged as a definitive network for\nanalyzing and inferring time series data. LSTM has the capability to extract\nspectral features and a mixture of temporal features. Due to this benefit, a\nsimilar feature extraction method is explored for the spiking counterparts\ntargeting time-series data. Though LSTMs perform well in their spiking form,\nthey tend to be compute and power intensive. Addressing this issue, this work\nproposes Multi-Compartment Leaky (MCLeaky) neuron as a viable alternative for\nefficient processing of time series data. The MCLeaky neuron, derived from the\nLeaky Integrate and Fire (LIF) neuron model, contains multiple memristive\nsynapses interlinked to form a memory component, which emulates the human\nbrain's Hippocampus region. The proposed MCLeaky neuron based Spiking Neural\nNetwork model and its quantized variant were benchmarked against\nstate-of-the-art (SOTA) Spiking LSTMs to perform human stress detection, by\ncomparing compute requirements, latency and real-world performances on unseen\ndata with models derived through Neural Architecture Search (NAS). Results show\nthat networks with MCLeaky activation neuron managed a superior accuracy of\n98.8% to detect stress based on Electrodermal Activity (EDA) signals, better\nthan any other investigated models, while using 20% less parameters on average.\nMCLeaky neuron was also tested for various signals including EDA Wrist and\nChest, Temperature, ECG, and combinations of them. Quantized MCLeaky model was\nalso derived and validated to forecast their performance on hardware\narchitectures, which resulted in 91.84% accuracy. The neurons were evaluated\nfor multiple modalities of data towards stress detection, which resulted in\nenergy savings of 25.12x to 39.20x and EDP gains of 52.37x to 81.9x over ANNs,\nwhile offering a best accuracy of 98.8% when compared with the rest of the SOTA\nimplementations."
                },
                "authors": [
                    {
                        "name": "Ajay B S"
                    },
                    {
                        "name": "Phani Pavan K"
                    },
                    {
                        "name": "Madhav Rao"
                    }
                ],
                "author_detail": {
                    "name": "Madhav Rao"
                },
                "author": "Madhav Rao",
                "arxiv_doi": "10.1109/TCAD.2024.3484353",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TCAD.2024.3484353",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.04992v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04992v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "13 pages, 15 figures. Published to IEEE Transactions on Computer\n  Aided Design",
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19420v1",
                "updated": "2024-10-25T09:23:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    9,
                    23,
                    59,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T09:23:59Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    9,
                    23,
                    59,
                    4,
                    299,
                    0
                ],
                "title": "Doppler correlation-driven vetoes for the Frequency Hough analysis in\n  continuous gravitational-wave searches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doppler correlation-driven vetoes for the Frequency Hough analysis in\n  continuous gravitational-wave searches"
                },
                "summary": "We present an improved method for vetoing candidates of continuous\ngravitational-wave sources during all-sky searches utilizing the Frequency\nHough pipeline. This approach leverages linear correlations between source\nparameters induced by the Earth Doppler effect, which can be effectively\nidentified through the Hough Transform. Candidates that do not align with these\npatterns are considered spurious and can thus be vetoed, enhancing the depth\nand statistical significance of follow-up analyses. Additionally, we provide a\ncomprehensive explanation of the method calibration, which intrinsically linked\nto the total duration of the observing run. On average, the procedure\nsuccessfully vetoes $56\\%$ of candidates. To assess the method performance, we\nconducted a Monte-Carlo simulation injecting fake continuous-wave signals into\ndata from the third observing run of the LIGO detectors. This analysis allowed\nus to infer strain amplitude upper limits at a $90\\%$ confidence level. We\nfound that the optimal sensitivity is $h_0^{90\\%} = 3.62^{+0.23}_{-0.22}\\times\n10^{-26}$ in the [128, 200] Hz band, which is within the most sensible\nfrequency band of the LIGO detectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an improved method for vetoing candidates of continuous\ngravitational-wave sources during all-sky searches utilizing the Frequency\nHough pipeline. This approach leverages linear correlations between source\nparameters induced by the Earth Doppler effect, which can be effectively\nidentified through the Hough Transform. Candidates that do not align with these\npatterns are considered spurious and can thus be vetoed, enhancing the depth\nand statistical significance of follow-up analyses. Additionally, we provide a\ncomprehensive explanation of the method calibration, which intrinsically linked\nto the total duration of the observing run. On average, the procedure\nsuccessfully vetoes $56\\%$ of candidates. To assess the method performance, we\nconducted a Monte-Carlo simulation injecting fake continuous-wave signals into\ndata from the third observing run of the LIGO detectors. This analysis allowed\nus to infer strain amplitude upper limits at a $90\\%$ confidence level. We\nfound that the optimal sensitivity is $h_0^{90\\%} = 3.62^{+0.23}_{-0.22}\\times\n10^{-26}$ in the [128, 200] Hz band, which is within the most sensible\nfrequency band of the LIGO detectors."
                },
                "authors": [
                    {
                        "name": "Matteo Di Giovanni"
                    },
                    {
                        "name": "Paola Leaci"
                    },
                    {
                        "name": "Pia Astone"
                    },
                    {
                        "name": "Stefano Dal Pra"
                    },
                    {
                        "name": "Sabrina D'Atonio"
                    },
                    {
                        "name": "Luca D'Onofrio"
                    },
                    {
                        "name": "Sergio Frasca"
                    },
                    {
                        "name": "Federico Muciaccia"
                    },
                    {
                        "name": "Cristiano Palomba"
                    },
                    {
                        "name": "Lorenzo Pierini"
                    },
                    {
                        "name": "Francesco Safai Tehrani"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Safai Tehrani"
                },
                "author": "Francesco Safai Tehrani",
                "arxiv_comment": "13 pages, 9 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19419v1",
                "updated": "2024-10-25T09:23:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    9,
                    23,
                    24,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T09:23:24Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    9,
                    23,
                    24,
                    4,
                    299,
                    0
                ],
                "title": "KAHANI: Culturally-Nuanced Visual Storytelling Pipeline for Non-Western\n  Cultures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KAHANI: Culturally-Nuanced Visual Storytelling Pipeline for Non-Western\n  Cultures"
                },
                "summary": "Large Language Models (LLMs) and Text-To-Image (T2I) models have demonstrated\nthe ability to generate compelling text and visual stories. However, their\noutputs are predominantly aligned with the sensibilities of the Global North,\noften resulting in an outsider's gaze on other cultures. As a result,\nnon-Western communities have to put extra effort into generating culturally\nspecific stories. To address this challenge, we developed a visual storytelling\npipeline called KAHANI that generates culturally grounded visual stories for\nnon-Western cultures. Our pipeline leverages off-the-shelf models GPT-4 Turbo\nand Stable Diffusion XL (SDXL). By using Chain of Thought (CoT) and T2I\nprompting techniques, we capture the cultural context from user's prompt and\ngenerate vivid descriptions of the characters and scene compositions. To\nevaluate the effectiveness of KAHANI, we conducted a comparative user study\nwith ChatGPT-4 (with DALL-E3) in which participants from different regions of\nIndia compared the cultural relevance of stories generated by the two tools.\nResults from the qualitative and quantitative analysis performed on the user\nstudy showed that KAHANI was able to capture and incorporate more Culturally\nSpecific Items (CSIs) compared to ChatGPT-4. In terms of both its cultural\ncompetence and visual story generation quality, our pipeline outperformed\nChatGPT-4 in 27 out of the 36 comparisons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and Text-To-Image (T2I) models have demonstrated\nthe ability to generate compelling text and visual stories. However, their\noutputs are predominantly aligned with the sensibilities of the Global North,\noften resulting in an outsider's gaze on other cultures. As a result,\nnon-Western communities have to put extra effort into generating culturally\nspecific stories. To address this challenge, we developed a visual storytelling\npipeline called KAHANI that generates culturally grounded visual stories for\nnon-Western cultures. Our pipeline leverages off-the-shelf models GPT-4 Turbo\nand Stable Diffusion XL (SDXL). By using Chain of Thought (CoT) and T2I\nprompting techniques, we capture the cultural context from user's prompt and\ngenerate vivid descriptions of the characters and scene compositions. To\nevaluate the effectiveness of KAHANI, we conducted a comparative user study\nwith ChatGPT-4 (with DALL-E3) in which participants from different regions of\nIndia compared the cultural relevance of stories generated by the two tools.\nResults from the qualitative and quantitative analysis performed on the user\nstudy showed that KAHANI was able to capture and incorporate more Culturally\nSpecific Items (CSIs) compared to ChatGPT-4. In terms of both its cultural\ncompetence and visual story generation quality, our pipeline outperformed\nChatGPT-4 in 27 out of the 36 comparisons."
                },
                "authors": [
                    {
                        "name": "Hamna"
                    },
                    {
                        "name": "Deepthi Sudharsan"
                    },
                    {
                        "name": "Agrima Seth"
                    },
                    {
                        "name": "Ritvik Budhiraja"
                    },
                    {
                        "name": "Deepika Khullar"
                    },
                    {
                        "name": "Vyshak Jain"
                    },
                    {
                        "name": "Kalika Bali"
                    },
                    {
                        "name": "Aditya Vashistha"
                    },
                    {
                        "name": "Sameer Segal"
                    }
                ],
                "author_detail": {
                    "name": "Sameer Segal"
                },
                "author": "Sameer Segal",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04838v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04838v2",
                "updated": "2024-10-25T09:11:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    9,
                    11,
                    41,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-07T08:53:00Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    8,
                    53,
                    0,
                    0,
                    281,
                    0
                ],
                "title": "Rationale-Aware Answer Verification by Pairwise Self-Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rationale-Aware Answer Verification by Pairwise Self-Evaluation"
                },
                "summary": "Answer verification identifies correct solutions among candidates generated\nby large language models (LLMs). Current approaches typically train verifier\nmodels by labeling solutions as correct or incorrect based solely on whether\nthe final answer matches the gold answer. However, this approach neglects any\nflawed rationale in the solution yielding the correct answer, undermining the\nverifier's ability to distinguish between sound and flawed rationales. We\nempirically show that in StrategyQA, only 19% of LLM-generated solutions with\ncorrect answers have valid rationales, thus leading to an unreliable verifier.\nFurthermore, we demonstrate that training a verifier on valid rationales\nsignificantly improves its ability to distinguish valid and flawed rationale.\nTo make a better verifier without extra human supervision, we introduce REPS\n(Rationale Enhancement through Pairwise Selection), a method for selecting\nvalid rationales from candidates by iteratively applying pairwise\nself-evaluation using the same LLM that generates the solutions. Verifiers\ntrained on solutions selected by REPS outperform those trained using\nconventional training methods on three reasoning benchmarks (ARC-Challenge,\nDROP, and StrategyQA). Our results suggest that training reliable verifiers\nrequires ensuring the validity of rationales in addition to the correctness of\nthe final answers, which would be critical for models assisting humans in\nsolving complex reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Answer verification identifies correct solutions among candidates generated\nby large language models (LLMs). Current approaches typically train verifier\nmodels by labeling solutions as correct or incorrect based solely on whether\nthe final answer matches the gold answer. However, this approach neglects any\nflawed rationale in the solution yielding the correct answer, undermining the\nverifier's ability to distinguish between sound and flawed rationales. We\nempirically show that in StrategyQA, only 19% of LLM-generated solutions with\ncorrect answers have valid rationales, thus leading to an unreliable verifier.\nFurthermore, we demonstrate that training a verifier on valid rationales\nsignificantly improves its ability to distinguish valid and flawed rationale.\nTo make a better verifier without extra human supervision, we introduce REPS\n(Rationale Enhancement through Pairwise Selection), a method for selecting\nvalid rationales from candidates by iteratively applying pairwise\nself-evaluation using the same LLM that generates the solutions. Verifiers\ntrained on solutions selected by REPS outperform those trained using\nconventional training methods on three reasoning benchmarks (ARC-Challenge,\nDROP, and StrategyQA). Our results suggest that training reliable verifiers\nrequires ensuring the validity of rationales in addition to the correctness of\nthe final answers, which would be critical for models assisting humans in\nsolving complex reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Akira Kawabata"
                    },
                    {
                        "name": "Saku Sugawara"
                    }
                ],
                "author_detail": {
                    "name": "Saku Sugawara"
                },
                "author": "Saku Sugawara",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04838v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04838v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18792v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18792v2",
                "updated": "2024-10-25T09:00:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    9,
                    0,
                    7,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-24T14:47:25Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    47,
                    25,
                    3,
                    298,
                    0
                ],
                "title": "An LLM Agent for Automatic Geospatial Data Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An LLM Agent for Automatic Geospatial Data Analysis"
                },
                "summary": "Large language models (LLMs) are being used in data science code generation\ntasks, but they often struggle with complex sequential tasks, leading to\nlogical errors. Their application to geospatial data processing is particularly\nchallenging due to difficulties in incorporating complex data structures and\nspatial constraints, effectively utilizing diverse function calls, and the\ntendency to hallucinate less-used geospatial libraries. To tackle these\nproblems, we introduce GeoAgent, a new interactive framework designed to help\nLLMs handle geospatial data processing more effectively. GeoAgent pioneers the\nintegration of a code interpreter, static analysis, and Retrieval-Augmented\nGeneration (RAG) techniques within a Monte Carlo Tree Search (MCTS) algorithm,\noffering a novel approach to geospatial data processing. In addition, we\ncontribute a new benchmark specifically designed to evaluate the LLM-based\napproach in geospatial tasks. This benchmark leverages a variety of Python\nlibraries and includes both single-turn and multi-turn tasks such as data\nacquisition, data analysis, and visualization. By offering a comprehensive\nevaluation among diverse geospatial contexts, this benchmark sets a new\nstandard for developing LLM-based approaches in geospatial data analysis tasks.\nOur findings suggest that relying solely on knowledge of LLM is insufficient\nfor accurate geospatial task programming, which requires coherent multi-step\nprocesses and multiple function calls. Compared to the baseline LLMs, the\nproposed GeoAgent has demonstrated superior performance, yielding notable\nimprovements in function calls and task completion. In addition, these results\noffer valuable insights for the future development of LLM agents in automatic\ngeospatial data analysis task programming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are being used in data science code generation\ntasks, but they often struggle with complex sequential tasks, leading to\nlogical errors. Their application to geospatial data processing is particularly\nchallenging due to difficulties in incorporating complex data structures and\nspatial constraints, effectively utilizing diverse function calls, and the\ntendency to hallucinate less-used geospatial libraries. To tackle these\nproblems, we introduce GeoAgent, a new interactive framework designed to help\nLLMs handle geospatial data processing more effectively. GeoAgent pioneers the\nintegration of a code interpreter, static analysis, and Retrieval-Augmented\nGeneration (RAG) techniques within a Monte Carlo Tree Search (MCTS) algorithm,\noffering a novel approach to geospatial data processing. In addition, we\ncontribute a new benchmark specifically designed to evaluate the LLM-based\napproach in geospatial tasks. This benchmark leverages a variety of Python\nlibraries and includes both single-turn and multi-turn tasks such as data\nacquisition, data analysis, and visualization. By offering a comprehensive\nevaluation among diverse geospatial contexts, this benchmark sets a new\nstandard for developing LLM-based approaches in geospatial data analysis tasks.\nOur findings suggest that relying solely on knowledge of LLM is insufficient\nfor accurate geospatial task programming, which requires coherent multi-step\nprocesses and multiple function calls. Compared to the baseline LLMs, the\nproposed GeoAgent has demonstrated superior performance, yielding notable\nimprovements in function calls and task completion. In addition, these results\noffer valuable insights for the future development of LLM agents in automatic\ngeospatial data analysis task programming."
                },
                "authors": [
                    {
                        "name": "Yuxing Chen"
                    },
                    {
                        "name": "Weijie Wang"
                    },
                    {
                        "name": "Sylvain Lobry"
                    },
                    {
                        "name": "Camille Kurtz"
                    }
                ],
                "author_detail": {
                    "name": "Camille Kurtz"
                },
                "author": "Camille Kurtz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18792v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18792v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19393v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19393v1",
                "updated": "2024-10-25T08:48:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    8,
                    48,
                    27,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T08:48:27Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    8,
                    48,
                    27,
                    4,
                    299,
                    0
                ],
                "title": "On low frequency inference for diffusions without the hot spots\n  conjecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On low frequency inference for diffusions without the hot spots\n  conjecture"
                },
                "summary": "We remove the dependence on the `hot-spots' conjecture in two of the main\ntheorems of the recent paper of Nickl (2024, Annals of Statistics).\nSpecifically, we characterise the minimax convergence rates for estimation of\nthe transition operator $P_{f}$ arising from the Neumann Laplacian with\ndiffusion coefficient $f$ on arbitrary convex domains with smooth boundary, and\nfurther show that a general Lipschitz stability estimate holds for the inverse\nmap $P_f\\mapsto f$ from $H^2\\to H^2$ to $L^1$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We remove the dependence on the `hot-spots' conjecture in two of the main\ntheorems of the recent paper of Nickl (2024, Annals of Statistics).\nSpecifically, we characterise the minimax convergence rates for estimation of\nthe transition operator $P_{f}$ arising from the Neumann Laplacian with\ndiffusion coefficient $f$ on arbitrary convex domains with smooth boundary, and\nfurther show that a general Lipschitz stability estimate holds for the inverse\nmap $P_f\\mapsto f$ from $H^2\\to H^2$ to $L^1$."
                },
                "authors": [
                    {
                        "name": "Giovanni S. Alberti"
                    },
                    {
                        "name": "Douglas Barnes"
                    },
                    {
                        "name": "Aditya Jambhale"
                    },
                    {
                        "name": "Richard Nickl"
                    }
                ],
                "author_detail": {
                    "name": "Richard Nickl"
                },
                "author": "Richard Nickl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19393v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18153v2",
                "updated": "2024-10-25T08:47:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    8,
                    47,
                    6,
                    4,
                    299,
                    0
                ],
                "published": "2024-02-28T08:34:23Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    8,
                    34,
                    23,
                    2,
                    59,
                    0
                ],
                "title": "Diffusion-Based Neural Network Weights Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-Based Neural Network Weights Generation"
                },
                "summary": "Transfer learning has gained significant attention in recent deep learning\nresearch due to its ability to accelerate convergence and enhance performance\non new tasks. However, its success is often contingent on the similarity\nbetween source and target data, and training on numerous datasets can be\ncostly, leading to blind selection of pretrained models with limited insight\ninto their effectiveness. To address these challenges, we introduce D2NWG, a\ndiffusion-based neural network weights generation technique that efficiently\nproduces high-performing weights for transfer learning, conditioned on the\ntarget dataset. Our method extends generative hyper-representation learning to\nrecast the latent diffusion paradigm for neural network weights generation,\nlearning the weight distributions of models pretrained on various datasets.\nThis allows for automatic generation of weights that generalize well across\nboth seen and unseen tasks, outperforming state-of-the-art meta-learning\nmethods and pretrained models. Moreover, our approach is scalable to large\narchitectures such as large language models (LLMs), overcoming the limitations\nof current parameter generation techniques that rely on task-specific model\ncollections or access to original training data. By modeling the parameter\ndistribution of LLMs, D2NWG enables task-specific parameter generation without\nrequiring additional fine-tuning or large collections of model variants.\nExtensive experiments show that our method consistently enhances the\nperformance of diverse base models, regardless of their size or complexity,\npositioning it as a robust solution for scalable transfer learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transfer learning has gained significant attention in recent deep learning\nresearch due to its ability to accelerate convergence and enhance performance\non new tasks. However, its success is often contingent on the similarity\nbetween source and target data, and training on numerous datasets can be\ncostly, leading to blind selection of pretrained models with limited insight\ninto their effectiveness. To address these challenges, we introduce D2NWG, a\ndiffusion-based neural network weights generation technique that efficiently\nproduces high-performing weights for transfer learning, conditioned on the\ntarget dataset. Our method extends generative hyper-representation learning to\nrecast the latent diffusion paradigm for neural network weights generation,\nlearning the weight distributions of models pretrained on various datasets.\nThis allows for automatic generation of weights that generalize well across\nboth seen and unseen tasks, outperforming state-of-the-art meta-learning\nmethods and pretrained models. Moreover, our approach is scalable to large\narchitectures such as large language models (LLMs), overcoming the limitations\nof current parameter generation techniques that rely on task-specific model\ncollections or access to original training data. By modeling the parameter\ndistribution of LLMs, D2NWG enables task-specific parameter generation without\nrequiring additional fine-tuning or large collections of model variants.\nExtensive experiments show that our method consistently enhances the\nperformance of diverse base models, regardless of their size or complexity,\npositioning it as a robust solution for scalable transfer learning."
                },
                "authors": [
                    {
                        "name": "Bedionita Soro"
                    },
                    {
                        "name": "Bruno Andreis"
                    },
                    {
                        "name": "Hayeon Lee"
                    },
                    {
                        "name": "Wonyong Jeong"
                    },
                    {
                        "name": "Song Chong"
                    },
                    {
                        "name": "Frank Hutter"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "32 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19385v1",
                "updated": "2024-10-25T08:34:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    8,
                    34,
                    53,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T08:34:53Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    8,
                    34,
                    53,
                    4,
                    299,
                    0
                ],
                "title": "Investigating the Role of Prompting and External Tools in Hallucination\n  Rates of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating the Role of Prompting and External Tools in Hallucination\n  Rates of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are powerful computational models trained on\nextensive corpora of human-readable text, enabling them to perform\ngeneral-purpose language understanding and generation. LLMs have garnered\nsignificant attention in both industry and academia due to their exceptional\nperformance across various natural language processing (NLP) tasks. Despite\nthese successes, LLMs often produce inaccuracies, commonly referred to as\nhallucinations. Prompt engineering, the process of designing and formulating\ninstructions for LLMs to perform specific tasks, has emerged as a key approach\nto mitigating hallucinations. This paper provides a comprehensive empirical\nevaluation of different prompting strategies and frameworks aimed at reducing\nhallucinations in LLMs. Various prompting techniques are applied to a broad set\nof benchmark datasets to assess the accuracy and hallucination rate of each\nmethod. Additionally, the paper investigates the influence of tool-calling\nagents (LLMs augmented with external tools to enhance their capabilities beyond\nlanguage generation) on hallucination rates in the same benchmarks. The\nfindings demonstrate that the optimal prompting technique depends on the type\nof problem, and that simpler techniques often outperform more complex methods\nin reducing hallucinations. Furthermore, it is shown that LLM agents can\nexhibit significantly higher hallucination rates due to the added complexity of\nexternal tool usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are powerful computational models trained on\nextensive corpora of human-readable text, enabling them to perform\ngeneral-purpose language understanding and generation. LLMs have garnered\nsignificant attention in both industry and academia due to their exceptional\nperformance across various natural language processing (NLP) tasks. Despite\nthese successes, LLMs often produce inaccuracies, commonly referred to as\nhallucinations. Prompt engineering, the process of designing and formulating\ninstructions for LLMs to perform specific tasks, has emerged as a key approach\nto mitigating hallucinations. This paper provides a comprehensive empirical\nevaluation of different prompting strategies and frameworks aimed at reducing\nhallucinations in LLMs. Various prompting techniques are applied to a broad set\nof benchmark datasets to assess the accuracy and hallucination rate of each\nmethod. Additionally, the paper investigates the influence of tool-calling\nagents (LLMs augmented with external tools to enhance their capabilities beyond\nlanguage generation) on hallucination rates in the same benchmarks. The\nfindings demonstrate that the optimal prompting technique depends on the type\nof problem, and that simpler techniques often outperform more complex methods\nin reducing hallucinations. Furthermore, it is shown that LLM agents can\nexhibit significantly higher hallucination rates due to the added complexity of\nexternal tool usage."
                },
                "authors": [
                    {
                        "name": "Liam Barkley"
                    },
                    {
                        "name": "Brink van der Merwe"
                    }
                ],
                "author_detail": {
                    "name": "Brink van der Merwe"
                },
                "author": "Brink van der Merwe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13836v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13836v2",
                "updated": "2024-10-25T08:31:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    8,
                    31,
                    33,
                    4,
                    299,
                    0
                ],
                "published": "2024-08-25T13:42:47Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    13,
                    42,
                    47,
                    6,
                    238,
                    0
                ],
                "title": "PAM: A Propagation-Based Model for Segmenting Any 3D Objects across\n  Multi-Modal Medical Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAM: A Propagation-Based Model for Segmenting Any 3D Objects across\n  Multi-Modal Medical Images"
                },
                "summary": "Volumetric segmentation is important in medical imaging, but current methods\nface challenges like requiring lots of manual annotations and being tailored to\nspecific tasks, which limits their versatility. General segmentation models\nused for natural images don't perform well with the unique features of medical\nimages. There's a strong need for an adaptable approach that can effectively\nhandle different 3D medical structures and imaging modalities. In this study,\nwe present PAM (Propagating Anything Model), a segmentation approach that uses\na 2D prompt, like a bounding box or sketch, to create a complete 3D\nsegmentation of medical image volumes. PAM works by modeling relationships\nbetween slices, maintaining information flow across the 3D structure. It\ncombines a CNN-based UNet for processing within slices and a Transformer-based\nattention module for propagating information between slices, leading to better\ngeneralizability across various imaging modalities. PAM significantly\noutperformed existing models like MedSAM and SegVol, with an average\nimprovement of over 18.1% in dice similarity coefficient (DSC) across 44\nmedical datasets and various object types. It also showed stable performance\ndespite prompt deviations and different propagation setups, and faster\ninference speeds compared to other models. PAM's one-view prompt design made it\nmore efficient, reducing interaction time by about 63.6% compared to two-view\nprompts. Thanks to its focus on structural relationships, PAM handled unseen\nand complex objects well, showing a unique ability to generalize to new\nsituations. PAM represents an advancement in medical image segmentation,\neffectively reducing the need for extensive manual work and specialized\ntraining. Its adaptability makes it a promising tool for more automated and\nreliable analysis in clinical settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Volumetric segmentation is important in medical imaging, but current methods\nface challenges like requiring lots of manual annotations and being tailored to\nspecific tasks, which limits their versatility. General segmentation models\nused for natural images don't perform well with the unique features of medical\nimages. There's a strong need for an adaptable approach that can effectively\nhandle different 3D medical structures and imaging modalities. In this study,\nwe present PAM (Propagating Anything Model), a segmentation approach that uses\na 2D prompt, like a bounding box or sketch, to create a complete 3D\nsegmentation of medical image volumes. PAM works by modeling relationships\nbetween slices, maintaining information flow across the 3D structure. It\ncombines a CNN-based UNet for processing within slices and a Transformer-based\nattention module for propagating information between slices, leading to better\ngeneralizability across various imaging modalities. PAM significantly\noutperformed existing models like MedSAM and SegVol, with an average\nimprovement of over 18.1% in dice similarity coefficient (DSC) across 44\nmedical datasets and various object types. It also showed stable performance\ndespite prompt deviations and different propagation setups, and faster\ninference speeds compared to other models. PAM's one-view prompt design made it\nmore efficient, reducing interaction time by about 63.6% compared to two-view\nprompts. Thanks to its focus on structural relationships, PAM handled unseen\nand complex objects well, showing a unique ability to generalize to new\nsituations. PAM represents an advancement in medical image segmentation,\neffectively reducing the need for extensive manual work and specialized\ntraining. Its adaptability makes it a promising tool for more automated and\nreliable analysis in clinical settings."
                },
                "authors": [
                    {
                        "name": "Zifan Chen"
                    },
                    {
                        "name": "Xinyu Nan"
                    },
                    {
                        "name": "Jiazheng Li"
                    },
                    {
                        "name": "Jie Zhao"
                    },
                    {
                        "name": "Haifeng Li"
                    },
                    {
                        "name": "Ziling Lin"
                    },
                    {
                        "name": "Haoshen Li"
                    },
                    {
                        "name": "Heyun Chen"
                    },
                    {
                        "name": "Yiting Liu"
                    },
                    {
                        "name": "Lei Tang"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Bin Dong"
                    }
                ],
                "author_detail": {
                    "name": "Bin Dong"
                },
                "author": "Bin Dong",
                "arxiv_comment": "28 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13836v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13836v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18908v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18908v2",
                "updated": "2024-10-25T08:30:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    8,
                    30,
                    21,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-24T16:59:28Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    59,
                    28,
                    3,
                    298,
                    0
                ],
                "title": "A Survey on Speech Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Speech Large Language Models"
                },
                "summary": "Large Language Models (LLMs) exhibit strong contextual understanding and\nremarkable multi-task performance. Therefore, researchers have been seeking to\nintegrate LLMs in the broad sense of Spoken Language Understanding (SLU) field.\nDifferent from the traditional method of cascading LLMs to process text\ngenerated by Automatic Speech Recognition(ASR), new efforts have focused on\ndesigning architectures centered around Audio Feature Extraction - Multimodal\nInformation Fusion - LLM Inference(Speech LLMs). This approach enables richer\naudio feature extraction while simultaneously facilitating end-to-end fusion of\naudio and text modalities, thereby achieving deeper understanding and reasoning\nfrom audio data. This paper elucidates the development of Speech LLMs, offering\nan in-depth analysis of system architectures and training strategies. Through\nextensive research and a series of targeted experiments, the paper assesses\nSpeech LLMs' advancements in Rich Audio Transcription and its potential for\nCross-task Integration within the SLU field. Additionally, it indicates key\nchallenges uncovered through experimentation, such as the Dormancy of LLMs\nunder certain conditions. The paper further delves into the training strategies\nfor Speech LLMs, proposing potential solutions based on these findings, and\noffering valuable insights and references for future research in this domain,\nas well as LLM applications in multimodal contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit strong contextual understanding and\nremarkable multi-task performance. Therefore, researchers have been seeking to\nintegrate LLMs in the broad sense of Spoken Language Understanding (SLU) field.\nDifferent from the traditional method of cascading LLMs to process text\ngenerated by Automatic Speech Recognition(ASR), new efforts have focused on\ndesigning architectures centered around Audio Feature Extraction - Multimodal\nInformation Fusion - LLM Inference(Speech LLMs). This approach enables richer\naudio feature extraction while simultaneously facilitating end-to-end fusion of\naudio and text modalities, thereby achieving deeper understanding and reasoning\nfrom audio data. This paper elucidates the development of Speech LLMs, offering\nan in-depth analysis of system architectures and training strategies. Through\nextensive research and a series of targeted experiments, the paper assesses\nSpeech LLMs' advancements in Rich Audio Transcription and its potential for\nCross-task Integration within the SLU field. Additionally, it indicates key\nchallenges uncovered through experimentation, such as the Dormancy of LLMs\nunder certain conditions. The paper further delves into the training strategies\nfor Speech LLMs, proposing potential solutions based on these findings, and\noffering valuable insights and references for future research in this domain,\nas well as LLM applications in multimodal contexts."
                },
                "authors": [
                    {
                        "name": "Jing Peng"
                    },
                    {
                        "name": "Yucheng Wang"
                    },
                    {
                        "name": "Yu Xi"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Xizhuo Zhang"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18908v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18908v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19375v1",
                "updated": "2024-10-25T08:27:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    8,
                    27,
                    38,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T08:27:38Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    8,
                    27,
                    38,
                    4,
                    299,
                    0
                ],
                "title": "COMSPLIT: A Communication-Aware Split Learning Design for Heterogeneous\n  IoT Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COMSPLIT: A Communication-Aware Split Learning Design for Heterogeneous\n  IoT Platforms"
                },
                "summary": "The significance of distributed learning and inference algorithms in Internet\nof Things (IoT) network is growing since they flexibly distribute computation\nload between IoT devices and the infrastructure, enhance data privacy, and\nminimize latency. However, a notable challenge stems from the influence of\ncommunication channel conditions on their performance. In this work, we\nintroduce COMSPLIT: a novel communication-aware design for split learning (SL)\nand inference paradigm tailored to processing time series data in IoT networks.\nCOMSPLIT provides a versatile framework for deploying adaptable SL in IoT\nnetworks affected by diverse channel conditions. In conjunction with the\nintegration of an early-exit strategy, and addressing IoT scenarios containing\ndevices with heterogeneous computational capabilities, COMSPLIT represents a\ncomprehensive design solution for communication-aware SL in IoT networks.\nNumerical results show superior performance of COMSPLIT compared to vanilla SL\napproaches (that assume ideal communication channel), demonstrating its ability\nto offer both design simplicity and adaptability to different channel\nconditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The significance of distributed learning and inference algorithms in Internet\nof Things (IoT) network is growing since they flexibly distribute computation\nload between IoT devices and the infrastructure, enhance data privacy, and\nminimize latency. However, a notable challenge stems from the influence of\ncommunication channel conditions on their performance. In this work, we\nintroduce COMSPLIT: a novel communication-aware design for split learning (SL)\nand inference paradigm tailored to processing time series data in IoT networks.\nCOMSPLIT provides a versatile framework for deploying adaptable SL in IoT\nnetworks affected by diverse channel conditions. In conjunction with the\nintegration of an early-exit strategy, and addressing IoT scenarios containing\ndevices with heterogeneous computational capabilities, COMSPLIT represents a\ncomprehensive design solution for communication-aware SL in IoT networks.\nNumerical results show superior performance of COMSPLIT compared to vanilla SL\napproaches (that assume ideal communication channel), demonstrating its ability\nto offer both design simplicity and adaptability to different channel\nconditions."
                },
                "authors": [
                    {
                        "name": "Vukan Ninkovic"
                    },
                    {
                        "name": "Dejan Vukobratovic"
                    },
                    {
                        "name": "Dragisa Miskovic"
                    },
                    {
                        "name": "Marco Zennaro"
                    }
                ],
                "author_detail": {
                    "name": "Marco Zennaro"
                },
                "author": "Marco Zennaro",
                "arxiv_comment": "Accepted for publication in IEEE Internet of Things Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17558v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17558v2",
                "updated": "2024-10-25T08:21:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    8,
                    21,
                    59,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-23T04:55:08Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    4,
                    55,
                    8,
                    2,
                    297,
                    0
                ],
                "title": "CLR-Bench: Evaluating Large Language Models in College-level Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLR-Bench: Evaluating Large Language Models in College-level Reasoning"
                },
                "summary": "Large language models (LLMs) have demonstrated their remarkable performance\nacross various language understanding tasks. While emerging benchmarks have\nbeen proposed to evaluate LLMs in various domains such as mathematics and\ncomputer science, they merely measure the accuracy in terms of the final\nprediction on multi-choice questions. However, it remains insufficient to\nverify the essential understanding of LLMs given a chosen choice. To fill this\ngap, we present CLR-Bench to comprehensively evaluate the LLMs in complex\ncollege-level reasoning. Specifically, (i) we prioritize 16 challenging college\ndisciplines in computer science and artificial intelligence. The dataset\ncontains 5 types of questions, while each question is associated with detailed\nexplanations from experts. (ii) To quantify a fair evaluation of LLMs'\nreasoning ability, we formalize the criteria with two novel metrics.\nQ$\\rightarrow$A is utilized to measure the performance of direct answer\nprediction, and Q$\\rightarrow$AR effectively considers the joint ability to\nanswer the question and provide rationale simultaneously. Extensive experiments\nare conducted with 40 LLMs over 1,018 discipline-specific questions. The\nresults demonstrate the key insights that LLMs, even the best closed-source\nLLM, i.e., GPT-4 turbo, tend to `guess' the college-level answers. It shows a\ndramatic decrease in accuracy from 63.31% Q$\\rightarrow$A to 39.00%\nQ$\\rightarrow$AR, indicating an unsatisfactory reasoning ability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated their remarkable performance\nacross various language understanding tasks. While emerging benchmarks have\nbeen proposed to evaluate LLMs in various domains such as mathematics and\ncomputer science, they merely measure the accuracy in terms of the final\nprediction on multi-choice questions. However, it remains insufficient to\nverify the essential understanding of LLMs given a chosen choice. To fill this\ngap, we present CLR-Bench to comprehensively evaluate the LLMs in complex\ncollege-level reasoning. Specifically, (i) we prioritize 16 challenging college\ndisciplines in computer science and artificial intelligence. The dataset\ncontains 5 types of questions, while each question is associated with detailed\nexplanations from experts. (ii) To quantify a fair evaluation of LLMs'\nreasoning ability, we formalize the criteria with two novel metrics.\nQ$\\rightarrow$A is utilized to measure the performance of direct answer\nprediction, and Q$\\rightarrow$AR effectively considers the joint ability to\nanswer the question and provide rationale simultaneously. Extensive experiments\nare conducted with 40 LLMs over 1,018 discipline-specific questions. The\nresults demonstrate the key insights that LLMs, even the best closed-source\nLLM, i.e., GPT-4 turbo, tend to `guess' the college-level answers. It shows a\ndramatic decrease in accuracy from 63.31% Q$\\rightarrow$A to 39.00%\nQ$\\rightarrow$AR, indicating an unsatisfactory reasoning ability."
                },
                "authors": [
                    {
                        "name": "Junnan Dong"
                    },
                    {
                        "name": "Zijin Hong"
                    },
                    {
                        "name": "Yuanchen Bei"
                    },
                    {
                        "name": "Feiran Huang"
                    },
                    {
                        "name": "Xinrun Wang"
                    },
                    {
                        "name": "Xiao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Huang"
                },
                "author": "Xiao Huang",
                "arxiv_comment": "18 pages, 6 figures, dataset and evaluation framework will be\n  opensourced",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17558v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17558v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19371v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19371v1",
                "updated": "2024-10-25T08:18:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    8,
                    18,
                    49,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T08:18:49Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    8,
                    18,
                    49,
                    4,
                    299,
                    0
                ],
                "title": "Noise-Aware Differentially Private Variational Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noise-Aware Differentially Private Variational Inference"
                },
                "summary": "Differential privacy (DP) provides robust privacy guarantees for statistical\ninference, but this can lead to unreliable results and biases in downstream\napplications. While several noise-aware approaches have been proposed which\nintegrate DP perturbation into the inference, they are limited to specific\ntypes of simple probabilistic models. In this work, we propose a novel method\nfor noise-aware approximate Bayesian inference based on stochastic gradient\nvariational inference which can also be applied to high-dimensional and\nnon-conjugate models. We also propose a more accurate evaluation method for\nnoise-aware posteriors. Empirically, our inference method has similar\nperformance to existing methods in the domain where they are applicable.\nOutside this domain, we obtain accurate coverages on high-dimensional Bayesian\nlinear regression and well-calibrated predictive probabilities on Bayesian\nlogistic regression with the UCI Adult dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differential privacy (DP) provides robust privacy guarantees for statistical\ninference, but this can lead to unreliable results and biases in downstream\napplications. While several noise-aware approaches have been proposed which\nintegrate DP perturbation into the inference, they are limited to specific\ntypes of simple probabilistic models. In this work, we propose a novel method\nfor noise-aware approximate Bayesian inference based on stochastic gradient\nvariational inference which can also be applied to high-dimensional and\nnon-conjugate models. We also propose a more accurate evaluation method for\nnoise-aware posteriors. Empirically, our inference method has similar\nperformance to existing methods in the domain where they are applicable.\nOutside this domain, we obtain accurate coverages on high-dimensional Bayesian\nlinear regression and well-calibrated predictive probabilities on Bayesian\nlogistic regression with the UCI Adult dataset."
                },
                "authors": [
                    {
                        "name": "Talal Alrawajfeh"
                    },
                    {
                        "name": "Joonas Jälkö"
                    },
                    {
                        "name": "Antti Honkela"
                    }
                ],
                "author_detail": {
                    "name": "Antti Honkela"
                },
                "author": "Antti Honkela",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19371v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19371v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19370v1",
                "updated": "2024-10-25T08:16:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    8,
                    16,
                    19,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T08:16:19Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    8,
                    16,
                    19,
                    4,
                    299,
                    0
                ],
                "title": "Notes on the Mathematical Structure of GPT LLM Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Notes on the Mathematical Structure of GPT LLM Architectures"
                },
                "summary": "An exposition of the mathematics underpinning the neural network architecture\nof a GPT-3-style LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An exposition of the mathematics underpinning the neural network architecture\nof a GPT-3-style LLM."
                },
                "authors": [
                    {
                        "name": "Spencer Becker-Kahn"
                    }
                ],
                "author_detail": {
                    "name": "Spencer Becker-Kahn"
                },
                "author": "Spencer Becker-Kahn",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04834v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04834v2",
                "updated": "2024-10-25T07:41:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    41,
                    45,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-07T08:44:04Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    8,
                    44,
                    4,
                    0,
                    281,
                    0
                ],
                "title": "As Simple as Fine-tuning: LLM Alignment via Bidirectional Negative\n  Feedback Loss",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Simple as Fine-tuning: LLM Alignment via Bidirectional Negative\n  Feedback Loss"
                },
                "summary": "Direct Preference Optimization (DPO) has emerged as a more computationally\nefficient alternative to Reinforcement Learning from Human Feedback (RLHF) with\nProximal Policy Optimization (PPO), eliminating the need for reward models and\nonline sampling. Despite these benefits, DPO and its variants remain sensitive\nto hyper-parameters and prone to instability, particularly on mathematical\ndatasets. We argue that these issues arise from the unidirectional\nlikelihood-derivative negative feedback inherent in the log-likelihood loss\nfunction. To address this, we propose a novel LLM alignment loss that\nestablishes a stable Bidirectional Negative Feedback (BNF) during optimization.\nOur proposed BNF loss eliminates the need for pairwise contrastive losses and\ndoes not require any extra tunable hyper-parameters or pairwise preference\ndata, streamlining the alignment pipeline to be as simple as supervised\nfine-tuning. We conduct extensive experiments across two challenging QA\nbenchmarks and four reasoning benchmarks. The experimental results show that\nBNF achieves comparable performance to the best methods on QA benchmarks, while\nits performance decrease on the four reasoning benchmarks is significantly\nlower compared to the best methods, thus striking a better balance between\nvalue alignment and reasoning ability. In addition, we further validate the\nperformance of BNF on non-pairwise datasets, and conduct in-depth analysis of\nlog-likelihood and logit shifts across different preference optimization\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) has emerged as a more computationally\nefficient alternative to Reinforcement Learning from Human Feedback (RLHF) with\nProximal Policy Optimization (PPO), eliminating the need for reward models and\nonline sampling. Despite these benefits, DPO and its variants remain sensitive\nto hyper-parameters and prone to instability, particularly on mathematical\ndatasets. We argue that these issues arise from the unidirectional\nlikelihood-derivative negative feedback inherent in the log-likelihood loss\nfunction. To address this, we propose a novel LLM alignment loss that\nestablishes a stable Bidirectional Negative Feedback (BNF) during optimization.\nOur proposed BNF loss eliminates the need for pairwise contrastive losses and\ndoes not require any extra tunable hyper-parameters or pairwise preference\ndata, streamlining the alignment pipeline to be as simple as supervised\nfine-tuning. We conduct extensive experiments across two challenging QA\nbenchmarks and four reasoning benchmarks. The experimental results show that\nBNF achieves comparable performance to the best methods on QA benchmarks, while\nits performance decrease on the four reasoning benchmarks is significantly\nlower compared to the best methods, thus striking a better balance between\nvalue alignment and reasoning ability. In addition, we further validate the\nperformance of BNF on non-pairwise datasets, and conduct in-depth analysis of\nlog-likelihood and logit shifts across different preference optimization\nmethods."
                },
                "authors": [
                    {
                        "name": "Xin Mao"
                    },
                    {
                        "name": "Feng-Lin Li"
                    },
                    {
                        "name": "Huimin Xu"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Wang Chen"
                    },
                    {
                        "name": "Anh Tuan Luu"
                    }
                ],
                "author_detail": {
                    "name": "Anh Tuan Luu"
                },
                "author": "Anh Tuan Luu",
                "arxiv_comment": "20 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04834v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04834v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13185v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13185v4",
                "updated": "2024-10-25T07:34:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    34,
                    36,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-17T03:26:37Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    3,
                    26,
                    37,
                    3,
                    291,
                    0
                ],
                "title": "Chain of Ideas: Revolutionizing Research Via Novel Idea Development with\n  LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Ideas: Revolutionizing Research Via Novel Idea Development with\n  LLM Agents"
                },
                "summary": "Effective research ideation is a critical step for scientific research.\nHowever, the exponential increase in scientific literature makes it challenging\nfor researchers to stay current with recent advances and identify meaningful\nresearch directions. Recent developments in large language models~(LLMs)\nsuggest a promising avenue for automating the generation of novel research\nideas. However, existing methods for idea generation either trivially prompt\nLLMs or directly expose LLMs to extensive literature without indicating useful\ninformation. Inspired by the research process of human researchers, we propose\na Chain-of-Ideas~(CoI) agent, an LLM-based agent that organizes relevant\nliterature in a chain structure to effectively mirror the progressive\ndevelopment in a research domain. This organization facilitates LLMs to capture\nthe current advancements in research, thereby enhancing their ideation\ncapabilities. Furthermore, we propose Idea Arena, an evaluation protocol that\ncan comprehensively evaluate idea generation methods from different\nperspectives, aligning closely with the preferences of human researchers.\nExperimental results indicate that the CoI agent consistently outperforms other\nmethods and shows comparable quality as humans in research idea generation.\nMoreover, our CoI agent is budget-friendly, with a minimum cost of \\$0.50 to\ngenerate a candidate idea and its corresponding experimental design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective research ideation is a critical step for scientific research.\nHowever, the exponential increase in scientific literature makes it challenging\nfor researchers to stay current with recent advances and identify meaningful\nresearch directions. Recent developments in large language models~(LLMs)\nsuggest a promising avenue for automating the generation of novel research\nideas. However, existing methods for idea generation either trivially prompt\nLLMs or directly expose LLMs to extensive literature without indicating useful\ninformation. Inspired by the research process of human researchers, we propose\na Chain-of-Ideas~(CoI) agent, an LLM-based agent that organizes relevant\nliterature in a chain structure to effectively mirror the progressive\ndevelopment in a research domain. This organization facilitates LLMs to capture\nthe current advancements in research, thereby enhancing their ideation\ncapabilities. Furthermore, we propose Idea Arena, an evaluation protocol that\ncan comprehensively evaluate idea generation methods from different\nperspectives, aligning closely with the preferences of human researchers.\nExperimental results indicate that the CoI agent consistently outperforms other\nmethods and shows comparable quality as humans in research idea generation.\nMoreover, our CoI agent is budget-friendly, with a minimum cost of \\$0.50 to\ngenerate a candidate idea and its corresponding experimental design."
                },
                "authors": [
                    {
                        "name": "Long Li"
                    },
                    {
                        "name": "Weiwen Xu"
                    },
                    {
                        "name": "Jiayan Guo"
                    },
                    {
                        "name": "Ruochen Zhao"
                    },
                    {
                        "name": "Xinxuan Li"
                    },
                    {
                        "name": "Yuqian Yuan"
                    },
                    {
                        "name": "Boqiang Zhang"
                    },
                    {
                        "name": "Yuming Jiang"
                    },
                    {
                        "name": "Yifei Xin"
                    },
                    {
                        "name": "Ronghao Dang"
                    },
                    {
                        "name": "Deli Zhao"
                    },
                    {
                        "name": "Yu Rong"
                    },
                    {
                        "name": "Tian Feng"
                    },
                    {
                        "name": "Lidong Bing"
                    }
                ],
                "author_detail": {
                    "name": "Lidong Bing"
                },
                "author": "Lidong Bing",
                "arxiv_comment": "10 pages,5 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13185v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13185v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19356v1",
                "updated": "2024-10-25T07:24:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    49,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T07:24:49Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    49,
                    4,
                    299,
                    0
                ],
                "title": "FeBiM: Efficient and Compact Bayesian Inference Engine Empowered with\n  Ferroelectric In-Memory Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FeBiM: Efficient and Compact Bayesian Inference Engine Empowered with\n  Ferroelectric In-Memory Computing"
                },
                "summary": "In scenarios with limited training data or where explainability is crucial,\nconventional neural network-based machine learning models often face\nchallenges. In contrast, Bayesian inference-based algorithms excel in providing\ninterpretable predictions and reliable uncertainty estimation in these\nscenarios. While many state-of-the-art in-memory computing (IMC) architectures\nleverage emerging non-volatile memory (NVM) technologies to offer unparalleled\ncomputing capacity and energy efficiency for neural network workloads, their\napplication in Bayesian inference is limited. This is because the core\noperations in Bayesian inference differ significantly from the\nmultiplication-accumulation (MAC) operations common in neural networks,\nrendering them generally unsuitable for direct implementation in most existing\nIMC designs. In this paper, we propose FeBiM, an efficient and compact Bayesian\ninference engine powered by multi-bit ferroelectric field-effect transistor\n(FeFET)-based IMC. FeBiM effectively encodes the trained probabilities of a\nBayesian inference model within a compact FeFET-based crossbar. It maps\nquantized logarithmic probabilities to discrete FeFET states. As a result, the\naccumulated outputs of the crossbar naturally represent the posterior\nprobabilities, i.e., the Bayesian inference model's output given a set of\nobservations. This approach enables efficient in-memory Bayesian inference\nwithout the need for additional calculation circuitry. As the first FeFET-based\nin-memory Bayesian inference engine, FeBiM achieves an impressive storage\ndensity of 26.32 Mb/mm$^{2}$ and a computing efficiency of 581.40 TOPS/W in a\nrepresentative Bayesian classification task. These results demonstrate\n10.7$\\times$/43.4$\\times$ improvement in compactness/efficiency compared to the\nstate-of-the-art hardware implementation of Bayesian inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In scenarios with limited training data or where explainability is crucial,\nconventional neural network-based machine learning models often face\nchallenges. In contrast, Bayesian inference-based algorithms excel in providing\ninterpretable predictions and reliable uncertainty estimation in these\nscenarios. While many state-of-the-art in-memory computing (IMC) architectures\nleverage emerging non-volatile memory (NVM) technologies to offer unparalleled\ncomputing capacity and energy efficiency for neural network workloads, their\napplication in Bayesian inference is limited. This is because the core\noperations in Bayesian inference differ significantly from the\nmultiplication-accumulation (MAC) operations common in neural networks,\nrendering them generally unsuitable for direct implementation in most existing\nIMC designs. In this paper, we propose FeBiM, an efficient and compact Bayesian\ninference engine powered by multi-bit ferroelectric field-effect transistor\n(FeFET)-based IMC. FeBiM effectively encodes the trained probabilities of a\nBayesian inference model within a compact FeFET-based crossbar. It maps\nquantized logarithmic probabilities to discrete FeFET states. As a result, the\naccumulated outputs of the crossbar naturally represent the posterior\nprobabilities, i.e., the Bayesian inference model's output given a set of\nobservations. This approach enables efficient in-memory Bayesian inference\nwithout the need for additional calculation circuitry. As the first FeFET-based\nin-memory Bayesian inference engine, FeBiM achieves an impressive storage\ndensity of 26.32 Mb/mm$^{2}$ and a computing efficiency of 581.40 TOPS/W in a\nrepresentative Bayesian classification task. These results demonstrate\n10.7$\\times$/43.4$\\times$ improvement in compactness/efficiency compared to the\nstate-of-the-art hardware implementation of Bayesian inference."
                },
                "authors": [
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Zhicheng Xu"
                    },
                    {
                        "name": "Bo Wen"
                    },
                    {
                        "name": "Ruibin Mao"
                    },
                    {
                        "name": "Can Li"
                    },
                    {
                        "name": "Thomas Kämpfe"
                    },
                    {
                        "name": "Kai Ni"
                    },
                    {
                        "name": "Xunzhao Yin"
                    }
                ],
                "author_detail": {
                    "name": "Xunzhao Yin"
                },
                "author": "Xunzhao Yin",
                "arxiv_doi": "10.1145/3649329.3656538",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3649329.3656538",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.19356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages, 8 figures, to be published in the 61st DAC (Design\n  Automation Conference) proceedings",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19355v1",
                "updated": "2024-10-25T07:24:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T07:24:38Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "title": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality"
                },
                "summary": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality."
                },
                "authors": [
                    {
                        "name": "Zhengyao Lv"
                    },
                    {
                        "name": "Chenyang Si"
                    },
                    {
                        "name": "Junhao Song"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Kwan-Yee K. Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Yee K. Wong"
                },
                "author": "Kwan-Yee K. Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19353v1",
                "updated": "2024-10-25T07:21:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    21,
                    57,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T07:21:57Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    21,
                    57,
                    4,
                    299,
                    0
                ],
                "title": "Interleaving Text and Number Embeddings to Solve Mathemathics Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interleaving Text and Number Embeddings to Solve Mathemathics Problems"
                },
                "summary": "Integrating text and numbers effectively is a crucial step towards enhancing\nLarge Language Models (LLMs) capabilities in assisting in scientific tasks.\nWhile most current approaches rely on discrete tokenization of numbers, for\ninstance, conversion to scientific notation or base 10-decomposition, a recent\napproach proposed a continuous numerical encoding as an inductive bias. In this\npaper, we build upon this approach by introducing more expressive numerical\nembeddings. Our method addresses key shortcomings, including the elimination of\nnumerical artefacts and the ability to handle a wide range of magnitudes\nwithout clipping.\n  Our work presents two key contributions. First, we employ an MLP to assign\ndistinct directions in the embedding space to different numbers. Our second\ncontribution is the introduction of a routing layer that differentiates between\nnumerical and text embeddings. We hypothesise that this combined approach\nenables the model to distinguish between text and number distributions while\nmaintaining its capacity for arithmetic operations.\n  Using only a 45 M parameter encoder-decoder architecture our method achieves\na $R^2$=0.9988 over a wide range of magnitude ($10^{-3},10^{8}$). In addition,\nwe empirically observe a reduction of the numerical artefacts and biases\nobserved compared to the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating text and numbers effectively is a crucial step towards enhancing\nLarge Language Models (LLMs) capabilities in assisting in scientific tasks.\nWhile most current approaches rely on discrete tokenization of numbers, for\ninstance, conversion to scientific notation or base 10-decomposition, a recent\napproach proposed a continuous numerical encoding as an inductive bias. In this\npaper, we build upon this approach by introducing more expressive numerical\nembeddings. Our method addresses key shortcomings, including the elimination of\nnumerical artefacts and the ability to handle a wide range of magnitudes\nwithout clipping.\n  Our work presents two key contributions. First, we employ an MLP to assign\ndistinct directions in the embedding space to different numbers. Our second\ncontribution is the introduction of a routing layer that differentiates between\nnumerical and text embeddings. We hypothesise that this combined approach\nenables the model to distinguish between text and number distributions while\nmaintaining its capacity for arithmetic operations.\n  Using only a 45 M parameter encoder-decoder architecture our method achieves\na $R^2$=0.9988 over a wide range of magnitude ($10^{-3},10^{8}$). In addition,\nwe empirically observe a reduction of the numerical artefacts and biases\nobserved compared to the baselines."
                },
                "authors": [
                    {
                        "name": "Marvin Alberts"
                    },
                    {
                        "name": "Gianmarco Gabrieli"
                    },
                    {
                        "name": "Irina Espejo Morales"
                    }
                ],
                "author_detail": {
                    "name": "Irina Espejo Morales"
                },
                "author": "Irina Espejo Morales",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19346v1",
                "updated": "2024-10-25T07:04:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    4,
                    16,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T07:04:16Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    4,
                    16,
                    4,
                    299,
                    0
                ],
                "title": "AgentSense: Benchmarking Social Intelligence of Language Agents through\n  Interactive Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentSense: Benchmarking Social Intelligence of Language Agents through\n  Interactive Scenarios"
                },
                "summary": "Large language models (LLMs) are increasingly leveraged to empower autonomous\nagents to simulate human beings in various fields of behavioral research.\nHowever, evaluating their capacity to navigate complex social interactions\nremains a challenge. Previous studies face limitations due to insufficient\nscenario diversity, complexity, and a single-perspective focus. To this end, we\nintroduce AgentSense: Benchmarking Social Intelligence of Language Agents\nthrough Interactive Scenarios. Drawing on Dramaturgical Theory, AgentSense\nemploys a bottom-up approach to create 1,225 diverse social scenarios\nconstructed from extensive scripts. We evaluate LLM-driven agents through\nmulti-turn interactions, emphasizing both goal completion and implicit\nreasoning. We analyze goals using ERG theory and conduct comprehensive\nexperiments. Our findings highlight that LLMs struggle with goals in complex\nsocial scenarios, especially high-level growth needs, and even GPT-4o requires\nimprovement in private information reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly leveraged to empower autonomous\nagents to simulate human beings in various fields of behavioral research.\nHowever, evaluating their capacity to navigate complex social interactions\nremains a challenge. Previous studies face limitations due to insufficient\nscenario diversity, complexity, and a single-perspective focus. To this end, we\nintroduce AgentSense: Benchmarking Social Intelligence of Language Agents\nthrough Interactive Scenarios. Drawing on Dramaturgical Theory, AgentSense\nemploys a bottom-up approach to create 1,225 diverse social scenarios\nconstructed from extensive scripts. We evaluate LLM-driven agents through\nmulti-turn interactions, emphasizing both goal completion and implicit\nreasoning. We analyze goals using ERG theory and conduct comprehensive\nexperiments. Our findings highlight that LLMs struggle with goals in complex\nsocial scenarios, especially high-level growth needs, and even GPT-4o requires\nimprovement in private information reasoning."
                },
                "authors": [
                    {
                        "name": "Xinyi Mou"
                    },
                    {
                        "name": "Jingcong Liang"
                    },
                    {
                        "name": "Jiayu Lin"
                    },
                    {
                        "name": "Xinnong Zhang"
                    },
                    {
                        "name": "Xiawei Liu"
                    },
                    {
                        "name": "Shiyue Yang"
                    },
                    {
                        "name": "Rong Ye"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Haoyu Kuang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12312v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12312v2",
                "updated": "2024-10-25T06:56:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    6,
                    56,
                    54,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-16T07:25:24Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    7,
                    25,
                    24,
                    2,
                    290,
                    0
                ],
                "title": "FaceChain-FACT: Face Adapter with Decoupled Training for\n  Identity-preserved Personalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FaceChain-FACT: Face Adapter with Decoupled Training for\n  Identity-preserved Personalization"
                },
                "summary": "In the field of human-centric personalized image generation, the\nadapter-based method obtains the ability to customize and generate portraits by\ntext-to-image training on facial data. This allows for identity-preserved\npersonalization without additional fine-tuning in inference. Although there are\nimprovements in efficiency and fidelity, there is often a significant\nperformance decrease in test following ability, controllability, and diversity\nof generated faces compared to the base model. In this paper, we analyze that\nthe performance degradation is attributed to the failure to decouple identity\nfeatures from other attributes during extraction, as well as the failure to\ndecouple the portrait generation training from the overall generation task. To\naddress these issues, we propose the Face Adapter with deCoupled Training\n(FACT) framework, focusing on both model architecture and training strategy. To\ndecouple identity features from others, we leverage a transformer-based\nface-export encoder and harness fine-grained identity features. To decouple the\nportrait generation training, we propose Face Adapting Increment\nRegularization~(FAIR), which effectively constrains the effect of face adapters\non the facial region, preserving the generative ability of the base model.\nAdditionally, we incorporate a face condition drop and shuffle mechanism,\ncombined with curriculum learning, to enhance facial controllability and\ndiversity. As a result, FACT solely learns identity preservation from training\ndata, thereby minimizing the impact on the original text-to-image capabilities\nof the base model. Extensive experiments show that FACT has both\ncontrollability and fidelity in both text-to-image generation and inpainting\nsolutions for portrait generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of human-centric personalized image generation, the\nadapter-based method obtains the ability to customize and generate portraits by\ntext-to-image training on facial data. This allows for identity-preserved\npersonalization without additional fine-tuning in inference. Although there are\nimprovements in efficiency and fidelity, there is often a significant\nperformance decrease in test following ability, controllability, and diversity\nof generated faces compared to the base model. In this paper, we analyze that\nthe performance degradation is attributed to the failure to decouple identity\nfeatures from other attributes during extraction, as well as the failure to\ndecouple the portrait generation training from the overall generation task. To\naddress these issues, we propose the Face Adapter with deCoupled Training\n(FACT) framework, focusing on both model architecture and training strategy. To\ndecouple identity features from others, we leverage a transformer-based\nface-export encoder and harness fine-grained identity features. To decouple the\nportrait generation training, we propose Face Adapting Increment\nRegularization~(FAIR), which effectively constrains the effect of face adapters\non the facial region, preserving the generative ability of the base model.\nAdditionally, we incorporate a face condition drop and shuffle mechanism,\ncombined with curriculum learning, to enhance facial controllability and\ndiversity. As a result, FACT solely learns identity preservation from training\ndata, thereby minimizing the impact on the original text-to-image capabilities\nof the base model. Extensive experiments show that FACT has both\ncontrollability and fidelity in both text-to-image generation and inpainting\nsolutions for portrait generation."
                },
                "authors": [
                    {
                        "name": "Cheng Yu"
                    },
                    {
                        "name": "Haoyu Xie"
                    },
                    {
                        "name": "Lei Shang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Jun Dan"
                    },
                    {
                        "name": "Liefeng Bo"
                    },
                    {
                        "name": "Baigui Sun"
                    }
                ],
                "author_detail": {
                    "name": "Baigui Sun"
                },
                "author": "Baigui Sun",
                "arxiv_comment": "12 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12312v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12312v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06040v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06040v2",
                "updated": "2024-10-25T06:32:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    6,
                    32,
                    9,
                    4,
                    299,
                    0
                ],
                "published": "2024-06-10T06:17:55Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    6,
                    17,
                    55,
                    0,
                    162,
                    0
                ],
                "title": "Vript: A Video Is Worth Thousands of Words",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vript: A Video Is Worth Thousands of Words"
                },
                "summary": "Advancements in multimodal learning, particularly in video understanding and\ngeneration, require high-quality video-text datasets for improved model\nperformance. Vript addresses this issue with a meticulously annotated corpus of\n12K high-resolution videos, offering detailed, dense, and script-like captions\nfor over 420K clips. Each clip has a caption of ~145 words, which is over 10x\nlonger than most video-text datasets. Unlike captions only documenting static\ncontent in previous datasets, we enhance video captioning to video scripting by\ndocumenting not just the content, but also the camera operations, which include\nthe shot types (medium shot, close-up, etc) and camera movements (panning,\ntilting, etc). By utilizing the Vript, we explore three training paradigms of\naligning more text with the video modality rather than clip-caption pairs. This\nresults in Vriptor, a top-performing video captioning model among open-source\nmodels, comparable to GPT-4V in performance. Vriptor is also a powerful model\ncapable of end-to-end generation of dense and detailed captions for long\nvideos. Moreover, we introduce Vript-Hard, a benchmark consisting of three\nvideo understanding tasks that are more challenging than existing benchmarks:\nVript-HAL is the first benchmark evaluating action and object hallucinations in\nvideo LLMs, Vript-RR combines reasoning with retrieval resolving question\nambiguity in long-video QAs, and Vript-ERO is a new task to evaluate the\ntemporal understanding of events in long videos rather than actions in short\nvideos in previous works. All code, models, and datasets are available in\nhttps://github.com/mutonix/Vript. PS: We have included more video-text datasets\n(Vript_CN & Vript_Multilingual) in the Vript series.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in multimodal learning, particularly in video understanding and\ngeneration, require high-quality video-text datasets for improved model\nperformance. Vript addresses this issue with a meticulously annotated corpus of\n12K high-resolution videos, offering detailed, dense, and script-like captions\nfor over 420K clips. Each clip has a caption of ~145 words, which is over 10x\nlonger than most video-text datasets. Unlike captions only documenting static\ncontent in previous datasets, we enhance video captioning to video scripting by\ndocumenting not just the content, but also the camera operations, which include\nthe shot types (medium shot, close-up, etc) and camera movements (panning,\ntilting, etc). By utilizing the Vript, we explore three training paradigms of\naligning more text with the video modality rather than clip-caption pairs. This\nresults in Vriptor, a top-performing video captioning model among open-source\nmodels, comparable to GPT-4V in performance. Vriptor is also a powerful model\ncapable of end-to-end generation of dense and detailed captions for long\nvideos. Moreover, we introduce Vript-Hard, a benchmark consisting of three\nvideo understanding tasks that are more challenging than existing benchmarks:\nVript-HAL is the first benchmark evaluating action and object hallucinations in\nvideo LLMs, Vript-RR combines reasoning with retrieval resolving question\nambiguity in long-video QAs, and Vript-ERO is a new task to evaluate the\ntemporal understanding of events in long videos rather than actions in short\nvideos in previous works. All code, models, and datasets are available in\nhttps://github.com/mutonix/Vript. PS: We have included more video-text datasets\n(Vript_CN & Vript_Multilingual) in the Vript series."
                },
                "authors": [
                    {
                        "name": "Dongjie Yang"
                    },
                    {
                        "name": "Suyuan Huang"
                    },
                    {
                        "name": "Chengqiang Lu"
                    },
                    {
                        "name": "Xiaodong Han"
                    },
                    {
                        "name": "Haoxin Zhang"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "Accepted by NeurIPS Dataset & Benchmark track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06040v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06040v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01436v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01436v2",
                "updated": "2024-10-25T06:20:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    6,
                    20,
                    16,
                    4,
                    299,
                    0
                ],
                "published": "2024-06-03T15:28:21Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    15,
                    28,
                    21,
                    0,
                    155,
                    0
                ],
                "title": "Editing the Mind of Giants: An In-Depth Exploration of Pitfalls of\n  Knowledge Editing in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Editing the Mind of Giants: An In-Depth Exploration of Pitfalls of\n  Knowledge Editing in Large Language Models"
                },
                "summary": "Knowledge editing is a rising technique for efficiently updating factual\nknowledge in large language models (LLMs) with minimal alteration of\nparameters. However, recent studies have identified side effects, such as\nknowledge distortion and the deterioration of general abilities, that have\nemerged after editing. Despite these findings, evaluating the pitfalls of\nknowledge editing often relies on inconsistent metrics and benchmarks, lacking\na uniform standard. In response, this survey presents a comprehensive study of\nthese side effects, providing a unified perspective on the challenges of\nknowledge editing in LLMs by conducting experiments with consistent metrics and\nbenchmarks. Additionally, we review related works and outline potential\nresearch directions to address these limitations. Our survey highlights the\nlimitations of current knowledge editing methods, emphasizing the need for a\ndeeper understanding of the inner knowledge structures of LLMs and improved\nknowledge editing methods. To foster future research, we have released the\ncomplementary materials publicly in https://github.com/MiuLab/EditLLM-Survey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge editing is a rising technique for efficiently updating factual\nknowledge in large language models (LLMs) with minimal alteration of\nparameters. However, recent studies have identified side effects, such as\nknowledge distortion and the deterioration of general abilities, that have\nemerged after editing. Despite these findings, evaluating the pitfalls of\nknowledge editing often relies on inconsistent metrics and benchmarks, lacking\na uniform standard. In response, this survey presents a comprehensive study of\nthese side effects, providing a unified perspective on the challenges of\nknowledge editing in LLMs by conducting experiments with consistent metrics and\nbenchmarks. Additionally, we review related works and outline potential\nresearch directions to address these limitations. Our survey highlights the\nlimitations of current knowledge editing methods, emphasizing the need for a\ndeeper understanding of the inner knowledge structures of LLMs and improved\nknowledge editing methods. To foster future research, we have released the\ncomplementary materials publicly in https://github.com/MiuLab/EditLLM-Survey."
                },
                "authors": [
                    {
                        "name": "Cheng-Hsun Hsueh"
                    },
                    {
                        "name": "Paul Kuo-Ming Huang"
                    },
                    {
                        "name": "Tzu-Han Lin"
                    },
                    {
                        "name": "Che-Wei Liao"
                    },
                    {
                        "name": "Hung-Chieh Fang"
                    },
                    {
                        "name": "Chao-Wei Huang"
                    },
                    {
                        "name": "Yun-Nung Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yun-Nung Chen"
                },
                "author": "Yun-Nung Chen",
                "arxiv_comment": "EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01436v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01436v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16236v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16236v2",
                "updated": "2024-10-25T06:19:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    6,
                    19,
                    13,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-21T17:41:28Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    41,
                    28,
                    0,
                    295,
                    0
                ],
                "title": "LLaVA-KD: A Framework of Distilling Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaVA-KD: A Framework of Distilling Multimodal Large Language Models"
                },
                "summary": "The success of Large Language Models (LLM) has led researchers to explore\nMultimodal Large Language Models (MLLM) for unified visual and linguistic\nunderstanding. However, the increasing model size and computational complexity\nof MLLM limit their use in resource-constrained environments. Small-scale MLLM\n(s-MLLM) aims to retain the capabilities of the large-scale model (l-MLLM)\nwhile reducing computational demands, but resulting in a significant decline in\nperformance. To address the aforementioned issues, we propose a novel LLaVA-KD\nframework to transfer knowledge from l-MLLM to s-MLLM. Specifically, we\nintroduce Multimodal Distillation (MDist) to minimize the divergence between\nthe visual-textual output distributions of l-MLLM and s-MLLM, and Relation\nDistillation (RDist) to transfer l-MLLM's ability to model correlations between\nvisual features. Additionally, we propose a three-stage training scheme to\nfully exploit the potential of s-MLLM: 1) Distilled Pre-Training to align\nvisual-textual representations, 2) Supervised Fine-Tuning to equip the model\nwith multimodal understanding, and 3) Distilled Fine-Tuning to further transfer\nl-MLLM capabilities. Our approach significantly improves performance without\naltering the small model's architecture. Extensive experiments and ablation\nstudies validate the effectiveness of each proposed component. Code will be\navailable at https://github.com/Fantasyele/LLaVA-KD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of Large Language Models (LLM) has led researchers to explore\nMultimodal Large Language Models (MLLM) for unified visual and linguistic\nunderstanding. However, the increasing model size and computational complexity\nof MLLM limit their use in resource-constrained environments. Small-scale MLLM\n(s-MLLM) aims to retain the capabilities of the large-scale model (l-MLLM)\nwhile reducing computational demands, but resulting in a significant decline in\nperformance. To address the aforementioned issues, we propose a novel LLaVA-KD\nframework to transfer knowledge from l-MLLM to s-MLLM. Specifically, we\nintroduce Multimodal Distillation (MDist) to minimize the divergence between\nthe visual-textual output distributions of l-MLLM and s-MLLM, and Relation\nDistillation (RDist) to transfer l-MLLM's ability to model correlations between\nvisual features. Additionally, we propose a three-stage training scheme to\nfully exploit the potential of s-MLLM: 1) Distilled Pre-Training to align\nvisual-textual representations, 2) Supervised Fine-Tuning to equip the model\nwith multimodal understanding, and 3) Distilled Fine-Tuning to further transfer\nl-MLLM capabilities. Our approach significantly improves performance without\naltering the small model's architecture. Extensive experiments and ablation\nstudies validate the effectiveness of each proposed component. Code will be\navailable at https://github.com/Fantasyele/LLaVA-KD."
                },
                "authors": [
                    {
                        "name": "Yuxuan Cai"
                    },
                    {
                        "name": "Jiangning Zhang"
                    },
                    {
                        "name": "Haoyang He"
                    },
                    {
                        "name": "Xinwei He"
                    },
                    {
                        "name": "Ao Tong"
                    },
                    {
                        "name": "Zhenye Gan"
                    },
                    {
                        "name": "Chengjie Wang"
                    },
                    {
                        "name": "Xiang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Bai"
                },
                "author": "Xiang Bai",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16236v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16236v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03856v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03856v3",
                "updated": "2024-10-25T06:12:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    6,
                    12,
                    49,
                    4,
                    299,
                    0
                ],
                "published": "2024-07-04T11:42:36Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    11,
                    42,
                    36,
                    3,
                    186,
                    0
                ],
                "title": "Q-Adapter: Customizing Pre-trained LLMs to New Preferences with\n  Forgetting Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-Adapter: Customizing Pre-trained LLMs to New Preferences with\n  Forgetting Mitigation"
                },
                "summary": "Large Language Models (LLMs), trained on a large amount of corpus, have\ndemonstrated remarkable abilities. However, it may not be sufficient to\ndirectly apply open-source LLMs like Llama to certain real-world scenarios,\nsince most of them are trained for \\emph{general} purposes. Thus, the demands\nfor customizing publicly available LLMs emerge, but are currently\nunder-studied. In this work, we consider customizing pre-trained LLMs with new\nhuman preferences. Specifically, the LLM should not only meet the new\npreference but also preserve its original capabilities after customization.\nDrawing inspiration from the observation that human preference can be expressed\nas a reward model, we propose to cast LLM customization as optimizing the sum\nof two reward functions, one of which (denoted as $r_1$) was used to pre-train\nthe LLM while the other (denoted as $r_2$) characterizes the new human\npreference. The obstacle here is that both reward functions are unknown, making\nthe application of modern reinforcement learning methods infeasible. Thanks to\nthe residual Q-learning framework, we can restore the customized LLM with the\npre-trained LLM and the \\emph{residual Q-function} without the reward function\n$r_1$. Moreover, we find that for a fixed pre-trained LLM, the reward function\n$r_2$ can be derived from the residual Q-function, enabling us to directly\nlearn the residual Q-function from the new human preference data upon the\nBradley-Terry model. We name our method Q-Adapter as it introduces an adapter\nmodule to approximate the residual Q-function for customizing the pre-trained\nLLM towards the new preference. Experiments based on the Llama-3.1 model on the\nDSP dataset and HH-RLHF dataset illustrate the superior effectiveness of\nQ-Adapter on both retaining existing knowledge and learning new preferences.\nCode is available at \\url{https://github.com/mansicer/Q-Adapter}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), trained on a large amount of corpus, have\ndemonstrated remarkable abilities. However, it may not be sufficient to\ndirectly apply open-source LLMs like Llama to certain real-world scenarios,\nsince most of them are trained for \\emph{general} purposes. Thus, the demands\nfor customizing publicly available LLMs emerge, but are currently\nunder-studied. In this work, we consider customizing pre-trained LLMs with new\nhuman preferences. Specifically, the LLM should not only meet the new\npreference but also preserve its original capabilities after customization.\nDrawing inspiration from the observation that human preference can be expressed\nas a reward model, we propose to cast LLM customization as optimizing the sum\nof two reward functions, one of which (denoted as $r_1$) was used to pre-train\nthe LLM while the other (denoted as $r_2$) characterizes the new human\npreference. The obstacle here is that both reward functions are unknown, making\nthe application of modern reinforcement learning methods infeasible. Thanks to\nthe residual Q-learning framework, we can restore the customized LLM with the\npre-trained LLM and the \\emph{residual Q-function} without the reward function\n$r_1$. Moreover, we find that for a fixed pre-trained LLM, the reward function\n$r_2$ can be derived from the residual Q-function, enabling us to directly\nlearn the residual Q-function from the new human preference data upon the\nBradley-Terry model. We name our method Q-Adapter as it introduces an adapter\nmodule to approximate the residual Q-function for customizing the pre-trained\nLLM towards the new preference. Experiments based on the Llama-3.1 model on the\nDSP dataset and HH-RLHF dataset illustrate the superior effectiveness of\nQ-Adapter on both retaining existing knowledge and learning new preferences.\nCode is available at \\url{https://github.com/mansicer/Q-Adapter}."
                },
                "authors": [
                    {
                        "name": "Yi-Chen Li"
                    },
                    {
                        "name": "Fuxiang Zhang"
                    },
                    {
                        "name": "Wenjie Qiu"
                    },
                    {
                        "name": "Lei Yuan"
                    },
                    {
                        "name": "Chengxing Jia"
                    },
                    {
                        "name": "Zongzhang Zhang"
                    },
                    {
                        "name": "Yang Yu"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03856v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03856v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19318v1",
                "updated": "2024-10-25T06:08:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    6,
                    8,
                    59,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T06:08:59Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    6,
                    8,
                    59,
                    4,
                    299,
                    0
                ],
                "title": "Two are better than one: Context window extension with multi-grained\n  self-injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two are better than one: Context window extension with multi-grained\n  self-injection"
                },
                "summary": "The limited context window of contemporary large language models (LLMs)\nremains a huge barrier to their broader application across various domains.\nWhile continual pre-training on long-context data is a straightforward and\neffective solution, it incurs substantial costs in terms of data acquisition\nand computational resources. To alleviate this issue, we propose SharedLLM, a\nnovel approach grounded in the design philosophy of multi-grained context\ncompression and query-aware information retrieval. SharedLLM is composed of two\nshort-context LLMs such as LLaMA-2, termed upper model and lower model. The\nlower model functions as a compressor while the upper model acts as a decoder.\nThe upper model receives compressed, multi-grained context information from the\nlower model and performs context-aware modeling on the running text.\nInformation transfer between the compressor and decoder occurs only at the\nlowest layers to refrain from long forward paths in the lower model and\nredundant cross-attention modules in the upper model. Based on this\narchitecture, we introduce a specialized tree-style data structure to\nefficiently encode, store and retrieve multi-grained contextual information for\ntext chunks. This structure, combined with a search algorithm, enables rapid\nencoding and retrieval of relevant information from various levels of the tree\nbased on the input query. This entire process, wherein the sender and receiver\nare derived from the same LLM layer, is referred to as self-injection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The limited context window of contemporary large language models (LLMs)\nremains a huge barrier to their broader application across various domains.\nWhile continual pre-training on long-context data is a straightforward and\neffective solution, it incurs substantial costs in terms of data acquisition\nand computational resources. To alleviate this issue, we propose SharedLLM, a\nnovel approach grounded in the design philosophy of multi-grained context\ncompression and query-aware information retrieval. SharedLLM is composed of two\nshort-context LLMs such as LLaMA-2, termed upper model and lower model. The\nlower model functions as a compressor while the upper model acts as a decoder.\nThe upper model receives compressed, multi-grained context information from the\nlower model and performs context-aware modeling on the running text.\nInformation transfer between the compressor and decoder occurs only at the\nlowest layers to refrain from long forward paths in the lower model and\nredundant cross-attention modules in the upper model. Based on this\narchitecture, we introduce a specialized tree-style data structure to\nefficiently encode, store and retrieve multi-grained contextual information for\ntext chunks. This structure, combined with a search algorithm, enables rapid\nencoding and retrieval of relevant information from various levels of the tree\nbased on the input query. This entire process, wherein the sender and receiver\nare derived from the same LLM layer, is referred to as self-injection."
                },
                "authors": [
                    {
                        "name": "Wei Han"
                    },
                    {
                        "name": "Pan Zhou"
                    },
                    {
                        "name": "Soujanya Poria"
                    },
                    {
                        "name": "Shuicheng Yan"
                    }
                ],
                "author_detail": {
                    "name": "Shuicheng Yan"
                },
                "author": "Shuicheng Yan",
                "arxiv_comment": "The code is available at https://github.com/Clement25/SharedLLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19317v1",
                "updated": "2024-10-25T06:06:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    6,
                    6,
                    31,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T06:06:31Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    6,
                    6,
                    31,
                    4,
                    299,
                    0
                ],
                "title": "FairMT-Bench: Benchmarking Fairness for Multi-turn Dialogue in\n  Conversational LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FairMT-Bench: Benchmarking Fairness for Multi-turn Dialogue in\n  Conversational LLMs"
                },
                "summary": "The growing use of large language model (LLM)-based chatbots has raised\nconcerns about fairness. Fairness issues in LLMs can lead to severe\nconsequences, such as bias amplification, discrimination, and harm to\nmarginalized communities. While existing fairness benchmarks mainly focus on\nsingle-turn dialogues, multi-turn scenarios, which in fact better reflect\nreal-world conversations, present greater challenges due to conversational\ncomplexity and potential bias accumulation. In this paper, we propose a\ncomprehensive fairness benchmark for LLMs in multi-turn dialogue scenarios,\n\\textbf{FairMT-Bench}. Specifically, we formulate a task taxonomy targeting LLM\nfairness capabilities across three stages: context understanding, user\ninteraction, and instruction trade-offs, with each stage comprising two tasks.\nTo ensure coverage of diverse bias types and attributes, we draw from existing\nfairness datasets and employ our template to construct a multi-turn dialogue\ndataset, \\texttt{FairMT-10K}. For evaluation, GPT-4 is applied, alongside bias\nclassifiers including Llama-Guard-3 and human validation to ensure robustness.\nExperiments and analyses on \\texttt{FairMT-10K} reveal that in multi-turn\ndialogue scenarios, current LLMs are more likely to generate biased responses,\nand there is significant variation in performance across different tasks and\nmodels. Based on this, we curate a challenging dataset, \\texttt{FairMT-1K}, and\ntest 15 current state-of-the-art (SOTA) LLMs on this dataset. The results show\nthe current state of fairness in LLMs and showcase the utility of this novel\napproach for assessing fairness in more realistic multi-turn dialogue contexts,\ncalling for future work to focus on LLM fairness improvement and the adoption\nof \\texttt{FairMT-1K} in such efforts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing use of large language model (LLM)-based chatbots has raised\nconcerns about fairness. Fairness issues in LLMs can lead to severe\nconsequences, such as bias amplification, discrimination, and harm to\nmarginalized communities. While existing fairness benchmarks mainly focus on\nsingle-turn dialogues, multi-turn scenarios, which in fact better reflect\nreal-world conversations, present greater challenges due to conversational\ncomplexity and potential bias accumulation. In this paper, we propose a\ncomprehensive fairness benchmark for LLMs in multi-turn dialogue scenarios,\n\\textbf{FairMT-Bench}. Specifically, we formulate a task taxonomy targeting LLM\nfairness capabilities across three stages: context understanding, user\ninteraction, and instruction trade-offs, with each stage comprising two tasks.\nTo ensure coverage of diverse bias types and attributes, we draw from existing\nfairness datasets and employ our template to construct a multi-turn dialogue\ndataset, \\texttt{FairMT-10K}. For evaluation, GPT-4 is applied, alongside bias\nclassifiers including Llama-Guard-3 and human validation to ensure robustness.\nExperiments and analyses on \\texttt{FairMT-10K} reveal that in multi-turn\ndialogue scenarios, current LLMs are more likely to generate biased responses,\nand there is significant variation in performance across different tasks and\nmodels. Based on this, we curate a challenging dataset, \\texttt{FairMT-1K}, and\ntest 15 current state-of-the-art (SOTA) LLMs on this dataset. The results show\nthe current state of fairness in LLMs and showcase the utility of this novel\napproach for assessing fairness in more realistic multi-turn dialogue contexts,\ncalling for future work to focus on LLM fairness improvement and the adoption\nof \\texttt{FairMT-1K} in such efforts."
                },
                "authors": [
                    {
                        "name": "Zhiting Fan"
                    },
                    {
                        "name": "Ruizhe Chen"
                    },
                    {
                        "name": "Tianxiang Hu"
                    },
                    {
                        "name": "Zuozhu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zuozhu Liu"
                },
                "author": "Zuozhu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19315v1",
                "updated": "2024-10-25T06:00:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    6,
                    0,
                    18,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T06:00:18Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    6,
                    0,
                    18,
                    4,
                    299,
                    0
                ],
                "title": "A prescriptive theory for brain-like inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A prescriptive theory for brain-like inference"
                },
                "summary": "The Evidence Lower Bound (ELBO) is a widely used objective for training deep\ngenerative models, such as Variational Autoencoders (VAEs). In the neuroscience\nliterature, an identical objective is known as the variational free energy,\nhinting at a potential unified framework for brain function and machine\nlearning. Despite its utility in interpreting generative models, including\ndiffusion models, ELBO maximization is often seen as too broad to offer\nprescriptive guidance for specific architectures in neuroscience or machine\nlearning. In this work, we show that maximizing ELBO under Poisson assumptions\nfor general sequence data leads to a spiking neural network that performs\nBayesian posterior inference through its membrane potential dynamics. The\nresulting model, the iterative Poisson VAE (iP-VAE), has a closer connection to\nbiological neurons than previous brain-inspired predictive coding models based\non Gaussian assumptions. Compared to amortized and iterative VAEs, iP-VAElearns\nsparser representations and exhibits superior generalization to\nout-of-distribution samples. These findings suggest that optimizing ELBO,\ncombined with Poisson assumptions, provides a solid foundation for developing\nprescriptive theories in NeuroAI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Evidence Lower Bound (ELBO) is a widely used objective for training deep\ngenerative models, such as Variational Autoencoders (VAEs). In the neuroscience\nliterature, an identical objective is known as the variational free energy,\nhinting at a potential unified framework for brain function and machine\nlearning. Despite its utility in interpreting generative models, including\ndiffusion models, ELBO maximization is often seen as too broad to offer\nprescriptive guidance for specific architectures in neuroscience or machine\nlearning. In this work, we show that maximizing ELBO under Poisson assumptions\nfor general sequence data leads to a spiking neural network that performs\nBayesian posterior inference through its membrane potential dynamics. The\nresulting model, the iterative Poisson VAE (iP-VAE), has a closer connection to\nbiological neurons than previous brain-inspired predictive coding models based\non Gaussian assumptions. Compared to amortized and iterative VAEs, iP-VAElearns\nsparser representations and exhibits superior generalization to\nout-of-distribution samples. These findings suggest that optimizing ELBO,\ncombined with Poisson assumptions, provides a solid foundation for developing\nprescriptive theories in NeuroAI."
                },
                "authors": [
                    {
                        "name": "Hadi Vafaii"
                    },
                    {
                        "name": "Dekel Galor"
                    },
                    {
                        "name": "Jacob L. Yates"
                    }
                ],
                "author_detail": {
                    "name": "Jacob L. Yates"
                },
                "author": "Jacob L. Yates",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19314v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19314v1",
                "updated": "2024-10-25T05:59:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    5,
                    59,
                    44,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T05:59:44Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    5,
                    59,
                    44,
                    4,
                    299,
                    0
                ],
                "title": "Revealing and Reducing Gender Biases in Vision and Language Assistants\n  (VLAs)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing and Reducing Gender Biases in Vision and Language Assistants\n  (VLAs)"
                },
                "summary": "Pre-trained large language models (LLMs) have been reliably integrated with\nvisual input for multimodal tasks. The widespread adoption of instruction-tuned\nimage-to-text vision-language assistants (VLAs) like LLaVA and InternVL\nnecessitates evaluating gender biases. We study gender bias in 22 popular\nopen-source VLAs with respect to personality traits, skills, and occupations.\nOur results show that VLAs replicate human biases likely present in the data,\nsuch as real-world occupational imbalances. Similarly, they tend to attribute\nmore skills and positive personality traits to women than to men, and we see a\nconsistent tendency to associate negative personality traits with men. To\neliminate the gender bias in these models, we find that finetuning-based\ndebiasing methods achieve the best tradeoff between debiasing and retaining\nperformance on downstream tasks. We argue for pre-deploying gender bias\nassessment in VLAs and motivate further development of debiasing strategies to\nensure equitable societal outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained large language models (LLMs) have been reliably integrated with\nvisual input for multimodal tasks. The widespread adoption of instruction-tuned\nimage-to-text vision-language assistants (VLAs) like LLaVA and InternVL\nnecessitates evaluating gender biases. We study gender bias in 22 popular\nopen-source VLAs with respect to personality traits, skills, and occupations.\nOur results show that VLAs replicate human biases likely present in the data,\nsuch as real-world occupational imbalances. Similarly, they tend to attribute\nmore skills and positive personality traits to women than to men, and we see a\nconsistent tendency to associate negative personality traits with men. To\neliminate the gender bias in these models, we find that finetuning-based\ndebiasing methods achieve the best tradeoff between debiasing and retaining\nperformance on downstream tasks. We argue for pre-deploying gender bias\nassessment in VLAs and motivate further development of debiasing strategies to\nensure equitable societal outcomes."
                },
                "authors": [
                    {
                        "name": "Leander Girrbach"
                    },
                    {
                        "name": "Yiran Huang"
                    },
                    {
                        "name": "Stephan Alaniz"
                    },
                    {
                        "name": "Trevor Darrell"
                    },
                    {
                        "name": "Zeynep Akata"
                    }
                ],
                "author_detail": {
                    "name": "Zeynep Akata"
                },
                "author": "Zeynep Akata",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19314v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19314v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.10254v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10254v2",
                "updated": "2024-10-25T17:59:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    59,
                    4,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-14T08:10:34Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    8,
                    10,
                    34,
                    0,
                    288,
                    0
                ],
                "title": "LoLCATs: On Low-Rank Linearizing of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoLCATs: On Low-Rank Linearizing of Large Language Models"
                },
                "summary": "Recent works show we can linearize large language models (LLMs) -- swapping\nthe quadratic attentions of popular Transformer-based LLMs with subquadratic\nanalogs, such as linear attention -- avoiding the expensive pretraining costs.\nHowever, linearizing LLMs often significantly degrades model quality, still\nrequires training over billions of tokens, and remains limited to smaller 1.3B\nto 7B LLMs. We thus propose Low-rank Linear Conversion via Attention Transfer\n(LoLCATs), a simple two-step method that improves LLM linearizing quality with\norders of magnitudes less memory and compute. We base these steps on two\nfindings. First, we can replace an LLM's softmax attentions with\nclosely-approximating linear attentions, simply by training the linear\nattentions to match their softmax counterparts with an output MSE loss\n(\"attention transfer\"). Then, this enables adjusting for approximation errors\nand recovering LLM quality simply with low-rank adaptation (LoRA). LoLCATs\nsignificantly improves linearizing quality, training efficiency, and\nscalability. We significantly reduce the linearizing quality gap and produce\nstate-of-the-art subquadratic LLMs from Llama 3 8B and Mistral 7B v0.1, leading\nto 20+ points of improvement on 5-shot MMLU. Furthermore, LoLCATs does so with\nonly 0.2% of past methods' model parameters and 0.4% of their training tokens.\nFinally, we apply LoLCATs to create the first linearized 70B and 405B LLMs (50x\nlarger than prior work). When compared with prior approaches under the same\ncompute budgets, LoLCATs significantly improves linearizing quality, closing\nthe gap between linearized and original Llama 3.1 70B and 405B LLMs by 77.8%\nand 78.1% on 5-shot MMLU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works show we can linearize large language models (LLMs) -- swapping\nthe quadratic attentions of popular Transformer-based LLMs with subquadratic\nanalogs, such as linear attention -- avoiding the expensive pretraining costs.\nHowever, linearizing LLMs often significantly degrades model quality, still\nrequires training over billions of tokens, and remains limited to smaller 1.3B\nto 7B LLMs. We thus propose Low-rank Linear Conversion via Attention Transfer\n(LoLCATs), a simple two-step method that improves LLM linearizing quality with\norders of magnitudes less memory and compute. We base these steps on two\nfindings. First, we can replace an LLM's softmax attentions with\nclosely-approximating linear attentions, simply by training the linear\nattentions to match their softmax counterparts with an output MSE loss\n(\"attention transfer\"). Then, this enables adjusting for approximation errors\nand recovering LLM quality simply with low-rank adaptation (LoRA). LoLCATs\nsignificantly improves linearizing quality, training efficiency, and\nscalability. We significantly reduce the linearizing quality gap and produce\nstate-of-the-art subquadratic LLMs from Llama 3 8B and Mistral 7B v0.1, leading\nto 20+ points of improvement on 5-shot MMLU. Furthermore, LoLCATs does so with\nonly 0.2% of past methods' model parameters and 0.4% of their training tokens.\nFinally, we apply LoLCATs to create the first linearized 70B and 405B LLMs (50x\nlarger than prior work). When compared with prior approaches under the same\ncompute budgets, LoLCATs significantly improves linearizing quality, closing\nthe gap between linearized and original Llama 3.1 70B and 405B LLMs by 77.8%\nand 78.1% on 5-shot MMLU."
                },
                "authors": [
                    {
                        "name": "Michael Zhang"
                    },
                    {
                        "name": "Simran Arora"
                    },
                    {
                        "name": "Rahul Chalamala"
                    },
                    {
                        "name": "Alan Wu"
                    },
                    {
                        "name": "Benjamin Spector"
                    },
                    {
                        "name": "Aaryan Singhal"
                    },
                    {
                        "name": "Krithik Ramesh"
                    },
                    {
                        "name": "Christopher Ré"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Ré"
                },
                "author": "Christopher Ré",
                "arxiv_comment": "47 pages, 20 figures, 18 tables, preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10254v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10254v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19730v1",
                "updated": "2024-10-25T17:56:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    56,
                    24,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T17:56:24Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    56,
                    24,
                    4,
                    299,
                    0
                ],
                "title": "Counting Ability of Large Language Models and Impact of Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counting Ability of Large Language Models and Impact of Tokenization"
                },
                "summary": "Transformers, the backbone of modern large language models (LLMs), face\ninherent architectural limitations that impede their reasoning capabilities.\nUnlike recurrent networks, Transformers lack recurrent connections, confining\nthem to constant-depth computation. This restriction places them in the\ncomplexity class TC$^0$, making them theoretically incapable of solving tasks\nthat demand increasingly deep reasoning as input length grows. Counting, a\nfundamental component of many reasoning tasks, also requires reasoning depth to\ngrow linearly to be performed inductively. While previous studies have\nestablished the upper limits of counting ability in Transformer-based expert\nmodels (i.e., models specifically trained for counting tasks), these findings\ndo not directly extend to general-purpose LLMs due to differences in reasoning\nmechanisms. Recent work has highlighted how Chain of Thought (CoT) reasoning\ncan help alleviate some of the architectural limitations of Transformers in\ncounting tasks. However, little attention has been paid to the role of\ntokenization in these models. Unlike expert models that often use\ncharacter-level tokenization, LLMs typically rely on byte-level (BPE)\ntokenizers, which fundamentally alters the way reasoning is processed. Our work\ninvestigates the impact of tokenization on the counting abilities of LLMs,\nuncovering substantial performance variations based on input tokenization\ndifferences. We provide both theoretical and experimental analyses, offering\ninsights into how tokenization choices can undermine models' theoretical\ncomputability, thereby inspiring the design of new tokenization methods to\nenhance reasoning in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, the backbone of modern large language models (LLMs), face\ninherent architectural limitations that impede their reasoning capabilities.\nUnlike recurrent networks, Transformers lack recurrent connections, confining\nthem to constant-depth computation. This restriction places them in the\ncomplexity class TC$^0$, making them theoretically incapable of solving tasks\nthat demand increasingly deep reasoning as input length grows. Counting, a\nfundamental component of many reasoning tasks, also requires reasoning depth to\ngrow linearly to be performed inductively. While previous studies have\nestablished the upper limits of counting ability in Transformer-based expert\nmodels (i.e., models specifically trained for counting tasks), these findings\ndo not directly extend to general-purpose LLMs due to differences in reasoning\nmechanisms. Recent work has highlighted how Chain of Thought (CoT) reasoning\ncan help alleviate some of the architectural limitations of Transformers in\ncounting tasks. However, little attention has been paid to the role of\ntokenization in these models. Unlike expert models that often use\ncharacter-level tokenization, LLMs typically rely on byte-level (BPE)\ntokenizers, which fundamentally alters the way reasoning is processed. Our work\ninvestigates the impact of tokenization on the counting abilities of LLMs,\nuncovering substantial performance variations based on input tokenization\ndifferences. We provide both theoretical and experimental analyses, offering\ninsights into how tokenization choices can undermine models' theoretical\ncomputability, thereby inspiring the design of new tokenization methods to\nenhance reasoning in LLMs."
                },
                "authors": [
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Juntai Cao"
                    },
                    {
                        "name": "Chenyu You"
                    }
                ],
                "author_detail": {
                    "name": "Chenyu You"
                },
                "author": "Chenyu You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19727v1",
                "updated": "2024-10-25T17:53:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    53,
                    47,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T17:53:47Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    53,
                    47,
                    4,
                    299,
                    0
                ],
                "title": "FISHNET: Financial Intelligence from Sub-querying, Harmonizing,\n  Neural-Conditioning, Expert Swarms, and Task Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FISHNET: Financial Intelligence from Sub-querying, Harmonizing,\n  Neural-Conditioning, Expert Swarms, and Task Planning"
                },
                "summary": "Financial intelligence generation from vast data sources has typically relied\non traditional methods of knowledge-graph construction or database engineering.\nRecently, fine-tuned financial domain-specific Large Language Models (LLMs),\nhave emerged. While these advancements are promising, limitations such as high\ninference costs, hallucinations, and the complexity of concurrently analyzing\nhigh-dimensional financial data, emerge. This motivates our invention FISHNET\n(Financial Intelligence from Sub-querying, Harmonizing, Neural-Conditioning,\nExpert swarming, and Task planning), an agentic architecture that accomplishes\nhighly complex analytical tasks for more than 98,000 regulatory filings that\nvary immensely in terms of semantics, data hierarchy, or format. FISHNET shows\nremarkable performance for financial insight generation (61.8% success rate\nover 5.0% Routing, 45.6% RAG R-Precision). We conduct rigorous ablations to\nempirically prove the success of FISHNET, each agent's importance, and the\noptimized performance of assembling all agents. Our modular architecture can be\nleveraged for a myriad of use-cases, enabling scalability, flexibility, and\ndata integrity that are critical for financial tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Financial intelligence generation from vast data sources has typically relied\non traditional methods of knowledge-graph construction or database engineering.\nRecently, fine-tuned financial domain-specific Large Language Models (LLMs),\nhave emerged. While these advancements are promising, limitations such as high\ninference costs, hallucinations, and the complexity of concurrently analyzing\nhigh-dimensional financial data, emerge. This motivates our invention FISHNET\n(Financial Intelligence from Sub-querying, Harmonizing, Neural-Conditioning,\nExpert swarming, and Task planning), an agentic architecture that accomplishes\nhighly complex analytical tasks for more than 98,000 regulatory filings that\nvary immensely in terms of semantics, data hierarchy, or format. FISHNET shows\nremarkable performance for financial insight generation (61.8% success rate\nover 5.0% Routing, 45.6% RAG R-Precision). We conduct rigorous ablations to\nempirically prove the success of FISHNET, each agent's importance, and the\noptimized performance of assembling all agents. Our modular architecture can be\nleveraged for a myriad of use-cases, enabling scalability, flexibility, and\ndata integrity that are critical for financial tasks."
                },
                "authors": [
                    {
                        "name": "Nicole Cho"
                    },
                    {
                        "name": "Nishan Srishankar"
                    },
                    {
                        "name": "Lucas Cecchi"
                    },
                    {
                        "name": "William Watson"
                    }
                ],
                "author_detail": {
                    "name": "William Watson"
                },
                "author": "William Watson",
                "arxiv_doi": "10.1145/3677052.3698597",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3677052.3698597",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.19727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at the 5th ACM International Conference on AI in Finance\n  (ICAIF '24)",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19720v1",
                "updated": "2024-10-25T17:47:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    47,
                    35,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T17:47:35Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    47,
                    35,
                    4,
                    299,
                    0
                ],
                "title": "2D-DPO: Scaling Direct Preference Optimization with 2-Dimensional\n  Supervision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2D-DPO: Scaling Direct Preference Optimization with 2-Dimensional\n  Supervision"
                },
                "summary": "Recent advancements in Direct Preference Optimization (DPO) have\nsignificantly enhanced the alignment of Large Language Models (LLMs) with human\npreferences, owing to its simplicity and effectiveness. However, existing\nmethods typically optimize a scalar score or ranking reward, thereby\noverlooking the multi-dimensional nature of human preferences. In this work, we\npropose to extend the preference of DPO to two dimensions: segments and\naspects. We first introduce a 2D supervision dataset called HelpSteer-2D. For\nthe segment dimension, we divide the response into sentences and assign scores\nto each segment. For the aspect dimension, we meticulously design several\ncriteria covering the response quality rubrics. With the 2-dimensional signals\nas feedback, we develop a 2D-DPO framework, decomposing the overall objective\ninto multi-segment and multi-aspect objectives. Extensive experiments on\npopular benchmarks demonstrate that 2D-DPO performs better than methods that\noptimize for scalar or 1-dimensional preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Direct Preference Optimization (DPO) have\nsignificantly enhanced the alignment of Large Language Models (LLMs) with human\npreferences, owing to its simplicity and effectiveness. However, existing\nmethods typically optimize a scalar score or ranking reward, thereby\noverlooking the multi-dimensional nature of human preferences. In this work, we\npropose to extend the preference of DPO to two dimensions: segments and\naspects. We first introduce a 2D supervision dataset called HelpSteer-2D. For\nthe segment dimension, we divide the response into sentences and assign scores\nto each segment. For the aspect dimension, we meticulously design several\ncriteria covering the response quality rubrics. With the 2-dimensional signals\nas feedback, we develop a 2D-DPO framework, decomposing the overall objective\ninto multi-segment and multi-aspect objectives. Extensive experiments on\npopular benchmarks demonstrate that 2D-DPO performs better than methods that\noptimize for scalar or 1-dimensional preferences."
                },
                "authors": [
                    {
                        "name": "Shilong Li"
                    },
                    {
                        "name": "Yancheng He"
                    },
                    {
                        "name": "Hui Huang"
                    },
                    {
                        "name": "Xingyuan Bu"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Hangyu Guo"
                    },
                    {
                        "name": "Weixun Wang"
                    },
                    {
                        "name": "Jihao Gu"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "The first four authors contributed equally, 25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09187v2",
                "updated": "2024-10-25T17:37:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    37,
                    59,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-11T18:41:15Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    18,
                    41,
                    15,
                    4,
                    285,
                    0
                ],
                "title": "Automated Rewards via LLM-Generated Progress Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Rewards via LLM-Generated Progress Functions"
                },
                "summary": "Large Language Models (LLMs) have the potential to automate reward\nengineering by leveraging their broad domain knowledge across various tasks.\nHowever, they often need many iterations of trial-and-error to generate\neffective reward functions. This process is costly because evaluating every\nsampled reward function requires completing the full policy optimization\nprocess for each function. In this paper, we introduce an LLM-driven reward\ngeneration framework that is able to produce state-of-the-art policies on the\nchallenging Bi-DexHands benchmark with 20x fewer reward function samples than\nthe prior state-of-the-art work. Our key insight is that we reduce the problem\nof generating task-specific rewards to the problem of coarsely estimating task\nprogress. Our two-step solution leverages the task domain knowledge and the\ncode synthesis abilities of LLMs to author progress functions that estimate\ntask progress from a given state. Then, we use this notion of progress to\ndiscretize states, and generate count-based intrinsic rewards using the\nlow-dimensional state space. We show that the combination of LLM-generated\nprogress functions and count-based intrinsic rewards is essential for our\nperformance gains, while alternatives such as generic hash-based counts or\nusing progress directly as a reward function fall short.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have the potential to automate reward\nengineering by leveraging their broad domain knowledge across various tasks.\nHowever, they often need many iterations of trial-and-error to generate\neffective reward functions. This process is costly because evaluating every\nsampled reward function requires completing the full policy optimization\nprocess for each function. In this paper, we introduce an LLM-driven reward\ngeneration framework that is able to produce state-of-the-art policies on the\nchallenging Bi-DexHands benchmark with 20x fewer reward function samples than\nthe prior state-of-the-art work. Our key insight is that we reduce the problem\nof generating task-specific rewards to the problem of coarsely estimating task\nprogress. Our two-step solution leverages the task domain knowledge and the\ncode synthesis abilities of LLMs to author progress functions that estimate\ntask progress from a given state. Then, we use this notion of progress to\ndiscretize states, and generate count-based intrinsic rewards using the\nlow-dimensional state space. We show that the combination of LLM-generated\nprogress functions and count-based intrinsic rewards is essential for our\nperformance gains, while alternatives such as generic hash-based counts or\nusing progress directly as a reward function fall short."
                },
                "authors": [
                    {
                        "name": "Vishnu Sarukkai"
                    },
                    {
                        "name": "Brennan Shacklett"
                    },
                    {
                        "name": "Zander Majercik"
                    },
                    {
                        "name": "Kush Bhatia"
                    },
                    {
                        "name": "Christopher Ré"
                    },
                    {
                        "name": "Kayvon Fatahalian"
                    }
                ],
                "author_detail": {
                    "name": "Kayvon Fatahalian"
                },
                "author": "Kayvon Fatahalian",
                "arxiv_comment": "26 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15115v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15115v2",
                "updated": "2024-10-25T17:34:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    34,
                    22,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-19T13:53:50Z",
                "published_parsed": [
                    2024,
                    10,
                    19,
                    13,
                    53,
                    50,
                    5,
                    293,
                    0
                ],
                "title": "On Designing Effective RL Reward at Training Time for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Designing Effective RL Reward at Training Time for LLM Reasoning"
                },
                "summary": "Reward models have been increasingly critical for improving the reasoning\ncapability of LLMs. Existing research has shown that a well-trained reward\nmodel can substantially improve model performances at inference time via\nsearch. However, the potential of reward models during RL training time still\nremains largely under-explored. It is currently unclear whether these reward\nmodels can provide additional training signals to enhance the reasoning\ncapabilities of LLMs in RL training that uses sparse success rewards, which\nverify the correctness of solutions. In this work, we evaluate popular reward\nmodels for RL training, including the Outcome-supervised Reward Model (ORM) and\nthe Process-supervised Reward Model (PRM), and train a collection of LLMs for\nmath problems using RL by combining these learned rewards with success rewards.\nSurprisingly, even though these learned reward models have strong\ninference-time performances, they may NOT help or even hurt RL training,\nproducing worse performances than LLMs trained with the success reward only.\nOur analysis reveals that an LLM can receive high rewards from some of these\nreward models by repeating correct but unnecessary reasoning steps, leading to\na severe reward hacking issue. Therefore, we introduce two novel reward\nrefinement techniques, including Clipping and Delta. The key idea is to ensure\nthe accumulative reward of any reasoning trajectory is upper-bounded to keep a\nlearned reward model effective without being exploited. We evaluate our\ntechniques with multiple reward models over a set of 1.5B and 7B LLMs on MATH\nand GSM8K benchmarks and demonstrate that with a carefully designed reward\nfunction, RL training without any additional supervised tuning can improve all\nthe evaluated LLMs, including the state-of-the-art 7B LLM\nQwen2.5-Math-7B-Instruct on MATH and GSM8K benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models have been increasingly critical for improving the reasoning\ncapability of LLMs. Existing research has shown that a well-trained reward\nmodel can substantially improve model performances at inference time via\nsearch. However, the potential of reward models during RL training time still\nremains largely under-explored. It is currently unclear whether these reward\nmodels can provide additional training signals to enhance the reasoning\ncapabilities of LLMs in RL training that uses sparse success rewards, which\nverify the correctness of solutions. In this work, we evaluate popular reward\nmodels for RL training, including the Outcome-supervised Reward Model (ORM) and\nthe Process-supervised Reward Model (PRM), and train a collection of LLMs for\nmath problems using RL by combining these learned rewards with success rewards.\nSurprisingly, even though these learned reward models have strong\ninference-time performances, they may NOT help or even hurt RL training,\nproducing worse performances than LLMs trained with the success reward only.\nOur analysis reveals that an LLM can receive high rewards from some of these\nreward models by repeating correct but unnecessary reasoning steps, leading to\na severe reward hacking issue. Therefore, we introduce two novel reward\nrefinement techniques, including Clipping and Delta. The key idea is to ensure\nthe accumulative reward of any reasoning trajectory is upper-bounded to keep a\nlearned reward model effective without being exploited. We evaluate our\ntechniques with multiple reward models over a set of 1.5B and 7B LLMs on MATH\nand GSM8K benchmarks and demonstrate that with a carefully designed reward\nfunction, RL training without any additional supervised tuning can improve all\nthe evaluated LLMs, including the state-of-the-art 7B LLM\nQwen2.5-Math-7B-Instruct on MATH and GSM8K benchmarks."
                },
                "authors": [
                    {
                        "name": "Jiaxuan Gao"
                    },
                    {
                        "name": "Shusheng Xu"
                    },
                    {
                        "name": "Wenjie Ye"
                    },
                    {
                        "name": "Weilin Liu"
                    },
                    {
                        "name": "Chuyi He"
                    },
                    {
                        "name": "Wei Fu"
                    },
                    {
                        "name": "Zhiyu Mei"
                    },
                    {
                        "name": "Guangju Wang"
                    },
                    {
                        "name": "Yi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Wu"
                },
                "author": "Yi Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15115v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15115v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14262v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14262v3",
                "updated": "2024-10-25T17:24:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    24,
                    16,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-18T08:18:18Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    18,
                    18,
                    4,
                    292,
                    0
                ],
                "title": "Good Parenting is all you need -- Multi-agentic LLM Hallucination\n  Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Good Parenting is all you need -- Multi-agentic LLM Hallucination\n  Mitigation"
                },
                "summary": "This study explores the ability of Large Language Model (LLM) agents to\ndetect and correct hallucinations in AI-generated content. A primary agent was\ntasked with creating a blog about a fictional Danish artist named Flipfloppidy,\nwhich was then reviewed by another agent for factual inaccuracies. Most LLMs\nhallucinated the existence of this artist. Across 4,900 test runs involving\nvarious combinations of primary and reviewing agents, advanced AI models such\nas Llama3-70b and GPT-4 variants demonstrated near-perfect accuracy in\nidentifying hallucinations and successfully revised outputs in 85% to 100% of\ncases following feedback. These findings underscore the potential of advanced\nAI models to significantly enhance the accuracy and reliability of generated\ncontent, providing a promising approach to improving AI workflow orchestration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the ability of Large Language Model (LLM) agents to\ndetect and correct hallucinations in AI-generated content. A primary agent was\ntasked with creating a blog about a fictional Danish artist named Flipfloppidy,\nwhich was then reviewed by another agent for factual inaccuracies. Most LLMs\nhallucinated the existence of this artist. Across 4,900 test runs involving\nvarious combinations of primary and reviewing agents, advanced AI models such\nas Llama3-70b and GPT-4 variants demonstrated near-perfect accuracy in\nidentifying hallucinations and successfully revised outputs in 85% to 100% of\ncases following feedback. These findings underscore the potential of advanced\nAI models to significantly enhance the accuracy and reliability of generated\ncontent, providing a promising approach to improving AI workflow orchestration."
                },
                "authors": [
                    {
                        "name": "Ted Kwartler"
                    },
                    {
                        "name": "Matthew Berman"
                    },
                    {
                        "name": "Alan Aqrawi"
                    }
                ],
                "author_detail": {
                    "name": "Alan Aqrawi"
                },
                "author": "Alan Aqrawi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14262v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14262v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19702v1",
                "updated": "2024-10-25T17:19:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    19,
                    55,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T17:19:55Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    19,
                    55,
                    4,
                    299,
                    0
                ],
                "title": "TimeSuite: Improving MLLMs for Long Video Understanding via Grounded\n  Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TimeSuite: Improving MLLMs for Long Video Understanding via Grounded\n  Tuning"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\nperformance in short video understanding. However, understanding long-form\nvideos still remains challenging for MLLMs. This paper proposes TimeSuite, a\ncollection of new designs to adapt the existing short-form video MLLMs for long\nvideo understanding, including a simple yet efficient framework to process long\nvideo sequence, a high-quality video dataset for grounded tuning of MLLMs, and\na carefully-designed instruction tuning task to explicitly incorporate the\ngrounding supervision in the traditional QA format. Specifically, based on\nVideoChat, we propose our long-video MLLM, coined as VideoChat-T, by\nimplementing a token shuffling to compress long video tokens and introducing\nTemporal Adaptive Position Encoding (TAPE) to enhance the temporal awareness of\nvisual representation. Meanwhile, we introduce the TimePro, a comprehensive\ngrounding-centric instruction tuning dataset composed of 9 tasks and 349k\nhigh-quality grounded annotations. Notably, we design a new instruction tuning\ntask type, called Temporal Grounded Caption, to peform detailed video\ndescriptions with the corresponding time stamps prediction. This explicit\ntemporal location prediction will guide MLLM to correctly attend on the visual\ncontent when generating description, and thus reduce the hallucination risk\ncaused by the LLMs. Experimental results demonstrate that our TimeSuite\nprovides a successful solution to enhance the long video understanding\ncapability of short-form MLLM, achieving improvement of 5.6% and 6.8% on the\nbenchmarks of Egoschema and VideoMME, respectively. In addition, VideoChat-T\nexhibits robust zero-shot temporal grounding capabilities, significantly\noutperforming the existing state-of-the-art MLLMs. After fine-tuning, it\nperforms on par with the traditional supervised expert models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\nperformance in short video understanding. However, understanding long-form\nvideos still remains challenging for MLLMs. This paper proposes TimeSuite, a\ncollection of new designs to adapt the existing short-form video MLLMs for long\nvideo understanding, including a simple yet efficient framework to process long\nvideo sequence, a high-quality video dataset for grounded tuning of MLLMs, and\na carefully-designed instruction tuning task to explicitly incorporate the\ngrounding supervision in the traditional QA format. Specifically, based on\nVideoChat, we propose our long-video MLLM, coined as VideoChat-T, by\nimplementing a token shuffling to compress long video tokens and introducing\nTemporal Adaptive Position Encoding (TAPE) to enhance the temporal awareness of\nvisual representation. Meanwhile, we introduce the TimePro, a comprehensive\ngrounding-centric instruction tuning dataset composed of 9 tasks and 349k\nhigh-quality grounded annotations. Notably, we design a new instruction tuning\ntask type, called Temporal Grounded Caption, to peform detailed video\ndescriptions with the corresponding time stamps prediction. This explicit\ntemporal location prediction will guide MLLM to correctly attend on the visual\ncontent when generating description, and thus reduce the hallucination risk\ncaused by the LLMs. Experimental results demonstrate that our TimeSuite\nprovides a successful solution to enhance the long video understanding\ncapability of short-form MLLM, achieving improvement of 5.6% and 6.8% on the\nbenchmarks of Egoschema and VideoMME, respectively. In addition, VideoChat-T\nexhibits robust zero-shot temporal grounding capabilities, significantly\noutperforming the existing state-of-the-art MLLMs. After fine-tuning, it\nperforms on par with the traditional supervised expert models."
                },
                "authors": [
                    {
                        "name": "Xiangyu Zeng"
                    },
                    {
                        "name": "Kunchang Li"
                    },
                    {
                        "name": "Chenting Wang"
                    },
                    {
                        "name": "Xinhao Li"
                    },
                    {
                        "name": "Tianxiang Jiang"
                    },
                    {
                        "name": "Ziang Yan"
                    },
                    {
                        "name": "Songze Li"
                    },
                    {
                        "name": "Yansong Shi"
                    },
                    {
                        "name": "Zhengrong Yue"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Yali Wang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19694v1",
                "updated": "2024-10-25T17:07:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    7,
                    13,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T17:07:13Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    7,
                    13,
                    4,
                    299,
                    0
                ],
                "title": "Less is More: Extreme Gradient Boost Rank-1 Adaption for Efficient\n  Finetuning of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less is More: Extreme Gradient Boost Rank-1 Adaption for Efficient\n  Finetuning of LLMs"
                },
                "summary": "Fine-tuning Large Language Models (LLMs) has become a crucial technique for\nadapting pre-trained models to downstream tasks. However, the enormous size of\nLLMs poses significant challenges in terms of computational complexity and\nresource requirements. Low-Rank Adaptation (LoRA) has emerged as a promising\nsolution. However, there exists a gap between the practical performance of\nlow-rank adaptations and its theoretical optimum. In this work, we propose\neXtreme Gradient Boosting LoRA (XGBLoRA), a novel framework that bridges this\ngap by leveraging the power of ensemble learning. Inspired by gradient\nboosting, XGBLoRA iteratively learns and merges a sequence of LoRA adaptations\nto refine model predictions. It achieves better performance than the standard\nLoRA, while enjoying the computational efficiency of rank-1 adaptations. We\nprovide theoretical analysis to show the convergence and optimality of our\napproach, and conduct extensive experiments on a range of natural language\nprocessing tasks. The results demonstrate that XGBLoRA consistently outperforms\nstandard LoRA and achieves performance comparable to full fine-tuning with\nsignificantly fewer trainable parameters. This work advances\nparameter-efficient fine-tuning for LLMs, and offers a promising solution for\nadapting LLMs to downstream tasks while optimizing performance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning Large Language Models (LLMs) has become a crucial technique for\nadapting pre-trained models to downstream tasks. However, the enormous size of\nLLMs poses significant challenges in terms of computational complexity and\nresource requirements. Low-Rank Adaptation (LoRA) has emerged as a promising\nsolution. However, there exists a gap between the practical performance of\nlow-rank adaptations and its theoretical optimum. In this work, we propose\neXtreme Gradient Boosting LoRA (XGBLoRA), a novel framework that bridges this\ngap by leveraging the power of ensemble learning. Inspired by gradient\nboosting, XGBLoRA iteratively learns and merges a sequence of LoRA adaptations\nto refine model predictions. It achieves better performance than the standard\nLoRA, while enjoying the computational efficiency of rank-1 adaptations. We\nprovide theoretical analysis to show the convergence and optimality of our\napproach, and conduct extensive experiments on a range of natural language\nprocessing tasks. The results demonstrate that XGBLoRA consistently outperforms\nstandard LoRA and achieves performance comparable to full fine-tuning with\nsignificantly fewer trainable parameters. This work advances\nparameter-efficient fine-tuning for LLMs, and offers a promising solution for\nadapting LLMs to downstream tasks while optimizing performance and efficiency."
                },
                "authors": [
                    {
                        "name": "Yifei Zhang"
                    },
                    {
                        "name": "Hao Zhu"
                    },
                    {
                        "name": "Aiwei Liu"
                    },
                    {
                        "name": "Han Yu"
                    },
                    {
                        "name": "Piotr Koniusz"
                    },
                    {
                        "name": "Irwin King"
                    }
                ],
                "author_detail": {
                    "name": "Irwin King"
                },
                "author": "Irwin King",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19692v1",
                "updated": "2024-10-25T17:06:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    6,
                    27,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T17:06:27Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    6,
                    27,
                    4,
                    299,
                    0
                ],
                "title": "AGENT-CQ: Automatic Generation and Evaluation of Clarifying Questions\n  for Conversational Search with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGENT-CQ: Automatic Generation and Evaluation of Clarifying Questions\n  for Conversational Search with LLMs"
                },
                "summary": "Generating diverse and effective clarifying questions is crucial for\nimproving query understanding and retrieval performance in open-domain\nconversational search (CS) systems. We propose AGENT-CQ (Automatic GENeration,\nand evaluaTion of Clarifying Questions), an end-to-end LLM-based framework\naddressing the challenges of scalability and adaptability faced by existing\nmethods that rely on manual curation or template-based approaches. AGENT-CQ\nconsists of two stages: a generation stage employing LLM prompting strategies\nto generate clarifying questions, and an evaluation stage (CrowdLLM) that\nsimulates human crowdsourcing judgments using multiple LLM instances to assess\ngenerated questions and answers based on comprehensive quality metrics.\nExtensive experiments on the ClariQ dataset demonstrate CrowdLLM's\neffectiveness in evaluating question and answer quality. Human evaluation and\nCrowdLLM show that the AGENT-CQ - generation stage, consistently outperforms\nbaselines in various aspects of question and answer quality. In retrieval-based\nevaluation, LLM-generated questions significantly enhance retrieval\neffectiveness for both BM25 and cross-encoder models compared to\nhuman-generated questions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating diverse and effective clarifying questions is crucial for\nimproving query understanding and retrieval performance in open-domain\nconversational search (CS) systems. We propose AGENT-CQ (Automatic GENeration,\nand evaluaTion of Clarifying Questions), an end-to-end LLM-based framework\naddressing the challenges of scalability and adaptability faced by existing\nmethods that rely on manual curation or template-based approaches. AGENT-CQ\nconsists of two stages: a generation stage employing LLM prompting strategies\nto generate clarifying questions, and an evaluation stage (CrowdLLM) that\nsimulates human crowdsourcing judgments using multiple LLM instances to assess\ngenerated questions and answers based on comprehensive quality metrics.\nExtensive experiments on the ClariQ dataset demonstrate CrowdLLM's\neffectiveness in evaluating question and answer quality. Human evaluation and\nCrowdLLM show that the AGENT-CQ - generation stage, consistently outperforms\nbaselines in various aspects of question and answer quality. In retrieval-based\nevaluation, LLM-generated questions significantly enhance retrieval\neffectiveness for both BM25 and cross-encoder models compared to\nhuman-generated questions."
                },
                "authors": [
                    {
                        "name": "Clemencia Siro"
                    },
                    {
                        "name": "Yifei Yuan"
                    },
                    {
                        "name": "Mohammad Aliannejadi"
                    },
                    {
                        "name": "Maarten de Rijke"
                    }
                ],
                "author_detail": {
                    "name": "Maarten de Rijke"
                },
                "author": "Maarten de Rijke",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17141v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17141v2",
                "updated": "2024-10-25T16:58:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    16,
                    58,
                    37,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-22T16:18:41Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    16,
                    18,
                    41,
                    1,
                    296,
                    0
                ],
                "title": "Towards Automated Penetration Testing: Introducing LLM Benchmark,\n  Analysis, and Improvements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Automated Penetration Testing: Introducing LLM Benchmark,\n  Analysis, and Improvements"
                },
                "summary": "Hacking poses a significant threat to cybersecurity, inflicting billions of\ndollars in damages annually. To mitigate these risks, ethical hacking, or\npenetration testing, is employed to identify vulnerabilities in systems and\nnetworks. Recent advancements in large language models (LLMs) have shown\npotential across various domains, including cybersecurity. However, there is\ncurrently no comprehensive, open, end-to-end automated penetration testing\nbenchmark to drive progress and evaluate the capabilities of these models in\nsecurity contexts. This paper introduces a novel open benchmark for LLM-based\nautomated penetration testing, addressing this critical gap. We first evaluate\nthe performance of LLMs, including GPT-4o and Llama 3.1-405B, using the\nstate-of-the-art PentestGPT tool. Our findings reveal that while Llama 3.1\ndemonstrates an edge over GPT-4o, both models currently fall short of\nperforming fully automated, end-to-end penetration testing. Next, we advance\nthe state-of-the-art and present ablation studies that provide insights into\nimproving the PentestGPT tool. Our research illuminates the challenges LLMs\nface in each aspect of Pentesting, e.g. enumeration, exploitation, and\nprivilege escalation. This work contributes to the growing body of knowledge on\nAI-assisted cybersecurity and lays the foundation for future research in\nautomated penetration testing using large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hacking poses a significant threat to cybersecurity, inflicting billions of\ndollars in damages annually. To mitigate these risks, ethical hacking, or\npenetration testing, is employed to identify vulnerabilities in systems and\nnetworks. Recent advancements in large language models (LLMs) have shown\npotential across various domains, including cybersecurity. However, there is\ncurrently no comprehensive, open, end-to-end automated penetration testing\nbenchmark to drive progress and evaluate the capabilities of these models in\nsecurity contexts. This paper introduces a novel open benchmark for LLM-based\nautomated penetration testing, addressing this critical gap. We first evaluate\nthe performance of LLMs, including GPT-4o and Llama 3.1-405B, using the\nstate-of-the-art PentestGPT tool. Our findings reveal that while Llama 3.1\ndemonstrates an edge over GPT-4o, both models currently fall short of\nperforming fully automated, end-to-end penetration testing. Next, we advance\nthe state-of-the-art and present ablation studies that provide insights into\nimproving the PentestGPT tool. Our research illuminates the challenges LLMs\nface in each aspect of Pentesting, e.g. enumeration, exploitation, and\nprivilege escalation. This work contributes to the growing body of knowledge on\nAI-assisted cybersecurity and lays the foundation for future research in\nautomated penetration testing using large language models."
                },
                "authors": [
                    {
                        "name": "Isamu Isozaki"
                    },
                    {
                        "name": "Manil Shrestha"
                    },
                    {
                        "name": "Rick Console"
                    },
                    {
                        "name": "Edward Kim"
                    }
                ],
                "author_detail": {
                    "name": "Edward Kim"
                },
                "author": "Edward Kim",
                "arxiv_comment": "Main Paper 1-9 pages, Supplementary Materials: 10-17, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17141v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17141v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18682v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18682v2",
                "updated": "2024-10-25T16:57:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    16,
                    57,
                    43,
                    4,
                    299,
                    0
                ],
                "published": "2024-05-29T01:12:53Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    1,
                    12,
                    53,
                    2,
                    150,
                    0
                ],
                "title": "Can GPT Redefine Medical Understanding? Evaluating GPT on Biomedical\n  Machine Reading Comprehension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can GPT Redefine Medical Understanding? Evaluating GPT on Biomedical\n  Machine Reading Comprehension"
                },
                "summary": "Large language models (LLMs) have shown remarkable performance on many tasks\nin different domains. However, their performance in closed-book biomedical\nmachine reading comprehension (MRC) has not been evaluated in depth. In this\nwork, we evaluate GPT on four closed-book biomedical MRC benchmarks. We\nexperiment with different conventional prompting techniques as well as\nintroduce our own novel prompting method. To solve some of the retrieval\nproblems inherent to LLMs, we propose a prompting strategy named Implicit\nRetrieval Augmented Generation (RAG) that alleviates the need for using vector\ndatabases to retrieve important chunks in traditional RAG setups. Moreover, we\nreport qualitative assessments on the natural language generation outputs from\nour approach. The results show that our new prompting technique is able to get\nthe best performance in two out of four datasets and ranks second in rest of\nthem. Experiments show that modern-day LLMs like GPT even in a zero-shot\nsetting can outperform supervised models, leading to new state-of-the-art\n(SoTA) results on two of the benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable performance on many tasks\nin different domains. However, their performance in closed-book biomedical\nmachine reading comprehension (MRC) has not been evaluated in depth. In this\nwork, we evaluate GPT on four closed-book biomedical MRC benchmarks. We\nexperiment with different conventional prompting techniques as well as\nintroduce our own novel prompting method. To solve some of the retrieval\nproblems inherent to LLMs, we propose a prompting strategy named Implicit\nRetrieval Augmented Generation (RAG) that alleviates the need for using vector\ndatabases to retrieve important chunks in traditional RAG setups. Moreover, we\nreport qualitative assessments on the natural language generation outputs from\nour approach. The results show that our new prompting technique is able to get\nthe best performance in two out of four datasets and ranks second in rest of\nthem. Experiments show that modern-day LLMs like GPT even in a zero-shot\nsetting can outperform supervised models, leading to new state-of-the-art\n(SoTA) results on two of the benchmarks."
                },
                "authors": [
                    {
                        "name": "Shubham Vatsal"
                    },
                    {
                        "name": "Ayush Singh"
                    }
                ],
                "author_detail": {
                    "name": "Ayush Singh"
                },
                "author": "Ayush Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18682v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18682v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00066v2",
                "updated": "2024-10-25T16:57:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    16,
                    57,
                    10,
                    4,
                    299,
                    0
                ],
                "published": "2024-06-17T15:21:35Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    15,
                    21,
                    35,
                    0,
                    169,
                    0
                ],
                "title": "Compress then Serve: Serving Thousands of LoRA Adapters with Little\n  Overhead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compress then Serve: Serving Thousands of LoRA Adapters with Little\n  Overhead"
                },
                "summary": "Fine-tuning large language models (LLMs) with low-rank adaptations (LoRAs)\nhas become common practice, often yielding numerous copies of the same LLM\ndiffering only in their LoRA updates. This paradigm presents challenges for\nsystems that serve real-time responses to queries that each involve a different\nLoRA. Prior works optimize the design of such systems but still require\ncontinuous loading and offloading of LoRAs, as it is infeasible to store\nthousands of LoRAs in GPU memory. To mitigate this issue, we investigate the\nefficacy of model compression when serving LoRAs. We propose a method for joint\ncompression of LoRAs into a shared basis paired with LoRA-specific scaling\nmatrices. We extend our algorithm to learn clusters of LoRAs that are more\namenable to joint compression, allowing it to scale gracefully to large LoRA\ncollections. Our experiments with up to 500 LoRAs demonstrate that compressed\nLoRAs preserve performance while offering major throughput gains in realistic\nserving scenarios with over a thousand LoRAs, maintaining 80% of the throughput\nof serving a single LoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) with low-rank adaptations (LoRAs)\nhas become common practice, often yielding numerous copies of the same LLM\ndiffering only in their LoRA updates. This paradigm presents challenges for\nsystems that serve real-time responses to queries that each involve a different\nLoRA. Prior works optimize the design of such systems but still require\ncontinuous loading and offloading of LoRAs, as it is infeasible to store\nthousands of LoRAs in GPU memory. To mitigate this issue, we investigate the\nefficacy of model compression when serving LoRAs. We propose a method for joint\ncompression of LoRAs into a shared basis paired with LoRA-specific scaling\nmatrices. We extend our algorithm to learn clusters of LoRAs that are more\namenable to joint compression, allowing it to scale gracefully to large LoRA\ncollections. Our experiments with up to 500 LoRAs demonstrate that compressed\nLoRAs preserve performance while offering major throughput gains in realistic\nserving scenarios with over a thousand LoRAs, maintaining 80% of the throughput\nof serving a single LoRA."
                },
                "authors": [
                    {
                        "name": "Rickard Brüel-Gabrielsson"
                    },
                    {
                        "name": "Jiacheng Zhu"
                    },
                    {
                        "name": "Onkar Bhardwaj"
                    },
                    {
                        "name": "Leshem Choshen"
                    },
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Mikhail Yurochkin"
                    },
                    {
                        "name": "Justin Solomon"
                    }
                ],
                "author_detail": {
                    "name": "Justin Solomon"
                },
                "author": "Justin Solomon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19656v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19656v1",
                "updated": "2024-10-25T16:08:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    16,
                    8,
                    5,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T16:08:05Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    16,
                    8,
                    5,
                    4,
                    299,
                    0
                ],
                "title": "APRICOT: Active Preference Learning and Constraint-Aware Task Planning\n  with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APRICOT: Active Preference Learning and Constraint-Aware Task Planning\n  with LLMs"
                },
                "summary": "Home robots performing personalized tasks must adeptly balance user\npreferences with environmental affordances. We focus on organization tasks\nwithin constrained spaces, such as arranging items into a refrigerator, where\npreferences for placement collide with physical limitations. The robot must\ninfer user preferences based on a small set of demonstrations, which is easier\nfor users to provide than extensively defining all their requirements. While\nrecent works use Large Language Models (LLMs) to learn preferences from user\ndemonstrations, they encounter two fundamental challenges. First, there is\ninherent ambiguity in interpreting user actions, as multiple preferences can\noften explain a single observed behavior. Second, not all user preferences are\npractically feasible due to geometric constraints in the environment. To\naddress these challenges, we introduce APRICOT, a novel approach that merges\nLLM-based Bayesian active preference learning with constraint-aware task\nplanning. APRICOT refines its generated preferences by actively querying the\nuser and dynamically adapts its plan to respect environmental constraints. We\nevaluate APRICOT on a dataset of diverse organization tasks and demonstrate its\neffectiveness in real-world scenarios, showing significant improvements in both\npreference satisfaction and plan feasibility. The project website is at\nhttps://portal-cornell.github.io/apricot/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Home robots performing personalized tasks must adeptly balance user\npreferences with environmental affordances. We focus on organization tasks\nwithin constrained spaces, such as arranging items into a refrigerator, where\npreferences for placement collide with physical limitations. The robot must\ninfer user preferences based on a small set of demonstrations, which is easier\nfor users to provide than extensively defining all their requirements. While\nrecent works use Large Language Models (LLMs) to learn preferences from user\ndemonstrations, they encounter two fundamental challenges. First, there is\ninherent ambiguity in interpreting user actions, as multiple preferences can\noften explain a single observed behavior. Second, not all user preferences are\npractically feasible due to geometric constraints in the environment. To\naddress these challenges, we introduce APRICOT, a novel approach that merges\nLLM-based Bayesian active preference learning with constraint-aware task\nplanning. APRICOT refines its generated preferences by actively querying the\nuser and dynamically adapts its plan to respect environmental constraints. We\nevaluate APRICOT on a dataset of diverse organization tasks and demonstrate its\neffectiveness in real-world scenarios, showing significant improvements in both\npreference satisfaction and plan feasibility. The project website is at\nhttps://portal-cornell.github.io/apricot/"
                },
                "authors": [
                    {
                        "name": "Huaxiaoyue Wang"
                    },
                    {
                        "name": "Nathaniel Chin"
                    },
                    {
                        "name": "Gonzalo Gonzalez-Pumariega"
                    },
                    {
                        "name": "Xiangwan Sun"
                    },
                    {
                        "name": "Neha Sunkara"
                    },
                    {
                        "name": "Maximus Adrian Pace"
                    },
                    {
                        "name": "Jeannette Bohg"
                    },
                    {
                        "name": "Sanjiban Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Sanjiban Choudhury"
                },
                "author": "Sanjiban Choudhury",
                "arxiv_comment": "Conference on Robot Learning (CoRL) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19656v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19656v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18652v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18652v2",
                "updated": "2024-10-25T15:23:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    15,
                    23,
                    54,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-24T11:32:00Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    11,
                    32,
                    0,
                    3,
                    298,
                    0
                ],
                "title": "$C^2$: Scalable Auto-Feedback for LLM-based Chart Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$C^2$: Scalable Auto-Feedback for LLM-based Chart Generation"
                },
                "summary": "Generating high-quality charts with Large Language Models presents\nsignificant challenges due to limited data and the high cost of scaling through\nhuman curation. Instruction, data, and code triplets are scarce and expensive\nto manually curate as their creation demands technical expertise. To address\nthis scalability issue, we introduce a reference-free automatic feedback\ngenerator, which eliminates the need for costly human intervention. Our novel\nframework, $C^2$, consists of (1) an automatic feedback provider (ChartAF) and\n(2) a diverse, reference-free dataset (ChartUIE-8K). Quantitative results are\ncompelling: in our first experiment, 74% of respondents strongly preferred, and\n10% preferred, the results after feedback. The second post-feedback experiment\ndemonstrates that ChartAF outperforms nine baselines. Moreover, ChartUIE-8K\nsignificantly improves data diversity by increasing queries, datasets, and\nchart types by 5982%, 1936%, and 91%, respectively, over benchmarks. Finally,\nan LLM user study revealed that 94% of participants preferred ChartUIE-8K's\nqueries, with 93% deeming them aligned with real-world use cases. Core\ncontributions are available as open-source at an anonymized project site, with\nample qualitative examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating high-quality charts with Large Language Models presents\nsignificant challenges due to limited data and the high cost of scaling through\nhuman curation. Instruction, data, and code triplets are scarce and expensive\nto manually curate as their creation demands technical expertise. To address\nthis scalability issue, we introduce a reference-free automatic feedback\ngenerator, which eliminates the need for costly human intervention. Our novel\nframework, $C^2$, consists of (1) an automatic feedback provider (ChartAF) and\n(2) a diverse, reference-free dataset (ChartUIE-8K). Quantitative results are\ncompelling: in our first experiment, 74% of respondents strongly preferred, and\n10% preferred, the results after feedback. The second post-feedback experiment\ndemonstrates that ChartAF outperforms nine baselines. Moreover, ChartUIE-8K\nsignificantly improves data diversity by increasing queries, datasets, and\nchart types by 5982%, 1936%, and 91%, respectively, over benchmarks. Finally,\nan LLM user study revealed that 94% of participants preferred ChartUIE-8K's\nqueries, with 93% deeming them aligned with real-world use cases. Core\ncontributions are available as open-source at an anonymized project site, with\nample qualitative examples."
                },
                "authors": [
                    {
                        "name": "Woosung Koh"
                    },
                    {
                        "name": "Jang Han Yoon"
                    },
                    {
                        "name": "MinHyung Lee"
                    },
                    {
                        "name": "Youngjin Song"
                    },
                    {
                        "name": "Jaegwan Cho"
                    },
                    {
                        "name": "Jaehyun Kang"
                    },
                    {
                        "name": "Taehyeon Kim"
                    },
                    {
                        "name": "Se-young Yun"
                    },
                    {
                        "name": "Youngjae Yu"
                    },
                    {
                        "name": "Bongshin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Bongshin Lee"
                },
                "author": "Bongshin Lee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18652v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18652v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.06689v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.06689v4",
                "updated": "2024-10-25T15:08:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    15,
                    8,
                    3,
                    4,
                    299,
                    0
                ],
                "published": "2023-03-12T15:36:03Z",
                "published_parsed": [
                    2023,
                    3,
                    12,
                    15,
                    36,
                    3,
                    6,
                    71,
                    0
                ],
                "title": "Self-planning Code Generation with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-planning Code Generation with Large Language Models"
                },
                "summary": "Although large language models (LLMs) have demonstrated impressive ability in\ncode generation, they are still struggling to address the complicated intent\nprovided by humans. It is widely acknowledged that humans typically employ\nplanning to decompose complex problems and schedule solution steps prior to\nimplementation. To this end, we introduce planning into code generation to help\nthe model understand complex intent and reduce the difficulty of\nproblem-solving. This paper proposes a self-planning code generation approach\nwith large language models, which consists of two phases, namely planning phase\nand implementation phase. Specifically, in the planning phase, LLM plans out\nconcise solution steps from the intent combined with few-shot prompting.\nSubsequently, in the implementation phase, the model generates code step by\nstep, guided by the preceding solution steps. We conduct extensive experiments\non various code-generation benchmarks across multiple programming languages.\nExperimental results show that self-planning code generation achieves a\nrelative improvement of up to 25.4% in Pass@1 compared to direct code\ngeneration, and up to 11.9% compared to Chain-of-Thought of code generation.\nMoreover, our self-planning approach also enhances the quality of the generated\ncode with respect to correctness, readability, and robustness, as assessed by\nhumans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) have demonstrated impressive ability in\ncode generation, they are still struggling to address the complicated intent\nprovided by humans. It is widely acknowledged that humans typically employ\nplanning to decompose complex problems and schedule solution steps prior to\nimplementation. To this end, we introduce planning into code generation to help\nthe model understand complex intent and reduce the difficulty of\nproblem-solving. This paper proposes a self-planning code generation approach\nwith large language models, which consists of two phases, namely planning phase\nand implementation phase. Specifically, in the planning phase, LLM plans out\nconcise solution steps from the intent combined with few-shot prompting.\nSubsequently, in the implementation phase, the model generates code step by\nstep, guided by the preceding solution steps. We conduct extensive experiments\non various code-generation benchmarks across multiple programming languages.\nExperimental results show that self-planning code generation achieves a\nrelative improvement of up to 25.4% in Pass@1 compared to direct code\ngeneration, and up to 11.9% compared to Chain-of-Thought of code generation.\nMoreover, our self-planning approach also enhances the quality of the generated\ncode with respect to correctness, readability, and robustness, as assessed by\nhumans."
                },
                "authors": [
                    {
                        "name": "Xue Jiang"
                    },
                    {
                        "name": "Yihong Dong"
                    },
                    {
                        "name": "Lecheng Wang"
                    },
                    {
                        "name": "Zheng Fang"
                    },
                    {
                        "name": "Qiwei Shang"
                    },
                    {
                        "name": "Ge Li"
                    },
                    {
                        "name": "Zhi Jin"
                    },
                    {
                        "name": "Wenpin Jiao"
                    }
                ],
                "author_detail": {
                    "name": "Wenpin Jiao"
                },
                "author": "Wenpin Jiao",
                "arxiv_comment": "Accepted by TOSEM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.06689v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.06689v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19605v1",
                "updated": "2024-10-25T14:57:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    57,
                    29,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T14:57:29Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    57,
                    29,
                    4,
                    299,
                    0
                ],
                "title": "CoqPilot, a plugin for LLM-based generation of proofs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoqPilot, a plugin for LLM-based generation of proofs"
                },
                "summary": "We present CoqPilot, a VS Code extension designed to help automate writing of\nCoq proofs. The plugin collects the parts of proofs marked with the admit\ntactic in a Coq file, i.e., proof holes, and combines LLMs along with\nnon-machine-learning methods to generate proof candidates for the holes. Then,\nCoqPilot checks if each proof candidate solves the given subgoal and, if\nsuccessful, replaces the hole with it. The focus of CoqPilot is twofold.\nFirstly, we want to allow users to seamlessly combine multiple Coq generation\napproaches and provide a zero-setup experience for our tool. Secondly, we want\nto deliver a platform for LLM-based experiments on Coq proof generation. We\ndeveloped a benchmarking system for Coq generation methods, available in the\nplugin, and conducted an experiment using it, showcasing the framework's\npossibilities. Demo of CoqPilot is available at: https://youtu.be/oB1Lx-So9Lo.\nCode at: https://github.com/JetBrains-Research/coqpilot",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present CoqPilot, a VS Code extension designed to help automate writing of\nCoq proofs. The plugin collects the parts of proofs marked with the admit\ntactic in a Coq file, i.e., proof holes, and combines LLMs along with\nnon-machine-learning methods to generate proof candidates for the holes. Then,\nCoqPilot checks if each proof candidate solves the given subgoal and, if\nsuccessful, replaces the hole with it. The focus of CoqPilot is twofold.\nFirstly, we want to allow users to seamlessly combine multiple Coq generation\napproaches and provide a zero-setup experience for our tool. Secondly, we want\nto deliver a platform for LLM-based experiments on Coq proof generation. We\ndeveloped a benchmarking system for Coq generation methods, available in the\nplugin, and conducted an experiment using it, showcasing the framework's\npossibilities. Demo of CoqPilot is available at: https://youtu.be/oB1Lx-So9Lo.\nCode at: https://github.com/JetBrains-Research/coqpilot"
                },
                "authors": [
                    {
                        "name": "Andrei Kozyrev"
                    },
                    {
                        "name": "Gleb Solovev"
                    },
                    {
                        "name": "Nikita Khramov"
                    },
                    {
                        "name": "Anton Podkopaev"
                    }
                ],
                "author_detail": {
                    "name": "Anton Podkopaev"
                },
                "author": "Anton Podkopaev",
                "arxiv_doi": "10.1145/3691620.3695357",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3691620.3695357",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.19605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in the proceedings of the ASE'24 Tool Demonstrations Track",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19599v1",
                "updated": "2024-10-25T14:46:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    46,
                    7,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T14:46:07Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    46,
                    7,
                    4,
                    299,
                    0
                ],
                "title": "Take Caution in Using LLMs as Human Surrogates: Scylla Ex Machina",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Take Caution in Using LLMs as Human Surrogates: Scylla Ex Machina"
                },
                "summary": "Recent studies suggest large language models (LLMs) can exhibit human-like\nreasoning, aligning with human behavior in economic experiments, surveys, and\npolitical discourse. This has led many to propose that LLMs can be used as\nsurrogates for humans in social science research. However, LLMs differ\nfundamentally from humans, relying on probabilistic patterns, absent the\nembodied experiences or survival objectives that shape human cognition. We\nassess the reasoning depth of LLMs using the 11-20 money request game. Almost\nall advanced approaches fail to replicate human behavior distributions across\nmany models, except in one case involving fine-tuning using a substantial\namount of human behavior data. Causes of failure are diverse, relating to input\nlanguage, roles, and safeguarding. These results caution against using LLMs to\nstudy human behaviors or as human surrogates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies suggest large language models (LLMs) can exhibit human-like\nreasoning, aligning with human behavior in economic experiments, surveys, and\npolitical discourse. This has led many to propose that LLMs can be used as\nsurrogates for humans in social science research. However, LLMs differ\nfundamentally from humans, relying on probabilistic patterns, absent the\nembodied experiences or survival objectives that shape human cognition. We\nassess the reasoning depth of LLMs using the 11-20 money request game. Almost\nall advanced approaches fail to replicate human behavior distributions across\nmany models, except in one case involving fine-tuning using a substantial\namount of human behavior data. Causes of failure are diverse, relating to input\nlanguage, roles, and safeguarding. These results caution against using LLMs to\nstudy human behaviors or as human surrogates."
                },
                "authors": [
                    {
                        "name": "Yuan Gao"
                    },
                    {
                        "name": "Dokyun Lee"
                    },
                    {
                        "name": "Gordon Burtch"
                    },
                    {
                        "name": "Sina Fazelpour"
                    }
                ],
                "author_detail": {
                    "name": "Sina Fazelpour"
                },
                "author": "Sina Fazelpour",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19589v1",
                "updated": "2024-10-25T14:31:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    31,
                    34,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T14:31:34Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    31,
                    34,
                    4,
                    299,
                    0
                ],
                "title": "Energy-Efficiency Architectural Enhancements for Sensing-Enabled Mobile\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Efficiency Architectural Enhancements for Sensing-Enabled Mobile\n  Networks"
                },
                "summary": "Sensing will be a key technology in 6G networks, enabling a plethora of new\nsensing-enabled use cases. Some of the use cases relate to deployments over a\nwide physical area that needs to be sensed by multiple sensing sources at\ndifferent locations. The efficient management of the sensing resources is\npivotal for sustainable sensing-enabled mobile network designs. In this paper,\nwe provide an example of such use case, and show the energy consumption due to\nsensing has potential to scale to prohibitive levels. We then propose\narchitectural enhancements to solve this problem, and discuss energy saving and\nenergy efficient strategies in sensing, that can only be properly quantified\nand applied with the proposed architectural enhancements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensing will be a key technology in 6G networks, enabling a plethora of new\nsensing-enabled use cases. Some of the use cases relate to deployments over a\nwide physical area that needs to be sensed by multiple sensing sources at\ndifferent locations. The efficient management of the sensing resources is\npivotal for sustainable sensing-enabled mobile network designs. In this paper,\nwe provide an example of such use case, and show the energy consumption due to\nsensing has potential to scale to prohibitive levels. We then propose\narchitectural enhancements to solve this problem, and discuss energy saving and\nenergy efficient strategies in sensing, that can only be properly quantified\nand applied with the proposed architectural enhancements."
                },
                "authors": [
                    {
                        "name": "Filipe Conceicao"
                    },
                    {
                        "name": "Filipe B. Teixeira"
                    },
                    {
                        "name": "Luis M. Pessoa"
                    },
                    {
                        "name": "Sebastian Robitzsch"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Robitzsch"
                },
                "author": "Sebastian Robitzsch",
                "arxiv_comment": "6 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19586v1",
                "updated": "2024-10-25T14:28:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    28,
                    20,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T14:28:20Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    28,
                    20,
                    4,
                    299,
                    0
                ],
                "title": "Diverse Sign Language Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diverse Sign Language Translation"
                },
                "summary": "Like spoken languages, a single sign language expression could correspond to\nmultiple valid textual interpretations. Hence, learning a rigid one-to-one\nmapping for sign language translation (SLT) models might be inadequate,\nparticularly in the case of limited data. In this work, we introduce a Diverse\nSign Language Translation (DivSLT) task, aiming to generate diverse yet\naccurate translations for sign language videos. Firstly, we employ large\nlanguage models (LLM) to generate multiple references for the widely-used\nCSL-Daily and PHOENIX14T SLT datasets. Here, native speakers are only invited\nto touch up inaccurate references, thus significantly improving the annotation\nefficiency. Secondly, we provide a benchmark model to spur research in this\ntask. Specifically, we investigate multi-reference training strategies to\nenable our DivSLT model to achieve diverse translations. Then, to enhance\ntranslation accuracy, we employ the max-reward-driven reinforcement learning\nobjective that maximizes the reward of the translated result. Additionally, we\nutilize multiple metrics to assess the accuracy, diversity, and semantic\nprecision of the DivSLT task. Experimental results on the enriched datasets\ndemonstrate that our DivSLT method achieves not only better translation\nperformance but also diverse translation results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Like spoken languages, a single sign language expression could correspond to\nmultiple valid textual interpretations. Hence, learning a rigid one-to-one\nmapping for sign language translation (SLT) models might be inadequate,\nparticularly in the case of limited data. In this work, we introduce a Diverse\nSign Language Translation (DivSLT) task, aiming to generate diverse yet\naccurate translations for sign language videos. Firstly, we employ large\nlanguage models (LLM) to generate multiple references for the widely-used\nCSL-Daily and PHOENIX14T SLT datasets. Here, native speakers are only invited\nto touch up inaccurate references, thus significantly improving the annotation\nefficiency. Secondly, we provide a benchmark model to spur research in this\ntask. Specifically, we investigate multi-reference training strategies to\nenable our DivSLT model to achieve diverse translations. Then, to enhance\ntranslation accuracy, we employ the max-reward-driven reinforcement learning\nobjective that maximizes the reward of the translated result. Additionally, we\nutilize multiple metrics to assess the accuracy, diversity, and semantic\nprecision of the DivSLT task. Experimental results on the enriched datasets\ndemonstrate that our DivSLT method achieves not only better translation\nperformance but also diverse translation results."
                },
                "authors": [
                    {
                        "name": "Xin Shen"
                    },
                    {
                        "name": "Lei Shen"
                    },
                    {
                        "name": "Shaozu Yuan"
                    },
                    {
                        "name": "Heming Du"
                    },
                    {
                        "name": "Haiyang Sun"
                    },
                    {
                        "name": "Xin Yu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Yu"
                },
                "author": "Xin Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09450v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09450v2",
                "updated": "2024-10-25T14:27:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    27,
                    23,
                    4,
                    299,
                    0
                ],
                "published": "2024-07-12T17:34:03Z",
                "published_parsed": [
                    2024,
                    7,
                    12,
                    17,
                    34,
                    3,
                    4,
                    194,
                    0
                ],
                "title": "Human-like Episodic Memory for Infinite Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-like Episodic Memory for Infinite Context LLMs"
                },
                "summary": "Large language models (LLMs) have shown remarkable capabilities, but still\nstruggle with processing extensive contexts, limiting their ability to maintain\ncoherence and accuracy over long sequences. In contrast, the human brain excels\nat organising and retrieving episodic experiences across vast temporal scales,\nspanning a lifetime. In this work, we introduce EM-LLM, a novel approach that\nintegrates key aspects of human episodic memory and event cognition into LLMs\nwith no fine-tuning, enabling them to handle practically infinite context\nlengths while maintaining computational efficiency. EM-LLM organises sequences\nof tokens into coherent episodic events using a combination of Bayesian\nsurprise and graph-theoretic boundary refinement in an online fashion. When\nneeded, these events are retrieved through a two-stage memory process,\ncombining similarity-based and temporally contiguous retrieval for efficient\nand human-like access to relevant information. Experiments on the LongBench and\nInfiniteBench benchmarks demonstrate EM-LLM's superior performance,\nconsistently outperforming the state-of-the-art retrieval model InfLLM across\nvarious baseline LLMs. In addition, EM-LLM outperforms its popular counterpart,\nRAG, in a wide range of tasks, while requiring similar resources. Notably,\nEM-LLM's performance even surpasses full-context models in most tasks, while\nsuccessfully performing retrieval across 10 million tokens - a scale\ncomputationally infeasible for such models. Finally, our analysis reveals\nstrong correlations between EM-LLM's event segmentation and human-perceived\nevents, suggesting a bridge between this artificial system and its biological\ncounterpart, thereby offering a novel computational framework for exploring\nhuman memory mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable capabilities, but still\nstruggle with processing extensive contexts, limiting their ability to maintain\ncoherence and accuracy over long sequences. In contrast, the human brain excels\nat organising and retrieving episodic experiences across vast temporal scales,\nspanning a lifetime. In this work, we introduce EM-LLM, a novel approach that\nintegrates key aspects of human episodic memory and event cognition into LLMs\nwith no fine-tuning, enabling them to handle practically infinite context\nlengths while maintaining computational efficiency. EM-LLM organises sequences\nof tokens into coherent episodic events using a combination of Bayesian\nsurprise and graph-theoretic boundary refinement in an online fashion. When\nneeded, these events are retrieved through a two-stage memory process,\ncombining similarity-based and temporally contiguous retrieval for efficient\nand human-like access to relevant information. Experiments on the LongBench and\nInfiniteBench benchmarks demonstrate EM-LLM's superior performance,\nconsistently outperforming the state-of-the-art retrieval model InfLLM across\nvarious baseline LLMs. In addition, EM-LLM outperforms its popular counterpart,\nRAG, in a wide range of tasks, while requiring similar resources. Notably,\nEM-LLM's performance even surpasses full-context models in most tasks, while\nsuccessfully performing retrieval across 10 million tokens - a scale\ncomputationally infeasible for such models. Finally, our analysis reveals\nstrong correlations between EM-LLM's event segmentation and human-perceived\nevents, suggesting a bridge between this artificial system and its biological\ncounterpart, thereby offering a novel computational framework for exploring\nhuman memory mechanisms."
                },
                "authors": [
                    {
                        "name": "Zafeirios Fountas"
                    },
                    {
                        "name": "Martin A Benfeghoul"
                    },
                    {
                        "name": "Adnan Oomerjee"
                    },
                    {
                        "name": "Fenia Christopoulou"
                    },
                    {
                        "name": "Gerasimos Lampouras"
                    },
                    {
                        "name": "Haitham Bou-Ammar"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09450v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09450v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.00953v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.00953v4",
                "updated": "2024-10-25T14:20:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    20,
                    32,
                    4,
                    299,
                    0
                ],
                "published": "2024-03-01T20:06:39Z",
                "published_parsed": [
                    2024,
                    3,
                    1,
                    20,
                    6,
                    39,
                    4,
                    61,
                    0
                ],
                "title": "AutoRD: An Automatic and End-to-End System for Rare Disease Knowledge\n  Graph Construction Based on Ontologies-enhanced Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoRD: An Automatic and End-to-End System for Rare Disease Knowledge\n  Graph Construction Based on Ontologies-enhanced Large Language Models"
                },
                "summary": "Rare diseases affect millions worldwide but often face limited research focus\ndue to their low prevalence. This results in prolonged diagnoses and a lack of\napproved therapies. Recent advancements in Large Language Models (LLMs) have\nshown promise in automating the extraction of medical information, offering\npotential to improve medical diagnosis and management. However, most LLMs lack\nprofessional medical knowledge, especially concerning rare diseases, and\nstruggle to handle the latest rare disease information. They also cannot\neffectively manage rare disease data and are not directly suitable for\ndiagnosis and management tasks. Our objective is to create an end-to-end system\ncalled AutoRD, which automates the extraction of information from medical texts\nabout rare diseases, focusing on entities and their relations. AutoRD\nintegrates up-to-date structured knowledge and demonstrates superior\nperformance in rare disease extraction tasks. We conduct various experiments to\nevaluate AutoRD's performance, aiming to surpass common LLMs and traditional\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rare diseases affect millions worldwide but often face limited research focus\ndue to their low prevalence. This results in prolonged diagnoses and a lack of\napproved therapies. Recent advancements in Large Language Models (LLMs) have\nshown promise in automating the extraction of medical information, offering\npotential to improve medical diagnosis and management. However, most LLMs lack\nprofessional medical knowledge, especially concerning rare diseases, and\nstruggle to handle the latest rare disease information. They also cannot\neffectively manage rare disease data and are not directly suitable for\ndiagnosis and management tasks. Our objective is to create an end-to-end system\ncalled AutoRD, which automates the extraction of information from medical texts\nabout rare diseases, focusing on entities and their relations. AutoRD\nintegrates up-to-date structured knowledge and demonstrates superior\nperformance in rare disease extraction tasks. We conduct various experiments to\nevaluate AutoRD's performance, aiming to surpass common LLMs and traditional\nmethods."
                },
                "authors": [
                    {
                        "name": "Lang Cao"
                    },
                    {
                        "name": "Jimeng Sun"
                    },
                    {
                        "name": "Adam Cross"
                    }
                ],
                "author_detail": {
                    "name": "Adam Cross"
                },
                "author": "Adam Cross",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.00953v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.00953v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18012v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18012v2",
                "updated": "2024-10-25T14:19:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    19,
                    22,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-23T16:40:38Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    40,
                    38,
                    2,
                    297,
                    0
                ],
                "title": "MiniFed : Integrating LLM-based Agentic-Workflow for Simulating FOMC\n  Meeting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniFed : Integrating LLM-based Agentic-Workflow for Simulating FOMC\n  Meeting"
                },
                "summary": "The Federal Funds rate in the United States plays a significant role in both\ndomestic and international financial markets. However, research has\npredominantly focused on the effects of adjustments to the Federal Funds rate\nrather than on the decision-making process itself. Recent advancements in large\nlanguage models(LLMs) offer a potential method for reconstructing the original\nFOMC meetings, which are responsible for setting the Federal Funds rate. In\nthis paper, we propose a five-stage FOMC meeting simulation framework, MiniFed,\nwhich employs LLM agents to simulate real-world FOMC meeting members and\noptimize the FOMC structure. This framework effectively revitalizes the FOMC\nmeeting process and facilitates projections of the Federal Funds rate.\nExperimental results demonstrate that our proposed MiniFed framework achieves\nboth high accuracy in Federal Funds rate projections and behavioral alignment\nwith the agents' real-world counterparts. Given that few studies have focused\non employing LLM agents to simulate large-scale real-world conferences, our\nwork can serve as a benchmark for future developments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Federal Funds rate in the United States plays a significant role in both\ndomestic and international financial markets. However, research has\npredominantly focused on the effects of adjustments to the Federal Funds rate\nrather than on the decision-making process itself. Recent advancements in large\nlanguage models(LLMs) offer a potential method for reconstructing the original\nFOMC meetings, which are responsible for setting the Federal Funds rate. In\nthis paper, we propose a five-stage FOMC meeting simulation framework, MiniFed,\nwhich employs LLM agents to simulate real-world FOMC meeting members and\noptimize the FOMC structure. This framework effectively revitalizes the FOMC\nmeeting process and facilitates projections of the Federal Funds rate.\nExperimental results demonstrate that our proposed MiniFed framework achieves\nboth high accuracy in Federal Funds rate projections and behavioral alignment\nwith the agents' real-world counterparts. Given that few studies have focused\non employing LLM agents to simulate large-scale real-world conferences, our\nwork can serve as a benchmark for future developments."
                },
                "authors": [
                    {
                        "name": "Sungil Seok"
                    },
                    {
                        "name": "Shuide Wen"
                    },
                    {
                        "name": "Qiyuan Yang"
                    },
                    {
                        "name": "Juan Feng"
                    },
                    {
                        "name": "Wenming Yang"
                    }
                ],
                "author_detail": {
                    "name": "Wenming Yang"
                },
                "author": "Wenming Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18012v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18012v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15999v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15999v2",
                "updated": "2024-10-25T14:17:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    17,
                    28,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-21T13:30:47Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    13,
                    30,
                    47,
                    0,
                    295,
                    0
                ],
                "title": "Steering Knowledge Selection Behaviours in LLMs via SAE-Based\n  Representation Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering Knowledge Selection Behaviours in LLMs via SAE-Based\n  Representation Engineering"
                },
                "summary": "Large language models (LLMs) can store a significant amount of factual\nknowledge in their parameters. However, their parametric knowledge may conflict\nwith the information provided in the context -- this phenomenon, known as\n\\emph{context-memory knowledge conflicts}, can lead to undesirable model\nbehaviour, such as reliance on outdated or incorrect information. Analysing the\ninternal activations of LLMs, we find that they can internally register the\nsignals of knowledge conflict at mid-layers. Such signals allow us to detect\nwhether a knowledge conflict occurs and use \\emph{inference-time} intervention\nstrategies to resolve it. In this work, we propose \\textsc{SpARE}, a\n\\emph{training-free} representation engineering method that uses pre-trained\nsparse auto-encoders (SAEs) to control the knowledge selection behaviour of\nLLMs. \\textsc{SpARE} identifies the functional features that control the\nknowledge selection behaviours and applies them to edit the internal\nactivations of LLMs at inference time. Our experimental results show that\n\\textsc{SpARE} can effectively control the usage of either knowledge source to\nresolve knowledge conflict in open-domain question-answering tasks, surpassing\nexisting representation engineering methods ($+10\\%$) as well as contrastive\ndecoding methods ($+15\\%$).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can store a significant amount of factual\nknowledge in their parameters. However, their parametric knowledge may conflict\nwith the information provided in the context -- this phenomenon, known as\n\\emph{context-memory knowledge conflicts}, can lead to undesirable model\nbehaviour, such as reliance on outdated or incorrect information. Analysing the\ninternal activations of LLMs, we find that they can internally register the\nsignals of knowledge conflict at mid-layers. Such signals allow us to detect\nwhether a knowledge conflict occurs and use \\emph{inference-time} intervention\nstrategies to resolve it. In this work, we propose \\textsc{SpARE}, a\n\\emph{training-free} representation engineering method that uses pre-trained\nsparse auto-encoders (SAEs) to control the knowledge selection behaviour of\nLLMs. \\textsc{SpARE} identifies the functional features that control the\nknowledge selection behaviours and applies them to edit the internal\nactivations of LLMs at inference time. Our experimental results show that\n\\textsc{SpARE} can effectively control the usage of either knowledge source to\nresolve knowledge conflict in open-domain question-answering tasks, surpassing\nexisting representation engineering methods ($+10\\%$) as well as contrastive\ndecoding methods ($+15\\%$)."
                },
                "authors": [
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Giwon Hong"
                    },
                    {
                        "name": "Xiaotang Du"
                    },
                    {
                        "name": "Aryo Pradipta Gema"
                    },
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Xuanli He"
                    },
                    {
                        "name": "Kam-Fai Wong"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15999v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15999v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19572v1",
                "updated": "2024-10-25T14:07:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    7,
                    53,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T14:07:53Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    7,
                    53,
                    4,
                    299,
                    0
                ],
                "title": "ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems using large language models\n(LLMs) often generate inaccurate responses due to the retrieval of irrelevant\nor loosely related information. Existing methods, which operate at the document\nlevel, fail to effectively filter out such content. We propose LLM-driven chunk\nfiltering, ChunkRAG, a framework that enhances RAG systems by evaluating and\nfiltering retrieved information at the chunk level. Our approach employs\nsemantic chunking to divide documents into coherent sections and utilizes\nLLM-based relevance scoring to assess each chunk's alignment with the user's\nquery. By filtering out less pertinent chunks before the generation phase, we\nsignificantly reduce hallucinations and improve factual accuracy. Experiments\nshow that our method outperforms existing RAG models, achieving higher accuracy\non tasks requiring precise information retrieval. This advancement enhances the\nreliability of RAG systems, making them particularly beneficial for\napplications like fact-checking and multi-hop reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems using large language models\n(LLMs) often generate inaccurate responses due to the retrieval of irrelevant\nor loosely related information. Existing methods, which operate at the document\nlevel, fail to effectively filter out such content. We propose LLM-driven chunk\nfiltering, ChunkRAG, a framework that enhances RAG systems by evaluating and\nfiltering retrieved information at the chunk level. Our approach employs\nsemantic chunking to divide documents into coherent sections and utilizes\nLLM-based relevance scoring to assess each chunk's alignment with the user's\nquery. By filtering out less pertinent chunks before the generation phase, we\nsignificantly reduce hallucinations and improve factual accuracy. Experiments\nshow that our method outperforms existing RAG models, achieving higher accuracy\non tasks requiring precise information retrieval. This advancement enhances the\nreliability of RAG systems, making them particularly beneficial for\napplications like fact-checking and multi-hop reasoning."
                },
                "authors": [
                    {
                        "name": "Ritvik Aggarwal Ishneet Sukhvinder Singh Ibrahim Allahverdiyev"
                    },
                    {
                        "name": "Muhammad Taha"
                    },
                    {
                        "name": "Aslihan Akalin"
                    },
                    {
                        "name": "Kevin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Zhu"
                },
                "author": "Kevin Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14807v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14807v4",
                "updated": "2024-10-25T13:34:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    13,
                    34,
                    14,
                    4,
                    299,
                    0
                ],
                "published": "2024-02-22T18:58:27Z",
                "published_parsed": [
                    2024,
                    2,
                    22,
                    18,
                    58,
                    27,
                    3,
                    53,
                    0
                ],
                "title": "A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit\n  Tasks in Public Health",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit\n  Tasks in Public Health"
                },
                "summary": "Restless multi-armed bandits (RMAB) have demonstrated success in optimizing\nresource allocation for large beneficiary populations in public health\nsettings. Unfortunately, RMAB models lack flexibility to adapt to evolving\npublic health policy priorities. Concurrently, Large Language Models (LLMs)\nhave emerged as adept automated planners across domains of robotic control and\nnavigation. In this paper, we propose a Decision Language Model (DLM) for\nRMABs, enabling dynamic fine-tuning of RMAB policies in public health settings\nusing human-language commands. We propose using LLMs as automated planners to\n(1) interpret human policy preference prompts, (2) propose reward functions as\ncode for a multi-agent RMAB environment, and (3) iterate on the generated\nreward functions using feedback from grounded RMAB simulations. We illustrate\nthe application of DLM in collaboration with ARMMAN, an India-based non-profit\npromoting preventative care for pregnant mothers, that currently relies on RMAB\npolicies to optimally allocate health worker calls to low-resource populations.\nWe conduct a technology demonstration in simulation using the Gemini Pro model,\nshowing DLM can dynamically shape policy outcomes using only human prompts as\ninput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Restless multi-armed bandits (RMAB) have demonstrated success in optimizing\nresource allocation for large beneficiary populations in public health\nsettings. Unfortunately, RMAB models lack flexibility to adapt to evolving\npublic health policy priorities. Concurrently, Large Language Models (LLMs)\nhave emerged as adept automated planners across domains of robotic control and\nnavigation. In this paper, we propose a Decision Language Model (DLM) for\nRMABs, enabling dynamic fine-tuning of RMAB policies in public health settings\nusing human-language commands. We propose using LLMs as automated planners to\n(1) interpret human policy preference prompts, (2) propose reward functions as\ncode for a multi-agent RMAB environment, and (3) iterate on the generated\nreward functions using feedback from grounded RMAB simulations. We illustrate\nthe application of DLM in collaboration with ARMMAN, an India-based non-profit\npromoting preventative care for pregnant mothers, that currently relies on RMAB\npolicies to optimally allocate health worker calls to low-resource populations.\nWe conduct a technology demonstration in simulation using the Gemini Pro model,\nshowing DLM can dynamically shape policy outcomes using only human prompts as\ninput."
                },
                "authors": [
                    {
                        "name": "Nikhil Behari"
                    },
                    {
                        "name": "Edwin Zhang"
                    },
                    {
                        "name": "Yunfan Zhao"
                    },
                    {
                        "name": "Aparna Taneja"
                    },
                    {
                        "name": "Dheeraj Nagaraj"
                    },
                    {
                        "name": "Milind Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Milind Tambe"
                },
                "author": "Milind Tambe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14807v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14807v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05827v2",
                "updated": "2024-10-25T13:24:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    13,
                    24,
                    58,
                    4,
                    299,
                    0
                ],
                "published": "2024-02-08T17:06:45Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    17,
                    6,
                    45,
                    3,
                    39,
                    0
                ],
                "title": "On the Robustness of Editing Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Robustness of Editing Large Language Models"
                },
                "summary": "Large language models (LLMs) have played a pivotal role in building\ncommunicative AI, yet they encounter the challenge of efficient updates. Model\nediting enables the manipulation of specific knowledge memories and the\nbehavior of language generation without retraining. However, the robustness of\nmodel editing remains an open question. This work seeks to understand the\nstrengths and limitations of editing methods, facilitating practical\napplications of communicative AI. We focus on three key research questions.\nRQ1: Can edited LLMs behave consistently resembling communicative AI in\nrealistic situations? RQ2: To what extent does the rephrasing of prompts lead\nLLMs to deviate from the edited knowledge memory? RQ3: Which knowledge features\nare correlated with the performance and robustness of editing? Our empirical\nstudies uncover a substantial disparity between existing editing methods and\nthe practical application of LLMs. On rephrased prompts that are flexible but\ncommon in realistic applications, the performance of editing experiences a\nsignificant decline. Further analysis shows that more popular knowledge is\nmemorized better, easier to recall, and more challenging to edit effectively.\nCode is publicly available at https://github.com/xbmxb/edit_analysis .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have played a pivotal role in building\ncommunicative AI, yet they encounter the challenge of efficient updates. Model\nediting enables the manipulation of specific knowledge memories and the\nbehavior of language generation without retraining. However, the robustness of\nmodel editing remains an open question. This work seeks to understand the\nstrengths and limitations of editing methods, facilitating practical\napplications of communicative AI. We focus on three key research questions.\nRQ1: Can edited LLMs behave consistently resembling communicative AI in\nrealistic situations? RQ2: To what extent does the rephrasing of prompts lead\nLLMs to deviate from the edited knowledge memory? RQ3: Which knowledge features\nare correlated with the performance and robustness of editing? Our empirical\nstudies uncover a substantial disparity between existing editing methods and\nthe practical application of LLMs. On rephrased prompts that are flexible but\ncommon in realistic applications, the performance of editing experiences a\nsignificant decline. Further analysis shows that more popular knowledge is\nmemorized better, easier to recall, and more challenging to edit effectively.\nCode is publicly available at https://github.com/xbmxb/edit_analysis ."
                },
                "authors": [
                    {
                        "name": "Xinbei Ma"
                    },
                    {
                        "name": "Tianjie Ju"
                    },
                    {
                        "name": "Jiyang Qiu"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Lifeng Liu"
                    },
                    {
                        "name": "Yulong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yulong Wang"
                },
                "author": "Yulong Wang",
                "arxiv_comment": "EMNLP2024. Code is publicly available at\n  https://github.com/xbmxb/edit_analysis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19542v1",
                "updated": "2024-10-25T13:15:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    13,
                    15,
                    17,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T13:15:17Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    13,
                    15,
                    17,
                    4,
                    299,
                    0
                ],
                "title": "Brain-like Functional Organization within Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brain-like Functional Organization within Large Language Models"
                },
                "summary": "The human brain has long inspired the pursuit of artificial intelligence\n(AI). Recently, neuroimaging studies provide compelling evidence of alignment\nbetween the computational representation of artificial neural networks (ANNs)\nand the neural responses of the human brain to stimuli, suggesting that ANNs\nmay employ brain-like information processing strategies. While such alignment\nhas been observed across sensory modalities--visual, auditory, and\nlinguistic--much of the focus has been on the behaviors of artificial neurons\n(ANs) at the population level, leaving the functional organization of\nindividual ANs that facilitates such brain-like processes largely unexplored.\nIn this study, we bridge this gap by directly coupling sub-groups of artificial\nneurons with functional brain networks (FBNs), the foundational organizational\nstructure of the human brain. Specifically, we extract representative patterns\nfrom temporal responses of ANs in large language models (LLMs), and use them as\nfixed regressors to construct voxel-wise encoding models to predict brain\nactivity recorded by functional magnetic resonance imaging (fMRI). This\nframework links the AN sub-groups to FBNs, enabling the delineation of\nbrain-like functional organization within LLMs. Our findings reveal that LLMs\n(BERT and Llama 1-3) exhibit brain-like functional architecture, with\nsub-groups of artificial neurons mirroring the organizational patterns of\nwell-established FBNs. Notably, the brain-like functional organization of LLMs\nevolves with the increased sophistication and capability, achieving an improved\nbalance between the diversity of computational behaviors and the consistency of\nfunctional specializations. This research represents the first exploration of\nbrain-like functional organization within LLMs, offering novel insights to\ninform the development of artificial general intelligence (AGI) with human\nbrain principles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The human brain has long inspired the pursuit of artificial intelligence\n(AI). Recently, neuroimaging studies provide compelling evidence of alignment\nbetween the computational representation of artificial neural networks (ANNs)\nand the neural responses of the human brain to stimuli, suggesting that ANNs\nmay employ brain-like information processing strategies. While such alignment\nhas been observed across sensory modalities--visual, auditory, and\nlinguistic--much of the focus has been on the behaviors of artificial neurons\n(ANs) at the population level, leaving the functional organization of\nindividual ANs that facilitates such brain-like processes largely unexplored.\nIn this study, we bridge this gap by directly coupling sub-groups of artificial\nneurons with functional brain networks (FBNs), the foundational organizational\nstructure of the human brain. Specifically, we extract representative patterns\nfrom temporal responses of ANs in large language models (LLMs), and use them as\nfixed regressors to construct voxel-wise encoding models to predict brain\nactivity recorded by functional magnetic resonance imaging (fMRI). This\nframework links the AN sub-groups to FBNs, enabling the delineation of\nbrain-like functional organization within LLMs. Our findings reveal that LLMs\n(BERT and Llama 1-3) exhibit brain-like functional architecture, with\nsub-groups of artificial neurons mirroring the organizational patterns of\nwell-established FBNs. Notably, the brain-like functional organization of LLMs\nevolves with the increased sophistication and capability, achieving an improved\nbalance between the diversity of computational behaviors and the consistency of\nfunctional specializations. This research represents the first exploration of\nbrain-like functional organization within LLMs, offering novel insights to\ninform the development of artificial general intelligence (AGI) with human\nbrain principles."
                },
                "authors": [
                    {
                        "name": "H. Sun"
                    },
                    {
                        "name": "L. Zhao"
                    },
                    {
                        "name": "Z. Wu"
                    },
                    {
                        "name": "X. Gao"
                    },
                    {
                        "name": "Y. Hu"
                    },
                    {
                        "name": "M. Zuo"
                    },
                    {
                        "name": "W. Zhang"
                    },
                    {
                        "name": "J. Han"
                    },
                    {
                        "name": "T. Liu"
                    },
                    {
                        "name": "X. Hu"
                    }
                ],
                "author_detail": {
                    "name": "X. Hu"
                },
                "author": "X. Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17519v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17519v2",
                "updated": "2024-10-25T13:14:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    13,
                    14,
                    25,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-23T02:51:33Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    2,
                    51,
                    33,
                    2,
                    297,
                    0
                ],
                "title": "Large Language Models Still Exhibit Bias in Long Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Still Exhibit Bias in Long Text"
                },
                "summary": "Existing fairness benchmarks for large language models (LLMs) primarily focus\non simple tasks, such as multiple-choice questions, overlooking biases that may\narise in more complex scenarios like long-text generation. To address this gap,\nwe introduce the Long Text Fairness Test (LTF-TEST), a framework that evaluates\nbiases in LLMs through essay-style prompts. LTF-TEST covers 14 topics and 10\ndemographic axes, including gender and race, resulting in 11,948 samples. By\nassessing both model responses and the reasoning behind them, LTF-TEST uncovers\nsubtle biases that are difficult to detect in simple responses. In our\nevaluation of five recent LLMs, including GPT-4o and LLaMa3, we identify two\nkey patterns of bias. First, these models frequently favor certain demographic\ngroups in their responses. Second, they show excessive sensitivity toward\ntraditionally disadvantaged groups, often providing overly protective responses\nwhile neglecting others. To mitigate these biases, we propose FT-REGARD, a\nfinetuning approach that pairs biased prompts with neutral responses. FT-REGARD\nreduces gender bias by 34.6% and improves performance by 1.4 percentage points\non the BBQ benchmark, offering a promising approach to addressing biases in\nlong-text generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing fairness benchmarks for large language models (LLMs) primarily focus\non simple tasks, such as multiple-choice questions, overlooking biases that may\narise in more complex scenarios like long-text generation. To address this gap,\nwe introduce the Long Text Fairness Test (LTF-TEST), a framework that evaluates\nbiases in LLMs through essay-style prompts. LTF-TEST covers 14 topics and 10\ndemographic axes, including gender and race, resulting in 11,948 samples. By\nassessing both model responses and the reasoning behind them, LTF-TEST uncovers\nsubtle biases that are difficult to detect in simple responses. In our\nevaluation of five recent LLMs, including GPT-4o and LLaMa3, we identify two\nkey patterns of bias. First, these models frequently favor certain demographic\ngroups in their responses. Second, they show excessive sensitivity toward\ntraditionally disadvantaged groups, often providing overly protective responses\nwhile neglecting others. To mitigate these biases, we propose FT-REGARD, a\nfinetuning approach that pairs biased prompts with neutral responses. FT-REGARD\nreduces gender bias by 34.6% and improves performance by 1.4 percentage points\non the BBQ benchmark, offering a promising approach to addressing biases in\nlong-text generation tasks."
                },
                "authors": [
                    {
                        "name": "Wonje Jeung"
                    },
                    {
                        "name": "Dongjae Jeon"
                    },
                    {
                        "name": "Ashkan Yousefpour"
                    },
                    {
                        "name": "Jonghyun Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jonghyun Choi"
                },
                "author": "Jonghyun Choi",
                "arxiv_comment": "22 page, 38 figures, Neurips (SoLaR Workshop)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17519v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17519v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.16210v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.16210v4",
                "updated": "2024-10-25T12:51:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    12,
                    51,
                    37,
                    4,
                    299,
                    0
                ],
                "published": "2023-10-24T21:57:59Z",
                "published_parsed": [
                    2023,
                    10,
                    24,
                    21,
                    57,
                    59,
                    1,
                    297,
                    0
                ],
                "title": "Semantic Segmentation in Satellite Hyperspectral Imagery by Deep\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Segmentation in Satellite Hyperspectral Imagery by Deep\n  Learning"
                },
                "summary": "Satellites are increasingly adopting on-board AI to optimize operations and\nincrease autonomy through in-orbit inference. The use of Deep Learning (DL)\nmodels for segmentation in hyperspectral imagery offers advantages for remote\nsensing applications. In this work, we train and test 20 models for multi-class\nsegmentation in hyperspectral imagery, selected for their potential in future\nspace deployment. These models include 1D and 2D Convolutional Neural Networks\n(CNNs) and the latest vision transformers (ViTs). We propose a lightweight\n1D-CNN model, 1D-Justo-LiuNet, which outperforms state-of-the-art models in the\nhypespectral domain. 1D-Justo-LiuNet exceeds the performance of 2D-CNN UNets\nand outperforms Apple's lightweight vision transformers designed for mobile\ninference. 1D-Justo-LiuNet achieves the highest accuracy (0.93) with the\nsmallest model size (4,563 parameters) among all tested models, while\nmaintaining fast inference. Unlike 2D-CNNs and ViTs, which encode both spectral\nand spatial information, 1D-Justo-LiuNet focuses solely on the rich spectral\nfeatures in hyperspectral data, benefitting from the high-dimensional feature\nspace. Our findings are validated across various satellite datasets, with the\nHYPSO-1 mission serving as the primary case study for sea, land, and cloud\nsegmentation. We further confirm our conclusions through generalization tests\non other hyperspectral missions, such as NASA's EO-1. Based on its superior\nperformance and compact size, we conclude that 1D-Justo-LiuNet is highly\nsuitable for in-orbit deployment, providing an effective solution for\noptimizing and automating satellite operations at edge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satellites are increasingly adopting on-board AI to optimize operations and\nincrease autonomy through in-orbit inference. The use of Deep Learning (DL)\nmodels for segmentation in hyperspectral imagery offers advantages for remote\nsensing applications. In this work, we train and test 20 models for multi-class\nsegmentation in hyperspectral imagery, selected for their potential in future\nspace deployment. These models include 1D and 2D Convolutional Neural Networks\n(CNNs) and the latest vision transformers (ViTs). We propose a lightweight\n1D-CNN model, 1D-Justo-LiuNet, which outperforms state-of-the-art models in the\nhypespectral domain. 1D-Justo-LiuNet exceeds the performance of 2D-CNN UNets\nand outperforms Apple's lightweight vision transformers designed for mobile\ninference. 1D-Justo-LiuNet achieves the highest accuracy (0.93) with the\nsmallest model size (4,563 parameters) among all tested models, while\nmaintaining fast inference. Unlike 2D-CNNs and ViTs, which encode both spectral\nand spatial information, 1D-Justo-LiuNet focuses solely on the rich spectral\nfeatures in hyperspectral data, benefitting from the high-dimensional feature\nspace. Our findings are validated across various satellite datasets, with the\nHYPSO-1 mission serving as the primary case study for sea, land, and cloud\nsegmentation. We further confirm our conclusions through generalization tests\non other hyperspectral missions, such as NASA's EO-1. Based on its superior\nperformance and compact size, we conclude that 1D-Justo-LiuNet is highly\nsuitable for in-orbit deployment, providing an effective solution for\noptimizing and automating satellite operations at edge."
                },
                "authors": [
                    {
                        "name": "Jon Alvarez Justo"
                    },
                    {
                        "name": "Alexandru Ghita"
                    },
                    {
                        "name": "Daniel Kovac"
                    },
                    {
                        "name": "Joseph L. Garrett"
                    },
                    {
                        "name": "Mariana-Iuliana Georgescu"
                    },
                    {
                        "name": "Jesus Gonzalez-Llorente"
                    },
                    {
                        "name": "Radu Tudor Ionescu"
                    },
                    {
                        "name": "Tor Arne Johansen"
                    }
                ],
                "author_detail": {
                    "name": "Tor Arne Johansen"
                },
                "author": "Tor Arne Johansen",
                "arxiv_comment": "Remote Sensing, Satellite Hyperspectral Imagery, Segmentation, Deep\n  Learning, 1D-CNNs, 2D-CNNs, ViTs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.16210v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.16210v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19517v1",
                "updated": "2024-10-25T12:42:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    12,
                    42,
                    7,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T12:42:07Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    12,
                    42,
                    7,
                    4,
                    299,
                    0
                ],
                "title": "Detection of Human and Machine-Authored Fake News in Urdu",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detection of Human and Machine-Authored Fake News in Urdu"
                },
                "summary": "The rise of social media has amplified the spread of fake news, now further\ncomplicated by large language models (LLMs) like ChatGPT, which ease the\ngeneration of highly convincing, error-free misinformation, making it\nincreasingly challenging for the public to discern truth from falsehood.\nTraditional fake news detection methods relying on linguistic cues also becomes\nless effective. Moreover, current detectors primarily focus on binary\nclassification and English texts, often overlooking the distinction between\nmachine-generated true vs. fake news and the detection in low-resource\nlanguages. To this end, we updated detection schema to include\nmachine-generated news with focus on the Urdu language. We further propose a\nhierarchical detection strategy to improve the accuracy and robustness.\nExperiments show its effectiveness across four datasets in various settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of social media has amplified the spread of fake news, now further\ncomplicated by large language models (LLMs) like ChatGPT, which ease the\ngeneration of highly convincing, error-free misinformation, making it\nincreasingly challenging for the public to discern truth from falsehood.\nTraditional fake news detection methods relying on linguistic cues also becomes\nless effective. Moreover, current detectors primarily focus on binary\nclassification and English texts, often overlooking the distinction between\nmachine-generated true vs. fake news and the detection in low-resource\nlanguages. To this end, we updated detection schema to include\nmachine-generated news with focus on the Urdu language. We further propose a\nhierarchical detection strategy to improve the accuracy and robustness.\nExperiments show its effectiveness across four datasets in various settings."
                },
                "authors": [
                    {
                        "name": "Muhammad Zain Ali"
                    },
                    {
                        "name": "Yuxia Wang"
                    },
                    {
                        "name": "Bernhard Pfahringer"
                    },
                    {
                        "name": "Tony Smith"
                    }
                ],
                "author_detail": {
                    "name": "Tony Smith"
                },
                "author": "Tony Smith",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07498v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07498v2",
                "updated": "2024-10-25T12:31:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    12,
                    31,
                    28,
                    4,
                    299,
                    0
                ],
                "published": "2024-05-13T06:33:12Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    6,
                    33,
                    12,
                    0,
                    134,
                    0
                ],
                "title": "Wafer-Scale Integration of Freestanding Photonic Devices with Color\n  Centers in Silicon Carbide",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wafer-Scale Integration of Freestanding Photonic Devices with Color\n  Centers in Silicon Carbide"
                },
                "summary": "Color center platforms have been at the forefront of quantum nanophotonics\nfor applications in quantum networking, computing, and sensing. However,\nlarge-scale deployment of this technology has been stifled by a lack of ability\nto integrate photonic devices at scale while maintaining the properties of\nquantum emitters. We address this challenge in silicon carbide which has both\ncommercially available wafer-scale substrates and is a host to color centers\nwith desirable optical and spin properties. Using ion beam etching at an angle,\nwe develop a 5-inch wafer process for the fabrication of triangular\ncross-section photonic devices in bulk 4H-SiC. The developed process has a\nvariability in etch rate and etch angle of 5.4% and 2.9%, respectively.\nFurthermore, the integrated color centers maintain their optical properties\nafter the etch, thus achieving the nanofabrication goal of wafer-scale\nnanofabrication in quantum-grade silicon carbide.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Color center platforms have been at the forefront of quantum nanophotonics\nfor applications in quantum networking, computing, and sensing. However,\nlarge-scale deployment of this technology has been stifled by a lack of ability\nto integrate photonic devices at scale while maintaining the properties of\nquantum emitters. We address this challenge in silicon carbide which has both\ncommercially available wafer-scale substrates and is a host to color centers\nwith desirable optical and spin properties. Using ion beam etching at an angle,\nwe develop a 5-inch wafer process for the fabrication of triangular\ncross-section photonic devices in bulk 4H-SiC. The developed process has a\nvariability in etch rate and etch angle of 5.4% and 2.9%, respectively.\nFurthermore, the integrated color centers maintain their optical properties\nafter the etch, thus achieving the nanofabrication goal of wafer-scale\nnanofabrication in quantum-grade silicon carbide."
                },
                "authors": [
                    {
                        "name": "Sridhar Majety"
                    },
                    {
                        "name": "Victoria A. Norman"
                    },
                    {
                        "name": "Pranta Saha"
                    },
                    {
                        "name": "Alex H. Rubin"
                    },
                    {
                        "name": "Scott Dhuey"
                    },
                    {
                        "name": "Marina Radulaski"
                    }
                ],
                "author_detail": {
                    "name": "Marina Radulaski"
                },
                "author": "Marina Radulaski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07498v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07498v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08815v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08815v2",
                "updated": "2024-10-25T12:18:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    12,
                    18,
                    37,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-11T13:52:44Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    13,
                    52,
                    44,
                    4,
                    285,
                    0
                ],
                "title": "StructRAG: Boosting Knowledge Intensive Reasoning of LLMs via\n  Inference-time Hybrid Information Structurization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StructRAG: Boosting Knowledge Intensive Reasoning of LLMs via\n  Inference-time Hybrid Information Structurization"
                },
                "summary": "Retrieval-augmented generation (RAG) is a key means to effectively enhance\nlarge language models (LLMs) in many knowledge-based tasks. However, existing\nRAG methods struggle with knowledge-intensive reasoning tasks, because useful\ninformation required to these tasks are badly scattered. This characteristic\nmakes it difficult for existing RAG methods to accurately identify key\ninformation and perform global reasoning with such noisy augmentation. In this\npaper, motivated by the cognitive theories that humans convert raw information\ninto various structured knowledge when tackling knowledge-intensive reasoning,\nwe proposes a new framework, StructRAG, which can identify the optimal\nstructure type for the task at hand, reconstruct original documents into this\nstructured format, and infer answers based on the resulting structure.\nExtensive experiments across various knowledge-intensive tasks show that\nStructRAG achieves state-of-the-art performance, particularly excelling in\nchallenging scenarios, demonstrating its potential as an effective solution for\nenhancing LLMs in complex real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) is a key means to effectively enhance\nlarge language models (LLMs) in many knowledge-based tasks. However, existing\nRAG methods struggle with knowledge-intensive reasoning tasks, because useful\ninformation required to these tasks are badly scattered. This characteristic\nmakes it difficult for existing RAG methods to accurately identify key\ninformation and perform global reasoning with such noisy augmentation. In this\npaper, motivated by the cognitive theories that humans convert raw information\ninto various structured knowledge when tackling knowledge-intensive reasoning,\nwe proposes a new framework, StructRAG, which can identify the optimal\nstructure type for the task at hand, reconstruct original documents into this\nstructured format, and infer answers based on the resulting structure.\nExtensive experiments across various knowledge-intensive tasks show that\nStructRAG achieves state-of-the-art performance, particularly excelling in\nchallenging scenarios, demonstrating its potential as an effective solution for\nenhancing LLMs in complex real-world applications."
                },
                "authors": [
                    {
                        "name": "Zhuoqun Li"
                    },
                    {
                        "name": "Xuanang Chen"
                    },
                    {
                        "name": "Haiyang Yu"
                    },
                    {
                        "name": "Hongyu Lin"
                    },
                    {
                        "name": "Yaojie Lu"
                    },
                    {
                        "name": "Qiaoyu Tang"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Xianpei Han"
                    },
                    {
                        "name": "Le Sun"
                    },
                    {
                        "name": "Yongbin Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongbin Li"
                },
                "author": "Yongbin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08815v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08815v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19503v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19503v1",
                "updated": "2024-10-25T12:10:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    12,
                    10,
                    49,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T12:10:49Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    12,
                    10,
                    49,
                    4,
                    299,
                    0
                ],
                "title": "SWITCH: Studying with Teacher for Knowledge Distillation of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWITCH: Studying with Teacher for Knowledge Distillation of Large\n  Language Models"
                },
                "summary": "Despite the success of Large Language Models (LLMs), they still face\nchallenges related to high inference costs and memory requirements. To address\nthese issues, Knowledge Distillation (KD) has emerged as a popular method for\nmodel compression, with student-generated outputs (SGOs) being particularly\nnotable for reducing the mismatch between training and inference. However, SGOs\noften produce noisy and biased sequences, which can lead to misguidance from\nthe teacher model, especially in long sequences. To mitigate these challenges,\nwe propose SWITCH (Studying WIth TeaCHer for Knowledge Distillation), a novel\napproach that strategically incorporates the teacher model during the student's\nsequence generation. SWITCH identifies discrepancies between the token\nprobabilities of the teacher and student models, allowing the teacher to\nintervene selectively, particularly in long sequences that are more prone to\nteacher misguidance. Extensive experimental results across three model families\nand five instruction-following datasets show that SWITCH surpasses traditional\nKD methods, particularly excelling in the generation of long sequential data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the success of Large Language Models (LLMs), they still face\nchallenges related to high inference costs and memory requirements. To address\nthese issues, Knowledge Distillation (KD) has emerged as a popular method for\nmodel compression, with student-generated outputs (SGOs) being particularly\nnotable for reducing the mismatch between training and inference. However, SGOs\noften produce noisy and biased sequences, which can lead to misguidance from\nthe teacher model, especially in long sequences. To mitigate these challenges,\nwe propose SWITCH (Studying WIth TeaCHer for Knowledge Distillation), a novel\napproach that strategically incorporates the teacher model during the student's\nsequence generation. SWITCH identifies discrepancies between the token\nprobabilities of the teacher and student models, allowing the teacher to\nintervene selectively, particularly in long sequences that are more prone to\nteacher misguidance. Extensive experimental results across three model families\nand five instruction-following datasets show that SWITCH surpasses traditional\nKD methods, particularly excelling in the generation of long sequential data."
                },
                "authors": [
                    {
                        "name": "Jahyun Koo"
                    },
                    {
                        "name": "Yerin Hwang"
                    },
                    {
                        "name": "Yongil Kim"
                    },
                    {
                        "name": "Taegwan Kang"
                    },
                    {
                        "name": "Hyunkyung Bae"
                    },
                    {
                        "name": "Kyomin Jung"
                    }
                ],
                "author_detail": {
                    "name": "Kyomin Jung"
                },
                "author": "Kyomin Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19503v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19503v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19499v1",
                "updated": "2024-10-25T11:58:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    11,
                    58,
                    12,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T11:58:12Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    11,
                    58,
                    12,
                    4,
                    299,
                    0
                ],
                "title": "Introducing MAPO: Momentum-Aided Gradient Descent Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introducing MAPO: Momentum-Aided Gradient Descent Prompt Optimization"
                },
                "summary": "Momentum-Aided Prompt Optimization (MAPO) enhances the efficiency and\nefficacy of prompt optimization for Large Language Models (LLMs). Building on\nProTeGi, MAPO uses positive natural language \"gradients\" and a momentum-based\nextension to refine prompts effectively. By tracking gradient history, MAPO\navoids local minima and oscillations. It also utilizes beam search and an Upper\nConfidence Bound (UCB) algorithm for balanced candidate expansion and\nselection. Benchmark testing shows that MAPO achieves faster convergence time\nwith fewer API calls and higher F1 scores than ProTeGi, proving it as a robust\nand scalable solution for automated prompt engineering in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Momentum-Aided Prompt Optimization (MAPO) enhances the efficiency and\nefficacy of prompt optimization for Large Language Models (LLMs). Building on\nProTeGi, MAPO uses positive natural language \"gradients\" and a momentum-based\nextension to refine prompts effectively. By tracking gradient history, MAPO\navoids local minima and oscillations. It also utilizes beam search and an Upper\nConfidence Bound (UCB) algorithm for balanced candidate expansion and\nselection. Benchmark testing shows that MAPO achieves faster convergence time\nwith fewer API calls and higher F1 scores than ProTeGi, proving it as a robust\nand scalable solution for automated prompt engineering in LLMs."
                },
                "authors": [
                    {
                        "name": "Anthony Cui"
                    },
                    {
                        "name": "Pranav Nandyalam"
                    },
                    {
                        "name": "Kevin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Zhu"
                },
                "author": "Kevin Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19494v1",
                "updated": "2024-10-25T11:51:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    11,
                    51,
                    37,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T11:51:37Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    11,
                    51,
                    37,
                    4,
                    299,
                    0
                ],
                "title": "Graph Linearization Methods for Reasoning on Graphs with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Linearization Methods for Reasoning on Graphs with Large Language\n  Models"
                },
                "summary": "Large language models have evolved to process multiple modalities beyond\ntext, such as images and audio, which motivates us to explore how to\neffectively leverage them for graph machine learning tasks. The key question,\ntherefore, is how to transform graphs into linear sequences of tokens, a\nprocess we term graph linearization, so that LLMs can handle graphs naturally.\nWe consider that graphs should be linearized meaningfully to reflect certain\nproperties of natural language text, such as local dependency and global\nalignment, in order to ease contemporary LLMs, trained on trillions of textual\ntokens, better understand graphs. To achieve this, we developed several graph\nlinearization methods based on graph centrality, degeneracy, and node\nrelabeling schemes. We then investigated their effect on LLM performance in\ngraph reasoning tasks. Experimental results on synthetic graphs demonstrate the\neffectiveness of our methods compared to random linearization baselines. Our\nwork introduces novel graph representations suitable for LLMs, contributing to\nthe potential integration of graph machine learning with the trend of\nmulti-modal processing using a unified transformer model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have evolved to process multiple modalities beyond\ntext, such as images and audio, which motivates us to explore how to\neffectively leverage them for graph machine learning tasks. The key question,\ntherefore, is how to transform graphs into linear sequences of tokens, a\nprocess we term graph linearization, so that LLMs can handle graphs naturally.\nWe consider that graphs should be linearized meaningfully to reflect certain\nproperties of natural language text, such as local dependency and global\nalignment, in order to ease contemporary LLMs, trained on trillions of textual\ntokens, better understand graphs. To achieve this, we developed several graph\nlinearization methods based on graph centrality, degeneracy, and node\nrelabeling schemes. We then investigated their effect on LLM performance in\ngraph reasoning tasks. Experimental results on synthetic graphs demonstrate the\neffectiveness of our methods compared to random linearization baselines. Our\nwork introduces novel graph representations suitable for LLMs, contributing to\nthe potential integration of graph machine learning with the trend of\nmulti-modal processing using a unified transformer model."
                },
                "authors": [
                    {
                        "name": "Christos Xypolopoulos"
                    },
                    {
                        "name": "Guokan Shang"
                    },
                    {
                        "name": "Xiao Fei"
                    },
                    {
                        "name": "Giannis Nikolentzos"
                    },
                    {
                        "name": "Hadi Abdine"
                    },
                    {
                        "name": "Iakovos Evdaimon"
                    },
                    {
                        "name": "Michail Chatzianastasis"
                    },
                    {
                        "name": "Giorgos Stamou"
                    },
                    {
                        "name": "Michalis Vazirgiannis"
                    }
                ],
                "author_detail": {
                    "name": "Michalis Vazirgiannis"
                },
                "author": "Michalis Vazirgiannis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19485v1",
                "updated": "2024-10-25T11:41:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    11,
                    41,
                    27,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T11:41:27Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    11,
                    41,
                    27,
                    4,
                    299,
                    0
                ],
                "title": "A Debate-Driven Experiment on LLM Hallucinations and Accuracy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Debate-Driven Experiment on LLM Hallucinations and Accuracy"
                },
                "summary": "Large language models (LLMs) have achieved a degree of success in generating\ncoherent and contextually relevant text, yet they remain prone to a significant\nchallenge known as hallucination: producing information that is not\nsubstantiated by the input or external knowledge. Previous efforts to mitigate\nhallucinations have focused on techniques such as fine-tuning models on\nhigh-quality datasets, incorporating fact-checking mechanisms, and developing\nadversarial training methods. While these approaches have shown some promise,\nthey often address the issue at the level of individual model outputs, leaving\nunexplored the effects of inter-model interactions on hallucination. This study\ninvestigates the phenomenon of hallucination in LLMs through a novel\nexperimental framework where multiple instances of GPT-4o-Mini models engage in\na debate-like interaction prompted with questions from the TruthfulQA dataset.\nOne model is deliberately instructed to generate plausible but false answers\nwhile the other models are asked to respond truthfully. The experiment is\ndesigned to assess whether the introduction of misinformation by one model can\nchallenge the truthful majority to better justify their reasoning, improving\nperformance on the TruthfulQA benchmark. The findings suggest that inter-model\ninteractions can offer valuable insights into improving the accuracy and\nrobustness of LLM outputs, complementing existing mitigation strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved a degree of success in generating\ncoherent and contextually relevant text, yet they remain prone to a significant\nchallenge known as hallucination: producing information that is not\nsubstantiated by the input or external knowledge. Previous efforts to mitigate\nhallucinations have focused on techniques such as fine-tuning models on\nhigh-quality datasets, incorporating fact-checking mechanisms, and developing\nadversarial training methods. While these approaches have shown some promise,\nthey often address the issue at the level of individual model outputs, leaving\nunexplored the effects of inter-model interactions on hallucination. This study\ninvestigates the phenomenon of hallucination in LLMs through a novel\nexperimental framework where multiple instances of GPT-4o-Mini models engage in\na debate-like interaction prompted with questions from the TruthfulQA dataset.\nOne model is deliberately instructed to generate plausible but false answers\nwhile the other models are asked to respond truthfully. The experiment is\ndesigned to assess whether the introduction of misinformation by one model can\nchallenge the truthful majority to better justify their reasoning, improving\nperformance on the TruthfulQA benchmark. The findings suggest that inter-model\ninteractions can offer valuable insights into improving the accuracy and\nrobustness of LLM outputs, complementing existing mitigation strategies."
                },
                "authors": [
                    {
                        "name": "Ray Li"
                    },
                    {
                        "name": "Tanishka Bagade"
                    },
                    {
                        "name": "Kevin Martinez"
                    },
                    {
                        "name": "Flora Yasmin"
                    },
                    {
                        "name": "Grant Ayala"
                    },
                    {
                        "name": "Michael Lam"
                    },
                    {
                        "name": "Kevin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Zhu"
                },
                "author": "Kevin Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19483v1",
                "updated": "2024-10-25T11:39:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    11,
                    39,
                    55,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T11:39:55Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    11,
                    39,
                    55,
                    4,
                    299,
                    0
                ],
                "title": "Content-Aware Radiance Fields: Aligning Model Complexity with Scene\n  Intricacy Through Learned Bitwidth Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content-Aware Radiance Fields: Aligning Model Complexity with Scene\n  Intricacy Through Learned Bitwidth Quantization"
                },
                "summary": "The recent popular radiance field models, exemplified by Neural Radiance\nFields (NeRF), Instant-NGP and 3D Gaussian Splat?ting, are designed to\nrepresent 3D content by that training models for each individual scene. This\nunique characteristic of scene representation and per-scene training\ndistinguishes radiance field models from other neural models, because complex\nscenes necessitate models with higher representational capacity and vice versa.\nIn this paper, we propose content?aware radiance fields, aligning the model\ncomplexity with the scene intricacies through Adversarial Content-Aware\nQuantization (A-CAQ). Specifically, we make the bitwidth of parameters\ndifferentiable and train?able, tailored to the unique characteristics of\nspecific scenes and requirements. The proposed framework has been assessed on\nInstant-NGP, a well-known NeRF variant and evaluated using various datasets.\nExperimental results demonstrate a notable reduction in computational\ncomplexity, while preserving the requisite reconstruction and rendering\nquality, making it beneficial for practical deployment of radiance fields\nmodels. Codes are available at\nhttps://github.com/WeihangLiu2024/Content_Aware_NeRF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent popular radiance field models, exemplified by Neural Radiance\nFields (NeRF), Instant-NGP and 3D Gaussian Splat?ting, are designed to\nrepresent 3D content by that training models for each individual scene. This\nunique characteristic of scene representation and per-scene training\ndistinguishes radiance field models from other neural models, because complex\nscenes necessitate models with higher representational capacity and vice versa.\nIn this paper, we propose content?aware radiance fields, aligning the model\ncomplexity with the scene intricacies through Adversarial Content-Aware\nQuantization (A-CAQ). Specifically, we make the bitwidth of parameters\ndifferentiable and train?able, tailored to the unique characteristics of\nspecific scenes and requirements. The proposed framework has been assessed on\nInstant-NGP, a well-known NeRF variant and evaluated using various datasets.\nExperimental results demonstrate a notable reduction in computational\ncomplexity, while preserving the requisite reconstruction and rendering\nquality, making it beneficial for practical deployment of radiance fields\nmodels. Codes are available at\nhttps://github.com/WeihangLiu2024/Content_Aware_NeRF."
                },
                "authors": [
                    {
                        "name": "Weihang Liu"
                    },
                    {
                        "name": "Xue Xian Zheng"
                    },
                    {
                        "name": "Jingyi Yu"
                    },
                    {
                        "name": "Xin Lou"
                    }
                ],
                "author_detail": {
                    "name": "Xin Lou"
                },
                "author": "Xin Lou",
                "arxiv_comment": "accepted by ECCV2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19482v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19482v1",
                "updated": "2024-10-25T11:37:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    11,
                    37,
                    4,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T11:37:04Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    11,
                    37,
                    4,
                    4,
                    299,
                    0
                ],
                "title": "Measuring memorization through probabilistic discoverable extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring memorization through probabilistic discoverable extraction"
                },
                "summary": "Large language models (LLMs) are susceptible to memorizing training data,\nraising concerns due to the potential extraction of sensitive information.\nCurrent methods to measure memorization rates of LLMs, primarily discoverable\nextraction (Carlini et al., 2022), rely on single-sequence greedy sampling,\npotentially underestimating the true extent of memorization. This paper\nintroduces a probabilistic relaxation of discoverable extraction that\nquantifies the probability of extracting a target sequence within a set of\ngenerated samples, considering various sampling schemes and multiple attempts.\nThis approach addresses the limitations of reporting memorization rates through\ndiscoverable extraction by accounting for the probabilistic nature of LLMs and\nuser interaction patterns. Our experiments demonstrate that this probabilistic\nmeasure can reveal cases of higher memorization rates compared to rates found\nthrough discoverable extraction. We further investigate the impact of different\nsampling schemes on extractability, providing a more comprehensive and\nrealistic assessment of LLM memorization and its associated risks. Our\ncontributions include a new probabilistic memorization definition, empirical\nevidence of its effectiveness, and a thorough evaluation across different\nmodels, sizes, sampling schemes, and training data repetitions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are susceptible to memorizing training data,\nraising concerns due to the potential extraction of sensitive information.\nCurrent methods to measure memorization rates of LLMs, primarily discoverable\nextraction (Carlini et al., 2022), rely on single-sequence greedy sampling,\npotentially underestimating the true extent of memorization. This paper\nintroduces a probabilistic relaxation of discoverable extraction that\nquantifies the probability of extracting a target sequence within a set of\ngenerated samples, considering various sampling schemes and multiple attempts.\nThis approach addresses the limitations of reporting memorization rates through\ndiscoverable extraction by accounting for the probabilistic nature of LLMs and\nuser interaction patterns. Our experiments demonstrate that this probabilistic\nmeasure can reveal cases of higher memorization rates compared to rates found\nthrough discoverable extraction. We further investigate the impact of different\nsampling schemes on extractability, providing a more comprehensive and\nrealistic assessment of LLM memorization and its associated risks. Our\ncontributions include a new probabilistic memorization definition, empirical\nevidence of its effectiveness, and a thorough evaluation across different\nmodels, sizes, sampling schemes, and training data repetitions."
                },
                "authors": [
                    {
                        "name": "Jamie Hayes"
                    },
                    {
                        "name": "Marika Swanberg"
                    },
                    {
                        "name": "Harsh Chaudhari"
                    },
                    {
                        "name": "Itay Yona"
                    },
                    {
                        "name": "Ilia Shumailov"
                    }
                ],
                "author_detail": {
                    "name": "Ilia Shumailov"
                },
                "author": "Ilia Shumailov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19482v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19482v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19461v1",
                "updated": "2024-10-25T10:46:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    10,
                    46,
                    17,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T10:46:17Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    10,
                    46,
                    17,
                    4,
                    299,
                    0
                ],
                "title": "EDGE: Enhanced Grounded GUI Understanding with Enriched\n  Multi-Granularity Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EDGE: Enhanced Grounded GUI Understanding with Enriched\n  Multi-Granularity Synthetic Data"
                },
                "summary": "Autonomous agents operating on the graphical user interfaces (GUIs) of\nvarious applications hold immense practical value. Unlike the large language\nmodel (LLM)-based methods which rely on structured texts and customized\nbackends, the approaches using large vision-language models (LVLMs) are more\nintuitive and adaptable as they can visually perceive and directly interact\nwith screens, making them indispensable in general scenarios without text\nmetadata and tailored backends. Given the lack of high-quality training data\nfor GUI-related tasks in existing work, this paper aims to enhance the GUI\nunderstanding and interacting capabilities of LVLMs through a data-driven\napproach. We propose EDGE, a general data synthesis framework that\nautomatically generates large-scale, multi-granularity training data from\nwebpages across the Web. Evaluation results on various GUI and agent benchmarks\ndemonstrate that the model trained with the dataset generated through EDGE\nexhibits superior webpage understanding capabilities, which can then be easily\ntransferred to previously unseen desktop and mobile environments. Our approach\nsignificantly reduces the dependence on manual annotations, empowering\nresearchers to harness the vast public resources available on the Web to\nadvance their work. Our source code, the dataset and the model are available at\nhttps://anonymous.4open.science/r/EDGE-1CDB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous agents operating on the graphical user interfaces (GUIs) of\nvarious applications hold immense practical value. Unlike the large language\nmodel (LLM)-based methods which rely on structured texts and customized\nbackends, the approaches using large vision-language models (LVLMs) are more\nintuitive and adaptable as they can visually perceive and directly interact\nwith screens, making them indispensable in general scenarios without text\nmetadata and tailored backends. Given the lack of high-quality training data\nfor GUI-related tasks in existing work, this paper aims to enhance the GUI\nunderstanding and interacting capabilities of LVLMs through a data-driven\napproach. We propose EDGE, a general data synthesis framework that\nautomatically generates large-scale, multi-granularity training data from\nwebpages across the Web. Evaluation results on various GUI and agent benchmarks\ndemonstrate that the model trained with the dataset generated through EDGE\nexhibits superior webpage understanding capabilities, which can then be easily\ntransferred to previously unseen desktop and mobile environments. Our approach\nsignificantly reduces the dependence on manual annotations, empowering\nresearchers to harness the vast public resources available on the Web to\nadvance their work. Our source code, the dataset and the model are available at\nhttps://anonymous.4open.science/r/EDGE-1CDB."
                },
                "authors": [
                    {
                        "name": "Xuetian Chen"
                    },
                    {
                        "name": "Hangcheng Li"
                    },
                    {
                        "name": "Jiaqing Liang"
                    },
                    {
                        "name": "Sihang Jiang"
                    },
                    {
                        "name": "Deqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Deqing Yang"
                },
                "author": "Deqing Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19456v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19456v1",
                "updated": "2024-10-25T10:30:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    10,
                    30,
                    21,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T10:30:21Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    10,
                    30,
                    21,
                    4,
                    299,
                    0
                ],
                "title": "Computational Bottlenecks of Training Small-scale Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Bottlenecks of Training Small-scale Large Language Models"
                },
                "summary": "While large language models (LLMs) dominate the AI landscape, Small-scale\nlarge Language Models (SLMs) are gaining attention due to cost and efficiency\ndemands from consumers. However, there is limited research on the training\nbehavior and computational requirements of SLMs. In this study, we explore the\ncomputational bottlenecks of training SLMs (up to 2B parameters) by examining\nthe effects of various hyperparameters and configurations, including GPU type,\nbatch size, model size, communication protocol, attention type, and the number\nof GPUs. We assess these factors on popular cloud services using metrics such\nas loss per dollar and tokens per second. Our findings aim to support the\nbroader adoption and optimization of language model training for low-resource\nAI research institutes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) dominate the AI landscape, Small-scale\nlarge Language Models (SLMs) are gaining attention due to cost and efficiency\ndemands from consumers. However, there is limited research on the training\nbehavior and computational requirements of SLMs. In this study, we explore the\ncomputational bottlenecks of training SLMs (up to 2B parameters) by examining\nthe effects of various hyperparameters and configurations, including GPU type,\nbatch size, model size, communication protocol, attention type, and the number\nof GPUs. We assess these factors on popular cloud services using metrics such\nas loss per dollar and tokens per second. Our findings aim to support the\nbroader adoption and optimization of language model training for low-resource\nAI research institutes."
                },
                "authors": [
                    {
                        "name": "Saleh Ashkboos"
                    },
                    {
                        "name": "Iman Mirzadeh"
                    },
                    {
                        "name": "Keivan Alizadeh"
                    },
                    {
                        "name": "Mohammad Hossein Sekhavat"
                    },
                    {
                        "name": "Moin Nabi"
                    },
                    {
                        "name": "Mehrdad Farajtabar"
                    },
                    {
                        "name": "Fartash Faghri"
                    }
                ],
                "author_detail": {
                    "name": "Fartash Faghri"
                },
                "author": "Fartash Faghri",
                "arxiv_comment": "8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19456v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19453v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19453v1",
                "updated": "2024-10-25T10:28:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    10,
                    28,
                    59,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T10:28:59Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    10,
                    28,
                    59,
                    4,
                    299,
                    0
                ],
                "title": "ShifCon: Enhancing Non-Dominant Language Capabilities with a Shift-based\n  Contrastive Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShifCon: Enhancing Non-Dominant Language Capabilities with a Shift-based\n  Contrastive Framework"
                },
                "summary": "Although fine-tuning Large Language Models (LLMs) with multilingual data can\nrapidly enhance the multilingual capabilities of LLMs, they still exhibit a\nperformance gap between the dominant language (e.g., English) and non-dominant\nones due to the imbalance of training data across languages. To further enhance\nthe performance of non-dominant languages, we propose ShifCon, a Shift-based\nContrastive framework that aligns the internal forward process of other\nlanguages toward that of the dominant one. Specifically, it shifts the\nrepresentations of non-dominant languages into the dominant language subspace,\nallowing them to access relatively rich information encoded in the model\nparameters. The enriched representations are then shifted back into their\noriginal language subspace before generation. Moreover, we introduce a subspace\ndistance metric to pinpoint the optimal layer area for shifting representations\nand employ multilingual contrastive learning to further enhance the alignment\nof representations within this area. Experiments demonstrate that our ShifCon\nframework significantly enhances the performance of non-dominant languages,\nparticularly for low-resource ones. Further analysis offers extra insights to\nverify the effectiveness of ShifCon and propel future research",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although fine-tuning Large Language Models (LLMs) with multilingual data can\nrapidly enhance the multilingual capabilities of LLMs, they still exhibit a\nperformance gap between the dominant language (e.g., English) and non-dominant\nones due to the imbalance of training data across languages. To further enhance\nthe performance of non-dominant languages, we propose ShifCon, a Shift-based\nContrastive framework that aligns the internal forward process of other\nlanguages toward that of the dominant one. Specifically, it shifts the\nrepresentations of non-dominant languages into the dominant language subspace,\nallowing them to access relatively rich information encoded in the model\nparameters. The enriched representations are then shifted back into their\noriginal language subspace before generation. Moreover, we introduce a subspace\ndistance metric to pinpoint the optimal layer area for shifting representations\nand employ multilingual contrastive learning to further enhance the alignment\nof representations within this area. Experiments demonstrate that our ShifCon\nframework significantly enhances the performance of non-dominant languages,\nparticularly for low-resource ones. Further analysis offers extra insights to\nverify the effectiveness of ShifCon and propel future research"
                },
                "authors": [
                    {
                        "name": "Hengyuan Zhang"
                    },
                    {
                        "name": "Chenming Shang"
                    },
                    {
                        "name": "Sizhe Wang"
                    },
                    {
                        "name": "Dongdong Zhang"
                    },
                    {
                        "name": "Feng Yao"
                    },
                    {
                        "name": "Renliang Sun"
                    },
                    {
                        "name": "Yiyao Yu"
                    },
                    {
                        "name": "Yujiu Yang"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "23 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19453v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19453v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19451v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19451v1",
                "updated": "2024-10-25T10:24:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    10,
                    24,
                    30,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T10:24:30Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    10,
                    24,
                    30,
                    4,
                    299,
                    0
                ],
                "title": "Intelligent Understanding of Large Language Models in Traditional\n  Chinese Medicine Based on Prompt Engineering Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent Understanding of Large Language Models in Traditional\n  Chinese Medicine Based on Prompt Engineering Framework"
                },
                "summary": "This paper explores the application of prompt engineering to enhance the\nperformance of large language models (LLMs) in the domain of Traditional\nChinese Medicine (TCM). We propose TCM-Prompt, a framework that integrates\nvarious pre-trained language models (PLMs), templates, tokenization, and\nverbalization methods, allowing researchers to easily construct and fine-tune\nmodels for specific TCM-related tasks. We conducted experiments on disease\nclassification, syndrome identification, herbal medicine recommendation, and\ngeneral NLP tasks, demonstrating the effectiveness and superiority of our\napproach compared to baseline methods. Our findings suggest that prompt\nengineering is a promising technique for improving the performance of LLMs in\nspecialized domains like TCM, with potential applications in digitalization,\nmodernization, and personalized medicine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the application of prompt engineering to enhance the\nperformance of large language models (LLMs) in the domain of Traditional\nChinese Medicine (TCM). We propose TCM-Prompt, a framework that integrates\nvarious pre-trained language models (PLMs), templates, tokenization, and\nverbalization methods, allowing researchers to easily construct and fine-tune\nmodels for specific TCM-related tasks. We conducted experiments on disease\nclassification, syndrome identification, herbal medicine recommendation, and\ngeneral NLP tasks, demonstrating the effectiveness and superiority of our\napproach compared to baseline methods. Our findings suggest that prompt\nengineering is a promising technique for improving the performance of LLMs in\nspecialized domains like TCM, with potential applications in digitalization,\nmodernization, and personalized medicine."
                },
                "authors": [
                    {
                        "name": "Yirui Chen"
                    },
                    {
                        "name": "Qinyu Xiao"
                    },
                    {
                        "name": "Jia Yi"
                    },
                    {
                        "name": "Jing Chen"
                    },
                    {
                        "name": "Mengyang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengyang Wang"
                },
                "author": "Mengyang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19451v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10645v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10645v3",
                "updated": "2024-10-25T10:23:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    10,
                    23,
                    2,
                    4,
                    299,
                    0
                ],
                "published": "2024-08-20T08:36:59Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    8,
                    36,
                    59,
                    1,
                    233,
                    0
                ],
                "title": "CoRA: Collaborative Information Perception by Large Language Model's\n  Weights for Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoRA: Collaborative Information Perception by Large Language Model's\n  Weights for Recommendation"
                },
                "summary": "Involving collaborative information in Large Language Models (LLMs) is a\npromising technique for adapting LLMs for recommendation. Existing methods\nachieve this by concatenating collaborative features with text tokens into a\nunified sequence input and then fine-tuning to align these features with LLM's\ninput space. Although effective, in this work, we identify two limitations when\nadapting LLMs to recommendation tasks, which hinder the integration of general\nknowledge and collaborative information, resulting in sub-optimal\nrecommendation performance. (1) Fine-tuning LLM with recommendation data can\nundermine its inherent world knowledge and fundamental competencies, which are\ncrucial for interpreting and inferring recommendation text. (2) Incorporating\ncollaborative features into textual prompts disrupts the semantics of the\noriginal prompts, preventing LLM from generating appropriate outputs. In this\npaper, we propose a new paradigm, \\textbf{Co}llaborative \\textbf{Lo}RA (CoRA),\nwith a collaborative query generator. Rather than input space alignment, this\nmethod aligns collaborative information with LLM's parameter space,\nrepresenting them as incremental weights to update LLM's output. This way, LLM\nperceives collaborative information without altering its general knowledge and\ntext inference capabilities. Specifically, we employ a collaborative filtering\nmodel to extract user and item embeddings and inject them into a set number of\nlearnable queries. We then convert collaborative queries into collaborative\nweights with low-rank properties and merge the collaborative weights into LLM's\nweights, enabling LLM to perceive the collaborative signals and generate\npersonalized recommendations without fine-tuning or extra collaborative tokens\nin prompts. Extensive experiments confirm that CoRA effectively integrates\ncollaborative information into LLM, enhancing recommendation performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Involving collaborative information in Large Language Models (LLMs) is a\npromising technique for adapting LLMs for recommendation. Existing methods\nachieve this by concatenating collaborative features with text tokens into a\nunified sequence input and then fine-tuning to align these features with LLM's\ninput space. Although effective, in this work, we identify two limitations when\nadapting LLMs to recommendation tasks, which hinder the integration of general\nknowledge and collaborative information, resulting in sub-optimal\nrecommendation performance. (1) Fine-tuning LLM with recommendation data can\nundermine its inherent world knowledge and fundamental competencies, which are\ncrucial for interpreting and inferring recommendation text. (2) Incorporating\ncollaborative features into textual prompts disrupts the semantics of the\noriginal prompts, preventing LLM from generating appropriate outputs. In this\npaper, we propose a new paradigm, \\textbf{Co}llaborative \\textbf{Lo}RA (CoRA),\nwith a collaborative query generator. Rather than input space alignment, this\nmethod aligns collaborative information with LLM's parameter space,\nrepresenting them as incremental weights to update LLM's output. This way, LLM\nperceives collaborative information without altering its general knowledge and\ntext inference capabilities. Specifically, we employ a collaborative filtering\nmodel to extract user and item embeddings and inject them into a set number of\nlearnable queries. We then convert collaborative queries into collaborative\nweights with low-rank properties and merge the collaborative weights into LLM's\nweights, enabling LLM to perceive the collaborative signals and generate\npersonalized recommendations without fine-tuning or extra collaborative tokens\nin prompts. Extensive experiments confirm that CoRA effectively integrates\ncollaborative information into LLM, enhancing recommendation performance."
                },
                "authors": [
                    {
                        "name": "Yuting Liu"
                    },
                    {
                        "name": "Jinghao Zhang"
                    },
                    {
                        "name": "Yizhou Dang"
                    },
                    {
                        "name": "Yuliang Liang"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Guibing Guo"
                    },
                    {
                        "name": "Jianzhe Zhao"
                    },
                    {
                        "name": "Xingwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xingwei Wang"
                },
                "author": "Xingwei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10645v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10645v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19440v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19440v1",
                "updated": "2024-10-25T09:58:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    9,
                    58,
                    19,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T09:58:19Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    9,
                    58,
                    19,
                    4,
                    299,
                    0
                ],
                "title": "Strategic deployment of solar photovoltaics for achieving\n  self-sufficiency in Europe throughout the energy transition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strategic deployment of solar photovoltaics for achieving\n  self-sufficiency in Europe throughout the energy transition"
                },
                "summary": "Transition pathways for Europe to achieve carbon neutrality emphasize the\nneed for a massive deployment of solar and wind energy. Global cost\noptimization would lead to installing most of the renewable capacity in a few\nresource-rich countries, but policy decisions could prioritize other factors.\nIn this study, we focus on the effect of energy independence on Europe's energy\nsystem design. We show that self-sufficiency constraints lead to a more\nequitable distribution of costs and installed capacities across Europe.\nHowever, countries that typically depend on energy imports face cost increases\nof up to 150% to achieve complete self-sufficiency. Self-sufficiency\nparticularly favours solar photovoltaic (PV) energy, and with declining PV\nmodule prices, alternative configurations like inverter dimensioning and\nhorizontal tracking are beneficial enough to be part of the optimal solution\nfor many countries. Moreover, we found that very large solar and wind annual\ninstallation rates are required, but they seem feasible in light of recent\nhistorical trends.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transition pathways for Europe to achieve carbon neutrality emphasize the\nneed for a massive deployment of solar and wind energy. Global cost\noptimization would lead to installing most of the renewable capacity in a few\nresource-rich countries, but policy decisions could prioritize other factors.\nIn this study, we focus on the effect of energy independence on Europe's energy\nsystem design. We show that self-sufficiency constraints lead to a more\nequitable distribution of costs and installed capacities across Europe.\nHowever, countries that typically depend on energy imports face cost increases\nof up to 150% to achieve complete self-sufficiency. Self-sufficiency\nparticularly favours solar photovoltaic (PV) energy, and with declining PV\nmodule prices, alternative configurations like inverter dimensioning and\nhorizontal tracking are beneficial enough to be part of the optimal solution\nfor many countries. Moreover, we found that very large solar and wind annual\ninstallation rates are required, but they seem feasible in light of recent\nhistorical trends."
                },
                "authors": [
                    {
                        "name": "Parisa Rahdan"
                    },
                    {
                        "name": "Elisabeth Zeyen"
                    },
                    {
                        "name": "Marta Victoria"
                    }
                ],
                "author_detail": {
                    "name": "Marta Victoria"
                },
                "author": "Marta Victoria",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19440v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19437v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19437v1",
                "updated": "2024-10-25T09:56:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    9,
                    56,
                    15,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T09:56:15Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    9,
                    56,
                    15,
                    4,
                    299,
                    0
                ],
                "title": "Transductive Learning for Near-Duplicate Image Detection in Scanned\n  Photo Collections",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transductive Learning for Near-Duplicate Image Detection in Scanned\n  Photo Collections"
                },
                "summary": "This paper presents a comparative study of near-duplicate image detection\ntechniques in a real-world use case scenario, where a document management\ncompany is commissioned to manually annotate a collection of scanned\nphotographs. Detecting duplicate and near-duplicate photographs can reduce the\ntime spent on manual annotation by archivists. This real use case differs from\nlaboratory settings as the deployment dataset is available in advance, allowing\nthe use of transductive learning. We propose a transductive learning approach\nthat leverages state-of-the-art deep learning architectures such as\nconvolutional neural networks (CNNs) and Vision Transformers (ViTs). Our\napproach involves pre-training a deep neural network on a large dataset and\nthen fine-tuning the network on the unlabeled target collection with\nself-supervised learning. The results show that the proposed approach\noutperforms the baseline methods in the task of near-duplicate image detection\nin the UKBench and an in-house private dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comparative study of near-duplicate image detection\ntechniques in a real-world use case scenario, where a document management\ncompany is commissioned to manually annotate a collection of scanned\nphotographs. Detecting duplicate and near-duplicate photographs can reduce the\ntime spent on manual annotation by archivists. This real use case differs from\nlaboratory settings as the deployment dataset is available in advance, allowing\nthe use of transductive learning. We propose a transductive learning approach\nthat leverages state-of-the-art deep learning architectures such as\nconvolutional neural networks (CNNs) and Vision Transformers (ViTs). Our\napproach involves pre-training a deep neural network on a large dataset and\nthen fine-tuning the network on the unlabeled target collection with\nself-supervised learning. The results show that the proposed approach\noutperforms the baseline methods in the task of near-duplicate image detection\nin the UKBench and an in-house private dataset."
                },
                "authors": [
                    {
                        "name": "Francesc Net"
                    },
                    {
                        "name": "Marc Folia"
                    },
                    {
                        "name": "Pep Casals"
                    },
                    {
                        "name": "Lluis Gomez"
                    }
                ],
                "author_detail": {
                    "name": "Lluis Gomez"
                },
                "author": "Lluis Gomez",
                "arxiv_comment": "Published in ICDAR 2023",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19437v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19437v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01469v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01469v2",
                "updated": "2024-10-25T09:30:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    9,
                    30,
                    32,
                    4,
                    299,
                    0
                ],
                "published": "2024-02-02T14:56:48Z",
                "published_parsed": [
                    2024,
                    2,
                    2,
                    14,
                    56,
                    48,
                    4,
                    33,
                    0
                ],
                "title": "AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through\n  Process Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through\n  Process Feedback"
                },
                "summary": "The notable success of large language models (LLMs) has sparked an upsurge in\nbuilding language agents to complete various complex tasks. We present AMOR, an\nagent framework based on open-source LLMs, which reasons with external\nknowledge bases and adapts to specific domains through human supervision to the\nreasoning process. AMOR builds reasoning logic over a finite state machine\n(FSM) that solves problems through autonomous executions and transitions over\ndisentangled modules. This allows humans to provide direct feedback to the\nindividual modules, and thus naturally forms process supervision. Based on this\nreasoning and feedback framework, we develop AMOR through two-stage\nfine-tuning: warm-up and adaptation. The former fine-tunes the LLM with\nexamples automatically constructed from various public datasets, enabling AMOR\nto generalize across different knowledge environments, while the latter tailors\nAMOR to specific domains using process feedback. Extensive experiments across\nmultiple domains demonstrate the advantage of AMOR to strong baselines, thanks\nto its FSM-based reasoning and process feedback mechanism. The code and data\nare publicly available at \\url{https://github.com/JianGuanTHU/AMOR}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The notable success of large language models (LLMs) has sparked an upsurge in\nbuilding language agents to complete various complex tasks. We present AMOR, an\nagent framework based on open-source LLMs, which reasons with external\nknowledge bases and adapts to specific domains through human supervision to the\nreasoning process. AMOR builds reasoning logic over a finite state machine\n(FSM) that solves problems through autonomous executions and transitions over\ndisentangled modules. This allows humans to provide direct feedback to the\nindividual modules, and thus naturally forms process supervision. Based on this\nreasoning and feedback framework, we develop AMOR through two-stage\nfine-tuning: warm-up and adaptation. The former fine-tunes the LLM with\nexamples automatically constructed from various public datasets, enabling AMOR\nto generalize across different knowledge environments, while the latter tailors\nAMOR to specific domains using process feedback. Extensive experiments across\nmultiple domains demonstrate the advantage of AMOR to strong baselines, thanks\nto its FSM-based reasoning and process feedback mechanism. The code and data\nare publicly available at \\url{https://github.com/JianGuanTHU/AMOR}."
                },
                "authors": [
                    {
                        "name": "Jian Guan"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zujie Wen"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Hongning Wang"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01469v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01469v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19419v1",
                "updated": "2024-10-25T09:23:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    9,
                    23,
                    24,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T09:23:24Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    9,
                    23,
                    24,
                    4,
                    299,
                    0
                ],
                "title": "KAHANI: Culturally-Nuanced Visual Storytelling Pipeline for Non-Western\n  Cultures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KAHANI: Culturally-Nuanced Visual Storytelling Pipeline for Non-Western\n  Cultures"
                },
                "summary": "Large Language Models (LLMs) and Text-To-Image (T2I) models have demonstrated\nthe ability to generate compelling text and visual stories. However, their\noutputs are predominantly aligned with the sensibilities of the Global North,\noften resulting in an outsider's gaze on other cultures. As a result,\nnon-Western communities have to put extra effort into generating culturally\nspecific stories. To address this challenge, we developed a visual storytelling\npipeline called KAHANI that generates culturally grounded visual stories for\nnon-Western cultures. Our pipeline leverages off-the-shelf models GPT-4 Turbo\nand Stable Diffusion XL (SDXL). By using Chain of Thought (CoT) and T2I\nprompting techniques, we capture the cultural context from user's prompt and\ngenerate vivid descriptions of the characters and scene compositions. To\nevaluate the effectiveness of KAHANI, we conducted a comparative user study\nwith ChatGPT-4 (with DALL-E3) in which participants from different regions of\nIndia compared the cultural relevance of stories generated by the two tools.\nResults from the qualitative and quantitative analysis performed on the user\nstudy showed that KAHANI was able to capture and incorporate more Culturally\nSpecific Items (CSIs) compared to ChatGPT-4. In terms of both its cultural\ncompetence and visual story generation quality, our pipeline outperformed\nChatGPT-4 in 27 out of the 36 comparisons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and Text-To-Image (T2I) models have demonstrated\nthe ability to generate compelling text and visual stories. However, their\noutputs are predominantly aligned with the sensibilities of the Global North,\noften resulting in an outsider's gaze on other cultures. As a result,\nnon-Western communities have to put extra effort into generating culturally\nspecific stories. To address this challenge, we developed a visual storytelling\npipeline called KAHANI that generates culturally grounded visual stories for\nnon-Western cultures. Our pipeline leverages off-the-shelf models GPT-4 Turbo\nand Stable Diffusion XL (SDXL). By using Chain of Thought (CoT) and T2I\nprompting techniques, we capture the cultural context from user's prompt and\ngenerate vivid descriptions of the characters and scene compositions. To\nevaluate the effectiveness of KAHANI, we conducted a comparative user study\nwith ChatGPT-4 (with DALL-E3) in which participants from different regions of\nIndia compared the cultural relevance of stories generated by the two tools.\nResults from the qualitative and quantitative analysis performed on the user\nstudy showed that KAHANI was able to capture and incorporate more Culturally\nSpecific Items (CSIs) compared to ChatGPT-4. In terms of both its cultural\ncompetence and visual story generation quality, our pipeline outperformed\nChatGPT-4 in 27 out of the 36 comparisons."
                },
                "authors": [
                    {
                        "name": "Hamna"
                    },
                    {
                        "name": "Deepthi Sudharsan"
                    },
                    {
                        "name": "Agrima Seth"
                    },
                    {
                        "name": "Ritvik Budhiraja"
                    },
                    {
                        "name": "Deepika Khullar"
                    },
                    {
                        "name": "Vyshak Jain"
                    },
                    {
                        "name": "Kalika Bali"
                    },
                    {
                        "name": "Aditya Vashistha"
                    },
                    {
                        "name": "Sameer Segal"
                    }
                ],
                "author_detail": {
                    "name": "Sameer Segal"
                },
                "author": "Sameer Segal",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04838v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04838v2",
                "updated": "2024-10-25T09:11:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    9,
                    11,
                    41,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-07T08:53:00Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    8,
                    53,
                    0,
                    0,
                    281,
                    0
                ],
                "title": "Rationale-Aware Answer Verification by Pairwise Self-Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rationale-Aware Answer Verification by Pairwise Self-Evaluation"
                },
                "summary": "Answer verification identifies correct solutions among candidates generated\nby large language models (LLMs). Current approaches typically train verifier\nmodels by labeling solutions as correct or incorrect based solely on whether\nthe final answer matches the gold answer. However, this approach neglects any\nflawed rationale in the solution yielding the correct answer, undermining the\nverifier's ability to distinguish between sound and flawed rationales. We\nempirically show that in StrategyQA, only 19% of LLM-generated solutions with\ncorrect answers have valid rationales, thus leading to an unreliable verifier.\nFurthermore, we demonstrate that training a verifier on valid rationales\nsignificantly improves its ability to distinguish valid and flawed rationale.\nTo make a better verifier without extra human supervision, we introduce REPS\n(Rationale Enhancement through Pairwise Selection), a method for selecting\nvalid rationales from candidates by iteratively applying pairwise\nself-evaluation using the same LLM that generates the solutions. Verifiers\ntrained on solutions selected by REPS outperform those trained using\nconventional training methods on three reasoning benchmarks (ARC-Challenge,\nDROP, and StrategyQA). Our results suggest that training reliable verifiers\nrequires ensuring the validity of rationales in addition to the correctness of\nthe final answers, which would be critical for models assisting humans in\nsolving complex reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Answer verification identifies correct solutions among candidates generated\nby large language models (LLMs). Current approaches typically train verifier\nmodels by labeling solutions as correct or incorrect based solely on whether\nthe final answer matches the gold answer. However, this approach neglects any\nflawed rationale in the solution yielding the correct answer, undermining the\nverifier's ability to distinguish between sound and flawed rationales. We\nempirically show that in StrategyQA, only 19% of LLM-generated solutions with\ncorrect answers have valid rationales, thus leading to an unreliable verifier.\nFurthermore, we demonstrate that training a verifier on valid rationales\nsignificantly improves its ability to distinguish valid and flawed rationale.\nTo make a better verifier without extra human supervision, we introduce REPS\n(Rationale Enhancement through Pairwise Selection), a method for selecting\nvalid rationales from candidates by iteratively applying pairwise\nself-evaluation using the same LLM that generates the solutions. Verifiers\ntrained on solutions selected by REPS outperform those trained using\nconventional training methods on three reasoning benchmarks (ARC-Challenge,\nDROP, and StrategyQA). Our results suggest that training reliable verifiers\nrequires ensuring the validity of rationales in addition to the correctness of\nthe final answers, which would be critical for models assisting humans in\nsolving complex reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Akira Kawabata"
                    },
                    {
                        "name": "Saku Sugawara"
                    }
                ],
                "author_detail": {
                    "name": "Saku Sugawara"
                },
                "author": "Saku Sugawara",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04838v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04838v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19406v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19406v1",
                "updated": "2024-10-25T09:09:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    9,
                    9,
                    31,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T09:09:31Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    9,
                    9,
                    31,
                    4,
                    299,
                    0
                ],
                "title": "An Auditing Test To Detect Behavioral Shift in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Auditing Test To Detect Behavioral Shift in Language Models"
                },
                "summary": "As language models (LMs) approach human-level performance, a comprehensive\nunderstanding of their behavior becomes crucial. This includes evaluating\ncapabilities, biases, task performance, and alignment with societal values.\nExtensive initial evaluations, including red teaming and diverse benchmarking,\ncan establish a model's behavioral profile. However, subsequent fine-tuning or\ndeployment modifications may alter these behaviors in unintended ways. We\npresent a method for continual Behavioral Shift Auditing (BSA) in LMs. Building\non recent work in hypothesis testing, our auditing test detects behavioral\nshifts solely through model generations. Our test compares model generations\nfrom a baseline model to those of the model under scrutiny and provides\ntheoretical guarantees for change detection while controlling false positives.\nThe test features a configurable tolerance parameter that adjusts sensitivity\nto behavioral changes for different use cases. We evaluate our approach using\ntwo case studies: monitoring changes in (a) toxicity and (b) translation\nperformance. We find that the test is able to detect meaningful changes in\nbehavior distributions using just hundreds of examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As language models (LMs) approach human-level performance, a comprehensive\nunderstanding of their behavior becomes crucial. This includes evaluating\ncapabilities, biases, task performance, and alignment with societal values.\nExtensive initial evaluations, including red teaming and diverse benchmarking,\ncan establish a model's behavioral profile. However, subsequent fine-tuning or\ndeployment modifications may alter these behaviors in unintended ways. We\npresent a method for continual Behavioral Shift Auditing (BSA) in LMs. Building\non recent work in hypothesis testing, our auditing test detects behavioral\nshifts solely through model generations. Our test compares model generations\nfrom a baseline model to those of the model under scrutiny and provides\ntheoretical guarantees for change detection while controlling false positives.\nThe test features a configurable tolerance parameter that adjusts sensitivity\nto behavioral changes for different use cases. We evaluate our approach using\ntwo case studies: monitoring changes in (a) toxicity and (b) translation\nperformance. We find that the test is able to detect meaningful changes in\nbehavior distributions using just hundreds of examples."
                },
                "authors": [
                    {
                        "name": "Leo Richter"
                    },
                    {
                        "name": "Xuanli He"
                    },
                    {
                        "name": "Pasquale Minervini"
                    },
                    {
                        "name": "Matt J. Kusner"
                    }
                ],
                "author_detail": {
                    "name": "Matt J. Kusner"
                },
                "author": "Matt J. Kusner",
                "arxiv_comment": "25 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19406v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19406v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19403v1",
                "updated": "2024-10-25T09:04:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    9,
                    4,
                    50,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T09:04:50Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    9,
                    4,
                    50,
                    4,
                    299,
                    0
                ],
                "title": "Genetic Motifs as a Blueprint for Mismatch-Tolerant Neuromorphic\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genetic Motifs as a Blueprint for Mismatch-Tolerant Neuromorphic\n  Computing"
                },
                "summary": "Mixed-signal implementations of SNNs offer a promising solution to edge\ncomputing applications that require low-power and compact embedded processing\nsystems. However, device mismatch in the analog circuits of these neuromorphic\nprocessors poses a significant challenge to the deployment of robust processing\nin these systems. Here we introduce a novel architectural solution inspired by\nbiological development to address this issue. Specifically we propose to\nimplement architectures that incorporate network motifs found in developed\nbrains through a differentiable re-parameterization of weight matrices based on\ngene expression patterns and genetic rules. Thanks to the gradient descent\noptimization compatibility of the method proposed, we can apply the robustness\nof biological neural development to neuromorphic computing.\n  To validate this approach we benchmark it using the Yin-Yang classification\ndataset, and compare its performance with that of standard multilayer\nperceptrons trained with state-of-the-art hardware-aware training method. Our\nresults demonstrate that the proposed method mitigates mismatch-induced noise\nwithout requiring precise device mismatch measurements, effectively\noutperforming alternative hardware-aware techniques proposed in the literature,\nand providing a more general solution for improving the robustness of SNNs in\nneuromorphic hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed-signal implementations of SNNs offer a promising solution to edge\ncomputing applications that require low-power and compact embedded processing\nsystems. However, device mismatch in the analog circuits of these neuromorphic\nprocessors poses a significant challenge to the deployment of robust processing\nin these systems. Here we introduce a novel architectural solution inspired by\nbiological development to address this issue. Specifically we propose to\nimplement architectures that incorporate network motifs found in developed\nbrains through a differentiable re-parameterization of weight matrices based on\ngene expression patterns and genetic rules. Thanks to the gradient descent\noptimization compatibility of the method proposed, we can apply the robustness\nof biological neural development to neuromorphic computing.\n  To validate this approach we benchmark it using the Yin-Yang classification\ndataset, and compare its performance with that of standard multilayer\nperceptrons trained with state-of-the-art hardware-aware training method. Our\nresults demonstrate that the proposed method mitigates mismatch-induced noise\nwithout requiring precise device mismatch measurements, effectively\noutperforming alternative hardware-aware techniques proposed in the literature,\nand providing a more general solution for improving the robustness of SNNs in\nneuromorphic hardware."
                },
                "authors": [
                    {
                        "name": "Tommaso Boccato"
                    },
                    {
                        "name": "Dmitrii Zendrikov"
                    },
                    {
                        "name": "Nicola Toschi"
                    },
                    {
                        "name": "Giacomo Indiveri"
                    }
                ],
                "author_detail": {
                    "name": "Giacomo Indiveri"
                },
                "author": "Giacomo Indiveri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18792v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18792v2",
                "updated": "2024-10-25T09:00:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    9,
                    0,
                    7,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-24T14:47:25Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    47,
                    25,
                    3,
                    298,
                    0
                ],
                "title": "An LLM Agent for Automatic Geospatial Data Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An LLM Agent for Automatic Geospatial Data Analysis"
                },
                "summary": "Large language models (LLMs) are being used in data science code generation\ntasks, but they often struggle with complex sequential tasks, leading to\nlogical errors. Their application to geospatial data processing is particularly\nchallenging due to difficulties in incorporating complex data structures and\nspatial constraints, effectively utilizing diverse function calls, and the\ntendency to hallucinate less-used geospatial libraries. To tackle these\nproblems, we introduce GeoAgent, a new interactive framework designed to help\nLLMs handle geospatial data processing more effectively. GeoAgent pioneers the\nintegration of a code interpreter, static analysis, and Retrieval-Augmented\nGeneration (RAG) techniques within a Monte Carlo Tree Search (MCTS) algorithm,\noffering a novel approach to geospatial data processing. In addition, we\ncontribute a new benchmark specifically designed to evaluate the LLM-based\napproach in geospatial tasks. This benchmark leverages a variety of Python\nlibraries and includes both single-turn and multi-turn tasks such as data\nacquisition, data analysis, and visualization. By offering a comprehensive\nevaluation among diverse geospatial contexts, this benchmark sets a new\nstandard for developing LLM-based approaches in geospatial data analysis tasks.\nOur findings suggest that relying solely on knowledge of LLM is insufficient\nfor accurate geospatial task programming, which requires coherent multi-step\nprocesses and multiple function calls. Compared to the baseline LLMs, the\nproposed GeoAgent has demonstrated superior performance, yielding notable\nimprovements in function calls and task completion. In addition, these results\noffer valuable insights for the future development of LLM agents in automatic\ngeospatial data analysis task programming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are being used in data science code generation\ntasks, but they often struggle with complex sequential tasks, leading to\nlogical errors. Their application to geospatial data processing is particularly\nchallenging due to difficulties in incorporating complex data structures and\nspatial constraints, effectively utilizing diverse function calls, and the\ntendency to hallucinate less-used geospatial libraries. To tackle these\nproblems, we introduce GeoAgent, a new interactive framework designed to help\nLLMs handle geospatial data processing more effectively. GeoAgent pioneers the\nintegration of a code interpreter, static analysis, and Retrieval-Augmented\nGeneration (RAG) techniques within a Monte Carlo Tree Search (MCTS) algorithm,\noffering a novel approach to geospatial data processing. In addition, we\ncontribute a new benchmark specifically designed to evaluate the LLM-based\napproach in geospatial tasks. This benchmark leverages a variety of Python\nlibraries and includes both single-turn and multi-turn tasks such as data\nacquisition, data analysis, and visualization. By offering a comprehensive\nevaluation among diverse geospatial contexts, this benchmark sets a new\nstandard for developing LLM-based approaches in geospatial data analysis tasks.\nOur findings suggest that relying solely on knowledge of LLM is insufficient\nfor accurate geospatial task programming, which requires coherent multi-step\nprocesses and multiple function calls. Compared to the baseline LLMs, the\nproposed GeoAgent has demonstrated superior performance, yielding notable\nimprovements in function calls and task completion. In addition, these results\noffer valuable insights for the future development of LLM agents in automatic\ngeospatial data analysis task programming."
                },
                "authors": [
                    {
                        "name": "Yuxing Chen"
                    },
                    {
                        "name": "Weijie Wang"
                    },
                    {
                        "name": "Sylvain Lobry"
                    },
                    {
                        "name": "Camille Kurtz"
                    }
                ],
                "author_detail": {
                    "name": "Camille Kurtz"
                },
                "author": "Camille Kurtz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18792v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18792v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18153v2",
                "updated": "2024-10-25T08:47:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    8,
                    47,
                    6,
                    4,
                    299,
                    0
                ],
                "published": "2024-02-28T08:34:23Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    8,
                    34,
                    23,
                    2,
                    59,
                    0
                ],
                "title": "Diffusion-Based Neural Network Weights Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-Based Neural Network Weights Generation"
                },
                "summary": "Transfer learning has gained significant attention in recent deep learning\nresearch due to its ability to accelerate convergence and enhance performance\non new tasks. However, its success is often contingent on the similarity\nbetween source and target data, and training on numerous datasets can be\ncostly, leading to blind selection of pretrained models with limited insight\ninto their effectiveness. To address these challenges, we introduce D2NWG, a\ndiffusion-based neural network weights generation technique that efficiently\nproduces high-performing weights for transfer learning, conditioned on the\ntarget dataset. Our method extends generative hyper-representation learning to\nrecast the latent diffusion paradigm for neural network weights generation,\nlearning the weight distributions of models pretrained on various datasets.\nThis allows for automatic generation of weights that generalize well across\nboth seen and unseen tasks, outperforming state-of-the-art meta-learning\nmethods and pretrained models. Moreover, our approach is scalable to large\narchitectures such as large language models (LLMs), overcoming the limitations\nof current parameter generation techniques that rely on task-specific model\ncollections or access to original training data. By modeling the parameter\ndistribution of LLMs, D2NWG enables task-specific parameter generation without\nrequiring additional fine-tuning or large collections of model variants.\nExtensive experiments show that our method consistently enhances the\nperformance of diverse base models, regardless of their size or complexity,\npositioning it as a robust solution for scalable transfer learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transfer learning has gained significant attention in recent deep learning\nresearch due to its ability to accelerate convergence and enhance performance\non new tasks. However, its success is often contingent on the similarity\nbetween source and target data, and training on numerous datasets can be\ncostly, leading to blind selection of pretrained models with limited insight\ninto their effectiveness. To address these challenges, we introduce D2NWG, a\ndiffusion-based neural network weights generation technique that efficiently\nproduces high-performing weights for transfer learning, conditioned on the\ntarget dataset. Our method extends generative hyper-representation learning to\nrecast the latent diffusion paradigm for neural network weights generation,\nlearning the weight distributions of models pretrained on various datasets.\nThis allows for automatic generation of weights that generalize well across\nboth seen and unseen tasks, outperforming state-of-the-art meta-learning\nmethods and pretrained models. Moreover, our approach is scalable to large\narchitectures such as large language models (LLMs), overcoming the limitations\nof current parameter generation techniques that rely on task-specific model\ncollections or access to original training data. By modeling the parameter\ndistribution of LLMs, D2NWG enables task-specific parameter generation without\nrequiring additional fine-tuning or large collections of model variants.\nExtensive experiments show that our method consistently enhances the\nperformance of diverse base models, regardless of their size or complexity,\npositioning it as a robust solution for scalable transfer learning."
                },
                "authors": [
                    {
                        "name": "Bedionita Soro"
                    },
                    {
                        "name": "Bruno Andreis"
                    },
                    {
                        "name": "Hayeon Lee"
                    },
                    {
                        "name": "Wonyong Jeong"
                    },
                    {
                        "name": "Song Chong"
                    },
                    {
                        "name": "Frank Hutter"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "32 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19385v1",
                "updated": "2024-10-25T08:34:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    8,
                    34,
                    53,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T08:34:53Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    8,
                    34,
                    53,
                    4,
                    299,
                    0
                ],
                "title": "Investigating the Role of Prompting and External Tools in Hallucination\n  Rates of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating the Role of Prompting and External Tools in Hallucination\n  Rates of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are powerful computational models trained on\nextensive corpora of human-readable text, enabling them to perform\ngeneral-purpose language understanding and generation. LLMs have garnered\nsignificant attention in both industry and academia due to their exceptional\nperformance across various natural language processing (NLP) tasks. Despite\nthese successes, LLMs often produce inaccuracies, commonly referred to as\nhallucinations. Prompt engineering, the process of designing and formulating\ninstructions for LLMs to perform specific tasks, has emerged as a key approach\nto mitigating hallucinations. This paper provides a comprehensive empirical\nevaluation of different prompting strategies and frameworks aimed at reducing\nhallucinations in LLMs. Various prompting techniques are applied to a broad set\nof benchmark datasets to assess the accuracy and hallucination rate of each\nmethod. Additionally, the paper investigates the influence of tool-calling\nagents (LLMs augmented with external tools to enhance their capabilities beyond\nlanguage generation) on hallucination rates in the same benchmarks. The\nfindings demonstrate that the optimal prompting technique depends on the type\nof problem, and that simpler techniques often outperform more complex methods\nin reducing hallucinations. Furthermore, it is shown that LLM agents can\nexhibit significantly higher hallucination rates due to the added complexity of\nexternal tool usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are powerful computational models trained on\nextensive corpora of human-readable text, enabling them to perform\ngeneral-purpose language understanding and generation. LLMs have garnered\nsignificant attention in both industry and academia due to their exceptional\nperformance across various natural language processing (NLP) tasks. Despite\nthese successes, LLMs often produce inaccuracies, commonly referred to as\nhallucinations. Prompt engineering, the process of designing and formulating\ninstructions for LLMs to perform specific tasks, has emerged as a key approach\nto mitigating hallucinations. This paper provides a comprehensive empirical\nevaluation of different prompting strategies and frameworks aimed at reducing\nhallucinations in LLMs. Various prompting techniques are applied to a broad set\nof benchmark datasets to assess the accuracy and hallucination rate of each\nmethod. Additionally, the paper investigates the influence of tool-calling\nagents (LLMs augmented with external tools to enhance their capabilities beyond\nlanguage generation) on hallucination rates in the same benchmarks. The\nfindings demonstrate that the optimal prompting technique depends on the type\nof problem, and that simpler techniques often outperform more complex methods\nin reducing hallucinations. Furthermore, it is shown that LLM agents can\nexhibit significantly higher hallucination rates due to the added complexity of\nexternal tool usage."
                },
                "authors": [
                    {
                        "name": "Liam Barkley"
                    },
                    {
                        "name": "Brink van der Merwe"
                    }
                ],
                "author_detail": {
                    "name": "Brink van der Merwe"
                },
                "author": "Brink van der Merwe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18908v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18908v2",
                "updated": "2024-10-25T08:30:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    8,
                    30,
                    21,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-24T16:59:28Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    59,
                    28,
                    3,
                    298,
                    0
                ],
                "title": "A Survey on Speech Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Speech Large Language Models"
                },
                "summary": "Large Language Models (LLMs) exhibit strong contextual understanding and\nremarkable multi-task performance. Therefore, researchers have been seeking to\nintegrate LLMs in the broad sense of Spoken Language Understanding (SLU) field.\nDifferent from the traditional method of cascading LLMs to process text\ngenerated by Automatic Speech Recognition(ASR), new efforts have focused on\ndesigning architectures centered around Audio Feature Extraction - Multimodal\nInformation Fusion - LLM Inference(Speech LLMs). This approach enables richer\naudio feature extraction while simultaneously facilitating end-to-end fusion of\naudio and text modalities, thereby achieving deeper understanding and reasoning\nfrom audio data. This paper elucidates the development of Speech LLMs, offering\nan in-depth analysis of system architectures and training strategies. Through\nextensive research and a series of targeted experiments, the paper assesses\nSpeech LLMs' advancements in Rich Audio Transcription and its potential for\nCross-task Integration within the SLU field. Additionally, it indicates key\nchallenges uncovered through experimentation, such as the Dormancy of LLMs\nunder certain conditions. The paper further delves into the training strategies\nfor Speech LLMs, proposing potential solutions based on these findings, and\noffering valuable insights and references for future research in this domain,\nas well as LLM applications in multimodal contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit strong contextual understanding and\nremarkable multi-task performance. Therefore, researchers have been seeking to\nintegrate LLMs in the broad sense of Spoken Language Understanding (SLU) field.\nDifferent from the traditional method of cascading LLMs to process text\ngenerated by Automatic Speech Recognition(ASR), new efforts have focused on\ndesigning architectures centered around Audio Feature Extraction - Multimodal\nInformation Fusion - LLM Inference(Speech LLMs). This approach enables richer\naudio feature extraction while simultaneously facilitating end-to-end fusion of\naudio and text modalities, thereby achieving deeper understanding and reasoning\nfrom audio data. This paper elucidates the development of Speech LLMs, offering\nan in-depth analysis of system architectures and training strategies. Through\nextensive research and a series of targeted experiments, the paper assesses\nSpeech LLMs' advancements in Rich Audio Transcription and its potential for\nCross-task Integration within the SLU field. Additionally, it indicates key\nchallenges uncovered through experimentation, such as the Dormancy of LLMs\nunder certain conditions. The paper further delves into the training strategies\nfor Speech LLMs, proposing potential solutions based on these findings, and\noffering valuable insights and references for future research in this domain,\nas well as LLM applications in multimodal contexts."
                },
                "authors": [
                    {
                        "name": "Jing Peng"
                    },
                    {
                        "name": "Yucheng Wang"
                    },
                    {
                        "name": "Yu Xi"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Xizhuo Zhang"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18908v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18908v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17558v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17558v2",
                "updated": "2024-10-25T08:21:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    8,
                    21,
                    59,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-23T04:55:08Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    4,
                    55,
                    8,
                    2,
                    297,
                    0
                ],
                "title": "CLR-Bench: Evaluating Large Language Models in College-level Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLR-Bench: Evaluating Large Language Models in College-level Reasoning"
                },
                "summary": "Large language models (LLMs) have demonstrated their remarkable performance\nacross various language understanding tasks. While emerging benchmarks have\nbeen proposed to evaluate LLMs in various domains such as mathematics and\ncomputer science, they merely measure the accuracy in terms of the final\nprediction on multi-choice questions. However, it remains insufficient to\nverify the essential understanding of LLMs given a chosen choice. To fill this\ngap, we present CLR-Bench to comprehensively evaluate the LLMs in complex\ncollege-level reasoning. Specifically, (i) we prioritize 16 challenging college\ndisciplines in computer science and artificial intelligence. The dataset\ncontains 5 types of questions, while each question is associated with detailed\nexplanations from experts. (ii) To quantify a fair evaluation of LLMs'\nreasoning ability, we formalize the criteria with two novel metrics.\nQ$\\rightarrow$A is utilized to measure the performance of direct answer\nprediction, and Q$\\rightarrow$AR effectively considers the joint ability to\nanswer the question and provide rationale simultaneously. Extensive experiments\nare conducted with 40 LLMs over 1,018 discipline-specific questions. The\nresults demonstrate the key insights that LLMs, even the best closed-source\nLLM, i.e., GPT-4 turbo, tend to `guess' the college-level answers. It shows a\ndramatic decrease in accuracy from 63.31% Q$\\rightarrow$A to 39.00%\nQ$\\rightarrow$AR, indicating an unsatisfactory reasoning ability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated their remarkable performance\nacross various language understanding tasks. While emerging benchmarks have\nbeen proposed to evaluate LLMs in various domains such as mathematics and\ncomputer science, they merely measure the accuracy in terms of the final\nprediction on multi-choice questions. However, it remains insufficient to\nverify the essential understanding of LLMs given a chosen choice. To fill this\ngap, we present CLR-Bench to comprehensively evaluate the LLMs in complex\ncollege-level reasoning. Specifically, (i) we prioritize 16 challenging college\ndisciplines in computer science and artificial intelligence. The dataset\ncontains 5 types of questions, while each question is associated with detailed\nexplanations from experts. (ii) To quantify a fair evaluation of LLMs'\nreasoning ability, we formalize the criteria with two novel metrics.\nQ$\\rightarrow$A is utilized to measure the performance of direct answer\nprediction, and Q$\\rightarrow$AR effectively considers the joint ability to\nanswer the question and provide rationale simultaneously. Extensive experiments\nare conducted with 40 LLMs over 1,018 discipline-specific questions. The\nresults demonstrate the key insights that LLMs, even the best closed-source\nLLM, i.e., GPT-4 turbo, tend to `guess' the college-level answers. It shows a\ndramatic decrease in accuracy from 63.31% Q$\\rightarrow$A to 39.00%\nQ$\\rightarrow$AR, indicating an unsatisfactory reasoning ability."
                },
                "authors": [
                    {
                        "name": "Junnan Dong"
                    },
                    {
                        "name": "Zijin Hong"
                    },
                    {
                        "name": "Yuanchen Bei"
                    },
                    {
                        "name": "Feiran Huang"
                    },
                    {
                        "name": "Xinrun Wang"
                    },
                    {
                        "name": "Xiao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Huang"
                },
                "author": "Xiao Huang",
                "arxiv_comment": "18 pages, 6 figures, dataset and evaluation framework will be\n  opensourced",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17558v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17558v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19370v1",
                "updated": "2024-10-25T08:16:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    8,
                    16,
                    19,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T08:16:19Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    8,
                    16,
                    19,
                    4,
                    299,
                    0
                ],
                "title": "Notes on the Mathematical Structure of GPT LLM Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Notes on the Mathematical Structure of GPT LLM Architectures"
                },
                "summary": "An exposition of the mathematics underpinning the neural network architecture\nof a GPT-3-style LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An exposition of the mathematics underpinning the neural network architecture\nof a GPT-3-style LLM."
                },
                "authors": [
                    {
                        "name": "Spencer Becker-Kahn"
                    }
                ],
                "author_detail": {
                    "name": "Spencer Becker-Kahn"
                },
                "author": "Spencer Becker-Kahn",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04834v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04834v2",
                "updated": "2024-10-25T07:41:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    41,
                    45,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-07T08:44:04Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    8,
                    44,
                    4,
                    0,
                    281,
                    0
                ],
                "title": "As Simple as Fine-tuning: LLM Alignment via Bidirectional Negative\n  Feedback Loss",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Simple as Fine-tuning: LLM Alignment via Bidirectional Negative\n  Feedback Loss"
                },
                "summary": "Direct Preference Optimization (DPO) has emerged as a more computationally\nefficient alternative to Reinforcement Learning from Human Feedback (RLHF) with\nProximal Policy Optimization (PPO), eliminating the need for reward models and\nonline sampling. Despite these benefits, DPO and its variants remain sensitive\nto hyper-parameters and prone to instability, particularly on mathematical\ndatasets. We argue that these issues arise from the unidirectional\nlikelihood-derivative negative feedback inherent in the log-likelihood loss\nfunction. To address this, we propose a novel LLM alignment loss that\nestablishes a stable Bidirectional Negative Feedback (BNF) during optimization.\nOur proposed BNF loss eliminates the need for pairwise contrastive losses and\ndoes not require any extra tunable hyper-parameters or pairwise preference\ndata, streamlining the alignment pipeline to be as simple as supervised\nfine-tuning. We conduct extensive experiments across two challenging QA\nbenchmarks and four reasoning benchmarks. The experimental results show that\nBNF achieves comparable performance to the best methods on QA benchmarks, while\nits performance decrease on the four reasoning benchmarks is significantly\nlower compared to the best methods, thus striking a better balance between\nvalue alignment and reasoning ability. In addition, we further validate the\nperformance of BNF on non-pairwise datasets, and conduct in-depth analysis of\nlog-likelihood and logit shifts across different preference optimization\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) has emerged as a more computationally\nefficient alternative to Reinforcement Learning from Human Feedback (RLHF) with\nProximal Policy Optimization (PPO), eliminating the need for reward models and\nonline sampling. Despite these benefits, DPO and its variants remain sensitive\nto hyper-parameters and prone to instability, particularly on mathematical\ndatasets. We argue that these issues arise from the unidirectional\nlikelihood-derivative negative feedback inherent in the log-likelihood loss\nfunction. To address this, we propose a novel LLM alignment loss that\nestablishes a stable Bidirectional Negative Feedback (BNF) during optimization.\nOur proposed BNF loss eliminates the need for pairwise contrastive losses and\ndoes not require any extra tunable hyper-parameters or pairwise preference\ndata, streamlining the alignment pipeline to be as simple as supervised\nfine-tuning. We conduct extensive experiments across two challenging QA\nbenchmarks and four reasoning benchmarks. The experimental results show that\nBNF achieves comparable performance to the best methods on QA benchmarks, while\nits performance decrease on the four reasoning benchmarks is significantly\nlower compared to the best methods, thus striking a better balance between\nvalue alignment and reasoning ability. In addition, we further validate the\nperformance of BNF on non-pairwise datasets, and conduct in-depth analysis of\nlog-likelihood and logit shifts across different preference optimization\nmethods."
                },
                "authors": [
                    {
                        "name": "Xin Mao"
                    },
                    {
                        "name": "Feng-Lin Li"
                    },
                    {
                        "name": "Huimin Xu"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Wang Chen"
                    },
                    {
                        "name": "Anh Tuan Luu"
                    }
                ],
                "author_detail": {
                    "name": "Anh Tuan Luu"
                },
                "author": "Anh Tuan Luu",
                "arxiv_comment": "20 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04834v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04834v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13185v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13185v4",
                "updated": "2024-10-25T07:34:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    34,
                    36,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-17T03:26:37Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    3,
                    26,
                    37,
                    3,
                    291,
                    0
                ],
                "title": "Chain of Ideas: Revolutionizing Research Via Novel Idea Development with\n  LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Ideas: Revolutionizing Research Via Novel Idea Development with\n  LLM Agents"
                },
                "summary": "Effective research ideation is a critical step for scientific research.\nHowever, the exponential increase in scientific literature makes it challenging\nfor researchers to stay current with recent advances and identify meaningful\nresearch directions. Recent developments in large language models~(LLMs)\nsuggest a promising avenue for automating the generation of novel research\nideas. However, existing methods for idea generation either trivially prompt\nLLMs or directly expose LLMs to extensive literature without indicating useful\ninformation. Inspired by the research process of human researchers, we propose\na Chain-of-Ideas~(CoI) agent, an LLM-based agent that organizes relevant\nliterature in a chain structure to effectively mirror the progressive\ndevelopment in a research domain. This organization facilitates LLMs to capture\nthe current advancements in research, thereby enhancing their ideation\ncapabilities. Furthermore, we propose Idea Arena, an evaluation protocol that\ncan comprehensively evaluate idea generation methods from different\nperspectives, aligning closely with the preferences of human researchers.\nExperimental results indicate that the CoI agent consistently outperforms other\nmethods and shows comparable quality as humans in research idea generation.\nMoreover, our CoI agent is budget-friendly, with a minimum cost of \\$0.50 to\ngenerate a candidate idea and its corresponding experimental design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective research ideation is a critical step for scientific research.\nHowever, the exponential increase in scientific literature makes it challenging\nfor researchers to stay current with recent advances and identify meaningful\nresearch directions. Recent developments in large language models~(LLMs)\nsuggest a promising avenue for automating the generation of novel research\nideas. However, existing methods for idea generation either trivially prompt\nLLMs or directly expose LLMs to extensive literature without indicating useful\ninformation. Inspired by the research process of human researchers, we propose\na Chain-of-Ideas~(CoI) agent, an LLM-based agent that organizes relevant\nliterature in a chain structure to effectively mirror the progressive\ndevelopment in a research domain. This organization facilitates LLMs to capture\nthe current advancements in research, thereby enhancing their ideation\ncapabilities. Furthermore, we propose Idea Arena, an evaluation protocol that\ncan comprehensively evaluate idea generation methods from different\nperspectives, aligning closely with the preferences of human researchers.\nExperimental results indicate that the CoI agent consistently outperforms other\nmethods and shows comparable quality as humans in research idea generation.\nMoreover, our CoI agent is budget-friendly, with a minimum cost of \\$0.50 to\ngenerate a candidate idea and its corresponding experimental design."
                },
                "authors": [
                    {
                        "name": "Long Li"
                    },
                    {
                        "name": "Weiwen Xu"
                    },
                    {
                        "name": "Jiayan Guo"
                    },
                    {
                        "name": "Ruochen Zhao"
                    },
                    {
                        "name": "Xinxuan Li"
                    },
                    {
                        "name": "Yuqian Yuan"
                    },
                    {
                        "name": "Boqiang Zhang"
                    },
                    {
                        "name": "Yuming Jiang"
                    },
                    {
                        "name": "Yifei Xin"
                    },
                    {
                        "name": "Ronghao Dang"
                    },
                    {
                        "name": "Deli Zhao"
                    },
                    {
                        "name": "Yu Rong"
                    },
                    {
                        "name": "Tian Feng"
                    },
                    {
                        "name": "Lidong Bing"
                    }
                ],
                "author_detail": {
                    "name": "Lidong Bing"
                },
                "author": "Lidong Bing",
                "arxiv_comment": "10 pages,5 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13185v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13185v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19353v1",
                "updated": "2024-10-25T07:21:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    21,
                    57,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T07:21:57Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    21,
                    57,
                    4,
                    299,
                    0
                ],
                "title": "Interleaving Text and Number Embeddings to Solve Mathemathics Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interleaving Text and Number Embeddings to Solve Mathemathics Problems"
                },
                "summary": "Integrating text and numbers effectively is a crucial step towards enhancing\nLarge Language Models (LLMs) capabilities in assisting in scientific tasks.\nWhile most current approaches rely on discrete tokenization of numbers, for\ninstance, conversion to scientific notation or base 10-decomposition, a recent\napproach proposed a continuous numerical encoding as an inductive bias. In this\npaper, we build upon this approach by introducing more expressive numerical\nembeddings. Our method addresses key shortcomings, including the elimination of\nnumerical artefacts and the ability to handle a wide range of magnitudes\nwithout clipping.\n  Our work presents two key contributions. First, we employ an MLP to assign\ndistinct directions in the embedding space to different numbers. Our second\ncontribution is the introduction of a routing layer that differentiates between\nnumerical and text embeddings. We hypothesise that this combined approach\nenables the model to distinguish between text and number distributions while\nmaintaining its capacity for arithmetic operations.\n  Using only a 45 M parameter encoder-decoder architecture our method achieves\na $R^2$=0.9988 over a wide range of magnitude ($10^{-3},10^{8}$). In addition,\nwe empirically observe a reduction of the numerical artefacts and biases\nobserved compared to the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating text and numbers effectively is a crucial step towards enhancing\nLarge Language Models (LLMs) capabilities in assisting in scientific tasks.\nWhile most current approaches rely on discrete tokenization of numbers, for\ninstance, conversion to scientific notation or base 10-decomposition, a recent\napproach proposed a continuous numerical encoding as an inductive bias. In this\npaper, we build upon this approach by introducing more expressive numerical\nembeddings. Our method addresses key shortcomings, including the elimination of\nnumerical artefacts and the ability to handle a wide range of magnitudes\nwithout clipping.\n  Our work presents two key contributions. First, we employ an MLP to assign\ndistinct directions in the embedding space to different numbers. Our second\ncontribution is the introduction of a routing layer that differentiates between\nnumerical and text embeddings. We hypothesise that this combined approach\nenables the model to distinguish between text and number distributions while\nmaintaining its capacity for arithmetic operations.\n  Using only a 45 M parameter encoder-decoder architecture our method achieves\na $R^2$=0.9988 over a wide range of magnitude ($10^{-3},10^{8}$). In addition,\nwe empirically observe a reduction of the numerical artefacts and biases\nobserved compared to the baselines."
                },
                "authors": [
                    {
                        "name": "Marvin Alberts"
                    },
                    {
                        "name": "Gianmarco Gabrieli"
                    },
                    {
                        "name": "Irina Espejo Morales"
                    }
                ],
                "author_detail": {
                    "name": "Irina Espejo Morales"
                },
                "author": "Irina Espejo Morales",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19346v1",
                "updated": "2024-10-25T07:04:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    4,
                    16,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T07:04:16Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    4,
                    16,
                    4,
                    299,
                    0
                ],
                "title": "AgentSense: Benchmarking Social Intelligence of Language Agents through\n  Interactive Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentSense: Benchmarking Social Intelligence of Language Agents through\n  Interactive Scenarios"
                },
                "summary": "Large language models (LLMs) are increasingly leveraged to empower autonomous\nagents to simulate human beings in various fields of behavioral research.\nHowever, evaluating their capacity to navigate complex social interactions\nremains a challenge. Previous studies face limitations due to insufficient\nscenario diversity, complexity, and a single-perspective focus. To this end, we\nintroduce AgentSense: Benchmarking Social Intelligence of Language Agents\nthrough Interactive Scenarios. Drawing on Dramaturgical Theory, AgentSense\nemploys a bottom-up approach to create 1,225 diverse social scenarios\nconstructed from extensive scripts. We evaluate LLM-driven agents through\nmulti-turn interactions, emphasizing both goal completion and implicit\nreasoning. We analyze goals using ERG theory and conduct comprehensive\nexperiments. Our findings highlight that LLMs struggle with goals in complex\nsocial scenarios, especially high-level growth needs, and even GPT-4o requires\nimprovement in private information reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly leveraged to empower autonomous\nagents to simulate human beings in various fields of behavioral research.\nHowever, evaluating their capacity to navigate complex social interactions\nremains a challenge. Previous studies face limitations due to insufficient\nscenario diversity, complexity, and a single-perspective focus. To this end, we\nintroduce AgentSense: Benchmarking Social Intelligence of Language Agents\nthrough Interactive Scenarios. Drawing on Dramaturgical Theory, AgentSense\nemploys a bottom-up approach to create 1,225 diverse social scenarios\nconstructed from extensive scripts. We evaluate LLM-driven agents through\nmulti-turn interactions, emphasizing both goal completion and implicit\nreasoning. We analyze goals using ERG theory and conduct comprehensive\nexperiments. Our findings highlight that LLMs struggle with goals in complex\nsocial scenarios, especially high-level growth needs, and even GPT-4o requires\nimprovement in private information reasoning."
                },
                "authors": [
                    {
                        "name": "Xinyi Mou"
                    },
                    {
                        "name": "Jingcong Liang"
                    },
                    {
                        "name": "Jiayu Lin"
                    },
                    {
                        "name": "Xinnong Zhang"
                    },
                    {
                        "name": "Xiawei Liu"
                    },
                    {
                        "name": "Shiyue Yang"
                    },
                    {
                        "name": "Rong Ye"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Haoyu Kuang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06040v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06040v2",
                "updated": "2024-10-25T06:32:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    6,
                    32,
                    9,
                    4,
                    299,
                    0
                ],
                "published": "2024-06-10T06:17:55Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    6,
                    17,
                    55,
                    0,
                    162,
                    0
                ],
                "title": "Vript: A Video Is Worth Thousands of Words",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vript: A Video Is Worth Thousands of Words"
                },
                "summary": "Advancements in multimodal learning, particularly in video understanding and\ngeneration, require high-quality video-text datasets for improved model\nperformance. Vript addresses this issue with a meticulously annotated corpus of\n12K high-resolution videos, offering detailed, dense, and script-like captions\nfor over 420K clips. Each clip has a caption of ~145 words, which is over 10x\nlonger than most video-text datasets. Unlike captions only documenting static\ncontent in previous datasets, we enhance video captioning to video scripting by\ndocumenting not just the content, but also the camera operations, which include\nthe shot types (medium shot, close-up, etc) and camera movements (panning,\ntilting, etc). By utilizing the Vript, we explore three training paradigms of\naligning more text with the video modality rather than clip-caption pairs. This\nresults in Vriptor, a top-performing video captioning model among open-source\nmodels, comparable to GPT-4V in performance. Vriptor is also a powerful model\ncapable of end-to-end generation of dense and detailed captions for long\nvideos. Moreover, we introduce Vript-Hard, a benchmark consisting of three\nvideo understanding tasks that are more challenging than existing benchmarks:\nVript-HAL is the first benchmark evaluating action and object hallucinations in\nvideo LLMs, Vript-RR combines reasoning with retrieval resolving question\nambiguity in long-video QAs, and Vript-ERO is a new task to evaluate the\ntemporal understanding of events in long videos rather than actions in short\nvideos in previous works. All code, models, and datasets are available in\nhttps://github.com/mutonix/Vript. PS: We have included more video-text datasets\n(Vript_CN & Vript_Multilingual) in the Vript series.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in multimodal learning, particularly in video understanding and\ngeneration, require high-quality video-text datasets for improved model\nperformance. Vript addresses this issue with a meticulously annotated corpus of\n12K high-resolution videos, offering detailed, dense, and script-like captions\nfor over 420K clips. Each clip has a caption of ~145 words, which is over 10x\nlonger than most video-text datasets. Unlike captions only documenting static\ncontent in previous datasets, we enhance video captioning to video scripting by\ndocumenting not just the content, but also the camera operations, which include\nthe shot types (medium shot, close-up, etc) and camera movements (panning,\ntilting, etc). By utilizing the Vript, we explore three training paradigms of\naligning more text with the video modality rather than clip-caption pairs. This\nresults in Vriptor, a top-performing video captioning model among open-source\nmodels, comparable to GPT-4V in performance. Vriptor is also a powerful model\ncapable of end-to-end generation of dense and detailed captions for long\nvideos. Moreover, we introduce Vript-Hard, a benchmark consisting of three\nvideo understanding tasks that are more challenging than existing benchmarks:\nVript-HAL is the first benchmark evaluating action and object hallucinations in\nvideo LLMs, Vript-RR combines reasoning with retrieval resolving question\nambiguity in long-video QAs, and Vript-ERO is a new task to evaluate the\ntemporal understanding of events in long videos rather than actions in short\nvideos in previous works. All code, models, and datasets are available in\nhttps://github.com/mutonix/Vript. PS: We have included more video-text datasets\n(Vript_CN & Vript_Multilingual) in the Vript series."
                },
                "authors": [
                    {
                        "name": "Dongjie Yang"
                    },
                    {
                        "name": "Suyuan Huang"
                    },
                    {
                        "name": "Chengqiang Lu"
                    },
                    {
                        "name": "Xiaodong Han"
                    },
                    {
                        "name": "Haoxin Zhang"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "Accepted by NeurIPS Dataset & Benchmark track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06040v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06040v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01436v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01436v2",
                "updated": "2024-10-25T06:20:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    6,
                    20,
                    16,
                    4,
                    299,
                    0
                ],
                "published": "2024-06-03T15:28:21Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    15,
                    28,
                    21,
                    0,
                    155,
                    0
                ],
                "title": "Editing the Mind of Giants: An In-Depth Exploration of Pitfalls of\n  Knowledge Editing in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Editing the Mind of Giants: An In-Depth Exploration of Pitfalls of\n  Knowledge Editing in Large Language Models"
                },
                "summary": "Knowledge editing is a rising technique for efficiently updating factual\nknowledge in large language models (LLMs) with minimal alteration of\nparameters. However, recent studies have identified side effects, such as\nknowledge distortion and the deterioration of general abilities, that have\nemerged after editing. Despite these findings, evaluating the pitfalls of\nknowledge editing often relies on inconsistent metrics and benchmarks, lacking\na uniform standard. In response, this survey presents a comprehensive study of\nthese side effects, providing a unified perspective on the challenges of\nknowledge editing in LLMs by conducting experiments with consistent metrics and\nbenchmarks. Additionally, we review related works and outline potential\nresearch directions to address these limitations. Our survey highlights the\nlimitations of current knowledge editing methods, emphasizing the need for a\ndeeper understanding of the inner knowledge structures of LLMs and improved\nknowledge editing methods. To foster future research, we have released the\ncomplementary materials publicly in https://github.com/MiuLab/EditLLM-Survey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge editing is a rising technique for efficiently updating factual\nknowledge in large language models (LLMs) with minimal alteration of\nparameters. However, recent studies have identified side effects, such as\nknowledge distortion and the deterioration of general abilities, that have\nemerged after editing. Despite these findings, evaluating the pitfalls of\nknowledge editing often relies on inconsistent metrics and benchmarks, lacking\na uniform standard. In response, this survey presents a comprehensive study of\nthese side effects, providing a unified perspective on the challenges of\nknowledge editing in LLMs by conducting experiments with consistent metrics and\nbenchmarks. Additionally, we review related works and outline potential\nresearch directions to address these limitations. Our survey highlights the\nlimitations of current knowledge editing methods, emphasizing the need for a\ndeeper understanding of the inner knowledge structures of LLMs and improved\nknowledge editing methods. To foster future research, we have released the\ncomplementary materials publicly in https://github.com/MiuLab/EditLLM-Survey."
                },
                "authors": [
                    {
                        "name": "Cheng-Hsun Hsueh"
                    },
                    {
                        "name": "Paul Kuo-Ming Huang"
                    },
                    {
                        "name": "Tzu-Han Lin"
                    },
                    {
                        "name": "Che-Wei Liao"
                    },
                    {
                        "name": "Hung-Chieh Fang"
                    },
                    {
                        "name": "Chao-Wei Huang"
                    },
                    {
                        "name": "Yun-Nung Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yun-Nung Chen"
                },
                "author": "Yun-Nung Chen",
                "arxiv_comment": "EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01436v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01436v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16236v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16236v2",
                "updated": "2024-10-25T06:19:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    6,
                    19,
                    13,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-21T17:41:28Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    41,
                    28,
                    0,
                    295,
                    0
                ],
                "title": "LLaVA-KD: A Framework of Distilling Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaVA-KD: A Framework of Distilling Multimodal Large Language Models"
                },
                "summary": "The success of Large Language Models (LLM) has led researchers to explore\nMultimodal Large Language Models (MLLM) for unified visual and linguistic\nunderstanding. However, the increasing model size and computational complexity\nof MLLM limit their use in resource-constrained environments. Small-scale MLLM\n(s-MLLM) aims to retain the capabilities of the large-scale model (l-MLLM)\nwhile reducing computational demands, but resulting in a significant decline in\nperformance. To address the aforementioned issues, we propose a novel LLaVA-KD\nframework to transfer knowledge from l-MLLM to s-MLLM. Specifically, we\nintroduce Multimodal Distillation (MDist) to minimize the divergence between\nthe visual-textual output distributions of l-MLLM and s-MLLM, and Relation\nDistillation (RDist) to transfer l-MLLM's ability to model correlations between\nvisual features. Additionally, we propose a three-stage training scheme to\nfully exploit the potential of s-MLLM: 1) Distilled Pre-Training to align\nvisual-textual representations, 2) Supervised Fine-Tuning to equip the model\nwith multimodal understanding, and 3) Distilled Fine-Tuning to further transfer\nl-MLLM capabilities. Our approach significantly improves performance without\naltering the small model's architecture. Extensive experiments and ablation\nstudies validate the effectiveness of each proposed component. Code will be\navailable at https://github.com/Fantasyele/LLaVA-KD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of Large Language Models (LLM) has led researchers to explore\nMultimodal Large Language Models (MLLM) for unified visual and linguistic\nunderstanding. However, the increasing model size and computational complexity\nof MLLM limit their use in resource-constrained environments. Small-scale MLLM\n(s-MLLM) aims to retain the capabilities of the large-scale model (l-MLLM)\nwhile reducing computational demands, but resulting in a significant decline in\nperformance. To address the aforementioned issues, we propose a novel LLaVA-KD\nframework to transfer knowledge from l-MLLM to s-MLLM. Specifically, we\nintroduce Multimodal Distillation (MDist) to minimize the divergence between\nthe visual-textual output distributions of l-MLLM and s-MLLM, and Relation\nDistillation (RDist) to transfer l-MLLM's ability to model correlations between\nvisual features. Additionally, we propose a three-stage training scheme to\nfully exploit the potential of s-MLLM: 1) Distilled Pre-Training to align\nvisual-textual representations, 2) Supervised Fine-Tuning to equip the model\nwith multimodal understanding, and 3) Distilled Fine-Tuning to further transfer\nl-MLLM capabilities. Our approach significantly improves performance without\naltering the small model's architecture. Extensive experiments and ablation\nstudies validate the effectiveness of each proposed component. Code will be\navailable at https://github.com/Fantasyele/LLaVA-KD."
                },
                "authors": [
                    {
                        "name": "Yuxuan Cai"
                    },
                    {
                        "name": "Jiangning Zhang"
                    },
                    {
                        "name": "Haoyang He"
                    },
                    {
                        "name": "Xinwei He"
                    },
                    {
                        "name": "Ao Tong"
                    },
                    {
                        "name": "Zhenye Gan"
                    },
                    {
                        "name": "Chengjie Wang"
                    },
                    {
                        "name": "Xiang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Bai"
                },
                "author": "Xiang Bai",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16236v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16236v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03856v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03856v3",
                "updated": "2024-10-25T06:12:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    6,
                    12,
                    49,
                    4,
                    299,
                    0
                ],
                "published": "2024-07-04T11:42:36Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    11,
                    42,
                    36,
                    3,
                    186,
                    0
                ],
                "title": "Q-Adapter: Customizing Pre-trained LLMs to New Preferences with\n  Forgetting Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-Adapter: Customizing Pre-trained LLMs to New Preferences with\n  Forgetting Mitigation"
                },
                "summary": "Large Language Models (LLMs), trained on a large amount of corpus, have\ndemonstrated remarkable abilities. However, it may not be sufficient to\ndirectly apply open-source LLMs like Llama to certain real-world scenarios,\nsince most of them are trained for \\emph{general} purposes. Thus, the demands\nfor customizing publicly available LLMs emerge, but are currently\nunder-studied. In this work, we consider customizing pre-trained LLMs with new\nhuman preferences. Specifically, the LLM should not only meet the new\npreference but also preserve its original capabilities after customization.\nDrawing inspiration from the observation that human preference can be expressed\nas a reward model, we propose to cast LLM customization as optimizing the sum\nof two reward functions, one of which (denoted as $r_1$) was used to pre-train\nthe LLM while the other (denoted as $r_2$) characterizes the new human\npreference. The obstacle here is that both reward functions are unknown, making\nthe application of modern reinforcement learning methods infeasible. Thanks to\nthe residual Q-learning framework, we can restore the customized LLM with the\npre-trained LLM and the \\emph{residual Q-function} without the reward function\n$r_1$. Moreover, we find that for a fixed pre-trained LLM, the reward function\n$r_2$ can be derived from the residual Q-function, enabling us to directly\nlearn the residual Q-function from the new human preference data upon the\nBradley-Terry model. We name our method Q-Adapter as it introduces an adapter\nmodule to approximate the residual Q-function for customizing the pre-trained\nLLM towards the new preference. Experiments based on the Llama-3.1 model on the\nDSP dataset and HH-RLHF dataset illustrate the superior effectiveness of\nQ-Adapter on both retaining existing knowledge and learning new preferences.\nCode is available at \\url{https://github.com/mansicer/Q-Adapter}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), trained on a large amount of corpus, have\ndemonstrated remarkable abilities. However, it may not be sufficient to\ndirectly apply open-source LLMs like Llama to certain real-world scenarios,\nsince most of them are trained for \\emph{general} purposes. Thus, the demands\nfor customizing publicly available LLMs emerge, but are currently\nunder-studied. In this work, we consider customizing pre-trained LLMs with new\nhuman preferences. Specifically, the LLM should not only meet the new\npreference but also preserve its original capabilities after customization.\nDrawing inspiration from the observation that human preference can be expressed\nas a reward model, we propose to cast LLM customization as optimizing the sum\nof two reward functions, one of which (denoted as $r_1$) was used to pre-train\nthe LLM while the other (denoted as $r_2$) characterizes the new human\npreference. The obstacle here is that both reward functions are unknown, making\nthe application of modern reinforcement learning methods infeasible. Thanks to\nthe residual Q-learning framework, we can restore the customized LLM with the\npre-trained LLM and the \\emph{residual Q-function} without the reward function\n$r_1$. Moreover, we find that for a fixed pre-trained LLM, the reward function\n$r_2$ can be derived from the residual Q-function, enabling us to directly\nlearn the residual Q-function from the new human preference data upon the\nBradley-Terry model. We name our method Q-Adapter as it introduces an adapter\nmodule to approximate the residual Q-function for customizing the pre-trained\nLLM towards the new preference. Experiments based on the Llama-3.1 model on the\nDSP dataset and HH-RLHF dataset illustrate the superior effectiveness of\nQ-Adapter on both retaining existing knowledge and learning new preferences.\nCode is available at \\url{https://github.com/mansicer/Q-Adapter}."
                },
                "authors": [
                    {
                        "name": "Yi-Chen Li"
                    },
                    {
                        "name": "Fuxiang Zhang"
                    },
                    {
                        "name": "Wenjie Qiu"
                    },
                    {
                        "name": "Lei Yuan"
                    },
                    {
                        "name": "Chengxing Jia"
                    },
                    {
                        "name": "Zongzhang Zhang"
                    },
                    {
                        "name": "Yang Yu"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03856v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03856v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15816v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15816v2",
                "updated": "2024-10-25T06:11:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    6,
                    11,
                    37,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-21T09:29:50Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    9,
                    29,
                    50,
                    0,
                    295,
                    0
                ],
                "title": "Software Frugality in an Accelerating World: the Case of Continuous\n  Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software Frugality in an Accelerating World: the Case of Continuous\n  Integration"
                },
                "summary": "The acceleration of software development and delivery requires rigorous\ncontinuous testing and deployment of software systems, which are being deployed\nin increasingly diverse, complex, and dynamic environments. In recent years,\nthe popularization of DevOps and integrated software forges like GitLab and\nGitHub has largely democratized Continuous Integration (CI) practices for a\ngrowing number of software. However, this trend intersects significantly with\nglobal energy consumption concerns and the growing demand for frugality in the\nInformation and Communication Technology (ICT) sector. CI pipelines typically\nrun in data centers which contribute significantly to the environmental\nfootprint of ICT, yet there is little information available regarding their\nenvironmental impact. This article aims to bridge this gap by conducting the\nfirst large-scale analysis of the energy footprint of CI pipelines implemented\nwith GitHub Actions and to provide a first overview of the energy impact of CI.\nWe collect, instrument, and reproduce 838 workflows from 396 Java repositories\nhosted on GitHub to measure their energy consumption. We observe that the\naverage unitary energy cost of a pipeline is relatively low, at 10 Wh. However,\ndue to repeated invocations of these pipelines in real settings, the aggregated\nenergy consumption cost per project is high, averaging 22 kWh. When evaluating\nCO2 emissions based on regional Wh-to-CO2 estimates, we observe that the\naverage aggregated CO2 emissions are significant, averaging 10.5 kg. To put\nthis into perspective, this is akin to the emissions produced by driving\napproximately 100 kilometers in a typical European car (110 gCO2/km). In light\nof our results, we advocate that developers should have the means to better\nanticipate and reflect on the environmental consequences of their CI choices\nwhen implementing DevOps practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The acceleration of software development and delivery requires rigorous\ncontinuous testing and deployment of software systems, which are being deployed\nin increasingly diverse, complex, and dynamic environments. In recent years,\nthe popularization of DevOps and integrated software forges like GitLab and\nGitHub has largely democratized Continuous Integration (CI) practices for a\ngrowing number of software. However, this trend intersects significantly with\nglobal energy consumption concerns and the growing demand for frugality in the\nInformation and Communication Technology (ICT) sector. CI pipelines typically\nrun in data centers which contribute significantly to the environmental\nfootprint of ICT, yet there is little information available regarding their\nenvironmental impact. This article aims to bridge this gap by conducting the\nfirst large-scale analysis of the energy footprint of CI pipelines implemented\nwith GitHub Actions and to provide a first overview of the energy impact of CI.\nWe collect, instrument, and reproduce 838 workflows from 396 Java repositories\nhosted on GitHub to measure their energy consumption. We observe that the\naverage unitary energy cost of a pipeline is relatively low, at 10 Wh. However,\ndue to repeated invocations of these pipelines in real settings, the aggregated\nenergy consumption cost per project is high, averaging 22 kWh. When evaluating\nCO2 emissions based on regional Wh-to-CO2 estimates, we observe that the\naverage aggregated CO2 emissions are significant, averaging 10.5 kg. To put\nthis into perspective, this is akin to the emissions produced by driving\napproximately 100 kilometers in a typical European car (110 gCO2/km). In light\nof our results, we advocate that developers should have the means to better\nanticipate and reflect on the environmental consequences of their CI choices\nwhen implementing DevOps practices."
                },
                "authors": [
                    {
                        "name": "Quentin Perez"
                    },
                    {
                        "name": "Romain Lefeuvre"
                    },
                    {
                        "name": "Thomas Degueule"
                    },
                    {
                        "name": "Olivier Barais"
                    },
                    {
                        "name": "Benoit Combemale"
                    }
                ],
                "author_detail": {
                    "name": "Benoit Combemale"
                },
                "author": "Benoit Combemale",
                "arxiv_comment": "This paper is currently under review by Communication of the ACM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15816v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15816v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19318v1",
                "updated": "2024-10-25T06:08:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    6,
                    8,
                    59,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T06:08:59Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    6,
                    8,
                    59,
                    4,
                    299,
                    0
                ],
                "title": "Two are better than one: Context window extension with multi-grained\n  self-injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two are better than one: Context window extension with multi-grained\n  self-injection"
                },
                "summary": "The limited context window of contemporary large language models (LLMs)\nremains a huge barrier to their broader application across various domains.\nWhile continual pre-training on long-context data is a straightforward and\neffective solution, it incurs substantial costs in terms of data acquisition\nand computational resources. To alleviate this issue, we propose SharedLLM, a\nnovel approach grounded in the design philosophy of multi-grained context\ncompression and query-aware information retrieval. SharedLLM is composed of two\nshort-context LLMs such as LLaMA-2, termed upper model and lower model. The\nlower model functions as a compressor while the upper model acts as a decoder.\nThe upper model receives compressed, multi-grained context information from the\nlower model and performs context-aware modeling on the running text.\nInformation transfer between the compressor and decoder occurs only at the\nlowest layers to refrain from long forward paths in the lower model and\nredundant cross-attention modules in the upper model. Based on this\narchitecture, we introduce a specialized tree-style data structure to\nefficiently encode, store and retrieve multi-grained contextual information for\ntext chunks. This structure, combined with a search algorithm, enables rapid\nencoding and retrieval of relevant information from various levels of the tree\nbased on the input query. This entire process, wherein the sender and receiver\nare derived from the same LLM layer, is referred to as self-injection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The limited context window of contemporary large language models (LLMs)\nremains a huge barrier to their broader application across various domains.\nWhile continual pre-training on long-context data is a straightforward and\neffective solution, it incurs substantial costs in terms of data acquisition\nand computational resources. To alleviate this issue, we propose SharedLLM, a\nnovel approach grounded in the design philosophy of multi-grained context\ncompression and query-aware information retrieval. SharedLLM is composed of two\nshort-context LLMs such as LLaMA-2, termed upper model and lower model. The\nlower model functions as a compressor while the upper model acts as a decoder.\nThe upper model receives compressed, multi-grained context information from the\nlower model and performs context-aware modeling on the running text.\nInformation transfer between the compressor and decoder occurs only at the\nlowest layers to refrain from long forward paths in the lower model and\nredundant cross-attention modules in the upper model. Based on this\narchitecture, we introduce a specialized tree-style data structure to\nefficiently encode, store and retrieve multi-grained contextual information for\ntext chunks. This structure, combined with a search algorithm, enables rapid\nencoding and retrieval of relevant information from various levels of the tree\nbased on the input query. This entire process, wherein the sender and receiver\nare derived from the same LLM layer, is referred to as self-injection."
                },
                "authors": [
                    {
                        "name": "Wei Han"
                    },
                    {
                        "name": "Pan Zhou"
                    },
                    {
                        "name": "Soujanya Poria"
                    },
                    {
                        "name": "Shuicheng Yan"
                    }
                ],
                "author_detail": {
                    "name": "Shuicheng Yan"
                },
                "author": "Shuicheng Yan",
                "arxiv_comment": "The code is available at https://github.com/Clement25/SharedLLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19317v1",
                "updated": "2024-10-25T06:06:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    6,
                    6,
                    31,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T06:06:31Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    6,
                    6,
                    31,
                    4,
                    299,
                    0
                ],
                "title": "FairMT-Bench: Benchmarking Fairness for Multi-turn Dialogue in\n  Conversational LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FairMT-Bench: Benchmarking Fairness for Multi-turn Dialogue in\n  Conversational LLMs"
                },
                "summary": "The growing use of large language model (LLM)-based chatbots has raised\nconcerns about fairness. Fairness issues in LLMs can lead to severe\nconsequences, such as bias amplification, discrimination, and harm to\nmarginalized communities. While existing fairness benchmarks mainly focus on\nsingle-turn dialogues, multi-turn scenarios, which in fact better reflect\nreal-world conversations, present greater challenges due to conversational\ncomplexity and potential bias accumulation. In this paper, we propose a\ncomprehensive fairness benchmark for LLMs in multi-turn dialogue scenarios,\n\\textbf{FairMT-Bench}. Specifically, we formulate a task taxonomy targeting LLM\nfairness capabilities across three stages: context understanding, user\ninteraction, and instruction trade-offs, with each stage comprising two tasks.\nTo ensure coverage of diverse bias types and attributes, we draw from existing\nfairness datasets and employ our template to construct a multi-turn dialogue\ndataset, \\texttt{FairMT-10K}. For evaluation, GPT-4 is applied, alongside bias\nclassifiers including Llama-Guard-3 and human validation to ensure robustness.\nExperiments and analyses on \\texttt{FairMT-10K} reveal that in multi-turn\ndialogue scenarios, current LLMs are more likely to generate biased responses,\nand there is significant variation in performance across different tasks and\nmodels. Based on this, we curate a challenging dataset, \\texttt{FairMT-1K}, and\ntest 15 current state-of-the-art (SOTA) LLMs on this dataset. The results show\nthe current state of fairness in LLMs and showcase the utility of this novel\napproach for assessing fairness in more realistic multi-turn dialogue contexts,\ncalling for future work to focus on LLM fairness improvement and the adoption\nof \\texttt{FairMT-1K} in such efforts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing use of large language model (LLM)-based chatbots has raised\nconcerns about fairness. Fairness issues in LLMs can lead to severe\nconsequences, such as bias amplification, discrimination, and harm to\nmarginalized communities. While existing fairness benchmarks mainly focus on\nsingle-turn dialogues, multi-turn scenarios, which in fact better reflect\nreal-world conversations, present greater challenges due to conversational\ncomplexity and potential bias accumulation. In this paper, we propose a\ncomprehensive fairness benchmark for LLMs in multi-turn dialogue scenarios,\n\\textbf{FairMT-Bench}. Specifically, we formulate a task taxonomy targeting LLM\nfairness capabilities across three stages: context understanding, user\ninteraction, and instruction trade-offs, with each stage comprising two tasks.\nTo ensure coverage of diverse bias types and attributes, we draw from existing\nfairness datasets and employ our template to construct a multi-turn dialogue\ndataset, \\texttt{FairMT-10K}. For evaluation, GPT-4 is applied, alongside bias\nclassifiers including Llama-Guard-3 and human validation to ensure robustness.\nExperiments and analyses on \\texttt{FairMT-10K} reveal that in multi-turn\ndialogue scenarios, current LLMs are more likely to generate biased responses,\nand there is significant variation in performance across different tasks and\nmodels. Based on this, we curate a challenging dataset, \\texttt{FairMT-1K}, and\ntest 15 current state-of-the-art (SOTA) LLMs on this dataset. The results show\nthe current state of fairness in LLMs and showcase the utility of this novel\napproach for assessing fairness in more realistic multi-turn dialogue contexts,\ncalling for future work to focus on LLM fairness improvement and the adoption\nof \\texttt{FairMT-1K} in such efforts."
                },
                "authors": [
                    {
                        "name": "Zhiting Fan"
                    },
                    {
                        "name": "Ruizhe Chen"
                    },
                    {
                        "name": "Tianxiang Hu"
                    },
                    {
                        "name": "Zuozhu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zuozhu Liu"
                },
                "author": "Zuozhu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19314v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19314v1",
                "updated": "2024-10-25T05:59:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    5,
                    59,
                    44,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T05:59:44Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    5,
                    59,
                    44,
                    4,
                    299,
                    0
                ],
                "title": "Revealing and Reducing Gender Biases in Vision and Language Assistants\n  (VLAs)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing and Reducing Gender Biases in Vision and Language Assistants\n  (VLAs)"
                },
                "summary": "Pre-trained large language models (LLMs) have been reliably integrated with\nvisual input for multimodal tasks. The widespread adoption of instruction-tuned\nimage-to-text vision-language assistants (VLAs) like LLaVA and InternVL\nnecessitates evaluating gender biases. We study gender bias in 22 popular\nopen-source VLAs with respect to personality traits, skills, and occupations.\nOur results show that VLAs replicate human biases likely present in the data,\nsuch as real-world occupational imbalances. Similarly, they tend to attribute\nmore skills and positive personality traits to women than to men, and we see a\nconsistent tendency to associate negative personality traits with men. To\neliminate the gender bias in these models, we find that finetuning-based\ndebiasing methods achieve the best tradeoff between debiasing and retaining\nperformance on downstream tasks. We argue for pre-deploying gender bias\nassessment in VLAs and motivate further development of debiasing strategies to\nensure equitable societal outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained large language models (LLMs) have been reliably integrated with\nvisual input for multimodal tasks. The widespread adoption of instruction-tuned\nimage-to-text vision-language assistants (VLAs) like LLaVA and InternVL\nnecessitates evaluating gender biases. We study gender bias in 22 popular\nopen-source VLAs with respect to personality traits, skills, and occupations.\nOur results show that VLAs replicate human biases likely present in the data,\nsuch as real-world occupational imbalances. Similarly, they tend to attribute\nmore skills and positive personality traits to women than to men, and we see a\nconsistent tendency to associate negative personality traits with men. To\neliminate the gender bias in these models, we find that finetuning-based\ndebiasing methods achieve the best tradeoff between debiasing and retaining\nperformance on downstream tasks. We argue for pre-deploying gender bias\nassessment in VLAs and motivate further development of debiasing strategies to\nensure equitable societal outcomes."
                },
                "authors": [
                    {
                        "name": "Leander Girrbach"
                    },
                    {
                        "name": "Yiran Huang"
                    },
                    {
                        "name": "Stephan Alaniz"
                    },
                    {
                        "name": "Trevor Darrell"
                    },
                    {
                        "name": "Zeynep Akata"
                    }
                ],
                "author_detail": {
                    "name": "Zeynep Akata"
                },
                "author": "Zeynep Akata",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19314v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19314v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14826v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14826v2",
                "updated": "2024-10-25T05:43:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    5,
                    43,
                    16,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-18T18:51:44Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    18,
                    51,
                    44,
                    4,
                    292,
                    0
                ],
                "title": "SPRIG: Improving Large Language Model Performance by System Prompt\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPRIG: Improving Large Language Model Performance by System Prompt\n  Optimization"
                },
                "summary": "Large Language Models (LLMs) have shown impressive capabilities in many\nscenarios, but their performance depends, in part, on the choice of prompt.\nPast research has focused on optimizing prompts specific to a task. However,\nmuch less attention has been given to optimizing the general instructions\nincluded in a prompt, known as a system prompt. To address this gap, we propose\nSPRIG, an edit-based genetic algorithm that iteratively constructs prompts from\nprespecified components to maximize the model's performance in general\nscenarios. We evaluate the performance of system prompts on a collection of 47\ndifferent types of tasks to ensure generalizability. Our study finds that a\nsingle optimized system prompt performs on par with task prompts optimized for\neach individual task. Moreover, combining system and task-level optimizations\nleads to further improvement, which showcases their complementary nature.\nExperiments also reveal that the optimized system prompts generalize\neffectively across model families, parameter sizes, and languages. This study\nprovides insights into the role of system-level instructions in maximizing LLM\npotential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive capabilities in many\nscenarios, but their performance depends, in part, on the choice of prompt.\nPast research has focused on optimizing prompts specific to a task. However,\nmuch less attention has been given to optimizing the general instructions\nincluded in a prompt, known as a system prompt. To address this gap, we propose\nSPRIG, an edit-based genetic algorithm that iteratively constructs prompts from\nprespecified components to maximize the model's performance in general\nscenarios. We evaluate the performance of system prompts on a collection of 47\ndifferent types of tasks to ensure generalizability. Our study finds that a\nsingle optimized system prompt performs on par with task prompts optimized for\neach individual task. Moreover, combining system and task-level optimizations\nleads to further improvement, which showcases their complementary nature.\nExperiments also reveal that the optimized system prompts generalize\neffectively across model families, parameter sizes, and languages. This study\nprovides insights into the role of system-level instructions in maximizing LLM\npotential."
                },
                "authors": [
                    {
                        "name": "Lechen Zhang"
                    },
                    {
                        "name": "Tolga Ergen"
                    },
                    {
                        "name": "Lajanugen Logeswaran"
                    },
                    {
                        "name": "Moontae Lee"
                    },
                    {
                        "name": "David Jurgens"
                    }
                ],
                "author_detail": {
                    "name": "David Jurgens"
                },
                "author": "David Jurgens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14826v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14826v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.09336v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.09336v5",
                "updated": "2024-10-25T04:28:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    4,
                    28,
                    6,
                    4,
                    299,
                    0
                ],
                "published": "2023-11-15T19:52:11Z",
                "published_parsed": [
                    2023,
                    11,
                    15,
                    19,
                    52,
                    11,
                    2,
                    319,
                    0
                ],
                "title": "LLMRefine: Pinpointing and Refining Large Language Models via\n  Fine-Grained Actionable Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMRefine: Pinpointing and Refining Large Language Models via\n  Fine-Grained Actionable Feedback"
                },
                "summary": "Recent large language models (LLM) are leveraging human feedback to improve\ntheir generation quality. However, human feedback is costly to obtain,\nespecially during inference. In this work, we propose LLMRefine, an inference\ntime optimization method to refine LLM's output. The core idea is to use a\nlearned fine-grained feedback model to pinpoint defects and guide LLM to refine\nthem iteratively. Using original LLM as a proposal of edits, LLMRefine searches\nfor defect-less text via simulated annealing, trading off the exploration and\nexploitation. We conduct experiments on three text generation tasks, including\nmachine translation, long-form question answering (QA), and topical\nsummarization. LLMRefine consistently outperforms all baseline approaches,\nachieving improvements up to 1.7 MetricX points on translation tasks, 8.1\nROUGE-L on ASQA, 2.2 ROUGE-L on topical summarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLM) are leveraging human feedback to improve\ntheir generation quality. However, human feedback is costly to obtain,\nespecially during inference. In this work, we propose LLMRefine, an inference\ntime optimization method to refine LLM's output. The core idea is to use a\nlearned fine-grained feedback model to pinpoint defects and guide LLM to refine\nthem iteratively. Using original LLM as a proposal of edits, LLMRefine searches\nfor defect-less text via simulated annealing, trading off the exploration and\nexploitation. We conduct experiments on three text generation tasks, including\nmachine translation, long-form question answering (QA), and topical\nsummarization. LLMRefine consistently outperforms all baseline approaches,\nachieving improvements up to 1.7 MetricX points on translation tasks, 8.1\nROUGE-L on ASQA, 2.2 ROUGE-L on topical summarization."
                },
                "authors": [
                    {
                        "name": "Wenda Xu"
                    },
                    {
                        "name": "Daniel Deutsch"
                    },
                    {
                        "name": "Mara Finkelstein"
                    },
                    {
                        "name": "Juraj Juraska"
                    },
                    {
                        "name": "Biao Zhang"
                    },
                    {
                        "name": "Zhongtao Liu"
                    },
                    {
                        "name": "William Yang Wang"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Markus Freitag"
                    }
                ],
                "author_detail": {
                    "name": "Markus Freitag"
                },
                "author": "Markus Freitag",
                "arxiv_comment": "Accepted to NAACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.09336v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.09336v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19302v1",
                "updated": "2024-10-25T04:26:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    4,
                    26,
                    0,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T04:26:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    4,
                    26,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "TEARS: Textual Representations for Scrutable Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TEARS: Textual Representations for Scrutable Recommendations"
                },
                "summary": "Traditional recommender systems rely on high-dimensional (latent) embeddings\nfor modeling user-item interactions, often resulting in opaque representations\nthat lack interpretability. Moreover, these systems offer limited control to\nusers over their recommendations. Inspired by recent work, we introduce TExtuAl\nRepresentations for Scrutable recommendations (TEARS) to address these\nchallenges. Instead of representing a user's interests through a latent\nembedding, TEARS encodes them in natural text, providing transparency and\nallowing users to edit them. To do so, TEARS uses a modern LLM to generate user\nsummaries based on user preferences. We find the summaries capture user\npreferences uniquely. Using these summaries, we take a hybrid approach where we\nuse an optimal transport procedure to align the summaries' representation with\nthe learned representation of a standard VAE for collaborative filtering. We\nfind this approach can surpass the performance of three popular VAE models\nwhile providing user-controllable recommendations. We also analyze the\ncontrollability of TEARS through three simulated user tasks to evaluate the\neffectiveness of a user editing its summary.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional recommender systems rely on high-dimensional (latent) embeddings\nfor modeling user-item interactions, often resulting in opaque representations\nthat lack interpretability. Moreover, these systems offer limited control to\nusers over their recommendations. Inspired by recent work, we introduce TExtuAl\nRepresentations for Scrutable recommendations (TEARS) to address these\nchallenges. Instead of representing a user's interests through a latent\nembedding, TEARS encodes them in natural text, providing transparency and\nallowing users to edit them. To do so, TEARS uses a modern LLM to generate user\nsummaries based on user preferences. We find the summaries capture user\npreferences uniquely. Using these summaries, we take a hybrid approach where we\nuse an optimal transport procedure to align the summaries' representation with\nthe learned representation of a standard VAE for collaborative filtering. We\nfind this approach can surpass the performance of three popular VAE models\nwhile providing user-controllable recommendations. We also analyze the\ncontrollability of TEARS through three simulated user tasks to evaluate the\neffectiveness of a user editing its summary."
                },
                "authors": [
                    {
                        "name": "Emiliano Penaloza"
                    },
                    {
                        "name": "Olivier Gouvert"
                    },
                    {
                        "name": "Haolun Wu"
                    },
                    {
                        "name": "Laurent Charlin"
                    }
                ],
                "author_detail": {
                    "name": "Laurent Charlin"
                },
                "author": "Laurent Charlin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.00227v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.00227v2",
                "updated": "2024-10-25T04:18:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    4,
                    18,
                    30,
                    4,
                    299,
                    0
                ],
                "published": "2023-05-31T22:46:48Z",
                "published_parsed": [
                    2023,
                    5,
                    31,
                    22,
                    46,
                    48,
                    2,
                    151,
                    0
                ],
                "title": "From human-centered to social-centered artificial intelligence:\n  Assessing ChatGPT's impact through disruptive events",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From human-centered to social-centered artificial intelligence:\n  Assessing ChatGPT's impact through disruptive events"
                },
                "summary": "Large language models (LLMs) and dialogue agents represent a significant\nshift in artificial intelligence (AI) research, particularly with the recent\nrelease of the GPT family of models. ChatGPT's generative capabilities and\nversatility across technical and creative domains led to its widespread\nadoption, marking a departure from more limited deployments of previous AI\nsystems. While society grapples with the emerging cultural impacts of this new\nsocietal-scale technology, critiques of ChatGPT's impact within machine\nlearning research communities have coalesced around its performance or other\nconventional safety evaluations relating to bias, toxicity, and\n\"hallucination.\" We argue that these critiques draw heavily on a particular\nconceptualization of the \"human-centered\" framework, which tends to cast\natomized individuals as the key recipients of technology's benefits and\ndetriments. In this article, we direct attention to another dimension of LLMs\nand dialogue agents' impact: their effects on social groups, institutions, and\naccompanying norms and practices. By analyzing ChatGPT's social impact through\na social-centered framework, we challenge individualistic approaches in AI\ndevelopment and contribute to ongoing debates around the ethical and\nresponsible deployment of AI systems. We hope this effort will call attention\nto more comprehensive and longitudinal evaluation tools (e.g., including more\nethnographic analyses and participatory approaches) and compel technologists to\ncomplement human-centered thinking with social-centered approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) and dialogue agents represent a significant\nshift in artificial intelligence (AI) research, particularly with the recent\nrelease of the GPT family of models. ChatGPT's generative capabilities and\nversatility across technical and creative domains led to its widespread\nadoption, marking a departure from more limited deployments of previous AI\nsystems. While society grapples with the emerging cultural impacts of this new\nsocietal-scale technology, critiques of ChatGPT's impact within machine\nlearning research communities have coalesced around its performance or other\nconventional safety evaluations relating to bias, toxicity, and\n\"hallucination.\" We argue that these critiques draw heavily on a particular\nconceptualization of the \"human-centered\" framework, which tends to cast\natomized individuals as the key recipients of technology's benefits and\ndetriments. In this article, we direct attention to another dimension of LLMs\nand dialogue agents' impact: their effects on social groups, institutions, and\naccompanying norms and practices. By analyzing ChatGPT's social impact through\na social-centered framework, we challenge individualistic approaches in AI\ndevelopment and contribute to ongoing debates around the ethical and\nresponsible deployment of AI systems. We hope this effort will call attention\nto more comprehensive and longitudinal evaluation tools (e.g., including more\nethnographic analyses and participatory approaches) and compel technologists to\ncomplement human-centered thinking with social-centered approaches."
                },
                "authors": [
                    {
                        "name": "Skyler Wang"
                    },
                    {
                        "name": "Ned Cooper"
                    },
                    {
                        "name": "Margaret Eby"
                    }
                ],
                "author_detail": {
                    "name": "Margaret Eby"
                },
                "author": "Margaret Eby",
                "arxiv_doi": "10.1177/20539517241290220",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1177/20539517241290220",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2306.00227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.00227v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages. Published version",
                "arxiv_journal_ref": "Big Data & Society 11(1), 1-14 (2024)",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05087v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05087v2",
                "updated": "2024-10-25T03:57:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    57,
                    7,
                    4,
                    299,
                    0
                ],
                "published": "2024-06-07T17:02:35Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    17,
                    2,
                    35,
                    4,
                    159,
                    0
                ],
                "title": "Corpus Poisoning via Approximate Greedy Gradient Descent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Corpus Poisoning via Approximate Greedy Gradient Descent"
                },
                "summary": "Dense retrievers are widely used in information retrieval and have also been\nsuccessfully extended to other knowledge intensive areas such as language\nmodels, e.g., Retrieval-Augmented Generation (RAG) systems. Unfortunately, they\nhave recently been shown to be vulnerable to corpus poisoning attacks in which\na malicious user injects a small fraction of adversarial passages into the\nretrieval corpus to trick the system into returning these passages among the\ntop-ranked results for a broad set of user queries. Further study is needed to\nunderstand the extent to which these attacks could limit the deployment of\ndense retrievers in real-world applications. In this work, we propose\nApproximate Greedy Gradient Descent (AGGD), a new attack on dense retrieval\nsystems based on the widely used HotFlip method for efficiently generating\nadversarial passages. We demonstrate that AGGD can select a higher quality set\nof token-level perturbations than HotFlip by replacing its random token\nsampling with a more structured search. Experimentally, we show that our method\nachieves a high attack success rate on several datasets and using several\nretrievers, and can generalize to unseen queries and new domains. Notably, our\nmethod is extremely effective in attacking the ANCE retrieval model, achieving\nattack success rates that are 15.24\\% and 17.44\\% higher on the NQ and MS MARCO\ndatasets, respectively, compared to HotFlip. Additionally, we demonstrate\nAGGD's potential to replace HotFlip in other adversarial attacks, such as\nknowledge poisoning of RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dense retrievers are widely used in information retrieval and have also been\nsuccessfully extended to other knowledge intensive areas such as language\nmodels, e.g., Retrieval-Augmented Generation (RAG) systems. Unfortunately, they\nhave recently been shown to be vulnerable to corpus poisoning attacks in which\na malicious user injects a small fraction of adversarial passages into the\nretrieval corpus to trick the system into returning these passages among the\ntop-ranked results for a broad set of user queries. Further study is needed to\nunderstand the extent to which these attacks could limit the deployment of\ndense retrievers in real-world applications. In this work, we propose\nApproximate Greedy Gradient Descent (AGGD), a new attack on dense retrieval\nsystems based on the widely used HotFlip method for efficiently generating\nadversarial passages. We demonstrate that AGGD can select a higher quality set\nof token-level perturbations than HotFlip by replacing its random token\nsampling with a more structured search. Experimentally, we show that our method\nachieves a high attack success rate on several datasets and using several\nretrievers, and can generalize to unseen queries and new domains. Notably, our\nmethod is extremely effective in attacking the ANCE retrieval model, achieving\nattack success rates that are 15.24\\% and 17.44\\% higher on the NQ and MS MARCO\ndatasets, respectively, compared to HotFlip. Additionally, we demonstrate\nAGGD's potential to replace HotFlip in other adversarial attacks, such as\nknowledge poisoning of RAG systems."
                },
                "authors": [
                    {
                        "name": "Jinyan Su"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Claire Cardie"
                    }
                ],
                "author_detail": {
                    "name": "Claire Cardie"
                },
                "author": "Claire Cardie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05087v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05087v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19290v1",
                "updated": "2024-10-25T03:48:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    48,
                    51,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T03:48:51Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    48,
                    51,
                    4,
                    299,
                    0
                ],
                "title": "Fictitious Synthetic Data Can Improve LLM Factuality via Prerequisite\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fictitious Synthetic Data Can Improve LLM Factuality via Prerequisite\n  Learning"
                },
                "summary": "Recent studies have identified one aggravating factor of LLM hallucinations\nas the knowledge inconsistency between pre-training and fine-tuning, where\nunfamiliar fine-tuning data mislead the LLM to fabricate plausible but wrong\noutputs. In this paper, we propose a novel fine-tuning strategy called\nPrereq-Tune to address this knowledge inconsistency and reduce hallucinations.\nFundamentally, Prereq-Tune disentangles the learning of skills and knowledge,\nso the model learns only the task skills without being impacted by the\nknowledge inconsistency. To achieve this, Prereq-Tune introduces an additional\nprerequisite learning stage to learn the necessary knowledge for SFT, allowing\nsubsequent SFT to focus only on task skills. Prereq-Tune can also be combined\nwith fictitious synthetic data to enhance the grounding of LLM outputs to their\ninternal knowledge. Experiments show that Prereq-Tune outperforms existing\nbaselines in improving LLM's factuality across short QA and long-form\ngeneration tasks. It also opens new possibilities for knowledge-controlled\ngeneration in LLMs. Our code is available at\nhttps://github.com/UCSB-NLP-Chang/Prereq_tune.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have identified one aggravating factor of LLM hallucinations\nas the knowledge inconsistency between pre-training and fine-tuning, where\nunfamiliar fine-tuning data mislead the LLM to fabricate plausible but wrong\noutputs. In this paper, we propose a novel fine-tuning strategy called\nPrereq-Tune to address this knowledge inconsistency and reduce hallucinations.\nFundamentally, Prereq-Tune disentangles the learning of skills and knowledge,\nso the model learns only the task skills without being impacted by the\nknowledge inconsistency. To achieve this, Prereq-Tune introduces an additional\nprerequisite learning stage to learn the necessary knowledge for SFT, allowing\nsubsequent SFT to focus only on task skills. Prereq-Tune can also be combined\nwith fictitious synthetic data to enhance the grounding of LLM outputs to their\ninternal knowledge. Experiments show that Prereq-Tune outperforms existing\nbaselines in improving LLM's factuality across short QA and long-form\ngeneration tasks. It also opens new possibilities for knowledge-controlled\ngeneration in LLMs. Our code is available at\nhttps://github.com/UCSB-NLP-Chang/Prereq_tune.git."
                },
                "authors": [
                    {
                        "name": "Yujian Liu"
                    },
                    {
                        "name": "Shiyu Chang"
                    },
                    {
                        "name": "Tommi Jaakkola"
                    },
                    {
                        "name": "Yang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Zhang"
                },
                "author": "Yang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07515v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07515v2",
                "updated": "2024-10-25T03:38:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    38,
                    41,
                    4,
                    299,
                    0
                ],
                "published": "2024-06-11T17:46:16Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    17,
                    46,
                    16,
                    1,
                    163,
                    0
                ],
                "title": "Beyond Model Collapse: Scaling Up with Synthesized Data Requires\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Model Collapse: Scaling Up with Synthesized Data Requires\n  Verification"
                },
                "summary": "Large Language Models (LLM) are increasingly trained on data generated by\nother LLM, either because generated text and images become part of the\npre-training corpus, or because synthetized data is used as a replacement for\nexpensive human-annotation. This raises concerns about \\emph{model collapse}, a\ndrop in model performance when their training sets include generated data.\nConsidering that it is easier for both humans and machines to tell between good\nand bad examples than to generate high-quality samples, we investigate the use\nof verification on synthesized data to prevent model collapse. We provide a\ntheoretical characterization using Gaussian mixtures, linear classifiers, and\nlinear verifiers to derive conditions with measurable proxies to assess whether\nthe verifier can effectively select synthesized data that leads to optimal\nperformance. We experiment with two practical tasks -- computing matrix\neigenvalues with transformers and news summarization with LLMs -- which both\nexhibit model collapse when trained on generated data, and show that verifiers,\neven imperfect ones, can indeed be harnessed to prevent model collapse and that\nour proposed proxy measure strongly correlates with performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) are increasingly trained on data generated by\nother LLM, either because generated text and images become part of the\npre-training corpus, or because synthetized data is used as a replacement for\nexpensive human-annotation. This raises concerns about \\emph{model collapse}, a\ndrop in model performance when their training sets include generated data.\nConsidering that it is easier for both humans and machines to tell between good\nand bad examples than to generate high-quality samples, we investigate the use\nof verification on synthesized data to prevent model collapse. We provide a\ntheoretical characterization using Gaussian mixtures, linear classifiers, and\nlinear verifiers to derive conditions with measurable proxies to assess whether\nthe verifier can effectively select synthesized data that leads to optimal\nperformance. We experiment with two practical tasks -- computing matrix\neigenvalues with transformers and news summarization with LLMs -- which both\nexhibit model collapse when trained on generated data, and show that verifiers,\neven imperfect ones, can indeed be harnessed to prevent model collapse and that\nour proposed proxy measure strongly correlates with performance."
                },
                "authors": [
                    {
                        "name": "Yunzhen Feng"
                    },
                    {
                        "name": "Elvis Dohmatob"
                    },
                    {
                        "name": "Pu Yang"
                    },
                    {
                        "name": "Francois Charton"
                    },
                    {
                        "name": "Julia Kempe"
                    }
                ],
                "author_detail": {
                    "name": "Julia Kempe"
                },
                "author": "Julia Kempe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07515v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07515v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11142v2",
                "updated": "2024-10-25T03:37:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    37,
                    12,
                    4,
                    299,
                    0
                ],
                "published": "2024-02-17T00:20:06Z",
                "published_parsed": [
                    2024,
                    2,
                    17,
                    0,
                    20,
                    6,
                    5,
                    48,
                    0
                ],
                "title": "Grasping the Essentials: Tailoring Large Language Models for Zero-Shot\n  Relation Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grasping the Essentials: Tailoring Large Language Models for Zero-Shot\n  Relation Extraction"
                },
                "summary": "Relation extraction (RE) aims to identify semantic relationships between\nentities within text. Despite considerable advancements, existing models\npredominantly require extensive annotated training data, which is both costly\nand labor-intensive to collect. Moreover, these models often struggle to adapt\nto new or unseen relations. Few-shot learning, aiming to lessen annotation\ndemands, typically provides incomplete and biased supervision for target\nrelations, leading to degraded and unstable performance. To accurately and\nexplicitly describe relation semantics while minimizing annotation demands, we\nexplore the definition only zero-shot RE setting where only relation\ndefinitions expressed in natural language are used to train a RE model. We\nintroduce REPaL, comprising three stages: (1) We leverage large language models\n(LLMs) to generate initial seed instances from relation definitions and an\nunlabeled corpus. (2) We fine-tune a bidirectional Small Language Model (SLM)\nwith initial seeds to learn relations for the target domain. (3) We expand\npattern coverage and mitigate bias from initial seeds by integrating feedback\nfrom the SLM's predictions on the unlabeled corpus and the synthesis history.\nTo accomplish this, we leverage the multi-turn conversation ability of LLMs to\ngenerate new instances in follow-up dialogues, informed by both the feedback\nand synthesis history. Studies reveal that definition-oriented seed synthesis\nenhances pattern coverage whereas indiscriminately increasing seed quantity\nleads to performance saturation. Experiments on two datasets show REPaL\nsignificantly improved cost-effective zero-shot performance by large margins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relation extraction (RE) aims to identify semantic relationships between\nentities within text. Despite considerable advancements, existing models\npredominantly require extensive annotated training data, which is both costly\nand labor-intensive to collect. Moreover, these models often struggle to adapt\nto new or unseen relations. Few-shot learning, aiming to lessen annotation\ndemands, typically provides incomplete and biased supervision for target\nrelations, leading to degraded and unstable performance. To accurately and\nexplicitly describe relation semantics while minimizing annotation demands, we\nexplore the definition only zero-shot RE setting where only relation\ndefinitions expressed in natural language are used to train a RE model. We\nintroduce REPaL, comprising three stages: (1) We leverage large language models\n(LLMs) to generate initial seed instances from relation definitions and an\nunlabeled corpus. (2) We fine-tune a bidirectional Small Language Model (SLM)\nwith initial seeds to learn relations for the target domain. (3) We expand\npattern coverage and mitigate bias from initial seeds by integrating feedback\nfrom the SLM's predictions on the unlabeled corpus and the synthesis history.\nTo accomplish this, we leverage the multi-turn conversation ability of LLMs to\ngenerate new instances in follow-up dialogues, informed by both the feedback\nand synthesis history. Studies reveal that definition-oriented seed synthesis\nenhances pattern coverage whereas indiscriminately increasing seed quantity\nleads to performance saturation. Experiments on two datasets show REPaL\nsignificantly improved cost-effective zero-shot performance by large margins."
                },
                "authors": [
                    {
                        "name": "Sizhe Zhou"
                    },
                    {
                        "name": "Yu Meng"
                    },
                    {
                        "name": "Bowen Jin"
                    },
                    {
                        "name": "Jiawei Han"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Han"
                },
                "author": "Jiawei Han",
                "arxiv_comment": "25 pages, 20 Tables, 9 Figures; Accepted to EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18319v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18319v2",
                "updated": "2024-10-25T03:17:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    17,
                    24,
                    4,
                    299,
                    0
                ],
                "published": "2024-09-26T21:59:11Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    21,
                    59,
                    11,
                    3,
                    270,
                    0
                ],
                "title": "Development and Validation of a Dynamic-Template-Constrained Large\n  Language Model for Generating Fully-Structured Radiology Reports",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Validation of a Dynamic-Template-Constrained Large\n  Language Model for Generating Fully-Structured Radiology Reports"
                },
                "summary": "Current LLMs for creating fully-structured reports face the challenges of\nformatting errors, content hallucinations, and privacy leakage issues when\nuploading data to external servers.We aim to develop an open-source, accurate\nLLM for creating fully-structured and standardized LCS reports from varying\nfree-text reports across institutions and demonstrate its utility in automatic\nstatistical analysis and individual lung nodule retrieval. With IRB approvals,\nour retrospective study included 5,442 de-identified LDCT LCS radiology reports\nfrom two institutions. We constructed two evaluation datasets by labeling 500\npairs of free-text and fully-structured radiology reports and one large-scale\nconsecutive dataset from January 2021 to December 2023. Two radiologists\ncreated a standardized template for recording 27 lung nodule features on LCS.\nWe designed a dynamic-template-constrained decoding method to enhance existing\nLLMs for creating fully-structured reports from free-text radiology reports.\nUsing consecutive structured reports, we automated descriptive statistical\nanalyses and a nodule retrieval prototype. Our best LLM for creating\nfully-structured reports achieved high performance on cross-institutional\ndatasets with an F1 score of about 97%, with neither formatting errors nor\ncontent hallucinations. Our method consistently improved the best open-source\nLLMs by up to 10.42%, and outperformed GPT-4o by 17.19%. The automatically\nderived statistical distributions were consistent with prior findings regarding\nattenuation, location, size, stability, and Lung-RADS. The retrieval system\nwith structured reports allowed flexible nodule-level search and complex\nstatistical analysis. Our developed software is publicly available for local\ndeployment and further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current LLMs for creating fully-structured reports face the challenges of\nformatting errors, content hallucinations, and privacy leakage issues when\nuploading data to external servers.We aim to develop an open-source, accurate\nLLM for creating fully-structured and standardized LCS reports from varying\nfree-text reports across institutions and demonstrate its utility in automatic\nstatistical analysis and individual lung nodule retrieval. With IRB approvals,\nour retrospective study included 5,442 de-identified LDCT LCS radiology reports\nfrom two institutions. We constructed two evaluation datasets by labeling 500\npairs of free-text and fully-structured radiology reports and one large-scale\nconsecutive dataset from January 2021 to December 2023. Two radiologists\ncreated a standardized template for recording 27 lung nodule features on LCS.\nWe designed a dynamic-template-constrained decoding method to enhance existing\nLLMs for creating fully-structured reports from free-text radiology reports.\nUsing consecutive structured reports, we automated descriptive statistical\nanalyses and a nodule retrieval prototype. Our best LLM for creating\nfully-structured reports achieved high performance on cross-institutional\ndatasets with an F1 score of about 97%, with neither formatting errors nor\ncontent hallucinations. Our method consistently improved the best open-source\nLLMs by up to 10.42%, and outperformed GPT-4o by 17.19%. The automatically\nderived statistical distributions were consistent with prior findings regarding\nattenuation, location, size, stability, and Lung-RADS. The retrieval system\nwith structured reports allowed flexible nodule-level search and complex\nstatistical analysis. Our developed software is publicly available for local\ndeployment and further research."
                },
                "authors": [
                    {
                        "name": "Chuang Niu"
                    },
                    {
                        "name": "Parisa Kaviani"
                    },
                    {
                        "name": "Qing Lyu"
                    },
                    {
                        "name": "Mannudeep K. Kalra"
                    },
                    {
                        "name": "Christopher T. Whitlow"
                    },
                    {
                        "name": "Ge Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ge Wang"
                },
                "author": "Ge Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18319v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18319v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19274v1",
                "updated": "2024-10-25T03:01:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    1,
                    19,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T03:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    1,
                    19,
                    4,
                    299,
                    0
                ],
                "title": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference."
                },
                "authors": [
                    {
                        "name": "Tuowei Wang"
                    },
                    {
                        "name": "Ruwen Fan"
                    },
                    {
                        "name": "Minxing Huang"
                    },
                    {
                        "name": "Zixu Hao"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Youyou Lu"
                    },
                    {
                        "name": "Yaoxue Zhang"
                    },
                    {
                        "name": "Ju Ren"
                    }
                ],
                "author_detail": {
                    "name": "Ju Ren"
                },
                "author": "Ju Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19262v1",
                "updated": "2024-10-25T02:34:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    34,
                    54,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T02:34:54Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    34,
                    54,
                    4,
                    299,
                    0
                ],
                "title": "Autonomous Building Cyber-Physical Systems Using Decentralized\n  Autonomous Organizations, Digital Twins, and Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Building Cyber-Physical Systems Using Decentralized\n  Autonomous Organizations, Digital Twins, and Large Language Model"
                },
                "summary": "Current autonomous building research primarily focuses on energy efficiency\nand automation. While traditional artificial intelligence has advanced\nautonomous building research, it often relies on predefined rules and struggles\nto adapt to complex, evolving building operations. Moreover, the centralized\norganizational structures of facilities management hinder transparency in\ndecision-making, limiting true building autonomy. Research on decentralized\ngovernance and adaptive building infrastructure, which could overcome these\nchallenges, remains relatively unexplored. This paper addresses these\nlimitations by introducing a novel Decentralized Autonomous Building\nCyber-Physical System framework that integrates Decentralized Autonomous\nOrganizations, Large Language Models, and digital twins to create a smart,\nself-managed, operational, and financially autonomous building infrastructure.\nThis study develops a full-stack decentralized application to facilitate\ndecentralized governance of building infrastructure. An LLM-based artificial\nintelligence assistant is developed to provide intuitive human-building\ninteraction for blockchain and building operation management-related tasks and\nenable autonomous building operation. Six real-world scenarios were tested to\nevaluate the autonomous building system's workability, including building\nrevenue and expense management, AI-assisted facility control, and autonomous\nadjustment of building systems. Results indicate that the prototype\nsuccessfully executes these operations, confirming the framework's suitability\nfor developing building infrastructure with decentralized governance and\nautonomous operation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current autonomous building research primarily focuses on energy efficiency\nand automation. While traditional artificial intelligence has advanced\nautonomous building research, it often relies on predefined rules and struggles\nto adapt to complex, evolving building operations. Moreover, the centralized\norganizational structures of facilities management hinder transparency in\ndecision-making, limiting true building autonomy. Research on decentralized\ngovernance and adaptive building infrastructure, which could overcome these\nchallenges, remains relatively unexplored. This paper addresses these\nlimitations by introducing a novel Decentralized Autonomous Building\nCyber-Physical System framework that integrates Decentralized Autonomous\nOrganizations, Large Language Models, and digital twins to create a smart,\nself-managed, operational, and financially autonomous building infrastructure.\nThis study develops a full-stack decentralized application to facilitate\ndecentralized governance of building infrastructure. An LLM-based artificial\nintelligence assistant is developed to provide intuitive human-building\ninteraction for blockchain and building operation management-related tasks and\nenable autonomous building operation. Six real-world scenarios were tested to\nevaluate the autonomous building system's workability, including building\nrevenue and expense management, AI-assisted facility control, and autonomous\nadjustment of building systems. Results indicate that the prototype\nsuccessfully executes these operations, confirming the framework's suitability\nfor developing building infrastructure with decentralized governance and\nautonomous operation."
                },
                "authors": [
                    {
                        "name": "Reachsak Ly"
                    },
                    {
                        "name": "Alireza Shojaei"
                    }
                ],
                "author_detail": {
                    "name": "Alireza Shojaei"
                },
                "author": "Alireza Shojaei",
                "arxiv_comment": "40 pages, 22 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19258v1",
                "updated": "2024-10-25T02:22:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T02:22:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning"
                },
                "summary": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark."
                },
                "authors": [
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "18pages,submitted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19250v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19250v1",
                "updated": "2024-10-25T01:58:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    1,
                    58,
                    29,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T01:58:29Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    1,
                    58,
                    29,
                    4,
                    299,
                    0
                ],
                "title": "The Reopening of Pandora's Box: Analyzing the Role of LLMs in the\n  Evolving Battle Against AI-Generated Fake News",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Reopening of Pandora's Box: Analyzing the Role of LLMs in the\n  Evolving Battle Against AI-Generated Fake News"
                },
                "summary": "With the rise of AI-generated content spewed at scale from large language\nmodels (LLMs), genuine concerns about the spread of fake news have intensified.\nThe perceived ability of LLMs to produce convincing fake news at scale poses\nnew challenges for both human and automated fake news detection systems. To\naddress this gap, this work presents the findings from a university-level\ncompetition which aimed to explore how LLMs can be used by humans to create\nfake news, and to assess the ability of human annotators and AI models to\ndetect it. A total of 110 participants used LLMs to create 252 unique fake news\nstories, and 84 annotators participated in the detection tasks. Our findings\nindicate that LLMs are ~68% more effective at detecting real news than humans.\nHowever, for fake news detection, the performance of LLMs and humans remains\ncomparable (~60% accuracy). Additionally, we examine the impact of visual\nelements (e.g., pictures) in news on the accuracy of detecting fake news\nstories. Finally, we also examine various strategies used by fake news creators\nto enhance the credibility of their AI-generated content. This work highlights\nthe increasing complexity of detecting AI-generated fake news, particularly in\ncollaborative human-AI settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of AI-generated content spewed at scale from large language\nmodels (LLMs), genuine concerns about the spread of fake news have intensified.\nThe perceived ability of LLMs to produce convincing fake news at scale poses\nnew challenges for both human and automated fake news detection systems. To\naddress this gap, this work presents the findings from a university-level\ncompetition which aimed to explore how LLMs can be used by humans to create\nfake news, and to assess the ability of human annotators and AI models to\ndetect it. A total of 110 participants used LLMs to create 252 unique fake news\nstories, and 84 annotators participated in the detection tasks. Our findings\nindicate that LLMs are ~68% more effective at detecting real news than humans.\nHowever, for fake news detection, the performance of LLMs and humans remains\ncomparable (~60% accuracy). Additionally, we examine the impact of visual\nelements (e.g., pictures) in news on the accuracy of detecting fake news\nstories. Finally, we also examine various strategies used by fake news creators\nto enhance the credibility of their AI-generated content. This work highlights\nthe increasing complexity of detecting AI-generated fake news, particularly in\ncollaborative human-AI settings."
                },
                "authors": [
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Wenbo Zhang"
                    },
                    {
                        "name": "Sai Koneru"
                    },
                    {
                        "name": "Hangzhi Guo"
                    },
                    {
                        "name": "Bonam Mingole"
                    },
                    {
                        "name": "S. Shyam Sundar"
                    },
                    {
                        "name": "Sarah Rajtmajer"
                    },
                    {
                        "name": "Amulya Yadav"
                    }
                ],
                "author_detail": {
                    "name": "Amulya Yadav"
                },
                "author": "Amulya Yadav",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19250v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19250v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19245v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19245v1",
                "updated": "2024-10-25T01:52:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    1,
                    52,
                    15,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T01:52:15Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    1,
                    52,
                    15,
                    4,
                    299,
                    0
                ],
                "title": "VisionCoder: Empowering Multi-Agent Auto-Programming for Image\n  Processing with Hybrid LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisionCoder: Empowering Multi-Agent Auto-Programming for Image\n  Processing with Hybrid LLMs"
                },
                "summary": "In the field of automated programming, large language models (LLMs) have\ndemonstrated foundational generative capabilities when given detailed task\ndescriptions. However, their current functionalities are primarily limited to\nfunction-level development, restricting their effectiveness in complex project\nenvironments and specific application scenarios, such as complicated\nimage-processing tasks. This paper presents a multi-agent framework that\nutilises a hybrid set of LLMs, including GPT-4o and locally deployed\nopen-source models, which collaboratively complete auto-programming tasks. Each\nagent plays a distinct role in the software development cycle, collectively\nforming a virtual organisation that works together to produce software\nproducts. By establishing a tree-structured thought distribution and\ndevelopment mechanism across project, module, and function levels, this\nframework offers a cost-effective and efficient solution for code generation.\nWe evaluated our approach using benchmark datasets, and the experimental\nresults demonstrate that VisionCoder significantly outperforms existing methods\nin image processing auto-programming tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of automated programming, large language models (LLMs) have\ndemonstrated foundational generative capabilities when given detailed task\ndescriptions. However, their current functionalities are primarily limited to\nfunction-level development, restricting their effectiveness in complex project\nenvironments and specific application scenarios, such as complicated\nimage-processing tasks. This paper presents a multi-agent framework that\nutilises a hybrid set of LLMs, including GPT-4o and locally deployed\nopen-source models, which collaboratively complete auto-programming tasks. Each\nagent plays a distinct role in the software development cycle, collectively\nforming a virtual organisation that works together to produce software\nproducts. By establishing a tree-structured thought distribution and\ndevelopment mechanism across project, module, and function levels, this\nframework offers a cost-effective and efficient solution for code generation.\nWe evaluated our approach using benchmark datasets, and the experimental\nresults demonstrate that VisionCoder significantly outperforms existing methods\nin image processing auto-programming tasks."
                },
                "authors": [
                    {
                        "name": "Zixiao Zhao"
                    },
                    {
                        "name": "Jing Sun"
                    },
                    {
                        "name": "Zhiyuan Wei"
                    },
                    {
                        "name": "Cheng-Hao Cai"
                    },
                    {
                        "name": "Zhe Hou"
                    },
                    {
                        "name": "Jin Song Dong"
                    }
                ],
                "author_detail": {
                    "name": "Jin Song Dong"
                },
                "author": "Jin Song Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19245v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00608v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00608v3",
                "updated": "2024-10-25T01:16:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    1,
                    16,
                    55,
                    4,
                    299,
                    0
                ],
                "published": "2024-09-01T04:23:48Z",
                "published_parsed": [
                    2024,
                    9,
                    1,
                    4,
                    23,
                    48,
                    6,
                    245,
                    0
                ],
                "title": "TinyAgent: Function Calling at the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinyAgent: Function Calling at the Edge"
                },
                "summary": "Recent large language models (LLMs) have enabled the development of advanced\nagentic systems that can integrate various tools and APIs to fulfill user\nqueries through function calling. However, the deployment of these LLMs on the\nedge has not been explored since they typically require cloud-based\ninfrastructure due to their substantial model size and computational demands.\nTo this end, we present TinyAgent, an end-to-end framework for training and\ndeploying task-specific small language model agents capable of function calling\nfor driving agentic systems at the edge. We first show how to enable accurate\nfunction calling for open-source models via the LLMCompiler framework. We then\nsystematically curate a high-quality dataset for function calling, which we use\nto fine-tune two small language models, TinyAgent-1.1B and 7B. For efficient\ninference, we introduce a novel tool retrieval method to reduce the input\nprompt length and utilize quantization to further accelerate the inference\nspeed. As a driving application, we demonstrate a local Siri-like system for\nApple's MacBook that can execute user commands through text or voice input. Our\nresults show that our models can achieve, and even surpass, the\nfunction-calling capabilities of larger models like GPT-4-Turbo, while being\nfully deployed at the edge. We open-source our dataset, models, and installable\npackage and provide a demo video for our MacBook assistant agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) have enabled the development of advanced\nagentic systems that can integrate various tools and APIs to fulfill user\nqueries through function calling. However, the deployment of these LLMs on the\nedge has not been explored since they typically require cloud-based\ninfrastructure due to their substantial model size and computational demands.\nTo this end, we present TinyAgent, an end-to-end framework for training and\ndeploying task-specific small language model agents capable of function calling\nfor driving agentic systems at the edge. We first show how to enable accurate\nfunction calling for open-source models via the LLMCompiler framework. We then\nsystematically curate a high-quality dataset for function calling, which we use\nto fine-tune two small language models, TinyAgent-1.1B and 7B. For efficient\ninference, we introduce a novel tool retrieval method to reduce the input\nprompt length and utilize quantization to further accelerate the inference\nspeed. As a driving application, we demonstrate a local Siri-like system for\nApple's MacBook that can execute user commands through text or voice input. Our\nresults show that our models can achieve, and even surpass, the\nfunction-calling capabilities of larger models like GPT-4-Turbo, while being\nfully deployed at the edge. We open-source our dataset, models, and installable\npackage and provide a demo video for our MacBook assistant agent."
                },
                "authors": [
                    {
                        "name": "Lutfi Eren Erdogan"
                    },
                    {
                        "name": "Nicholas Lee"
                    },
                    {
                        "name": "Siddharth Jha"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Ryan Tabrizi"
                    },
                    {
                        "name": "Suhong Moon"
                    },
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Gopala Anumanchipalli"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "EMNLP 2024 Demo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00608v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00608v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16531v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16531v2",
                "updated": "2024-10-25T01:08:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    1,
                    8,
                    6,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-21T21:45:22Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    21,
                    45,
                    22,
                    0,
                    295,
                    0
                ],
                "title": "Bayesian scaling laws for in-context learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian scaling laws for in-context learning"
                },
                "summary": "In-context learning (ICL) is a powerful technique for getting language models\nto perform complex tasks with no training updates. Prior work has established\nstrong correlations between the number of in-context examples provided and the\naccuracy of the model's predictions. In this paper, we seek to explain this\ncorrelation by showing that ICL approximates a Bayesian learner. This\nperspective gives rise to a family of novel Bayesian scaling laws for ICL. In\nexperiments with \\mbox{GPT-2} models of different sizes, our scaling laws\nexceed or match existing scaling laws in accuracy while also offering\ninterpretable terms for task priors, learning efficiency, and per-example\nprobabilities. To illustrate the analytic power that such interpretable scaling\nlaws provide, we report on controlled synthetic dataset experiments designed to\ninform real-world studies of safety alignment. In our experimental protocol, we\nuse SFT to suppress an unwanted existing model capability and then use ICL to\ntry to bring that capability back (many-shot jailbreaking). We then experiment\non real-world instruction-tuned LLMs using capabilities benchmarks as well as a\nnew many-shot jailbreaking dataset. In all cases, Bayesian scaling laws\naccurately predict the conditions under which ICL will cause the suppressed\nbehavior to reemerge, which sheds light on the ineffectiveness of post-training\nat increasing LLM safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) is a powerful technique for getting language models\nto perform complex tasks with no training updates. Prior work has established\nstrong correlations between the number of in-context examples provided and the\naccuracy of the model's predictions. In this paper, we seek to explain this\ncorrelation by showing that ICL approximates a Bayesian learner. This\nperspective gives rise to a family of novel Bayesian scaling laws for ICL. In\nexperiments with \\mbox{GPT-2} models of different sizes, our scaling laws\nexceed or match existing scaling laws in accuracy while also offering\ninterpretable terms for task priors, learning efficiency, and per-example\nprobabilities. To illustrate the analytic power that such interpretable scaling\nlaws provide, we report on controlled synthetic dataset experiments designed to\ninform real-world studies of safety alignment. In our experimental protocol, we\nuse SFT to suppress an unwanted existing model capability and then use ICL to\ntry to bring that capability back (many-shot jailbreaking). We then experiment\non real-world instruction-tuned LLMs using capabilities benchmarks as well as a\nnew many-shot jailbreaking dataset. In all cases, Bayesian scaling laws\naccurately predict the conditions under which ICL will cause the suppressed\nbehavior to reemerge, which sheds light on the ineffectiveness of post-training\nat increasing LLM safety."
                },
                "authors": [
                    {
                        "name": "Aryaman Arora"
                    },
                    {
                        "name": "Dan Jurafsky"
                    },
                    {
                        "name": "Christopher Potts"
                    },
                    {
                        "name": "Noah D. Goodman"
                    }
                ],
                "author_detail": {
                    "name": "Noah D. Goodman"
                },
                "author": "Noah D. Goodman",
                "arxiv_comment": "10 pages main text, 26 pages total",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16531v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16531v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19238v1",
                "updated": "2024-10-25T01:05:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    1,
                    5,
                    4,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T01:05:04Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    1,
                    5,
                    4,
                    4,
                    299,
                    0
                ],
                "title": "Designing LLM-Agents with Personalities: A Psychometric Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing LLM-Agents with Personalities: A Psychometric Approach"
                },
                "summary": "This research introduces a novel methodology for assigning quantifiable,\ncontrollable and psychometrically validated personalities to Large Language\nModels-Based Agents (Agents) using the Big Five personality framework. It seeks\nto overcome the constraints of human subject studies, proposing Agents as an\naccessible tool for social science inquiry. Through a series of four studies,\nthis research demonstrates the feasibility of assigning psychometrically valid\npersonality traits to Agents, enabling them to replicate complex human-like\nbehaviors. The first study establishes an understanding of personality\nconstructs and personality tests within the semantic space of an LLM. Two\nsubsequent studies -- using empirical and simulated data -- illustrate the\nprocess of creating Agents and validate the results by showing strong\ncorrespondence between human and Agent answers to personality tests. The final\nstudy further corroborates this correspondence by using Agents to replicate\nknown human correlations between personality traits and decision-making\nbehaviors in scenarios involving risk-taking and ethical dilemmas, thereby\nvalidating the effectiveness of the psychometric approach to design Agents and\nits applicability to social and behavioral research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research introduces a novel methodology for assigning quantifiable,\ncontrollable and psychometrically validated personalities to Large Language\nModels-Based Agents (Agents) using the Big Five personality framework. It seeks\nto overcome the constraints of human subject studies, proposing Agents as an\naccessible tool for social science inquiry. Through a series of four studies,\nthis research demonstrates the feasibility of assigning psychometrically valid\npersonality traits to Agents, enabling them to replicate complex human-like\nbehaviors. The first study establishes an understanding of personality\nconstructs and personality tests within the semantic space of an LLM. Two\nsubsequent studies -- using empirical and simulated data -- illustrate the\nprocess of creating Agents and validate the results by showing strong\ncorrespondence between human and Agent answers to personality tests. The final\nstudy further corroborates this correspondence by using Agents to replicate\nknown human correlations between personality traits and decision-making\nbehaviors in scenarios involving risk-taking and ethical dilemmas, thereby\nvalidating the effectiveness of the psychometric approach to design Agents and\nits applicability to social and behavioral research."
                },
                "authors": [
                    {
                        "name": "Muhua Huang"
                    },
                    {
                        "name": "Xijuan Zhang"
                    },
                    {
                        "name": "Christopher Soto"
                    },
                    {
                        "name": "James Evans"
                    }
                ],
                "author_detail": {
                    "name": "James Evans"
                },
                "author": "James Evans",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19231v1",
                "updated": "2024-10-25T00:40:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    0,
                    40,
                    21,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T00:40:21Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    0,
                    40,
                    21,
                    4,
                    299,
                    0
                ],
                "title": "Developing a Tutoring Dialog Dataset to Optimize LLMs for Educational\n  Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing a Tutoring Dialog Dataset to Optimize LLMs for Educational\n  Use"
                },
                "summary": "Recent advances in large language models (LLMs) have shown promise for\nscalable educational applications, but their use in dialog-based tutoring\nsystems remains challenging due to the need for effective pedagogical\nstrategies and the high costs associated with expert-curated datasets. Our\nstudy explores the use of smaller, more affordable LLMs for one-on-one tutoring\nin the context of solving reading comprehension problems. We developed a\nsynthetic tutoring dialog dataset, evaluated by human teachers, and fine-tuned\na smaller LLM using this dataset. Furthermore, we conducted an interactive\nexperiment comparing the performance of the fine-tuned model with a larger\nmodel in real-world tutoring scenarios. Our results show that the fine-tuned\nmodel performs on par with the larger model but at a lower cost, demonstrating\na viable, cost-effective approach for implementing LLM-based tutoring systems\nin educational settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have shown promise for\nscalable educational applications, but their use in dialog-based tutoring\nsystems remains challenging due to the need for effective pedagogical\nstrategies and the high costs associated with expert-curated datasets. Our\nstudy explores the use of smaller, more affordable LLMs for one-on-one tutoring\nin the context of solving reading comprehension problems. We developed a\nsynthetic tutoring dialog dataset, evaluated by human teachers, and fine-tuned\na smaller LLM using this dataset. Furthermore, we conducted an interactive\nexperiment comparing the performance of the fine-tuned model with a larger\nmodel in real-world tutoring scenarios. Our results show that the fine-tuned\nmodel performs on par with the larger model but at a lower cost, demonstrating\na viable, cost-effective approach for implementing LLM-based tutoring systems\nin educational settings."
                },
                "authors": [
                    {
                        "name": "Menna Fateen"
                    },
                    {
                        "name": "Tsunenori Mine"
                    }
                ],
                "author_detail": {
                    "name": "Tsunenori Mine"
                },
                "author": "Tsunenori Mine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19230v1",
                "updated": "2024-10-25T00:35:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    0,
                    35,
                    0,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T00:35:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    0,
                    35,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "Humanizing the Machine: Proxy Attacks to Mislead LLM Detectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humanizing the Machine: Proxy Attacks to Mislead LLM Detectors"
                },
                "summary": "The advent of large language models (LLMs) has revolutionized the field of\ntext generation, producing outputs that closely mimic human-like writing.\nAlthough academic and industrial institutions have developed detectors to\nprevent the malicious usage of LLM-generated texts, other research has doubt\nabout the robustness of these systems. To stress test these detectors, we\nintroduce a proxy-attack strategy that effortlessly compromises LLMs, causing\nthem to produce outputs that align with human-written text and mislead\ndetection systems. Our method attacks the source model by leveraging a\nreinforcement learning (RL) fine-tuned humanized small language model (SLM) in\nthe decoding phase. Through an in-depth analysis, we demonstrate that our\nattack strategy is capable of generating responses that are indistinguishable\nto detectors, preventing them from differentiating between machine-generated\nand human-written text. We conduct systematic evaluations on extensive datasets\nusing proxy-attacked open-source models, including Llama2-13B, Llama3-70B, and\nMixtral-8*7B in both white- and black-box settings. Our findings show that the\nproxy-attack strategy effectively deceives the leading detectors, resulting in\nan average AUROC drop of 70.4% across multiple datasets, with a maximum drop of\n90.3% on a single dataset. Furthermore, in cross-discipline scenarios, our\nstrategy also bypasses these detectors, leading to a significant relative\ndecrease of up to 90.9%, while in cross-language scenario, the drop reaches\n91.3%. Despite our proxy-attack strategy successfully bypassing the detectors\nwith such significant relative drops, we find that the generation quality of\nthe attacked models remains preserved, even within a modest utility budget,\nwhen compared to the text produced by the original, unattacked source model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of large language models (LLMs) has revolutionized the field of\ntext generation, producing outputs that closely mimic human-like writing.\nAlthough academic and industrial institutions have developed detectors to\nprevent the malicious usage of LLM-generated texts, other research has doubt\nabout the robustness of these systems. To stress test these detectors, we\nintroduce a proxy-attack strategy that effortlessly compromises LLMs, causing\nthem to produce outputs that align with human-written text and mislead\ndetection systems. Our method attacks the source model by leveraging a\nreinforcement learning (RL) fine-tuned humanized small language model (SLM) in\nthe decoding phase. Through an in-depth analysis, we demonstrate that our\nattack strategy is capable of generating responses that are indistinguishable\nto detectors, preventing them from differentiating between machine-generated\nand human-written text. We conduct systematic evaluations on extensive datasets\nusing proxy-attacked open-source models, including Llama2-13B, Llama3-70B, and\nMixtral-8*7B in both white- and black-box settings. Our findings show that the\nproxy-attack strategy effectively deceives the leading detectors, resulting in\nan average AUROC drop of 70.4% across multiple datasets, with a maximum drop of\n90.3% on a single dataset. Furthermore, in cross-discipline scenarios, our\nstrategy also bypasses these detectors, leading to a significant relative\ndecrease of up to 90.9%, while in cross-language scenario, the drop reaches\n91.3%. Despite our proxy-attack strategy successfully bypassing the detectors\nwith such significant relative drops, we find that the generation quality of\nthe attacked models remains preserved, even within a modest utility budget,\nwhen compared to the text produced by the original, unattacked source model."
                },
                "authors": [
                    {
                        "name": "Tianchun Wang"
                    },
                    {
                        "name": "Yuanzhou Chen"
                    },
                    {
                        "name": "Zichuan Liu"
                    },
                    {
                        "name": "Zhanwen Chen"
                    },
                    {
                        "name": "Haifeng Chen"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Wei Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Wei Cheng"
                },
                "author": "Wei Cheng",
                "arxiv_comment": "26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19223v1",
                "updated": "2024-10-25T00:21:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    0,
                    21,
                    45,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T00:21:45Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    0,
                    21,
                    45,
                    4,
                    299,
                    0
                ],
                "title": "Integrating Large Language Models with Internet of Things Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Large Language Models with Internet of Things Applications"
                },
                "summary": "This paper identifies and analyzes applications in which Large Language\nModels (LLMs) can make Internet of Things (IoT) networks more intelligent and\nresponsive through three case studies from critical topics: DDoS attack\ndetection, macroprogramming over IoT systems, and sensor data processing. Our\nresults reveal that the GPT model under few-shot learning achieves 87.6%\ndetection accuracy, whereas the fine-tuned GPT increases the value to 94.9%.\nGiven a macroprogramming framework, the GPT model is capable of writing scripts\nusing high-level functions from the framework to handle possible incidents.\nMoreover, the GPT model shows efficacy in processing a vast amount of sensor\ndata by offering fast and high-quality responses, which comprise expected\nresults and summarized insights. Overall, the model demonstrates its potential\nto power a natural language interface. We hope that researchers will find these\ncase studies inspiring to develop further.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper identifies and analyzes applications in which Large Language\nModels (LLMs) can make Internet of Things (IoT) networks more intelligent and\nresponsive through three case studies from critical topics: DDoS attack\ndetection, macroprogramming over IoT systems, and sensor data processing. Our\nresults reveal that the GPT model under few-shot learning achieves 87.6%\ndetection accuracy, whereas the fine-tuned GPT increases the value to 94.9%.\nGiven a macroprogramming framework, the GPT model is capable of writing scripts\nusing high-level functions from the framework to handle possible incidents.\nMoreover, the GPT model shows efficacy in processing a vast amount of sensor\ndata by offering fast and high-quality responses, which comprise expected\nresults and summarized insights. Overall, the model demonstrates its potential\nto power a natural language interface. We hope that researchers will find these\ncase studies inspiring to develop further."
                },
                "authors": [
                    {
                        "name": "Mingyu Zong"
                    },
                    {
                        "name": "Arvin Hekmati"
                    },
                    {
                        "name": "Michael Guastalla"
                    },
                    {
                        "name": "Yiyi Li"
                    },
                    {
                        "name": "Bhaskar Krishnamachari"
                    }
                ],
                "author_detail": {
                    "name": "Bhaskar Krishnamachari"
                },
                "author": "Bhaskar Krishnamachari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19221v1",
                "updated": "2024-10-25T00:13:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    0,
                    13,
                    15,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T00:13:15Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    0,
                    13,
                    15,
                    4,
                    299,
                    0
                ],
                "title": "Can Stories Help LLMs Reason? Curating Information Space Through\n  Narrative",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Stories Help LLMs Reason? Curating Information Space Through\n  Narrative"
                },
                "summary": "Narratives are widely recognized as a powerful tool for structuring\ninformation and facilitating comprehension of complex ideas in various domains\nsuch as science communication. This paper investigates whether incorporating\nnarrative elements can assist Large Language Models (LLMs) in solving complex\nproblems more effectively. We propose a novel approach, Story of Thought (SoT),\nintegrating narrative structures into prompting techniques for problem-solving.\nThis approach involves constructing narratives around problem statements and\ncreating a framework to identify and organize relevant information. Our\nexperiments show that using various LLMs with SoT consistently surpasses using\nthem with other techniques on physics, chemistry, math, and biology questions\nin both the GPQA and JEEBench datasets. The narrative-based information\ncuration process in SoT enhances problem comprehension by contextualizing\ncritical in-domain information and highlighting causal relationships within the\nproblem space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Narratives are widely recognized as a powerful tool for structuring\ninformation and facilitating comprehension of complex ideas in various domains\nsuch as science communication. This paper investigates whether incorporating\nnarrative elements can assist Large Language Models (LLMs) in solving complex\nproblems more effectively. We propose a novel approach, Story of Thought (SoT),\nintegrating narrative structures into prompting techniques for problem-solving.\nThis approach involves constructing narratives around problem statements and\ncreating a framework to identify and organize relevant information. Our\nexperiments show that using various LLMs with SoT consistently surpasses using\nthem with other techniques on physics, chemistry, math, and biology questions\nin both the GPQA and JEEBench datasets. The narrative-based information\ncuration process in SoT enhances problem comprehension by contextualizing\ncritical in-domain information and highlighting causal relationships within the\nproblem space."
                },
                "authors": [
                    {
                        "name": "Vahid Sadiri Javadi"
                    },
                    {
                        "name": "Johanne R. Trippas"
                    },
                    {
                        "name": "Yash Kumar Lal"
                    },
                    {
                        "name": "Lucie Flek"
                    }
                ],
                "author_detail": {
                    "name": "Lucie Flek"
                },
                "author": "Lucie Flek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19206v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19206v1",
                "updated": "2024-10-24T23:31:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    23,
                    31,
                    39,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T23:31:39Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    23,
                    31,
                    39,
                    3,
                    298,
                    0
                ],
                "title": "Inference time LLM alignment in single and multidomain preference\n  spectrum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference time LLM alignment in single and multidomain preference\n  spectrum"
                },
                "summary": "Aligning Large Language Models (LLM) to address subjectivity and nuanced\npreference levels requires adequate flexibility and control, which can be a\nresource-intensive and time-consuming procedure. Existing training-time\nalignment methods require full re-training when a change is needed and\ninference-time ones typically require access to the reward model at each\ninference step. To address these limitations, we introduce inference-time model\nalignment method that learns encoded representations of preference dimensions,\ncalled \\textit{Alignment Vectors} (AV). These representations are computed by\nsubtraction of the base model from the aligned model as in model editing\nenabling dynamically adjusting the model behavior during inference through\nsimple linear operations. Even though the preference dimensions can span\nvarious granularity levels, here we focus on three gradual response levels\nacross three specialized domains: medical, legal, and financial, exemplifying\nits practical potential. This new alignment paradigm introduces adjustable\npreference knobs during inference, allowing users to tailor their LLM outputs\nwhile reducing the inference cost by half compared to the prompt engineering\napproach. Additionally, we find that AVs are transferable across different\nfine-tuning stages of the same model, demonstrating their flexibility. AVs also\nfacilitate multidomain, diverse preference alignment, making the process 12x\nfaster than the retraining approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Large Language Models (LLM) to address subjectivity and nuanced\npreference levels requires adequate flexibility and control, which can be a\nresource-intensive and time-consuming procedure. Existing training-time\nalignment methods require full re-training when a change is needed and\ninference-time ones typically require access to the reward model at each\ninference step. To address these limitations, we introduce inference-time model\nalignment method that learns encoded representations of preference dimensions,\ncalled \\textit{Alignment Vectors} (AV). These representations are computed by\nsubtraction of the base model from the aligned model as in model editing\nenabling dynamically adjusting the model behavior during inference through\nsimple linear operations. Even though the preference dimensions can span\nvarious granularity levels, here we focus on three gradual response levels\nacross three specialized domains: medical, legal, and financial, exemplifying\nits practical potential. This new alignment paradigm introduces adjustable\npreference knobs during inference, allowing users to tailor their LLM outputs\nwhile reducing the inference cost by half compared to the prompt engineering\napproach. Additionally, we find that AVs are transferable across different\nfine-tuning stages of the same model, demonstrating their flexibility. AVs also\nfacilitate multidomain, diverse preference alignment, making the process 12x\nfaster than the retraining approach."
                },
                "authors": [
                    {
                        "name": "Sadat Shahriar"
                    },
                    {
                        "name": "Zheng Qi"
                    },
                    {
                        "name": "Nikolaos Pappas"
                    },
                    {
                        "name": "Srikanth Doss"
                    },
                    {
                        "name": "Monica Sunkara"
                    },
                    {
                        "name": "Kishaloy Halder"
                    },
                    {
                        "name": "Manuel Mager"
                    },
                    {
                        "name": "Yassine Benajiba"
                    }
                ],
                "author_detail": {
                    "name": "Yassine Benajiba"
                },
                "author": "Yassine Benajiba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19206v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19206v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19202v1",
                "updated": "2024-10-24T23:24:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    23,
                    24,
                    35,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T23:24:35Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    23,
                    24,
                    35,
                    3,
                    298,
                    0
                ],
                "title": "Prebunking Elections Rumors: Artificial Intelligence Assisted\n  Interventions Increase Confidence in American Elections",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prebunking Elections Rumors: Artificial Intelligence Assisted\n  Interventions Increase Confidence in American Elections"
                },
                "summary": "Large Language Models (LLMs) can assist in the prebunking of election\nmisinformation. Using results from a preregistered two-wave experimental study\nof 4,293 U.S. registered voters conducted in August 2024, we show that\nLLM-assisted prebunking significantly reduced belief in specific election\nmyths,with these effects persisting for at least one week. Confidence in\nelection integrity was also increased post-treatment. Notably, the effect was\nconsistent across partisan lines, even when controlling for demographic and\nattitudinal factors like conspiratorial thinking. LLM-assisted prebunking is a\npromising tool for rapidly responding to changing election misinformation\nnarratives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can assist in the prebunking of election\nmisinformation. Using results from a preregistered two-wave experimental study\nof 4,293 U.S. registered voters conducted in August 2024, we show that\nLLM-assisted prebunking significantly reduced belief in specific election\nmyths,with these effects persisting for at least one week. Confidence in\nelection integrity was also increased post-treatment. Notably, the effect was\nconsistent across partisan lines, even when controlling for demographic and\nattitudinal factors like conspiratorial thinking. LLM-assisted prebunking is a\npromising tool for rapidly responding to changing election misinformation\nnarratives."
                },
                "authors": [
                    {
                        "name": "Mitchell Linegar"
                    },
                    {
                        "name": "Betsy Sinclair"
                    },
                    {
                        "name": "Sander van der Linden"
                    },
                    {
                        "name": "R. Michael Alvarez"
                    }
                ],
                "author_detail": {
                    "name": "R. Michael Alvarez"
                },
                "author": "R. Michael Alvarez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.4.1; J.4.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04267v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04267v2",
                "updated": "2024-10-24T23:12:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    23,
                    12,
                    55,
                    3,
                    298,
                    0
                ],
                "published": "2024-06-06T17:14:44Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    17,
                    14,
                    44,
                    3,
                    158,
                    0
                ],
                "title": "Transformers need glasses! Information over-squashing in language tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers need glasses! Information over-squashing in language tasks"
                },
                "summary": "We study how information propagates in decoder-only Transformers, which are\nthe architectural backbone of most existing frontier large language models\n(LLMs). We rely on a theoretical signal propagation analysis -- specifically,\nwe analyse the representations of the last token in the final layer of the\nTransformer, as this is the representation used for next-token prediction. Our\nanalysis reveals a representational collapse phenomenon: we prove that certain\ndistinct sequences of inputs to the Transformer can yield arbitrarily close\nrepresentations in the final token. This effect is exacerbated by the\nlow-precision floating-point formats frequently used in modern LLMs. As a\nresult, the model is provably unable to respond to these sequences in different\nways -- leading to errors in, e.g., tasks involving counting or copying.\nFurther, we show that decoder-only Transformer language models can lose\nsensitivity to specific tokens in the input, which relates to the well-known\nphenomenon of over-squashing in graph neural networks. We provide empirical\nevidence supporting our claims on contemporary LLMs. Our theory also points to\nsimple solutions towards ameliorating these issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study how information propagates in decoder-only Transformers, which are\nthe architectural backbone of most existing frontier large language models\n(LLMs). We rely on a theoretical signal propagation analysis -- specifically,\nwe analyse the representations of the last token in the final layer of the\nTransformer, as this is the representation used for next-token prediction. Our\nanalysis reveals a representational collapse phenomenon: we prove that certain\ndistinct sequences of inputs to the Transformer can yield arbitrarily close\nrepresentations in the final token. This effect is exacerbated by the\nlow-precision floating-point formats frequently used in modern LLMs. As a\nresult, the model is provably unable to respond to these sequences in different\nways -- leading to errors in, e.g., tasks involving counting or copying.\nFurther, we show that decoder-only Transformer language models can lose\nsensitivity to specific tokens in the input, which relates to the well-known\nphenomenon of over-squashing in graph neural networks. We provide empirical\nevidence supporting our claims on contemporary LLMs. Our theory also points to\nsimple solutions towards ameliorating these issues."
                },
                "authors": [
                    {
                        "name": "Federico Barbero"
                    },
                    {
                        "name": "Andrea Banino"
                    },
                    {
                        "name": "Steven Kapturowski"
                    },
                    {
                        "name": "Dharshan Kumaran"
                    },
                    {
                        "name": "João G. M. Araújo"
                    },
                    {
                        "name": "Alex Vitvitskyi"
                    },
                    {
                        "name": "Razvan Pascanu"
                    },
                    {
                        "name": "Petar Veličković"
                    }
                ],
                "author_detail": {
                    "name": "Petar Veličković"
                },
                "author": "Petar Veličković",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04267v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04267v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04401v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04401v2",
                "updated": "2024-10-24T22:53:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    22,
                    53,
                    1,
                    3,
                    298,
                    0
                ],
                "published": "2024-02-06T21:03:52Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    21,
                    3,
                    52,
                    1,
                    37,
                    0
                ],
                "title": "Democratizing Large Language Models via Personalized Parameter-Efficient\n  Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Democratizing Large Language Models via Personalized Parameter-Efficient\n  Fine-tuning"
                },
                "summary": "Personalization in large language models (LLMs) is increasingly important,\naiming to align the LLMs' interactions, content, and recommendations with\nindividual user preferences. Recent advances have highlighted effective prompt\ndesign by enriching user queries with non-parametric knowledge through behavior\nhistory retrieval and textual profiles. However, these methods faced\nlimitations due to a lack of model ownership, resulting in constrained\ncustomization and privacy issues, and often failed to capture complex, dynamic\nuser behavior patterns. To address these shortcomings, we introduce One PEFT\nPer User (OPPU), employing personalized parameter-efficient fine-tuning (PEFT)\nmodules to store user-specific behavior patterns and preferences. By plugging\nin personal PEFT parameters, users can own and use their LLMs individually.\nOPPU integrates parametric user knowledge in the personal PEFT parameters with\nnon-parametric knowledge from retrieval and profiles, adapting LLMs to user\nbehavior shifts. Experimental results demonstrate that OPPU significantly\noutperforms existing prompt-based methods across seven diverse tasks in the\nLaMP benchmark. Further studies reveal OPPU's enhanced capabilities in handling\nuser behavior shifts, modeling users at different activity levels, maintaining\nrobustness across various user history formats, and displaying versatility with\ndifferent PEFT methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalization in large language models (LLMs) is increasingly important,\naiming to align the LLMs' interactions, content, and recommendations with\nindividual user preferences. Recent advances have highlighted effective prompt\ndesign by enriching user queries with non-parametric knowledge through behavior\nhistory retrieval and textual profiles. However, these methods faced\nlimitations due to a lack of model ownership, resulting in constrained\ncustomization and privacy issues, and often failed to capture complex, dynamic\nuser behavior patterns. To address these shortcomings, we introduce One PEFT\nPer User (OPPU), employing personalized parameter-efficient fine-tuning (PEFT)\nmodules to store user-specific behavior patterns and preferences. By plugging\nin personal PEFT parameters, users can own and use their LLMs individually.\nOPPU integrates parametric user knowledge in the personal PEFT parameters with\nnon-parametric knowledge from retrieval and profiles, adapting LLMs to user\nbehavior shifts. Experimental results demonstrate that OPPU significantly\noutperforms existing prompt-based methods across seven diverse tasks in the\nLaMP benchmark. Further studies reveal OPPU's enhanced capabilities in handling\nuser behavior shifts, modeling users at different activity levels, maintaining\nrobustness across various user history formats, and displaying versatility with\ndifferent PEFT methods."
                },
                "authors": [
                    {
                        "name": "Zhaoxuan Tan"
                    },
                    {
                        "name": "Qingkai Zeng"
                    },
                    {
                        "name": "Yijun Tian"
                    },
                    {
                        "name": "Zheyuan Liu"
                    },
                    {
                        "name": "Bing Yin"
                    },
                    {
                        "name": "Meng Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Jiang"
                },
                "author": "Meng Jiang",
                "arxiv_comment": "EMNLP 2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04401v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04401v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10471v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10471v2",
                "updated": "2024-10-24T22:46:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    22,
                    46,
                    35,
                    3,
                    298,
                    0
                ],
                "published": "2024-06-15T02:26:18Z",
                "published_parsed": [
                    2024,
                    6,
                    15,
                    2,
                    26,
                    18,
                    5,
                    167,
                    0
                ],
                "title": "Personalized Pieces: Efficient Personalized Large Language Models\n  through Collaborative Efforts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Pieces: Efficient Personalized Large Language Models\n  through Collaborative Efforts"
                },
                "summary": "Personalized large language models (LLMs) aim to tailor interactions,\ncontent, and recommendations to individual user preferences. While\nparameter-efficient fine-tuning (PEFT) methods excel in performance and\ngeneralization, they are costly and limit communal benefits when used\nindividually. To this end, we introduce Personalized Pieces (Per-Pcs), a\nframework that allows users to safely share and assemble personalized PEFT\nefficiently with collaborative efforts. Per-Pcs involves selecting sharers,\nbreaking their PEFT into pieces, and training gates for each piece. These\npieces are added to a pool, from which target users can select and assemble\npersonalized PEFT using their history data. This approach preserves privacy and\nenables fine-grained user modeling without excessive storage and computation\ndemands. Experimental results show Per-Pcs outperforms non-personalized and\nPEFT retrieval baselines, offering performance comparable to OPPU with\nsignificantly lower resource use across six tasks. Further analysis highlights\nPer-Pcs's robustness concerning sharer count and selection strategy, pieces\nsharing ratio, and scalability in computation time and storage space. Per-Pcs's\nmodularity promotes safe sharing, making LLM personalization more efficient,\neffective, and widely accessible through collaborative efforts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized large language models (LLMs) aim to tailor interactions,\ncontent, and recommendations to individual user preferences. While\nparameter-efficient fine-tuning (PEFT) methods excel in performance and\ngeneralization, they are costly and limit communal benefits when used\nindividually. To this end, we introduce Personalized Pieces (Per-Pcs), a\nframework that allows users to safely share and assemble personalized PEFT\nefficiently with collaborative efforts. Per-Pcs involves selecting sharers,\nbreaking their PEFT into pieces, and training gates for each piece. These\npieces are added to a pool, from which target users can select and assemble\npersonalized PEFT using their history data. This approach preserves privacy and\nenables fine-grained user modeling without excessive storage and computation\ndemands. Experimental results show Per-Pcs outperforms non-personalized and\nPEFT retrieval baselines, offering performance comparable to OPPU with\nsignificantly lower resource use across six tasks. Further analysis highlights\nPer-Pcs's robustness concerning sharer count and selection strategy, pieces\nsharing ratio, and scalability in computation time and storage space. Per-Pcs's\nmodularity promotes safe sharing, making LLM personalization more efficient,\neffective, and widely accessible through collaborative efforts."
                },
                "authors": [
                    {
                        "name": "Zhaoxuan Tan"
                    },
                    {
                        "name": "Zheyuan Liu"
                    },
                    {
                        "name": "Meng Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Jiang"
                },
                "author": "Meng Jiang",
                "arxiv_comment": "EMNLP 2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10471v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10471v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15625v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15625v3",
                "updated": "2024-10-24T22:24:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    22,
                    24,
                    57,
                    3,
                    298,
                    0
                ],
                "published": "2024-06-21T20:02:22Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    20,
                    2,
                    22,
                    4,
                    173,
                    0
                ],
                "title": "Shortcomings of LLMs for Low-Resource Translation: Retrieval and\n  Understanding are Both the Problem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shortcomings of LLMs for Low-Resource Translation: Retrieval and\n  Understanding are Both the Problem"
                },
                "summary": "This work investigates the in-context learning abilities of pretrained large\nlanguage models (LLMs) when instructed to translate text from a low-resource\nlanguage into a high-resource language as part of an automated machine\ntranslation pipeline. We conduct a set of experiments translating Southern\nQuechua to Spanish and examine the informativity of various types of context\nretrieved from a constrained database of digitized pedagogical materials\n(dictionaries and grammar lessons) and parallel corpora. Using both automatic\nand human evaluation of model output, we conduct ablation studies that\nmanipulate (1) context type (morpheme translations, grammar descriptions, and\ncorpus examples), (2) retrieval methods (automated vs. manual), and (3) model\ntype. Our results suggest that even relatively small LLMs are capable of\nutilizing prompt context for zero-shot low-resource translation when provided a\nminimally sufficient amount of relevant linguistic information. However, the\nvariable effects of context type, retrieval method, model type, and\nlanguage-specific factors highlight the limitations of using even the best LLMs\nas translation systems for the majority of the world's 7,000+ languages and\ntheir speakers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work investigates the in-context learning abilities of pretrained large\nlanguage models (LLMs) when instructed to translate text from a low-resource\nlanguage into a high-resource language as part of an automated machine\ntranslation pipeline. We conduct a set of experiments translating Southern\nQuechua to Spanish and examine the informativity of various types of context\nretrieved from a constrained database of digitized pedagogical materials\n(dictionaries and grammar lessons) and parallel corpora. Using both automatic\nand human evaluation of model output, we conduct ablation studies that\nmanipulate (1) context type (morpheme translations, grammar descriptions, and\ncorpus examples), (2) retrieval methods (automated vs. manual), and (3) model\ntype. Our results suggest that even relatively small LLMs are capable of\nutilizing prompt context for zero-shot low-resource translation when provided a\nminimally sufficient amount of relevant linguistic information. However, the\nvariable effects of context type, retrieval method, model type, and\nlanguage-specific factors highlight the limitations of using even the best LLMs\nas translation systems for the majority of the world's 7,000+ languages and\ntheir speakers."
                },
                "authors": [
                    {
                        "name": "Sara Court"
                    },
                    {
                        "name": "Micha Elsner"
                    }
                ],
                "author_detail": {
                    "name": "Micha Elsner"
                },
                "author": "Micha Elsner",
                "arxiv_comment": "Presented at the Ninth Conference on Machine Translation (WMT24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15625v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15625v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19174v1",
                "updated": "2024-10-24T22:03:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    22,
                    3,
                    36,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T22:03:36Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    22,
                    3,
                    36,
                    3,
                    298,
                    0
                ],
                "title": "Indication Finding: a novel use case for representation learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Indication Finding: a novel use case for representation learning"
                },
                "summary": "Many therapies are effective in treating multiple diseases. We present an\napproach that leverages methods developed in natural language processing and\nreal-world data to prioritize potential, new indications for a mechanism of\naction (MoA). We specifically use representation learning to generate\nembeddings of indications and prioritize them based on their proximity to the\nindications with the strongest available evidence for the MoA. We demonstrate\nthe successful deployment of our approach for anti-IL-17A using embeddings\ngenerated with SPPMI and present an evaluation framework to determine the\nquality of indication finding results and the derived embeddings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many therapies are effective in treating multiple diseases. We present an\napproach that leverages methods developed in natural language processing and\nreal-world data to prioritize potential, new indications for a mechanism of\naction (MoA). We specifically use representation learning to generate\nembeddings of indications and prioritize them based on their proximity to the\nindications with the strongest available evidence for the MoA. We demonstrate\nthe successful deployment of our approach for anti-IL-17A using embeddings\ngenerated with SPPMI and present an evaluation framework to determine the\nquality of indication finding results and the derived embeddings."
                },
                "authors": [
                    {
                        "name": "Maren Eckhoff"
                    },
                    {
                        "name": "Valmir Selimi"
                    },
                    {
                        "name": "Alexander Aranovitch"
                    },
                    {
                        "name": "Ian Lyons"
                    },
                    {
                        "name": "Emily Briggs"
                    },
                    {
                        "name": "Jennifer Hou"
                    },
                    {
                        "name": "Alex Devereson"
                    },
                    {
                        "name": "Matej Macak"
                    },
                    {
                        "name": "David Champagne"
                    },
                    {
                        "name": "Chris Anagnostopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Chris Anagnostopoulos"
                },
                "author": "Chris Anagnostopoulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04663v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04663v2",
                "updated": "2024-10-24T21:42:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    21,
                    42,
                    20,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-07T00:22:07Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    0,
                    22,
                    7,
                    0,
                    281,
                    0
                ],
                "title": "Adversarial Multi-Agent Evaluation of Large Language Models through\n  Iterative Debates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Multi-Agent Evaluation of Large Language Models through\n  Iterative Debates"
                },
                "summary": "This paper explores optimal architectures for evaluating the outputs of large\nlanguage models (LLMs) using LLMs themselves. We propose a novel framework that\ninterprets LLMs as advocates within an ensemble of interacting agents, allowing\nthem to defend their answers and reach conclusions through a judge and jury\nsystem. This approach offers a more dynamic and comprehensive evaluation\nprocess compared to traditional human-based assessments or automated metrics.\nWe discuss the motivation behind this framework, its key components, and\ncomparative advantages. We also present a probabilistic model to evaluate the\nerror reduction achieved by iterative advocate systems. Finally, we outline\nexperiments to validate the effectiveness of multi-advocate architectures and\ndiscuss future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores optimal architectures for evaluating the outputs of large\nlanguage models (LLMs) using LLMs themselves. We propose a novel framework that\ninterprets LLMs as advocates within an ensemble of interacting agents, allowing\nthem to defend their answers and reach conclusions through a judge and jury\nsystem. This approach offers a more dynamic and comprehensive evaluation\nprocess compared to traditional human-based assessments or automated metrics.\nWe discuss the motivation behind this framework, its key components, and\ncomparative advantages. We also present a probabilistic model to evaluate the\nerror reduction achieved by iterative advocate systems. Finally, we outline\nexperiments to validate the effectiveness of multi-advocate architectures and\ndiscuss future research directions."
                },
                "authors": [
                    {
                        "name": "Chaithanya Bandi"
                    },
                    {
                        "name": "Abir Harrasse"
                    }
                ],
                "author_detail": {
                    "name": "Abir Harrasse"
                },
                "author": "Abir Harrasse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04663v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04663v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.09335v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.09335v3",
                "updated": "2024-10-24T21:40:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    21,
                    40,
                    0,
                    3,
                    298,
                    0
                ],
                "published": "2023-11-15T19:49:24Z",
                "published_parsed": [
                    2023,
                    11,
                    15,
                    19,
                    49,
                    24,
                    2,
                    319,
                    0
                ],
                "title": "Investigating Hallucinations in Pruned Large Language Models for\n  Abstractive Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Hallucinations in Pruned Large Language Models for\n  Abstractive Summarization"
                },
                "summary": "Despite the remarkable performance of generative large language models (LLMs)\non abstractive summarization, they face two significant challenges: their\nconsiderable size and tendency to hallucinate. Hallucinations are concerning\nbecause they erode reliability and raise safety issues. Pruning is a technique\nthat reduces model size by removing redundant weights, enabling more efficient\nsparse inference. Pruned models yield downstream task performance comparable to\nthe original, making them ideal alternatives when operating on a limited\nbudget. However, the effect that pruning has upon hallucinations in abstractive\nsummarization with LLMs has yet to be explored. In this paper, we provide an\nextensive empirical study across five summarization datasets, two\nstate-of-the-art pruning methods, and five instruction-tuned LLMs.\nSurprisingly, we find that hallucinations are less prevalent from pruned LLMs\nthan the original models. Our analysis suggests that pruned models tend to\ndepend more on the source document for summary generation. This leads to a\nhigher lexical overlap between the generated summary and the source document,\nwhich could be a reason for the reduction in hallucination risk.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable performance of generative large language models (LLMs)\non abstractive summarization, they face two significant challenges: their\nconsiderable size and tendency to hallucinate. Hallucinations are concerning\nbecause they erode reliability and raise safety issues. Pruning is a technique\nthat reduces model size by removing redundant weights, enabling more efficient\nsparse inference. Pruned models yield downstream task performance comparable to\nthe original, making them ideal alternatives when operating on a limited\nbudget. However, the effect that pruning has upon hallucinations in abstractive\nsummarization with LLMs has yet to be explored. In this paper, we provide an\nextensive empirical study across five summarization datasets, two\nstate-of-the-art pruning methods, and five instruction-tuned LLMs.\nSurprisingly, we find that hallucinations are less prevalent from pruned LLMs\nthan the original models. Our analysis suggests that pruned models tend to\ndepend more on the source document for summary generation. This leads to a\nhigher lexical overlap between the generated summary and the source document,\nwhich could be a reason for the reduction in hallucination risk."
                },
                "authors": [
                    {
                        "name": "George Chrysostomou"
                    },
                    {
                        "name": "Zhixue Zhao"
                    },
                    {
                        "name": "Miles Williams"
                    },
                    {
                        "name": "Nikolaos Aletras"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaos Aletras"
                },
                "author": "Nikolaos Aletras",
                "arxiv_doi": "10.1162/tacl_a_00695",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1162/tacl_a_00695",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2311.09335v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.09335v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "TACL 2024 (Presented at EMNLP 2024)",
                "arxiv_journal_ref": "Transactions of the Association for Computational Linguistics\n  (2024) 12: 1163-1181",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.02469v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.02469v2",
                "updated": "2024-10-24T21:36:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    21,
                    36,
                    36,
                    3,
                    298,
                    0
                ],
                "published": "2023-10-03T22:37:01Z",
                "published_parsed": [
                    2023,
                    10,
                    3,
                    22,
                    37,
                    1,
                    1,
                    276,
                    0
                ],
                "title": "Large Language Models Can Be Contextual Privacy Protection Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Can Be Contextual Privacy Protection Learners"
                },
                "summary": "The proliferation of Large Language Models (LLMs) has driven considerable\ninterest in fine-tuning them with domain-specific data to create specialized\nlanguage models. Nevertheless, such domain-specific fine-tuning data often\ncontains contextually sensitive personally identifiable information (PII).\nDirect fine-tuning of LLMs on this data without privacy protection poses a risk\nof data leakage of sensitive PII during inference time. To address this\nchallenge, we introduce Contextual Privacy Protection Language Models (CPPLM),\na novel paradigm for fine-tuning LLMs that effectively injects domain-specific\nknowledge while safeguarding inference-time data privacy. Our work offers a\ntheoretical analysis for model design and benchmarks various techniques such as\ncorpus curation, penalty-based unlikelihood in training loss, instruction-based\ntuning, etc. Extensive experiments across diverse datasets and scenarios\ndemonstrate the effectiveness of our approaches. In particular, instruction\ntuning with both positive and negative examples stands out as a promising\nmethod, effectively protecting private data while enhancing the model's\nknowledge. Our work underscores the potential for Large Language Models as\nrobust contextual privacy protection learners. The complete code and data for\nthe work can be found at https://github.com/Yijia-Xiao/PPLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of Large Language Models (LLMs) has driven considerable\ninterest in fine-tuning them with domain-specific data to create specialized\nlanguage models. Nevertheless, such domain-specific fine-tuning data often\ncontains contextually sensitive personally identifiable information (PII).\nDirect fine-tuning of LLMs on this data without privacy protection poses a risk\nof data leakage of sensitive PII during inference time. To address this\nchallenge, we introduce Contextual Privacy Protection Language Models (CPPLM),\na novel paradigm for fine-tuning LLMs that effectively injects domain-specific\nknowledge while safeguarding inference-time data privacy. Our work offers a\ntheoretical analysis for model design and benchmarks various techniques such as\ncorpus curation, penalty-based unlikelihood in training loss, instruction-based\ntuning, etc. Extensive experiments across diverse datasets and scenarios\ndemonstrate the effectiveness of our approaches. In particular, instruction\ntuning with both positive and negative examples stands out as a promising\nmethod, effectively protecting private data while enhancing the model's\nknowledge. Our work underscores the potential for Large Language Models as\nrobust contextual privacy protection learners. The complete code and data for\nthe work can be found at https://github.com/Yijia-Xiao/PPLM."
                },
                "authors": [
                    {
                        "name": "Yijia Xiao"
                    },
                    {
                        "name": "Yiqiao Jin"
                    },
                    {
                        "name": "Yushi Bai"
                    },
                    {
                        "name": "Yue Wu"
                    },
                    {
                        "name": "Xianjun Yang"
                    },
                    {
                        "name": "Xiao Luo"
                    },
                    {
                        "name": "Wenchao Yu"
                    },
                    {
                        "name": "Xujiang Zhao"
                    },
                    {
                        "name": "Yanchi Liu"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Haifeng Chen"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Wei Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Wei Cheng"
                },
                "author": "Wei Cheng",
                "arxiv_comment": "Accepted at EMNLP 2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.02469v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.02469v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]